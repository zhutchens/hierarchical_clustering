"text","chapter","chapter-title"
"How many cities with more than 250,000 people lie within 500 miles of Dallas,","chapter-1","Data Structures and Algorithms"
"Texas? How many people in my company make over $100,000 per year? Can we","chapter-1","Data Structures and Algorithms"
"connect all of our telephone customers with less than 1,000 miles of cable? To","chapter-1","Data Structures and Algorithms"
"answer questions like these, it is not enough to have the necessary information. We","chapter-1","Data Structures and Algorithms"
"must organize that information in a way that allows us to find the answers in time","chapter-1","Data Structures and Algorithms"
"to satisfy our needs.","chapter-1","Data Structures and Algorithms"
"Representing information is fundamental to computer science. The primary","chapter-1","Data Structures and Algorithms"
"purpose of most computer programs is not to perform calculations, but to store and","chapter-1","Data Structures and Algorithms"
"retrieve information — usually as fast as possible. For this reason, the study of","chapter-1","Data Structures and Algorithms"
"data structures and the algorithms that manipulate them is at the heart of computer","chapter-1","Data Structures and Algorithms"
"science. And that is what this book is about — helping you to understand how to","chapter-1","Data Structures and Algorithms"
"structure information to support efficient processing.","chapter-1","Data Structures and Algorithms"
"This book has three primary goals. The first is to present the commonly used","chapter-1","Data Structures and Algorithms"
"data structures. These form a programmer’s basic data structure “toolkit.” For","chapter-1","Data Structures and Algorithms"
"many problems, some data structure in the toolkit provides a good solution.","chapter-1","Data Structures and Algorithms"
"The second goal is to introduce the idea of tradeoffs and reinforce the concept","chapter-1","Data Structures and Algorithms"
"that there are costs and benefits associated with every data structure. This is done","chapter-1","Data Structures and Algorithms"
"by describing, for each data structure, the amount of space and time required for","chapter-1","Data Structures and Algorithms"
"typical operations.","chapter-1","Data Structures and Algorithms"
"The third goal is to teach how to measure the effectiveness of a data structure or","chapter-1","Data Structures and Algorithms"
"algorithm. Only through such measurement can you determine which data structure","chapter-1","Data Structures and Algorithms"
"in your toolkit is most appropriate for a new problem. The techniques presented","chapter-1","Data Structures and Algorithms"
"also allow you to judge the merits of new data structures that you or others might","chapter-1","Data Structures and Algorithms"
"invent.","chapter-1","Data Structures and Algorithms"
"There are often many approaches to solving a problem. How do we choose","chapter-1","Data Structures and Algorithms"
"between them? At the heart of computer program design are two (sometimes con-","chapter-1","Data Structures and Algorithms"
"flicting) goals:","chapter-1","Data Structures and Algorithms"
"1. To design an algorithm that is easy to understand, code, and debug.","chapter-1","Data Structures and Algorithms"
"2. To design an algorithm that makes efficient use of the computer’s resources.","chapter-1","Data Structures and Algorithms"
"4 Chap. 1 Data Structures and Algorithms","chapter-1","Data Structures and Algorithms"
"Ideally, the resulting program is true to both of these goals. We might say that","chapter-1","Data Structures and Algorithms"
"such a program is “elegant.” While the algorithms and program code examples pre-","chapter-1","Data Structures and Algorithms"
"sented here attempt to be elegant in this sense, it is not the purpose of this book to","chapter-1","Data Structures and Algorithms"
"explicitly treat issues related to goal (1). These are primarily concerns of the disci-","chapter-1","Data Structures and Algorithms"
"pline of Software Engineering. Rather, this book is mostly about issues relating to goal (2).","chapter-1","Data Structures and Algorithms"
"How do we measure efficiency? Chapter 3 describes a method for evaluating","chapter-1","Data Structures and Algorithms"
"the efficiency of an algorithm or computer program, called asymptotic analysis.","chapter-1","Data Structures and Algorithms"
"Asymptotic analysis also allows you to measure the inherent difficulty of a problem.","chapter-1","Data Structures and Algorithms"
"The remaining chapters use asymptotic analysis techniques to estimate the time cost","chapter-1","Data Structures and Algorithms"
"for every algorithm presented. This allows you to see how each algorithm compares","chapter-1","Data Structures and Algorithms"
"to other algorithms for solving the same problem in terms of its efficiency.","chapter-1","Data Structures and Algorithms"
"This first chapter sets the stage for what is to follow, by presenting some higher-","chapter-1","Data Structures and Algorithms"
"order issues related to the selection and use of data structures. We first examine the","chapter-1","Data Structures and Algorithms"
"process by which a designer selects a data structure appropriate to the task at hand.","chapter-1","Data Structures and Algorithms"
"We then consider the role of abstraction in program design. We briefly consider","chapter-1","Data Structures and Algorithms"
"the concept of a design pattern and see some examples. The chapter ends with an","chapter-1","Data Structures and Algorithms"
"exploration of the relationship between problems, algorithms, and programs.","chapter-1","Data Structures and Algorithms"
"You might think that with ever more powerful computers, program efficiency is","chapter-1","Data Structures and Algorithms"
"becoming less important. After all, processor speed and memory size still con-","chapter-1","Data Structures and Algorithms"
"tinue to improve. Won’t any efficiency problem we might have today be solved by tomorrow’s hardware?","chapter-1","Data Structures and Algorithms"
"As we develop more powerful computers, our history so far has always been to","chapter-1","Data Structures and Algorithms"
"use that additional computing power to tackle more complex problems, be it in the","chapter-1","Data Structures and Algorithms"
"form of more sophisticated user interfaces, bigger problem sizes, or new problems","chapter-1","Data Structures and Algorithms"
"previously deemed computationally infeasible. More complex problems demand","chapter-1","Data Structures and Algorithms"
"more computation, making the need for efficient programs even greater. Worse yet,","chapter-1","Data Structures and Algorithms"
"as tasks become more complex, they become less like our everyday experience.","chapter-1","Data Structures and Algorithms"
"Today’s computer scientists must be trained to have a thorough understanding of the","chapter-1","Data Structures and Algorithms"
"principles behind efficient program design, because their ordinary life experiences","chapter-1","Data Structures and Algorithms"
"often do not apply when designing computer programs.","chapter-1","Data Structures and Algorithms"
"In the most general sense, a data structure is any data representation and its","chapter-1","Data Structures and Algorithms"
"associated operations. Even an integer or floating point number stored on the com-","chapter-1","Data Structures and Algorithms"
"puter can be viewed as a simple data structure. More commonly, people use the","chapter-1","Data Structures and Algorithms"
"term “data structure” to mean an organization or structuring for a collection of data","chapter-1","Data Structures and Algorithms"
"items. A sorted list of integers stored in an array is an example of such a structuring.","chapter-1","Data Structures and Algorithms"
"Given sufficient space to store a collection of data items, it is always possible to","chapter-1","Data Structures and Algorithms"
"search for specified items within the collection, print or otherwise process the data","chapter-1","Data Structures and Algorithms"
"items in any desired order, or modify the value of any particular data item. Thus,","chapter-1","Data Structures and Algorithms"
"it is possible to perform all necessary operations on any data structure. However,","chapter-1","Data Structures and Algorithms"
"using the proper data structure can make the difference between a program running","chapter-1","Data Structures and Algorithms"
"in a few seconds and one requiring many days.","chapter-1","Data Structures and Algorithms"
"A solution is said to be efficient if it solves the problem within the required","chapter-1","Data Structures and Algorithms"
"resource constraints. Examples of resource constraints include the total space","chapter-1","Data Structures and Algorithms"
"available to store the data — possibly divided into separate main memory and disk","chapter-1","Data Structures and Algorithms"
"space constraints — and the time allowed to perform each subtask. A solution is","chapter-1","Data Structures and Algorithms"
"sometimes said to be efficient if it requires fewer resources than known alternatives,","chapter-1","Data Structures and Algorithms"
"regardless of whether it meets any particular requirements. The cost of a solution is","chapter-1","Data Structures and Algorithms"
"the amount of resources that the solution consumes. Most often, cost is measured","chapter-1","Data Structures and Algorithms"
"in terms of one key resource such as time, with the implied assumption that the","chapter-1","Data Structures and Algorithms"
"solution meets the other resource constraints.","chapter-1","Data Structures and Algorithms"
"It should go without saying that people write programs to solve problems. How-","chapter-1","Data Structures and Algorithms"
"ever, it is crucial to keep this truism in mind when selecting a data structure to solve","chapter-1","Data Structures and Algorithms"
"a particular problem. Only by first analyzing the problem to determine the perfor-","chapter-1","Data Structures and Algorithms"
"mance goals that must be achieved can there be any hope of selecting the right data","chapter-1","Data Structures and Algorithms"
"structure for the job. Poor program designers ignore this analysis step and apply a","chapter-1","Data Structures and Algorithms"
"data structure that they are familiar with but which is inappropriate to the problem.","chapter-1","Data Structures and Algorithms"
"The result is typically a slow program. Conversely, there is no sense in adopting","chapter-1","Data Structures and Algorithms"
"a complex representation to “improve” a program that can meet its performance","chapter-1","Data Structures and Algorithms"
"goals when implemented using a simpler design.","chapter-1","Data Structures and Algorithms"
"When selecting a data structure to solve a problem, you should follow these","chapter-1","Data Structures and Algorithms"
"steps.","chapter-1","Data Structures and Algorithms"
"1. Analyze your problem to determine the basic operations that must be sup-","chapter-1","Data Structures and Algorithms"
"ported. Examples of basic operations include inserting a data item into the","chapter-1","Data Structures and Algorithms"
"data structure, deleting a data item from the data structure, and finding a","chapter-1","Data Structures and Algorithms"
"specified data item.","chapter-1","Data Structures and Algorithms"
"2. Quantify the resource constraints for each operation.","chapter-1","Data Structures and Algorithms"
"3. Select the data structure that best meets these requirements.","chapter-1","Data Structures and Algorithms"
"This three-step approach to selecting a data structure operationalizes a data-","chapter-1","Data Structures and Algorithms"
"centered view of the design process. The first concern is for the data and the op-","chapter-1","Data Structures and Algorithms"
"erations to be performed on them, the next concern is the representation for those","chapter-1","Data Structures and Algorithms"
"data, and the final concern is the implementation of that representation.","chapter-1","Data Structures and Algorithms"
"Resource constraints on certain key operations, such as search, inserting data","chapter-1","Data Structures and Algorithms"
"records, and deleting data records, normally drive the data structure selection pro-","chapter-1","Data Structures and Algorithms"
"cess. Many issues relating to the relative importance of these operations are ad-","chapter-1","Data Structures and Algorithms"
"dressed by the following three questions, which you should ask yourself whenever","chapter-1","Data Structures and Algorithms"
"you must choose a data structure:","chapter-1","Data Structures and Algorithms"
"6 Chap. 1 Data Structures and Algorithms","chapter-1","Data Structures and Algorithms"
"• Are all data items inserted into the data structure at the beginning, or are","chapter-1","Data Structures and Algorithms"
"insertions interspersed with other operations? Static applications (where the","chapter-1","Data Structures and Algorithms"
"data are loaded at the beginning and never change) typically require only","chapter-1","Data Structures and Algorithms"
"simpler data structures to get an efficient implementation than do dynamic","chapter-1","Data Structures and Algorithms"
"applications.","chapter-1","Data Structures and Algorithms"
"• Can data items be deleted? If so, this will probably make the implementation","chapter-1","Data Structures and Algorithms"
"more complicated.","chapter-1","Data Structures and Algorithms"
"• Are all data items processed in some well-defined order, or is search for spe-","chapter-1","Data Structures and Algorithms"
"cific data items allowed? “Random access” search generally requires more","chapter-1","Data Structures and Algorithms"
"complex data structures.","chapter-1","Data Structures and Algorithms"
"Each data structure has associated costs and benefits. In practice, it is hardly ever","chapter-1","Data Structures and Algorithms"
"true that one data structure is better than another for use in all situations. If one","chapter-1","Data Structures and Algorithms"
"data structure or algorithm is superior to another in all respects, the inferior one","chapter-1","Data Structures and Algorithms"
"will usually have long been forgotten. For nearly every data structure and algorithm","chapter-1","Data Structures and Algorithms"
"presented in this book, you will see examples of where it is the best choice. Some","chapter-1","Data Structures and Algorithms"
"of the examples might surprise you.","chapter-1","Data Structures and Algorithms"
"A data structure requires a certain amount of space for each data item it stores,","chapter-1","Data Structures and Algorithms"
"a certain amount of time to perform a single basic operation, and a certain amount","chapter-1","Data Structures and Algorithms"
"of programming effort. Each problem has constraints on available space and time.","chapter-1","Data Structures and Algorithms"
"Each solution to a problem makes use of the basic operations in some relative pro-","chapter-1","Data Structures and Algorithms"
"portion, and the data structure selection process must account for this. Only after a","chapter-1","Data Structures and Algorithms"
"careful analysis of your problem’s characteristics can you determine the best data","chapter-1","Data Structures and Algorithms"
"structure for the task.","chapter-1","Data Structures and Algorithms"
"Example 1.1 A bank must support many types of transactions with its","chapter-1","Data Structures and Algorithms"
"customers, but we will examine a simple model where customers wish to","chapter-1","Data Structures and Algorithms"
"open accounts, close accounts, and add money or withdraw money from","chapter-1","Data Structures and Algorithms"
"accounts. We can consider this problem at two distinct levels: (1) the re-","chapter-1","Data Structures and Algorithms"
"quirements for the physical infrastructure and workflow process that the","chapter-1","Data Structures and Algorithms"
"bank uses in its interactions with its customers, and (2) the requirements","chapter-1","Data Structures and Algorithms"
"for the database system that manages the accounts.","chapter-1","Data Structures and Algorithms"
"The typical customer opens and closes accounts far less often than he","chapter-1","Data Structures and Algorithms"
"or she accesses the account. Customers are willing to wait many minutes","chapter-1","Data Structures and Algorithms"
"while accounts are created or deleted but are typically not willing to wait","chapter-1","Data Structures and Algorithms"
"more than a brief time for individual account transactions such as a deposit","chapter-1","Data Structures and Algorithms"
"or withdrawal. These observations can be considered as informal specifica-","chapter-1","Data Structures and Algorithms"
"tions for the time constraints on the problem.","chapter-1","Data Structures and Algorithms"
"It is common practice for banks to provide two tiers of service. Hu-","chapter-1","Data Structures and Algorithms"
"man tellers or automated teller machines (ATMs) support customer access","chapter-1","Data Structures and Algorithms"
"Sec. 1.1 A Philosophy of Data Structures 7","chapter-1","Data Structures and Algorithms"
"to account balances and updates such as deposits and withdrawals. Spe-","chapter-1","Data Structures and Algorithms"
"cial service representatives are typically provided (during restricted hours)","chapter-1","Data Structures and Algorithms"
"to handle opening and closing accounts. Teller and ATM transactions are","chapter-1","Data Structures and Algorithms"
"expected to take little time. Opening or closing an account can take much","chapter-1","Data Structures and Algorithms"
"longer (perhaps up to an hour from the customer’s perspective).","chapter-1","Data Structures and Algorithms"
"From a database perspective, we see that ATM transactions do not mod-","chapter-1","Data Structures and Algorithms"
"ify the database significantly. For simplicity, assume that if money is added","chapter-1","Data Structures and Algorithms"
"or removed, this transaction simply changes the value stored in an account","chapter-1","Data Structures and Algorithms"
"record. Adding a new account to the database is allowed to take several","chapter-1","Data Structures and Algorithms"
"minutes. Deleting an account need have no time constraint, because from","chapter-1","Data Structures and Algorithms"
"the customer’s point of view all that matters is that all the money be re-","chapter-1","Data Structures and Algorithms"
"turned (equivalent to a withdrawal). From the bank’s point of view, the","chapter-1","Data Structures and Algorithms"
"account record might be removed from the database system after business","chapter-1","Data Structures and Algorithms"
"hours, or at the end of the monthly account cycle.","chapter-1","Data Structures and Algorithms"
"When considering the choice of data structure to use in the database","chapter-1","Data Structures and Algorithms"
"system that manages customer accounts, we see that a data structure that","chapter-1","Data Structures and Algorithms"
"has little concern for the cost of deletion, but is highly efficient for search","chapter-1","Data Structures and Algorithms"
"and moderately efficient for insertion, should meet the resource constraints","chapter-1","Data Structures and Algorithms"
"imposed by this problem. Records are accessible by unique account number","chapter-1","Data Structures and Algorithms"
"(sometimes called an exact-match query). One data structure that meets","chapter-1","Data Structures and Algorithms"
"these requirements is the hash table described in Chapter 9.4. Hash tables","chapter-1","Data Structures and Algorithms"
"allow for extremely fast exact-match search. A record can be modified","chapter-1","Data Structures and Algorithms"
"quickly when the modification does not affect its space requirements. Hash","chapter-1","Data Structures and Algorithms"
"tables also support efficient insertion of new records. While deletions can","chapter-1","Data Structures and Algorithms"
"also be supported efficiently, too many deletions lead to some degradation","chapter-1","Data Structures and Algorithms"
"in performance for the remaining operations. However, the hash table can","chapter-1","Data Structures and Algorithms"
"be reorganized periodically to restore the system to peak efficiency. Such","chapter-1","Data Structures and Algorithms"
"reorganization can occur offline so as not to affect ATM transactions.","chapter-1","Data Structures and Algorithms"
"Example 1.2 A company is developing a database system containing in-","chapter-1","Data Structures and Algorithms"
"formation about cities and towns in the United States. There are many","chapter-1","Data Structures and Algorithms"
"thousands of cities and towns, and the database program should allow users","chapter-1","Data Structures and Algorithms"
"to find information about a particular place by name (another example of","chapter-1","Data Structures and Algorithms"
"an exact-match query). Users should also be able to find all places that","chapter-1","Data Structures and Algorithms"
"match a particular value or range of values for attributes such as location or","chapter-1","Data Structures and Algorithms"
"population size. This is known as a range query.","chapter-1","Data Structures and Algorithms"
"A reasonable database system must answer queries quickly enough to","chapter-1","Data Structures and Algorithms"
"satisfy the patience of a typical user. For an exact-match query, a few sec-","chapter-1","Data Structures and Algorithms"
"onds is satisfactory. If the database is meant to support range queries that","chapter-1","Data Structures and Algorithms"
"can return many cities that match the query specification, the entire opera-","chapter-1","Data Structures and Algorithms"
"8 Chap. 1 Data Structures and Algorithms","chapter-1","Data Structures and Algorithms"
"tion may be allowed to take longer, perhaps on the order of a minute. To","chapter-1","Data Structures and Algorithms"
"meet this requirement, it will be necessary to support operations that pro-","chapter-1","Data Structures and Algorithms"
"cess range queries efficiently by processing all cities in the range as a batch,","chapter-1","Data Structures and Algorithms"
"rather than as a series of operations on individual cities.","chapter-1","Data Structures and Algorithms"
"The hash table suggested in the previous example is inappropriate for","chapter-1","Data Structures and Algorithms"
"implementing our city database, because it cannot perform efficient range","chapter-1","Data Structures and Algorithms"
"queries. The B+-tree of Section 10.5.1 supports large databases, insertion","chapter-1","Data Structures and Algorithms"
"and deletion of data records, and range queries. However, a simple linear in-","chapter-1","Data Structures and Algorithms"
"dex as described in Section 10.1 would be more appropriate if the database","chapter-1","Data Structures and Algorithms"
"is created once, and then never changed, such as an atlas distributed on a","chapter-1","Data Structures and Algorithms"
"CD or accessed from a website.","chapter-1","Data Structures and Algorithms"
"1.2 Abstract Data Types and Data Structures","chapter-1","Data Structures and Algorithms"
"The previous section used the terms “data item” and “data structure” without prop-","chapter-1","Data Structures and Algorithms"
"erly defining them. This section presents terminology and motivates the design","chapter-1","Data Structures and Algorithms"
"process embodied in the three-step approach to selecting a data structure. This mo-","chapter-1","Data Structures and Algorithms"
"tivation stems from the need to manage the tremendous complexity of computer","chapter-1","Data Structures and Algorithms"
"programs.","chapter-1","Data Structures and Algorithms"
"A type is a collection of values. For example, the Boolean type consists of the","chapter-1","Data Structures and Algorithms"
"values true and false. The integers also form a type. An integer is a simple","chapter-1","Data Structures and Algorithms"
"type because its values contain no subparts. A bank account record will typically","chapter-1","Data Structures and Algorithms"
"contain several pieces of information such as name, address, account number, and","chapter-1","Data Structures and Algorithms"
"account balance. Such a record is an example of an aggregate type or composite","chapter-1","Data Structures and Algorithms"
"type. A data item is a piece of information or a record whose value is drawn from","chapter-1","Data Structures and Algorithms"
"a type. A data item is said to be a member of a type.","chapter-1","Data Structures and Algorithms"
"A data type is a type together with a collection of operations to manipulate","chapter-1","Data Structures and Algorithms"
"the type. For example, an integer variable is a member of the integer data type.","chapter-1","Data Structures and Algorithms"
"Addition is an example of an operation on the integer data type.","chapter-1","Data Structures and Algorithms"
"A distinction should be made between the logical concept of a data type and its","chapter-1","Data Structures and Algorithms"
"physical implementation in a computer program. For example, there are two tra-","chapter-1","Data Structures and Algorithms"
"ditional implementations for the list data type: the linked list and the array-based","chapter-1","Data Structures and Algorithms"
"list. The list data type can therefore be implemented using a linked list or an ar-","chapter-1","Data Structures and Algorithms"
"ray. Even the term “array” is ambiguous in that it can refer either to a data type","chapter-1","Data Structures and Algorithms"
"or an implementation. “Array” is commonly used in computer programming to","chapter-1","Data Structures and Algorithms"
"mean a contiguous block of memory locations, where each memory location stores","chapter-1","Data Structures and Algorithms"
"one fixed-length data item. By this meaning, an array is a physical data structure.","chapter-1","Data Structures and Algorithms"
"However, array can also mean a logical data type composed of a (typically ho-","chapter-1","Data Structures and Algorithms"
"mogeneous) collection of data items, with each data item identified by an index","chapter-1","Data Structures and Algorithms"
"number. It is possible to implement arrays in many different ways. For exam-","chapter-1","Data Structures and Algorithms"
"Sec. 1.2 Abstract Data Types and Data Structures 9","chapter-1","Data Structures and Algorithms"
"ple, Section 12.2 describes the data structure used to implement a sparse matrix, a","chapter-1","Data Structures and Algorithms"
"large two-dimensional array that stores only a relatively few non-zero values. This","chapter-1","Data Structures and Algorithms"
"implementation is quite different from the physical representation of an array as","chapter-1","Data Structures and Algorithms"
"contiguous memory locations.","chapter-1","Data Structures and Algorithms"
"An abstract data type (ADT) is the realization of a data type as a software","chapter-1","Data Structures and Algorithms"
"component. The interface of the ADT is defined in terms of a type and a set of","chapter-1","Data Structures and Algorithms"
"operations on that type. The behavior of each operation is determined by its inputs","chapter-1","Data Structures and Algorithms"
"and outputs. An ADT does not specify how the data type is implemented. These","chapter-1","Data Structures and Algorithms"
"implementation details are hidden from the user of the ADT and protected from","chapter-1","Data Structures and Algorithms"
"outside access, a concept referred to as encapsulation.","chapter-1","Data Structures and Algorithms"
"A data structure is the implementation for an ADT. In an object-oriented lan-","chapter-1","Data Structures and Algorithms"
"guage such as Java, an ADT and its implementation together make up a class.","chapter-1","Data Structures and Algorithms"
"Each operation associated with the ADT is implemented by a member function or","chapter-1","Data Structures and Algorithms"
"method. The variables that define the space required by a data item are referred","chapter-1","Data Structures and Algorithms"
"to as data members. An object is an instance of a class, that is, something that is","chapter-1","Data Structures and Algorithms"
"created and takes up storage during the execution of a computer program.","chapter-1","Data Structures and Algorithms"
"The term “data structure” often refers to data stored in a computer’s main mem-","chapter-1","Data Structures and Algorithms"
"ory. The related term file structure often refers to the organization of data on","chapter-1","Data Structures and Algorithms"
"peripheral storage, such as a disk drive or CD.","chapter-1","Data Structures and Algorithms"
"Example 1.3 The mathematical concept of an integer, along with opera-","chapter-1","Data Structures and Algorithms"
"tions that manipulate integers, form a data type. The Java int variable type","chapter-1","Data Structures and Algorithms"
"is a physical representation of the abstract integer. The int variable type,","chapter-1","Data Structures and Algorithms"
"along with the operations that act on an int variable, form an ADT. Un-","chapter-1","Data Structures and Algorithms"
"fortunately, the int implementation is not completely true to the abstract","chapter-1","Data Structures and Algorithms"
"integer, as there are limitations on the range of values an int variable can","chapter-1","Data Structures and Algorithms"
"store. If these limitations prove unacceptable, then some other represen-","chapter-1","Data Structures and Algorithms"
"tation for the ADT “integer” must be devised, and a new implementation","chapter-1","Data Structures and Algorithms"
"must be used for the associated operations.","chapter-1","Data Structures and Algorithms"
"Example 1.4 An ADT for a list of integers might specify the following","chapter-1","Data Structures and Algorithms"
"operations:","chapter-1","Data Structures and Algorithms"
"• Insert a new integer at a particular position in the list.","chapter-1","Data Structures and Algorithms"
"• Return true if the list is empty.","chapter-1","Data Structures and Algorithms"
"• Reinitialize the list.","chapter-1","Data Structures and Algorithms"
"• Return the number of integers currently in the list.","chapter-1","Data Structures and Algorithms"
"• Delete the integer at a particular position in the list.","chapter-1","Data Structures and Algorithms"
"From this description, the input and output of each operation should be","chapter-1","Data Structures and Algorithms"
"clear, but the implementation for lists has not been specified.","chapter-1","Data Structures and Algorithms"
"10 Chap. 1 Data Structures and Algorithms","chapter-1","Data Structures and Algorithms"
"One application that makes use of some ADT might use particular member","chapter-1","Data Structures and Algorithms"
"functions of that ADT more than a second application, or the two applications might","chapter-1","Data Structures and Algorithms"
"have different time requirements for the various operations. These differences in the","chapter-1","Data Structures and Algorithms"
"requirements of applications are the reason why a given ADT might be supported","chapter-1","Data Structures and Algorithms"
"by more than one implementation.","chapter-1","Data Structures and Algorithms"
"Example 1.5 Two popular implementations for large disk-based database","chapter-1","Data Structures and Algorithms"
"applications are hashing (Section 9.4) and the B+-tree (Section 10.5). Both","chapter-1","Data Structures and Algorithms"
"support efficient insertion and deletion of records, and both support exact-","chapter-1","Data Structures and Algorithms"
"match queries. However, hashing is more efficient than the B+-tree for","chapter-1","Data Structures and Algorithms"
"exact-match queries. On the other hand, the B+-tree can perform range","chapter-1","Data Structures and Algorithms"
"queries efficiently, while hashing is hopelessly inefficient for range queries.","chapter-1","Data Structures and Algorithms"
"Thus, if the database application limits searches to exact-match queries,","chapter-1","Data Structures and Algorithms"
"hashing is preferred. On the other hand, if the application requires support","chapter-1","Data Structures and Algorithms"
"for range queries, the B+-tree is preferred. Despite these performance is-","chapter-1","Data Structures and Algorithms"
"sues, both implementations solve versions of the same problem: updating","chapter-1","Data Structures and Algorithms"
"and searching a large collection of records.","chapter-1","Data Structures and Algorithms"
"The concept of an ADT can help us to focus on key issues even in non-comp-","chapter-1","Data Structures and Algorithms"
"uting applications.","chapter-1","Data Structures and Algorithms"
"Example 1.6 When operating a car, the primary activities are steering,","chapter-1","Data Structures and Algorithms"
"accelerating, and braking. On nearly all passenger cars, you steer by turn-","chapter-1","Data Structures and Algorithms"
"ing the steering wheel, accelerate by pushing the gas pedal, and brake by","chapter-1","Data Structures and Algorithms"
"pushing the brake pedal. This design for cars can be viewed as an ADT","chapter-1","Data Structures and Algorithms"
"with operations “steer,” “accelerate,” and “brake.” Two cars might imple-","chapter-1","Data Structures and Algorithms"
"ment these operations in radically different ways, say with different types","chapter-1","Data Structures and Algorithms"
"of engine, or front- versus rear-wheel drive. Yet, most drivers can oper-","chapter-1","Data Structures and Algorithms"
"ate many different cars because the ADT presents a uniform method of","chapter-1","Data Structures and Algorithms"
"operation that does not require the driver to understand the specifics of any","chapter-1","Data Structures and Algorithms"
"particular engine or drive design. These differences are deliberately hidden.","chapter-1","Data Structures and Algorithms"
"The concept of an ADT is one instance of an important principle that must be","chapter-1","Data Structures and Algorithms"
"understood by any successful computer scientist: managing complexity through","chapter-1","Data Structures and Algorithms"
"abstraction. A central theme of computer science is complexity and techniques","chapter-1","Data Structures and Algorithms"
"for handling it. Humans deal with complexity by assigning a label to an assembly","chapter-1","Data Structures and Algorithms"
"of objects or concepts and then manipulating the label in place of the assembly.","chapter-1","Data Structures and Algorithms"
"Cognitive psychologists call such a label a metaphor. A particular label might be","chapter-1","Data Structures and Algorithms"
"related to other pieces of information or other labels. This collection can in turn be","chapter-1","Data Structures and Algorithms"
"given a label, forming a hierarchy of concepts and labels. This hierarchy of labels","chapter-1","Data Structures and Algorithms"
"allows us to focus on important issues while ignoring unnecessary details.","chapter-1","Data Structures and Algorithms"
"Sec. 1.2 Abstract Data Types and Data Structures 11","chapter-1","Data Structures and Algorithms"
"Example 1.7 We apply the label “hard drive” to a collection of hardware","chapter-1","Data Structures and Algorithms"
"that manipulates data on a particular type of storage device, and we ap-","chapter-1","Data Structures and Algorithms"
"ply the label “CPU” to the hardware that controls execution of computer","chapter-1","Data Structures and Algorithms"
"instructions. These and other labels are gathered together under the label","chapter-1","Data Structures and Algorithms"
"“computer.” Because even the smallest home computers today have mil-","chapter-1","Data Structures and Algorithms"
"lions of components, some form of abstraction is necessary to comprehend","chapter-1","Data Structures and Algorithms"
"how a computer operates.","chapter-1","Data Structures and Algorithms"
"Consider how you might go about the process of designing a complex computer","chapter-1","Data Structures and Algorithms"
"program that implements and manipulates an ADT. The ADT is implemented in","chapter-1","Data Structures and Algorithms"
"one part of the program by a particular data structure. While designing those parts","chapter-1","Data Structures and Algorithms"
"of the program that use the ADT, you can think in terms of operations on the data","chapter-1","Data Structures and Algorithms"
"type without concern for the data structure’s implementation. Without this ability","chapter-1","Data Structures and Algorithms"
"to simplify your thinking about a complex program, you would have no hope of","chapter-1","Data Structures and Algorithms"
"understanding or implementing it.","chapter-1","Data Structures and Algorithms"
"Example 1.8 Consider the design for a relatively simple database system","chapter-1","Data Structures and Algorithms"
"stored on disk. Typically, records on disk in such a program are accessed","chapter-1","Data Structures and Algorithms"
"through a buffer pool (see Section 8.3) rather than directly. Variable length","chapter-1","Data Structures and Algorithms"
"records might use a memory manager (see Section 12.3) to find an appro-","chapter-1","Data Structures and Algorithms"
"priate location within the disk file to place the record. Multiple index struc-","chapter-1","Data Structures and Algorithms"
"tures (see Chapter 10) will typically be used to access records in various","chapter-1","Data Structures and Algorithms"
"ways. Thus, we have a chain of classes, each with its own responsibili-","chapter-1","Data Structures and Algorithms"
"ties and access privileges. A database query from a user is implemented","chapter-1","Data Structures and Algorithms"
"by searching an index structure. This index requests access to the record","chapter-1","Data Structures and Algorithms"
"by means of a request to the buffer pool. If a record is being inserted or","chapter-1","Data Structures and Algorithms"
"deleted, such a request goes through the memory manager, which in turn","chapter-1","Data Structures and Algorithms"
"interacts with the buffer pool to gain access to the disk file. A program such","chapter-1","Data Structures and Algorithms"
"as this is far too complex for nearly any human programmer to keep all of","chapter-1","Data Structures and Algorithms"
"the details in his or her head at once. The only way to design and imple-","chapter-1","Data Structures and Algorithms"
"ment such a program is through proper use of abstraction and metaphors.","chapter-1","Data Structures and Algorithms"
"In object-oriented programming, such abstraction is handled using classes.","chapter-1","Data Structures and Algorithms"
"Data types have both a logical and a physical form. The definition of the data","chapter-1","Data Structures and Algorithms"
"type in terms of an ADT is its logical form. The implementation of the data type as","chapter-1","Data Structures and Algorithms"
"a data structure is its physical form. Figure 1.1 illustrates this relationship between","chapter-1","Data Structures and Algorithms"
"logical and physical forms for data types. When you implement an ADT, you","chapter-1","Data Structures and Algorithms"
"are dealing with the physical form of the associated data type. When you use an","chapter-1","Data Structures and Algorithms"
"ADT elsewhere in your program, you are concerned with the associated data type’s","chapter-1","Data Structures and Algorithms"
"logical form. Some sections of this book focus on physical implementations for a","chapter-1","Data Structures and Algorithms"
"12 Chap. 1 Data Structures and Algorithms","chapter-1","Data Structures and Algorithms"
"Data Type","chapter-1","Data Structures and Algorithms"
"Data Structure:","chapter-1","Data Structures and Algorithms"
"Storage Space","chapter-1","Data Structures and Algorithms"
"Subroutines","chapter-1","Data Structures and Algorithms"
"ADT:","chapter-1","Data Structures and Algorithms"
"Type","chapter-1","Data Structures and Algorithms"
"Operations","chapter-1","Data Structures and Algorithms"
"Data Items:","chapter-1","Data Structures and Algorithms"
"Data Items:","chapter-1","Data Structures and Algorithms"
"Physical Form","chapter-1","Data Structures and Algorithms"
"Logical Form","chapter-1","Data Structures and Algorithms"
"Figure 1.1 The relationship between data items, abstract data types, and data","chapter-1","Data Structures and Algorithms"
"structures. The ADT defines the logical form of the data type. The data structure","chapter-1","Data Structures and Algorithms"
"implements the physical form of the data type.","chapter-1","Data Structures and Algorithms"
"given data structure. Other sections use the logical ADT for the data structure in","chapter-1","Data Structures and Algorithms"
"the context of a higher-level task.","chapter-1","Data Structures and Algorithms"
"Example 1.9 A particular Java environment might provide a library that","chapter-1","Data Structures and Algorithms"
"includes a list class. The logical form of the list is defined by the public","chapter-1","Data Structures and Algorithms"
"functions, their inputs, and their outputs that define the class. This might be","chapter-1","Data Structures and Algorithms"
"all that you know about the list class implementation, and this should be all","chapter-1","Data Structures and Algorithms"
"you need to know. Within the class, a variety of physical implementations","chapter-1","Data Structures and Algorithms"
"for lists is possible. Several are described in Section 4.1.","chapter-1","Data Structures and Algorithms"
"1.3 Design Patterns","chapter-1","Data Structures and Algorithms"
"At a higher level of abstraction than ADTs are abstractions for describing the design","chapter-1","Data Structures and Algorithms"
"of programs — that is, the interactions of objects and classes. Experienced software","chapter-1","Data Structures and Algorithms"
"designers learn and reuse patterns for combining software components. These have","chapter-1","Data Structures and Algorithms"
"come to be referred to as design patterns.","chapter-1","Data Structures and Algorithms"
"A design pattern embodies and generalizes important design concepts for a","chapter-1","Data Structures and Algorithms"
"recurring problem. A primary goal of design patterns is to quickly transfer the","chapter-1","Data Structures and Algorithms"
"knowledge gained by expert designers to newer programmers. Another goal is","chapter-1","Data Structures and Algorithms"
"to allow for efficient communication between programmers. It is much easier to","chapter-1","Data Structures and Algorithms"
"discuss a design issue when you share a technical vocabulary relevant to the topic.","chapter-1","Data Structures and Algorithms"
"Specific design patterns emerge from the realization that a particular design","chapter-1","Data Structures and Algorithms"
"problem appears repeatedly in many contexts. They are meant to solve real prob-","chapter-1","Data Structures and Algorithms"
"lems. Design patterns are a bit like generics. They describe the structure for a","chapter-1","Data Structures and Algorithms"
"design solution, with the details filled in for any given problem. Design patterns","chapter-1","Data Structures and Algorithms"
"are a bit like data structures: Each one provides costs and benefits, which implies","chapter-1","Data Structures and Algorithms"
"Sec. 1.3 Design Patterns 13","chapter-1","Data Structures and Algorithms"
"that tradeoffs are possible. Therefore, a given design pattern might have variations","chapter-1","Data Structures and Algorithms"
"on its application to match the various tradeoffs inherent in a given situation.","chapter-1","Data Structures and Algorithms"
"The rest of this section introduces a few simple design patterns that are used","chapter-1","Data Structures and Algorithms"
"later in the book.","chapter-1","Data Structures and Algorithms"
"1.3.1 Flyweight","chapter-1","Data Structures and Algorithms"
"The Flyweight design pattern is meant to solve the following problem. You have an","chapter-1","Data Structures and Algorithms"
"application with many objects. Some of these objects are identical in the informa-","chapter-1","Data Structures and Algorithms"
"tion that they contain, and the role that they play. But they must be reached from","chapter-1","Data Structures and Algorithms"
"various places, and conceptually they really are distinct objects. Because there is","chapter-1","Data Structures and Algorithms"
"so much duplication of the same information, we would like to take advantage of","chapter-1","Data Structures and Algorithms"
"the opportunity to reduce memory cost by sharing that space. An example comes","chapter-1","Data Structures and Algorithms"
"from representing the layout for a document. The letter “C” might reasonably be","chapter-1","Data Structures and Algorithms"
"represented by an object that describes that character’s strokes and bounding box.","chapter-1","Data Structures and Algorithms"
"However, we do not want to create a separate “C” object everywhere in the doc-","chapter-1","Data Structures and Algorithms"
"ument that a “C” appears. The solution is to allocate a single copy of the shared","chapter-1","Data Structures and Algorithms"
"representation for “C” objects. Then, every place in the document that needs a","chapter-1","Data Structures and Algorithms"
"“C” in a given font, size, and typeface will reference this single copy. The various","chapter-1","Data Structures and Algorithms"
"instances of references to a specific form of “C” are called flyweights.","chapter-1","Data Structures and Algorithms"
"We could describe the layout of text on a page by using a tree structure. The","chapter-1","Data Structures and Algorithms"
"root of the tree represents the entire page. The page has multiple child nodes, one","chapter-1","Data Structures and Algorithms"
"for each column. The column nodes have child nodes for each row. And the rows","chapter-1","Data Structures and Algorithms"
"have child nodes for each character. These representations for characters are the fly-","chapter-1","Data Structures and Algorithms"
"weights. The flyweight includes the reference to the shared shape information, and","chapter-1","Data Structures and Algorithms"
"might contain additional information specific to that instance. For example, each","chapter-1","Data Structures and Algorithms"
"instance for “C” will contain a reference to the shared information about strokes","chapter-1","Data Structures and Algorithms"
"and shapes, and it might also contain the exact location for that instance of the","chapter-1","Data Structures and Algorithms"
"character on the page.","chapter-1","Data Structures and Algorithms"
"Flyweights are used in the implementation for the PR quadtree data structure","chapter-1","Data Structures and Algorithms"
"for storing collections of point objects, described in Section 13.3. In a PR quadtree,","chapter-1","Data Structures and Algorithms"
"we again have a tree with leaf nodes. Many of these leaf nodes represent empty","chapter-1","Data Structures and Algorithms"
"areas, and so the only information that they store is the fact that they are empty.","chapter-1","Data Structures and Algorithms"
"These identical nodes can be implemented using a reference to a single instance of","chapter-1","Data Structures and Algorithms"
"the flyweight for better memory efficiency.","chapter-1","Data Structures and Algorithms"
"Given a tree of objects to describe a page layout, we might wish to perform some","chapter-1","Data Structures and Algorithms"
"activity on every node in the tree. Section 5.2 discusses tree traversal, which is the","chapter-1","Data Structures and Algorithms"
"process of visiting every node in the tree in a defined order. A simple example for","chapter-1","Data Structures and Algorithms"
"our text composition application might be to count the number of nodes in the tree","chapter-1","Data Structures and Algorithms"
"14 Chap. 1 Data Structures and Algorithms","chapter-1","Data Structures and Algorithms"
"that represents the page. At another time, we might wish to print a listing of all the","chapter-1","Data Structures and Algorithms"
"nodes for debugging purposes.","chapter-1","Data Structures and Algorithms"
"We could write a separate traversal function for each such activity that we in-","chapter-1","Data Structures and Algorithms"
"tend to perform on the tree. A better approach would be to write a generic traversal","chapter-1","Data Structures and Algorithms"
"function, and pass in the activity to be performed at each node. This organization","chapter-1","Data Structures and Algorithms"
"constitutes the visitor design pattern. The visitor design pattern is used in Sec-","chapter-1","Data Structures and Algorithms"
"tions 5.2 (tree traversal) and 11.3 (graph traversal).","chapter-1","Data Structures and Algorithms"
"There are two fundamental approaches to dealing with the relationship between","chapter-1","Data Structures and Algorithms"
"a collection of actions and a hierarchy of object types. First consider the typical","chapter-1","Data Structures and Algorithms"
"procedural approach. Say we have a base class for page layout entities, with a sub-","chapter-1","Data Structures and Algorithms"
"class hierarchy to define specific subtypes (page, columns, rows, figures, charac-","chapter-1","Data Structures and Algorithms"
"ters, etc.). And say there are actions to be performed on a collection of such objects","chapter-1","Data Structures and Algorithms"
"(such as rendering the objects to the screen). The procedural design approach is for","chapter-1","Data Structures and Algorithms"
"each action to be implemented as a method that takes as a parameter a pointer to","chapter-1","Data Structures and Algorithms"
"the base class type. Each action such method will traverse through the collection","chapter-1","Data Structures and Algorithms"
"of objects, visiting each object in turn. Each action method contains something","chapter-1","Data Structures and Algorithms"
"like a switch statement that defines the details of the action for each subclass in the","chapter-1","Data Structures and Algorithms"
"collection (e.g., page, column, row, character). We can cut the code down some by","chapter-1","Data Structures and Algorithms"
"using the visitor design pattern so that we only need to write the traversal once, and","chapter-1","Data Structures and Algorithms"
"then write a visitor subroutine for each action that might be applied to the collec-","chapter-1","Data Structures and Algorithms"
"tion of objects. But each such visitor subroutine must still contain logic for dealing","chapter-1","Data Structures and Algorithms"
"with each of the possible subclasses.","chapter-1","Data Structures and Algorithms"
"In our page composition application, there are only a few activities that we","chapter-1","Data Structures and Algorithms"
"would like to perform on the page representation. We might render the objects in","chapter-1","Data Structures and Algorithms"
"full detail. Or we might want a “rough draft” rendering that prints only the bound-","chapter-1","Data Structures and Algorithms"
"ing boxes of the objects. If we come up with a new activity to apply to the collection","chapter-1","Data Structures and Algorithms"
"of objects, we do not need to change any of the code that implements the existing","chapter-1","Data Structures and Algorithms"
"activities. But adding new activities won’t happen often for this application. In","chapter-1","Data Structures and Algorithms"
"contrast, there could be many object types, and we might frequently add new ob-","chapter-1","Data Structures and Algorithms"
"ject types to our implementation. Unfortunately, adding a new object type requires","chapter-1","Data Structures and Algorithms"
"that we modify each activity, and the subroutines implementing the activities get","chapter-1","Data Structures and Algorithms"
"rather long switch statements to distinguish the behavior of the many subclasses.","chapter-1","Data Structures and Algorithms"
"An alternative design is to have each object subclass in the hierarchy embody","chapter-1","Data Structures and Algorithms"
"the action for each of the various activities that might be performed. Each subclass","chapter-1","Data Structures and Algorithms"
"will have code to perform each activity (such as full rendering or bounding box","chapter-1","Data Structures and Algorithms"
"rendering). Then, if we wish to apply the activity to the collection, we simply call","chapter-1","Data Structures and Algorithms"
"the first object in the collection and specify the action (as a method call on that","chapter-1","Data Structures and Algorithms"
"object). In the case of our page layout and its hierarchical collection of objects,","chapter-1","Data Structures and Algorithms"
"those objects that contain other objects (such as a row objects that contains letters)","chapter-1","Data Structures and Algorithms"
"Sec. 1.3 Design Patterns 15","chapter-1","Data Structures and Algorithms"
"will call the appropriate method for each child. If we want to add a new activity","chapter-1","Data Structures and Algorithms"
"with this organization, we have to change the code for every subclass. But this is","chapter-1","Data Structures and Algorithms"
"relatively rare for our text compositing application. In contrast, adding a new object","chapter-1","Data Structures and Algorithms"
"into the subclass hierarchy (which for this application is far more likely than adding","chapter-1","Data Structures and Algorithms"
"a new rendering function) is easy. Adding a new subclass does not require changing","chapter-1","Data Structures and Algorithms"
"any of the existing subclasses. It merely requires that we define the behavior of each","chapter-1","Data Structures and Algorithms"
"activity that can be performed on the new subclass.","chapter-1","Data Structures and Algorithms"
"This second design approach of burying the functional activity in the subclasses","chapter-1","Data Structures and Algorithms"
"is called the Composite design pattern. A detailed example for using the Composite","chapter-1","Data Structures and Algorithms"
"design pattern is presented in Section 5.3.1.","chapter-1","Data Structures and Algorithms"
"Our final example of a design pattern lets us encapsulate and make interchangeable","chapter-1","Data Structures and Algorithms"
"a set of alternative actions that might be performed as part of some larger activity.","chapter-1","Data Structures and Algorithms"
"Again continuing our text compositing example, each output device that we wish","chapter-1","Data Structures and Algorithms"
"to render to will require its own function for doing the actual rendering. That is,","chapter-1","Data Structures and Algorithms"
"the objects will be broken down into constituent pixels or strokes, but the actual","chapter-1","Data Structures and Algorithms"
"mechanics of rendering a pixel or stroke will depend on the output device. We","chapter-1","Data Structures and Algorithms"
"don’t want to build this rendering functionality into the object subclasses. Instead,","chapter-1","Data Structures and Algorithms"
"we want to pass to the subroutine performing the rendering action a method or class","chapter-1","Data Structures and Algorithms"
"that does the appropriate rendering details for that output device. That is, we wish","chapter-1","Data Structures and Algorithms"
"to hand to the object the appropriate “strategy” for accomplishing the details of the","chapter-1","Data Structures and Algorithms"
"rendering task. Thus, this approach is called the Strategy design pattern.","chapter-1","Data Structures and Algorithms"
"The Strategy design pattern can be used to create generalized sorting functions.","chapter-1","Data Structures and Algorithms"
"The sorting function can be called with an additional parameter. This parameter is","chapter-1","Data Structures and Algorithms"
"a class that understands how to extract and compare the key values for records to","chapter-1","Data Structures and Algorithms"
"be sorted. In this way, the sorting function does not need to know any details of","chapter-1","Data Structures and Algorithms"
"how its record type is implemented.","chapter-1","Data Structures and Algorithms"
"One of the biggest challenges to understanding design patterns is that some-","chapter-1","Data Structures and Algorithms"
"times one is only subtly different from another. For example, you might be con-","chapter-1","Data Structures and Algorithms"
"fused about the difference between the composite pattern and the visitor pattern.","chapter-1","Data Structures and Algorithms"
"The distinction is that the composite design pattern is about whether to give control","chapter-1","Data Structures and Algorithms"
"of the traversal process to the nodes of the tree or to the tree itself. Both approaches","chapter-1","Data Structures and Algorithms"
"can make use of the visitor design pattern to avoid rewriting the traversal function","chapter-1","Data Structures and Algorithms"
"many times, by encapsulating the activity performed at each node.","chapter-1","Data Structures and Algorithms"
"But isn’t the strategy design pattern doing the same thing? The difference be-","chapter-1","Data Structures and Algorithms"
"tween the visitor pattern and the strategy pattern is more subtle. Here the difference","chapter-1","Data Structures and Algorithms"
"is primarily one of intent and focus. In both the strategy design pattern and the visi-","chapter-1","Data Structures and Algorithms"
"tor design pattern, an activity is being passed in as a parameter. The strategy design","chapter-1","Data Structures and Algorithms"
"pattern is focused on encapsulating an activity that is part of a larger process, so","chapter-1","Data Structures and Algorithms"
"16 Chap. 1 Data Structures and Algorithms","chapter-1","Data Structures and Algorithms"
"that different ways of performing that activity can be substituted. The visitor de-","chapter-1","Data Structures and Algorithms"
"sign pattern is focused on encapsulating an activity that will be performed on all","chapter-1","Data Structures and Algorithms"
"members of a collection so that completely different activities can be substituted","chapter-1","Data Structures and Algorithms"
"within a generic method that accesses all of the collection members.","chapter-1","Data Structures and Algorithms"
"1.4 Problems, Algorithms, and Programs","chapter-1","Data Structures and Algorithms"
"Programmers commonly deal with problems, algorithms, and computer programs.","chapter-1","Data Structures and Algorithms"
"These are three distinct concepts.","chapter-1","Data Structures and Algorithms"
"Problems: As your intuition would suggest, a problem is a task to be performed.","chapter-1","Data Structures and Algorithms"
"It is best thought of in terms of inputs and matching outputs. A problem definition","chapter-1","Data Structures and Algorithms"
"should not include any constraints on how the problem is to be solved. The solution","chapter-1","Data Structures and Algorithms"
"method should be developed only after the problem is precisely defined and thor-","chapter-1","Data Structures and Algorithms"
"oughly understood. However, a problem definition should include constraints on","chapter-1","Data Structures and Algorithms"
"the resources that may be consumed by any acceptable solution. For any problem","chapter-1","Data Structures and Algorithms"
"to be solved by a computer, there are always such constraints, whether stated or","chapter-1","Data Structures and Algorithms"
"implied. For example, any computer program may use only the main memory and","chapter-1","Data Structures and Algorithms"
"disk space available, and it must run in a “reasonable” amount of time.","chapter-1","Data Structures and Algorithms"
"Problems can be viewed as functions in the mathematical sense. A function","chapter-1","Data Structures and Algorithms"
"is a matching between inputs (the domain) and outputs (the range). An input","chapter-1","Data Structures and Algorithms"
"to a function might be a single value or a collection of information. The values","chapter-1","Data Structures and Algorithms"
"making up an input are called the parameters of the function. A specific selection","chapter-1","Data Structures and Algorithms"
"of values for the parameters is called an instance of the problem. For example,","chapter-1","Data Structures and Algorithms"
"the input parameter to a sorting function might be an array of integers. A particular","chapter-1","Data Structures and Algorithms"
"array of integers, with a given size and specific values for each position in the array,","chapter-1","Data Structures and Algorithms"
"would be an instance of the sorting problem. Different instances might generate the","chapter-1","Data Structures and Algorithms"
"same output. However, any problem instance must always result in the same output","chapter-1","Data Structures and Algorithms"
"every time the function is computed using that particular input.","chapter-1","Data Structures and Algorithms"
"This concept of all problems behaving like mathematical functions might not","chapter-1","Data Structures and Algorithms"
"match your intuition for the behavior of computer programs. You might know of","chapter-1","Data Structures and Algorithms"
"programs to which you can give the same input value on two separate occasions,","chapter-1","Data Structures and Algorithms"
"and two different outputs will result. For example, if you type “date” to a typical","chapter-1","Data Structures and Algorithms"
"UNIX command line prompt, you will get the current date. Naturally the date will","chapter-1","Data Structures and Algorithms"
"be different on different days, even though the same command is given. However,","chapter-1","Data Structures and Algorithms"
"there is obviously more to the input for the date program than the command that you","chapter-1","Data Structures and Algorithms"
"type to run the program. The date program computes a function. In other words,","chapter-1","Data Structures and Algorithms"
"on any particular day there can only be a single answer returned by a properly","chapter-1","Data Structures and Algorithms"
"running date program on a completely specified input. For all computer programs,","chapter-1","Data Structures and Algorithms"
"the output is completely determined by the program’s full set of inputs. Even a","chapter-1","Data Structures and Algorithms"
"“random number generator” is completely determined by its inputs (although some","chapter-1","Data Structures and Algorithms"
"random number generating systems appear to get around this by accepting a random","chapter-1","Data Structures and Algorithms"
"Sec. 1.4 Problems, Algorithms, and Programs 17","chapter-1","Data Structures and Algorithms"
"input from a physical process beyond the user’s control). The relationship between","chapter-1","Data Structures and Algorithms"
"programs and functions is explored further in Section 17.3.","chapter-1","Data Structures and Algorithms"
"Algorithms: An algorithm is a method or a process followed to solve a problem.","chapter-1","Data Structures and Algorithms"
"If the problem is viewed as a function, then an algorithm is an implementation for","chapter-1","Data Structures and Algorithms"
"the function that transforms an input to the corresponding output. A problem can be","chapter-1","Data Structures and Algorithms"
"solved by many different algorithms. A given algorithm solves only one problem","chapter-1","Data Structures and Algorithms"
"(i.e., computes a particular function). This book covers many problems, and for","chapter-1","Data Structures and Algorithms"
"several of these problems I present more than one algorithm. For the important","chapter-1","Data Structures and Algorithms"
"problem of sorting I present nearly a dozen algorithms!","chapter-1","Data Structures and Algorithms"
"The advantage of knowing several solutions to a problem is that solution A","chapter-1","Data Structures and Algorithms"
"might be more efficient than solution B for a specific variation of the problem,","chapter-1","Data Structures and Algorithms"
"or for a specific class of inputs to the problem, while solution B might be more","chapter-1","Data Structures and Algorithms"
"efficient than A for another variation or class of inputs. For example, one sorting","chapter-1","Data Structures and Algorithms"
"algorithm might be the best for sorting a small collection of integers (which is","chapter-1","Data Structures and Algorithms"
"important if you need to do this many times). Another might be the best for sorting","chapter-1","Data Structures and Algorithms"
"a large collection of integers. A third might be the best for sorting a collection of","chapter-1","Data Structures and Algorithms"
"variable-length strings.","chapter-1","Data Structures and Algorithms"
"By definition, something can only be called an algorithm if it has all of the","chapter-1","Data Structures and Algorithms"
"following properties.","chapter-1","Data Structures and Algorithms"
"1. It must be correct. In other words, it must compute the desired function,","chapter-1","Data Structures and Algorithms"
"converting each input to the correct output. Note that every algorithm im-","chapter-1","Data Structures and Algorithms"
"plements some function, because every algorithm maps every input to some","chapter-1","Data Structures and Algorithms"
"output (even if that output is a program crash). At issue here is whether a","chapter-1","Data Structures and Algorithms"
"given algorithm implements the intended function.","chapter-1","Data Structures and Algorithms"
"2. It is composed of a series of concrete steps. Concrete means that the action","chapter-1","Data Structures and Algorithms"
"described by that step is completely understood — and doable — by the","chapter-1","Data Structures and Algorithms"
"person or machine that must perform the algorithm. Each step must also be","chapter-1","Data Structures and Algorithms"
"doable in a finite amount of time. Thus, the algorithm gives us a “recipe” for","chapter-1","Data Structures and Algorithms"
"solving the problem by performing a series of steps, where each such step","chapter-1","Data Structures and Algorithms"
"is within our capacity to perform. The ability to perform a step can depend","chapter-1","Data Structures and Algorithms"
"on who or what is intended to execute the recipe. For example, the steps of","chapter-1","Data Structures and Algorithms"
"a cookie recipe in a cookbook might be considered sufficiently concrete for","chapter-1","Data Structures and Algorithms"
"instructing a human cook, but not for programming an automated cookie-","chapter-1","Data Structures and Algorithms"
"making factory.","chapter-1","Data Structures and Algorithms"
"3. There can be no ambiguity as to which step will be performed next. Often it","chapter-1","Data Structures and Algorithms"
"is the next step of the algorithm description. Selection (e.g., the if statement","chapter-1","Data Structures and Algorithms"
"in Java) is normally a part of any language for describing algorithms. Selec-","chapter-1","Data Structures and Algorithms"
"tion allows a choice for which step will be performed next, but the selection","chapter-1","Data Structures and Algorithms"
"process is unambiguous at the time when the choice is made.","chapter-1","Data Structures and Algorithms"
"4. It must be composed of a finite number of steps. If the description for the","chapter-1","Data Structures and Algorithms"
"algorithm were made up of an infinite number of steps, we could never hope","chapter-1","Data Structures and Algorithms"
"18 Chap. 1 Data Structures and Algorithms","chapter-1","Data Structures and Algorithms"
"to write it down, nor implement it as a computer program. Most languages for","chapter-1","Data Structures and Algorithms"
"describing algorithms (including English and “pseudocode”) provide some","chapter-1","Data Structures and Algorithms"
"way to perform repeated actions, known as iteration. Examples of iteration","chapter-1","Data Structures and Algorithms"
"in programming languages include the while and for loop constructs of","chapter-1","Data Structures and Algorithms"
"Java. Iteration allows for short descriptions, with the number of steps actually","chapter-1","Data Structures and Algorithms"
"performed controlled by the input.","chapter-1","Data Structures and Algorithms"
"5. It must terminate. In other words, it may not go into an infinite loop.","chapter-1","Data Structures and Algorithms"
"Programs: We often think of a computer program as an instance, or concrete","chapter-1","Data Structures and Algorithms"
"representation, of an algorithm in some programming language. In this book,","chapter-1","Data Structures and Algorithms"
"nearly all of the algorithms are presented in terms of programs, or parts of pro-","chapter-1","Data Structures and Algorithms"
"grams. Naturally, there are many programs that are instances of the same alg-","chapter-1","Data Structures and Algorithms"
"orithm, because any modern computer programming language can be used to im-","chapter-1","Data Structures and Algorithms"
"plement the same collection of algorithms (although some programming languages","chapter-1","Data Structures and Algorithms"
"can make life easier for the programmer). To simplify presentation, I often use","chapter-1","Data Structures and Algorithms"
"the terms “algorithm” and “program” interchangeably, despite the fact that they are","chapter-1","Data Structures and Algorithms"
"really separate concepts. By definition, an algorithm must provide sufficient detail","chapter-1","Data Structures and Algorithms"
"that it can be converted into a program when needed.","chapter-1","Data Structures and Algorithms"
"The requirement that an algorithm must terminate means that not all computer","chapter-1","Data Structures and Algorithms"
"programs meet the technical definition of an algorithm. Your operating system is","chapter-1","Data Structures and Algorithms"
"one such program. However, you can think of the various tasks for an operating sys-","chapter-1","Data Structures and Algorithms"
"tem (each with associated inputs and outputs) as individual problems, each solved","chapter-1","Data Structures and Algorithms"
"by specific algorithms implemented by a part of the operating system program, and","chapter-1","Data Structures and Algorithms"
"each one of which terminates once its output is produced.","chapter-1","Data Structures and Algorithms"
"To summarize: A problem is a function or a mapping of inputs to outputs.","chapter-1","Data Structures and Algorithms"
"An algorithm is a recipe for solving a problem whose steps are concrete and un-","chapter-1","Data Structures and Algorithms"
"ambiguous. Algorithms must be correct, of finite length, and must terminate for all","chapter-1","Data Structures and Algorithms"
"inputs. A program is an instantiation of an algorithm in a programming language.","chapter-1","Data Structures and Algorithms"
"1.5 Further Reading","chapter-1","Data Structures and Algorithms"
"An early authoritative work on data structures and algorithms was the series of","chapter-1","Data Structures and Algorithms"
"books The Art of Computer Programming by Donald E. Knuth, with Volumes 1","chapter-1","Data Structures and Algorithms"
"and 3 being most relevant to the study of data structures [Knu97, Knu98]. A mod-","chapter-1","Data Structures and Algorithms"
"ern encyclopedic approach to data structures and algorithms that should be easy","chapter-1","Data Structures and Algorithms"
"to understand once you have mastered this book is Algorithms by Robert Sedge-","chapter-1","Data Structures and Algorithms"
"wick [Sed11]. For an excellent and highly readable (but more advanced) teaching","chapter-1","Data Structures and Algorithms"
"introduction to algorithms, their design, and their analysis, see Introduction to Al-","chapter-1","Data Structures and Algorithms"
"gorithms: A Creative Approach by Udi Manber [Man89]. For an advanced, en-","chapter-1","Data Structures and Algorithms"
"cyclopedic approach, see Introduction to Algorithms by Cormen, Leiserson, and","chapter-1","Data Structures and Algorithms"
"Rivest [CLRS09]. Steven S. Skiena’s The Algorithm Design Manual [Ski10] pro-","chapter-1","Data Structures and Algorithms"
"Sec. 1.5 Further Reading 19","chapter-1","Data Structures and Algorithms"
"vides pointers to many implementations for data structures and algorithms that are","chapter-1","Data Structures and Algorithms"
"available on the Web.","chapter-1","Data Structures and Algorithms"
"The claim that all modern programming languages can implement the same","chapter-1","Data Structures and Algorithms"
"algorithms (stated more precisely, any function that is computable by one program-","chapter-1","Data Structures and Algorithms"
"ming language is computable by any programming language with certain standard","chapter-1","Data Structures and Algorithms"
"capabilities) is a key result from computability theory. For an easy introduction to","chapter-1","Data Structures and Algorithms"
"this field see James L. Hein, Discrete Structures, Logic, and Computability [Hei09].","chapter-1","Data Structures and Algorithms"
"Much of computer science is devoted to problem solving. Indeed, this is what","chapter-1","Data Structures and Algorithms"
"attracts many people to the field. How to Solve It by George Polya [P  ́ ol57] is con-  ́","chapter-1","Data Structures and Algorithms"
"sidered to be the classic work on how to improve your problem-solving abilities. If","chapter-1","Data Structures and Algorithms"
"you want to be a better student (as well as a better problem solver in general), see","chapter-1","Data Structures and Algorithms"
"Strategies for Creative Problem Solving by Folger and LeBlanc [FL95], Effective","chapter-1","Data Structures and Algorithms"
"Problem Solving by Marvin Levine [Lev94], and Problem Solving & Comprehen-","chapter-1","Data Structures and Algorithms"
"sion by Arthur Whimbey and Jack Lochhead [WL99], and Puzzle-Based Learning","chapter-1","Data Structures and Algorithms"
"by Zbigniew and Matthew Michaelewicz [MM08].","chapter-1","Data Structures and Algorithms"
"See The Origin of Consciousness in the Breakdown of the Bicameral Mind by","chapter-1","Data Structures and Algorithms"
"Julian Jaynes [Jay90] for a good discussion on how humans use the concept of","chapter-1","Data Structures and Algorithms"
"metaphor to handle complexity. More directly related to computer science educa-","chapter-1","Data Structures and Algorithms"
"tion and programming, see “Cogito, Ergo Sum! Cognitive Processes of Students","chapter-1","Data Structures and Algorithms"
"Dealing with Data Structures” by Dan Aharoni [Aha00] for a discussion on mov-","chapter-1","Data Structures and Algorithms"
"ing from programming-context thinking to higher-level (and more design-oriented)","chapter-1","Data Structures and Algorithms"
"programming-free thinking.","chapter-1","Data Structures and Algorithms"
"On a more pragmatic level, most people study data structures to write better","chapter-1","Data Structures and Algorithms"
"programs. If you expect your program to work correctly and efficiently, it must","chapter-1","Data Structures and Algorithms"
"first be understandable to yourself and your co-workers. Kernighan and Pike’s The","chapter-1","Data Structures and Algorithms"
"Practice of Programming [KP99] discusses a number of practical issues related to","chapter-1","Data Structures and Algorithms"
"programming, including good coding and documentation style. For an excellent","chapter-1","Data Structures and Algorithms"
"(and entertaining!) introduction to the difficulties involved with writing large pro-","chapter-1","Data Structures and Algorithms"
"grams, read the classic The Mythical Man-Month: Essays on Software Engineering","chapter-1","Data Structures and Algorithms"
"by Frederick P. Brooks [Bro95].","chapter-1","Data Structures and Algorithms"
"If you want to be a successful Java programmer, you need good reference man-","chapter-1","Data Structures and Algorithms"
"uals close at hand. David Flanagan’s Java in a Nutshell [Fla05] provides a good","chapter-1","Data Structures and Algorithms"
"reference for those familiar with the basics of the language.","chapter-1","Data Structures and Algorithms"
"After gaining proficiency in the mechanics of program writing, the next step","chapter-1","Data Structures and Algorithms"
"is to become proficient in program design. Good design is difficult to learn in any","chapter-1","Data Structures and Algorithms"
"discipline, and good design for object-oriented software is one of the most difficult","chapter-1","Data Structures and Algorithms"
"of arts. The novice designer can jump-start the learning process by studying well-","chapter-1","Data Structures and Algorithms"
"known and well-used design patterns. The classic reference on design patterns","chapter-1","Data Structures and Algorithms"
"is Design Patterns: Elements of Reusable Object-Oriented Software by Gamma,","chapter-1","Data Structures and Algorithms"
"Helm, Johnson, and Vlissides [GHJV95] (this is commonly referred to as the “gang","chapter-1","Data Structures and Algorithms"
"of four” book). Unfortunately, this is an extremely difficult book to understand,","chapter-1","Data Structures and Algorithms"
"20 Chap. 1 Data Structures and Algorithms","chapter-1","Data Structures and Algorithms"
"in part because the concepts are inherently difficult. A number of Web sites are","chapter-1","Data Structures and Algorithms"
"available that discuss design patterns, and which provide study guides for the De-","chapter-1","Data Structures and Algorithms"
"sign Patterns book. Two other books that discuss object-oriented software design","chapter-1","Data Structures and Algorithms"
"are Object-Oriented Software Design and Construction with C++ by Dennis Ka-","chapter-1","Data Structures and Algorithms"
"fura [Kaf98], and Object-Oriented Design Heuristics by Arthur J. Riel [Rie96].","chapter-1","Data Structures and Algorithms"
"1.6 Exercises","chapter-1","Data Structures and Algorithms"
"The exercises for this chapter are different from those in the rest of the book. Most","chapter-1","Data Structures and Algorithms"
"of these exercises are answered in the following chapters. However, you should","chapter-1","Data Structures and Algorithms"
"not look up the answers in other parts of the book. These exercises are intended to","chapter-1","Data Structures and Algorithms"
"make you think about some of the issues to be covered later on. Answer them to","chapter-1","Data Structures and Algorithms"
"the best of your ability with your current knowledge.","chapter-1","Data Structures and Algorithms"
"1.1 Think of a program you have used that is unacceptably slow. Identify the spe-","chapter-1","Data Structures and Algorithms"
"cific operations that make the program slow. Identify other basic operations","chapter-1","Data Structures and Algorithms"
"that the program performs quickly enough.","chapter-1","Data Structures and Algorithms"
"1.2 Most programming languages have a built-in integer data type. Normally","chapter-1","Data Structures and Algorithms"
"this representation has a fixed size, thus placing a limit on how large a value","chapter-1","Data Structures and Algorithms"
"can be stored in an integer variable. Describe a representation for integers","chapter-1","Data Structures and Algorithms"
"that has no size restriction (other than the limits of the computer’s available","chapter-1","Data Structures and Algorithms"
"main memory), and thus no practical limit on how large an integer can be","chapter-1","Data Structures and Algorithms"
"stored. Briefly show how your representation can be used to implement the","chapter-1","Data Structures and Algorithms"
"operations of addition, multiplication, and exponentiation.","chapter-1","Data Structures and Algorithms"
"1.3 Define an ADT for character strings. Your ADT should consist of typical","chapter-1","Data Structures and Algorithms"
"functions that can be performed on strings, with each function defined in","chapter-1","Data Structures and Algorithms"
"terms of its input and output. Then define two different physical representa-","chapter-1","Data Structures and Algorithms"
"tions for strings.","chapter-1","Data Structures and Algorithms"
"1.4 Define an ADT for a list of integers. First, decide what functionality your","chapter-1","Data Structures and Algorithms"
"ADT should provide. Example 1.4 should give you some ideas. Then, spec-","chapter-1","Data Structures and Algorithms"
"ify your ADT in Java in the form of an abstract class declaration, showing","chapter-1","Data Structures and Algorithms"
"the functions, their parameters, and their return types.","chapter-1","Data Structures and Algorithms"
"1.5 Briefly describe how integer variables are typically represented on a com-","chapter-1","Data Structures and Algorithms"
"puter. (Look up one’s complement and two’s complement arithmetic in an","chapter-1","Data Structures and Algorithms"
"introductory computer science textbook if you are not familiar with these.)","chapter-1","Data Structures and Algorithms"
"Why does this representation for integers qualify as a data structure as de-","chapter-1","Data Structures and Algorithms"
"fined in Section 1.2?","chapter-1","Data Structures and Algorithms"
"1.6 Define an ADT for a two-dimensional array of integers. Specify precisely","chapter-1","Data Structures and Algorithms"
"the basic operations that can be performed on such arrays. Next, imagine an","chapter-1","Data Structures and Algorithms"
"application that stores an array with 1000 rows and 1000 columns, where less","chapter-1","Data Structures and Algorithms"
"Sec. 1.6 Exercises 21","chapter-1","Data Structures and Algorithms"
"than 10,000 of the array values are non-zero. Describe two different imple-","chapter-1","Data Structures and Algorithms"
"mentations for such arrays that would be more space efficient than a standard","chapter-1","Data Structures and Algorithms"
"two-dimensional array implementation requiring one million positions.","chapter-1","Data Structures and Algorithms"
"1.7 Imagine that you have been assigned to implement a sorting program. The","chapter-1","Data Structures and Algorithms"
"goal is to make this program general purpose, in that you don’t want to define","chapter-1","Data Structures and Algorithms"
"in advance what record or key types are used. Describe ways to generalize","chapter-1","Data Structures and Algorithms"
"a simple sorting algorithm (such as insertion sort, or any other sort you are","chapter-1","Data Structures and Algorithms"
"familiar with) to support this generalization.","chapter-1","Data Structures and Algorithms"
"1.8 Imagine that you have been assigned to implement a simple sequential search","chapter-1","Data Structures and Algorithms"
"on an array. The problem is that you want the search to be as general as pos-","chapter-1","Data Structures and Algorithms"
"sible. This means that you need to support arbitrary record and key types.","chapter-1","Data Structures and Algorithms"
"Describe ways to generalize the search function to support this goal. Con-","chapter-1","Data Structures and Algorithms"
"sider the possibility that the function will be used multiple times in the same","chapter-1","Data Structures and Algorithms"
"program, on differing record types. Consider the possibility that the func-","chapter-1","Data Structures and Algorithms"
"tion will need to be used on different keys (possibly with the same or differ-","chapter-1","Data Structures and Algorithms"
"ent types) of the same record. For example, a student data record might be","chapter-1","Data Structures and Algorithms"
"searched by zip code, by name, by salary, or by GPA.","chapter-1","Data Structures and Algorithms"
"1.9 Does every problem have an algorithm?","chapter-1","Data Structures and Algorithms"
"1.10 Does every algorithm have a Java program?","chapter-1","Data Structures and Algorithms"
"1.11 Consider the design for a spelling checker program meant to run on a home","chapter-1","Data Structures and Algorithms"
"computer. The spelling checker should be able to handle quickly a document","chapter-1","Data Structures and Algorithms"
"of less than twenty pages. Assume that the spelling checker comes with a","chapter-1","Data Structures and Algorithms"
"dictionary of about 20,000 words. What primitive operations must be imple-","chapter-1","Data Structures and Algorithms"
"mented on the dictionary, and what is a reasonable time constraint for each","chapter-1","Data Structures and Algorithms"
"operation?","chapter-1","Data Structures and Algorithms"
"1.12 Imagine that you have been hired to design a database service containing","chapter-1","Data Structures and Algorithms"
"information about cities and towns in the United States, as described in Ex-","chapter-1","Data Structures and Algorithms"
"ample 1.2. Suggest two possible implementations for the database.","chapter-1","Data Structures and Algorithms"
"1.13 Imagine that you are given an array of records that is sorted with respect to","chapter-1","Data Structures and Algorithms"
"some key field contained in each record. Give two different algorithms for","chapter-1","Data Structures and Algorithms"
"searching the array to find the record with a specified key value. Which one","chapter-1","Data Structures and Algorithms"
"do you consider “better” and why?","chapter-1","Data Structures and Algorithms"
"1.14 How would you go about comparing two proposed algorithms for sorting an","chapter-1","Data Structures and Algorithms"
"array of integers? In particular,","chapter-1","Data Structures and Algorithms"
"(a) What would be appropriate measures of cost to use as a basis for com-","chapter-1","Data Structures and Algorithms"
"paring the two sorting algorithms?","chapter-1","Data Structures and Algorithms"
"(b) What tests or analysis would you conduct to determine how the two","chapter-1","Data Structures and Algorithms"
"algorithms perform under these cost measures?","chapter-1","Data Structures and Algorithms"
"1.15 A common problem for compilers and text editors is to determine if the","chapter-1","Data Structures and Algorithms"
"parentheses (or other brackets) in a string are balanced and properly nested.","chapter-1","Data Structures and Algorithms"
"22 Chap. 1 Data Structures and Algorithms","chapter-1","Data Structures and Algorithms"
"For example, the string “((())())()” contains properly nested pairs of paren-","chapter-1","Data Structures and Algorithms"
"theses, but the string “()(” does not; and the string “())” does not contain","chapter-1","Data Structures and Algorithms"
"properly matching parentheses.","chapter-1","Data Structures and Algorithms"
"(a) Give an algorithm that returns true if a string contains properly nested","chapter-1","Data Structures and Algorithms"
"and balanced parentheses, and false if otherwise. Hint: At no time","chapter-1","Data Structures and Algorithms"
"while scanning a legal string from left to right will you have encoun-","chapter-1","Data Structures and Algorithms"
"tered more right parentheses than left parentheses.","chapter-1","Data Structures and Algorithms"
"(b) Give an algorithm that returns the position in the string of the first of-","chapter-1","Data Structures and Algorithms"
"fending parenthesis if the string is not properly nested and balanced.","chapter-1","Data Structures and Algorithms"
"That is, if an excess right parenthesis is found, return its position; if","chapter-1","Data Structures and Algorithms"
"there are too many left parentheses, return the position of the first ex-","chapter-1","Data Structures and Algorithms"
"cess left parenthesis. Return −1 if the string is properly balanced and","chapter-1","Data Structures and Algorithms"
"nested.","chapter-1","Data Structures and Algorithms"
"1.16 A graph consists of a set of objects (called vertices) and a set of edges, where","chapter-1","Data Structures and Algorithms"
"each edge connects two vertices. Any given pair of vertices can be connected","chapter-1","Data Structures and Algorithms"
"by only one edge. Describe at least two different ways to represent the con-","chapter-1","Data Structures and Algorithms"
"nections defined by the vertices and edges of a graph.","chapter-1","Data Structures and Algorithms"
"1.17 Imagine that you are a shipping clerk for a large company. You have just","chapter-1","Data Structures and Algorithms"
"been handed about 1000 invoices, each of which is a single sheet of paper","chapter-1","Data Structures and Algorithms"
"with a large number in the upper right corner. The invoices must be sorted by","chapter-1","Data Structures and Algorithms"
"this number, in order from lowest to highest. Write down as many different","chapter-1","Data Structures and Algorithms"
"approaches to sorting the invoices as you can think of.","chapter-1","Data Structures and Algorithms"
"1.18 How would you sort an array of about 1000 integers from lowest value to","chapter-1","Data Structures and Algorithms"
"highest value? Write down at least five approaches to sorting the array. Do","chapter-1","Data Structures and Algorithms"
"not write algorithms in Java or pseudocode. Just write a sentence or two for","chapter-1","Data Structures and Algorithms"
"each approach to describe how it would work.","chapter-1","Data Structures and Algorithms"
"1.19 Think of an algorithm to find the maximum value in an (unsorted) array.","chapter-1","Data Structures and Algorithms"
"Now, think of an algorithm to find the second largest value in the array.","chapter-1","Data Structures and Algorithms"
"Which is harder to implement? Which takes more time to run (as measured","chapter-1","Data Structures and Algorithms"
"by the number of comparisons performed)? Now, think of an algorithm to","chapter-1","Data Structures and Algorithms"
"find the third largest value. Finally, think of an algorithm to find the middle","chapter-1","Data Structures and Algorithms"
"value. Which is the most difficult of these problems to solve?","chapter-1","Data Structures and Algorithms"
"1.20 An unsorted list allows for constant-time insert by adding a new element at","chapter-1","Data Structures and Algorithms"
"the end of the list. Unfortunately, searching for the element with key value X","chapter-1","Data Structures and Algorithms"
"requires a sequential search through the unsorted list until X is found, which","chapter-1","Data Structures and Algorithms"
"on average requires looking at half the list element. On the other hand, a","chapter-1","Data Structures and Algorithms"
"sorted array-based list of n elements can be searched in log n time with a","chapter-1","Data Structures and Algorithms"
"binary search. Unfortunately, inserting a new element requires a lot of time","chapter-1","Data Structures and Algorithms"
"because many elements might be shifted in the array if we want to keep it","chapter-1","Data Structures and Algorithms"
"sorted. How might data be organized to support both insertion and search in","chapter-1","Data Structures and Algorithms"
"log n time?","chapter-1","Data Structures and Algorithms"
"This chapter presents mathematical notation, background, and techniques used","chapter-2","Mathematical Preliminaries"
"throughout the book. This material is provided primarily for review and reference.","chapter-2","Mathematical Preliminaries"
"You might wish to return to the relevant sections when you encounter unfamiliar","chapter-2","Mathematical Preliminaries"
"notation or mathematical techniques in later chapters.","chapter-2","Mathematical Preliminaries"
"Section 2.7 on estimation might be unfamiliar to many readers. Estimation is","chapter-2","Mathematical Preliminaries"
"not a mathematical technique, but rather a general engineering skill. It is enor-","chapter-2","Mathematical Preliminaries"
"mously useful to computer scientists doing design work, because any proposed","chapter-2","Mathematical Preliminaries"
"solution whose estimated resource requirements fall well outside the problem’s re-","chapter-2","Mathematical Preliminaries"
"source constraints can be discarded immediately, allowing time for greater analysis","chapter-2","Mathematical Preliminaries"
"of more promising solutions.","chapter-2","Mathematical Preliminaries"
"2.1 Sets and Relations","chapter-2","Mathematical Preliminaries"
"The concept of a set in the mathematical sense has wide application in computer","chapter-2","Mathematical Preliminaries"
"science. The notations and techniques of set theory are commonly used when de-","chapter-2","Mathematical Preliminaries"
"scribing and implementing algorithms because the abstractions associated with sets","chapter-2","Mathematical Preliminaries"
"often help to clarify and simplify algorithm design.","chapter-2","Mathematical Preliminaries"
"A set is a collection of distinguishable members or elements. The members","chapter-2","Mathematical Preliminaries"
"are typically drawn from some larger population known as the base type. Each","chapter-2","Mathematical Preliminaries"
"member of a set is either a primitive element of the base type or is a set itself.","chapter-2","Mathematical Preliminaries"
"There is no concept of duplication in a set. Each value from the base type is either","chapter-2","Mathematical Preliminaries"
"in the set or not in the set. For example, a set named P might consist of the three","chapter-2","Mathematical Preliminaries"
"integers 7, 11, and 42. In this case, P’s members are 7, 11, and 42, and the base","chapter-2","Mathematical Preliminaries"
"type is integer.","chapter-2","Mathematical Preliminaries"
"Figure 2.1 shows the symbols commonly used to express sets and their rela-","chapter-2","Mathematical Preliminaries"
"tionships. Here are some examples of this notation in use. First define two sets, P","chapter-2","Mathematical Preliminaries"
"and Q.","chapter-2","Mathematical Preliminaries"
"P = {2, 3, 5}, Q = {5, 10}.","chapter-2","Mathematical Preliminaries"
"23","chapter-2","Mathematical Preliminaries"
"24 Chap. 2 Mathematical Preliminaries","chapter-2","Mathematical Preliminaries"
"{1, 4} A set composed of the members 1 and 4","chapter-2","Mathematical Preliminaries"
"{x | x is a positive integer} A set definition using a set former","chapter-2","Mathematical Preliminaries"
"Example: the set of all positive integers","chapter-2","Mathematical Preliminaries"
"x ∈ P x is a member of set P","chapter-2","Mathematical Preliminaries"
"x ∈/ P x is not a member of set P","chapter-2","Mathematical Preliminaries"
"∅ The null or empty set","chapter-2","Mathematical Preliminaries"
"|P| Cardinality: size of set P","chapter-2","Mathematical Preliminaries"
"or number of members for set P","chapter-2","Mathematical Preliminaries"
"P ⊆ Q, Q ⊇ P Set P is included in set Q,","chapter-2","Mathematical Preliminaries"
"set P is a subset of set Q,","chapter-2","Mathematical Preliminaries"
"set Q is a superset of set P","chapter-2","Mathematical Preliminaries"
"P ∪ Q Set Union:","chapter-2","Mathematical Preliminaries"
"all elements appearing in P OR Q","chapter-2","Mathematical Preliminaries"
"P ∩ Q Set Intersection:","chapter-2","Mathematical Preliminaries"
"all elements appearing in P AND Q","chapter-2","Mathematical Preliminaries"
"P − Q Set difference:","chapter-2","Mathematical Preliminaries"
"all elements of set P NOT in set Q","chapter-2","Mathematical Preliminaries"
"Figure 2.1 Set notation.","chapter-2","Mathematical Preliminaries"
"|P| = 3 (because P has three members) and |Q| = 2 (because Q has two members).","chapter-2","Mathematical Preliminaries"
"The union of P and Q, written P ∪ Q, is the set of elements in either P or Q, which","chapter-2","Mathematical Preliminaries"
"is {2, 3, 5, 10}. The intersection of P and Q, written P ∩ Q, is the set of elements","chapter-2","Mathematical Preliminaries"
"that appear in both P and Q, which is {5}. The set difference of P and Q, written","chapter-2","Mathematical Preliminaries"
"P − Q, is the set of elements that occur in P but not in Q, which is {2, 3}. Note","chapter-2","Mathematical Preliminaries"
"that P ∪ Q = Q ∪ P and that P ∩ Q = Q ∩ P, but in general P − Q 6= Q − P.","chapter-2","Mathematical Preliminaries"
"In this example, Q − P = {10}. Note that the set {4, 3, 5} is indistinguishable","chapter-2","Mathematical Preliminaries"
"from set P, because sets have no concept of order. Likewise, set {4, 3, 4, 5} is also","chapter-2","Mathematical Preliminaries"
"indistinguishable from P, because sets have no concept of duplicate elements.","chapter-2","Mathematical Preliminaries"
"The powerset of a set S is the set of all possible subsets for S. Consider the set","chapter-2","Mathematical Preliminaries"
"S = {a, b, c}. The powerset of S is","chapter-2","Mathematical Preliminaries"
"{∅, {a}, {b}, {c}, {a, b}, {a, c}, {b, c}, {a, b, c}}.","chapter-2","Mathematical Preliminaries"
"A collection of elements with no order (like a set), but with duplicate-valued el-","chapter-2","Mathematical Preliminaries"
"ements is called a bag.","chapter-2","Mathematical Preliminaries"
"1 To distinguish bags from sets, I use square brackets []","chapter-2","Mathematical Preliminaries"
"around a bag’s elements. For example, bag [3, 4, 5, 4] is distinct from bag [3, 4, 5],","chapter-2","Mathematical Preliminaries"
"while set {3, 4, 5, 4} is indistinguishable from set {3, 4, 5}. However, bag [3, 4, 5,","chapter-2","Mathematical Preliminaries"
"4] is indistinguishable from bag [3, 4, 4, 5].","chapter-2","Mathematical Preliminaries"
"1The object referred to here as a bag is sometimes called a multilist. But, I reserve the term","chapter-2","Mathematical Preliminaries"
"multilist for a list that may contain sublists (see Section 12.1).","chapter-2","Mathematical Preliminaries"
"Sec. 2.1 Sets and Relations 25","chapter-2","Mathematical Preliminaries"
"A sequence is a collection of elements with an order, and which may contain","chapter-2","Mathematical Preliminaries"
"duplicate-valued elements. A sequence is also sometimes called a tuple or a vec-","chapter-2","Mathematical Preliminaries"
"tor. In a sequence, there is a 0th element, a 1st element, 2nd element, and so","chapter-2","Mathematical Preliminaries"
"on. I indicate a sequence by using angle brackets hi to enclose its elements. For","chapter-2","Mathematical Preliminaries"
"example, h3, 4, 5, 4i is a sequence. Note that sequence h3, 5, 4, 4i is distinct from","chapter-2","Mathematical Preliminaries"
"sequence h3, 4, 5, 4i, and both are distinct from sequence h3, 4, 5i.","chapter-2","Mathematical Preliminaries"
"A relation R over set S is a set of ordered pairs from S. As an example of a","chapter-2","Mathematical Preliminaries"
"relation, if S is {a, b, c}, then","chapter-2","Mathematical Preliminaries"
"{ha, ci,hb, ci,hc, bi}","chapter-2","Mathematical Preliminaries"
"is a relation, and","chapter-2","Mathematical Preliminaries"
"{ha, ai,ha, ci,hb, bi,hb, ci,hc, ci}","chapter-2","Mathematical Preliminaries"
"is a different relation. If tuple hx, yi is in relation R, we may use the infix notation","chapter-2","Mathematical Preliminaries"
"xRy. We often use relations such as the less than operator (<) on the natural","chapter-2","Mathematical Preliminaries"
"numbers, which includes ordered pairs such as h1, 3i and h2, 23i, but not h3, 2i or","chapter-2","Mathematical Preliminaries"
"h2, 2i. Rather than writing the relationship in terms of ordered pairs, we typically","chapter-2","Mathematical Preliminaries"
"use an infix notation for such relations, writing 1 < 3.","chapter-2","Mathematical Preliminaries"
"Define the properties of relations as follows, with R a binary relation over set S.","chapter-2","Mathematical Preliminaries"
"• R is reflexive if aRa for all a ∈ S.","chapter-2","Mathematical Preliminaries"
"• R is symmetric if whenever aRb, then bRa, for all a, b ∈ S.","chapter-2","Mathematical Preliminaries"
"• R is antisymmetric if whenever aRb and bRa, then a = b, for all a, b ∈ S.","chapter-2","Mathematical Preliminaries"
"• R is transitive if whenever aRb and bRc, then aRc, for all a, b, c ∈ S.","chapter-2","Mathematical Preliminaries"
"As examples, for the natural numbers, < is antisymmetric (because there is","chapter-2","Mathematical Preliminaries"
"no case where aRb and bRa) and transitive; ≤ is reflexive, antisymmetric, and","chapter-2","Mathematical Preliminaries"
"transitive, and = is reflexive, symmetric (and antisymmetric!), and transitive. For","chapter-2","Mathematical Preliminaries"
"people, the relation “is a sibling of” is symmetric and transitive. If we define a","chapter-2","Mathematical Preliminaries"
"person to be a sibling of himself, then it is reflexive; if we define a person not to be","chapter-2","Mathematical Preliminaries"
"a sibling of himself, then it is not reflexive.","chapter-2","Mathematical Preliminaries"
"R is an equivalence relation on set S if it is reflexive, symmetric, and transitive.","chapter-2","Mathematical Preliminaries"
"An equivalence relation can be used to partition a set into equivalence classes. If","chapter-2","Mathematical Preliminaries"
"two elements a and b are equivalent to each other, we write a ≡ b. A partition of","chapter-2","Mathematical Preliminaries"
"a set S is a collection of subsets that are disjoint from each other and whose union","chapter-2","Mathematical Preliminaries"
"is S. An equivalence relation on set S partitions the set into subsets whose elements","chapter-2","Mathematical Preliminaries"
"are equivalent. See Section 6.2 for a discussion on how to represent equivalence","chapter-2","Mathematical Preliminaries"
"classes on a set. One application for disjoint sets appears in Section 11.5.2.","chapter-2","Mathematical Preliminaries"
"Example 2.1 For the integers, = is an equivalence relation that partitions","chapter-2","Mathematical Preliminaries"
"each element into a distinct subset. In other words, for any integer a, three","chapter-2","Mathematical Preliminaries"
"things are true.","chapter-2","Mathematical Preliminaries"
"1. a = a,","chapter-2","Mathematical Preliminaries"
"26 Chap. 2 Mathematical Preliminaries","chapter-2","Mathematical Preliminaries"
"2. if a = b then b = a, and","chapter-2","Mathematical Preliminaries"
"3. if a = b and b = c, then a = c.","chapter-2","Mathematical Preliminaries"
"Of course, for distinct integers a, b, and c there are never cases where","chapter-2","Mathematical Preliminaries"
"a = b, b = a, or b = c. So the claims that = is symmetric and transitive are","chapter-2","Mathematical Preliminaries"
"vacuously true (there are never examples in the relation where these events","chapter-2","Mathematical Preliminaries"
"occur). But because the requirements for symmetry and transitivity are not","chapter-2","Mathematical Preliminaries"
"violated, the relation is symmetric and transitive.","chapter-2","Mathematical Preliminaries"
"Example 2.2 If we clarify the definition of sibling to mean that a person","chapter-2","Mathematical Preliminaries"
"is a sibling of him- or herself, then the sibling relation is an equivalence","chapter-2","Mathematical Preliminaries"
"relation that partitions the set of people.","chapter-2","Mathematical Preliminaries"
"Example 2.3 We can use the modulus function (defined in the next sec-","chapter-2","Mathematical Preliminaries"
"tion) to define an equivalence relation. For the set of integers, use the mod-","chapter-2","Mathematical Preliminaries"
"ulus function to define a binary relation such that two numbers x and y are","chapter-2","Mathematical Preliminaries"
"in the relation if and only if x mod m = y mod m. Thus, for m = 4,","chapter-2","Mathematical Preliminaries"
"h1, 5i is in the relation because 1 mod 4 = 5 mod 4. We see that modulus","chapter-2","Mathematical Preliminaries"
"used in this way defines an equivalence relation on the integers, and this re-","chapter-2","Mathematical Preliminaries"
"lation can be used to partition the integers into m equivalence classes. This","chapter-2","Mathematical Preliminaries"
"relation is an equivalence relation because","chapter-2","Mathematical Preliminaries"
"1. x mod m = x mod m for all x;","chapter-2","Mathematical Preliminaries"
"2. if x mod m = y mod m, then y mod m = x mod m; and","chapter-2","Mathematical Preliminaries"
"3. if x mod m = y mod m and y mod m = z mod m, then x mod","chapter-2","Mathematical Preliminaries"
"m = z mod m.","chapter-2","Mathematical Preliminaries"
"A binary relation is called a partial order if it is antisymmetric and transitive.2","chapter-2","Mathematical Preliminaries"
"The set on which the partial order is defined is called a partially ordered set or a","chapter-2","Mathematical Preliminaries"
"poset. Elements x and y of a set are comparable under a given relation if either","chapter-2","Mathematical Preliminaries"
"xRy or yRx. If every pair of distinct elements in a partial order are comparable,","chapter-2","Mathematical Preliminaries"
"then the order is called a total order or linear order.","chapter-2","Mathematical Preliminaries"
"Example 2.4 For the integers, relations < and ≤ define partial orders.","chapter-2","Mathematical Preliminaries"
"Operation < is a total order because, for every pair of integers x and y such","chapter-2","Mathematical Preliminaries"
"that x 6= y, either x < y or y < x. Likewise, ≤ is a total order because, for","chapter-2","Mathematical Preliminaries"
"every pair of integers x and y such that x 6= y, either x ≤ y or y ≤ x.","chapter-2","Mathematical Preliminaries"
"2Not all authors use this definition for partial order. I have seen at least three significantly different","chapter-2","Mathematical Preliminaries"
"definitions in the literature. I have selected the one that lets < and ≤ both define partial orders on the","chapter-2","Mathematical Preliminaries"
"integers, because this seems the most natural to me.","chapter-2","Mathematical Preliminaries"
"Sec. 2.2 Miscellaneous Notation 27","chapter-2","Mathematical Preliminaries"
"Example 2.5 For the powerset of the integers, the subset operator defines","chapter-2","Mathematical Preliminaries"
"a partial order (because it is antisymmetric and transitive). For example,","chapter-2","Mathematical Preliminaries"
"{1, 2} ⊆ {1, 2, 3}. However, sets {1, 2} and {1, 3} are not comparable by","chapter-2","Mathematical Preliminaries"
"the subset operator, because neither is a subset of the other. Therefore, the","chapter-2","Mathematical Preliminaries"
"subset operator does not define a total order on the powerset of the integers.","chapter-2","Mathematical Preliminaries"
"2.2 Miscellaneous Notation","chapter-2","Mathematical Preliminaries"
"Units of measure: I use the following notation for units of measure. “B” will","chapter-2","Mathematical Preliminaries"
"be used as an abbreviation for bytes, “b” for bits, “KB” for kilobytes (2","chapter-2","Mathematical Preliminaries"
"10 =","chapter-2","Mathematical Preliminaries"
"1024 bytes), “MB” for megabytes (2","chapter-2","Mathematical Preliminaries"
"20 bytes), “GB” for gigabytes (2","chapter-2","Mathematical Preliminaries"
"30 bytes), and","chapter-2","Mathematical Preliminaries"
"“ms” for milliseconds (a millisecond is 1","chapter-2","Mathematical Preliminaries"
"1000 of a second). Spaces are not placed be-","chapter-2","Mathematical Preliminaries"
"tween the number and the unit abbreviation when a power of two is intended. Thus","chapter-2","Mathematical Preliminaries"
"a disk drive of size 25 gigabytes (where a gigabyte is intended as 2","chapter-2","Mathematical Preliminaries"
"30 bytes) will be","chapter-2","Mathematical Preliminaries"
"written as “25GB.” Spaces are used when a decimal value is intended. An amount","chapter-2","Mathematical Preliminaries"
"of 2000 bits would therefore be written “2 Kb” while “2Kb” represents 2048 bits.","chapter-2","Mathematical Preliminaries"
"2000 milliseconds is written as 2000 ms. Note that in this book large amounts of","chapter-2","Mathematical Preliminaries"
"storage are nearly always measured in powers of two and times in powers of ten.","chapter-2","Mathematical Preliminaries"
"Factorial function: The factorial function, written n! for n an integer greater","chapter-2","Mathematical Preliminaries"
"than 0, is the product of the integers between 1 and n, inclusive. Thus, 5! =","chapter-2","Mathematical Preliminaries"
"1 · 2 · 3 · 4 · 5 = 120. As a special case, 0! = 1. The factorial function grows","chapter-2","Mathematical Preliminaries"
"quickly as n becomes larger. Because computing the factorial function directly","chapter-2","Mathematical Preliminaries"
"is a time-consuming process, it can be useful to have an equation that provides a","chapter-2","Mathematical Preliminaries"
"good approximation. Stirling’s approximation states that n! ≈","chapter-2","Mathematical Preliminaries"
"√","chapter-2","Mathematical Preliminaries"
"2πn(","chapter-2","Mathematical Preliminaries"
"n","chapter-2","Mathematical Preliminaries"
"e","chapter-2","Mathematical Preliminaries"
")","chapter-2","Mathematical Preliminaries"
"n","chapter-2","Mathematical Preliminaries"
", where","chapter-2","Mathematical Preliminaries"
"e ≈ 2.71828 (e is the base for the system of natural logarithms).3 Thus we see that","chapter-2","Mathematical Preliminaries"
"while n! grows slower than n","chapter-2","Mathematical Preliminaries"
"n","chapter-2","Mathematical Preliminaries"
"(because √","chapter-2","Mathematical Preliminaries"
"2πn/en < 1), it grows faster than c","chapter-2","Mathematical Preliminaries"
"n","chapter-2","Mathematical Preliminaries"
"for","chapter-2","Mathematical Preliminaries"
"any positive integer constant c.","chapter-2","Mathematical Preliminaries"
"Permutations: A permutation of a sequence S is simply the members of S ar-","chapter-2","Mathematical Preliminaries"
"ranged in some order. For example, a permutation of the integers 1 through n","chapter-2","Mathematical Preliminaries"
"would be those values arranged in some order. If the sequence contains n distinct","chapter-2","Mathematical Preliminaries"
"members, then there are n! different permutations for the sequence. This is because","chapter-2","Mathematical Preliminaries"
"there are n choices for the first member in the permutation; for each choice of first","chapter-2","Mathematical Preliminaries"
"member there are n − 1 choices for the second member, and so on. Sometimes","chapter-2","Mathematical Preliminaries"
"one would like to obtain a random permutation for a sequence, that is, one of the","chapter-2","Mathematical Preliminaries"
"n! possible permutations is selected in such a way that each permutation has equal","chapter-2","Mathematical Preliminaries"
"probability of being selected. A simple Java function for generating a random per-","chapter-2","Mathematical Preliminaries"
"mutation is as follows. Here, the n values of the sequence are stored in positions 0","chapter-2","Mathematical Preliminaries"
"3 The symbol “≈” means “approximately equal.”","chapter-2","Mathematical Preliminaries"
"28 Chap. 2 Mathematical Preliminaries","chapter-2","Mathematical Preliminaries"
"through n − 1 of array A, function swap(A, i, j) exchanges elements i and","chapter-2","Mathematical Preliminaries"
"j in array A, and Random(n) returns an integer value in the range 0 to n − 1 (see","chapter-2","Mathematical Preliminaries"
"the Appendix for more information on swap and Random).","chapter-2","Mathematical Preliminaries"
"/** Randomly permute the values in array A */","chapter-2","Mathematical Preliminaries"
"static <E> void permute(E[] A) {","chapter-2","Mathematical Preliminaries"
"for (int i = A.length; i > 0; i--) // for each i","chapter-2","Mathematical Preliminaries"
"swap(A, i-1, DSutil.random(i)); // swap A[i-1] with","chapter-2","Mathematical Preliminaries"
"} // a random element","chapter-2","Mathematical Preliminaries"
"Boolean variables: A Boolean variable is a variable (of type boolean in Java)","chapter-2","Mathematical Preliminaries"
"that takes on one of the two values true and false. These two values are often","chapter-2","Mathematical Preliminaries"
"associated with the values 1 and 0, respectively, although there is no reason why","chapter-2","Mathematical Preliminaries"
"this needs to be the case. It is poor programming practice to rely on the corre-","chapter-2","Mathematical Preliminaries"
"spondence between 0 and false, because these are logically distinct objects of","chapter-2","Mathematical Preliminaries"
"different types.","chapter-2","Mathematical Preliminaries"
"Logic Notation: We will occasionally make use of the notation of symbolic or","chapter-2","Mathematical Preliminaries"
"Boolean logic. A ⇒ B means “A implies B” or “If A then B.” A ⇔ B means “A","chapter-2","Mathematical Preliminaries"
"if and only if B” or “A is equivalent to B.” A ∨ B means “A or B” (useful both in","chapter-2","Mathematical Preliminaries"
"the context of symbolic logic or when performing a Boolean operation). A ∧ B","chapter-2","Mathematical Preliminaries"
"means “A and B.” ∼A and A both mean “not A” or the negation of A where A is a","chapter-2","Mathematical Preliminaries"
"Boolean variable.","chapter-2","Mathematical Preliminaries"
"Floor and ceiling: The floor of x (written bxc) takes real value x and returns the","chapter-2","Mathematical Preliminaries"
"greatest integer ≤ x. For example, b3.4c = 3, as does b3.0c, while b−3.4c = −4","chapter-2","Mathematical Preliminaries"
"and b−3.0c = −3. The ceiling of x (written dxe) takes real value x and returns","chapter-2","Mathematical Preliminaries"
"the least integer ≥ x. For example, d3.4e = 4, as does d4.0e, while d−3.4e =","chapter-2","Mathematical Preliminaries"
"d−3.0e = −3.","chapter-2","Mathematical Preliminaries"
"Modulus operator: The modulus (or mod) function returns the remainder of an","chapter-2","Mathematical Preliminaries"
"integer division. Sometimes written n mod m in mathematical expressions, the","chapter-2","Mathematical Preliminaries"
"syntax for the Java modulus operator is n % m. From the definition of remainder,","chapter-2","Mathematical Preliminaries"
"n mod m is the integer r such that n = qm + r for q an integer, and |r| < |m|.","chapter-2","Mathematical Preliminaries"
"Therefore, the result of n mod m must be between 0 and m − 1 when n and m are","chapter-2","Mathematical Preliminaries"
"positive integers. For example, 5 mod 3 = 2; 25 mod 3 = 1, 5 mod 7 = 5, and","chapter-2","Mathematical Preliminaries"
"5 mod 5 = 0.","chapter-2","Mathematical Preliminaries"
"There is more than one way to assign values to q and r, depending on how in-","chapter-2","Mathematical Preliminaries"
"teger division is interpreted. The most common mathematical definition computes","chapter-2","Mathematical Preliminaries"
"the mod function as n mod m = n − mbn/mc. In this case, −3 mod 5 = 2.","chapter-2","Mathematical Preliminaries"
"However, Java and C++ compilers typically use the underlying processor’s ma-","chapter-2","Mathematical Preliminaries"
"chine instruction for computing integer arithmetic. On many computers this is done","chapter-2","Mathematical Preliminaries"
"by truncating the resulting fraction, meaning n mod m = n − m(trunc(n/m)).","chapter-2","Mathematical Preliminaries"
"Under this definition, −3 mod 5 = −3.","chapter-2","Mathematical Preliminaries"
"Sec. 2.3 Logarithms 29","chapter-2","Mathematical Preliminaries"
"Unfortunately, for many applications this is not what the user wants or expects.","chapter-2","Mathematical Preliminaries"
"For example, many hash systems will perform some computation on a record’s key","chapter-2","Mathematical Preliminaries"
"value and then take the result modulo the hash table size. The expectation here","chapter-2","Mathematical Preliminaries"
"would be that the result is a legal index into the hash table, not a negative number.","chapter-2","Mathematical Preliminaries"
"Implementers of hash functions must either insure that the result of the computation","chapter-2","Mathematical Preliminaries"
"is always positive, or else add the hash table size to the result of the modulo function","chapter-2","Mathematical Preliminaries"
"when that result is negative.","chapter-2","Mathematical Preliminaries"
"2.3 Logarithms","chapter-2","Mathematical Preliminaries"
"A logarithm of base b for value y is the power to which b is raised to get y. Nor-","chapter-2","Mathematical Preliminaries"
"mally, this is written as logb y = x. Thus, if logb y = x then b","chapter-2","Mathematical Preliminaries"
"x = y, and b","chapter-2","Mathematical Preliminaries"
"logby = y.","chapter-2","Mathematical Preliminaries"
"Logarithms are used frequently by programmers. Here are two typical uses.","chapter-2","Mathematical Preliminaries"
"Example 2.6 Many programs require an encoding for a collection of ob-","chapter-2","Mathematical Preliminaries"
"jects. What is the minimum number of bits needed to represent n distinct","chapter-2","Mathematical Preliminaries"
"code values? The answer is dlog2 ne bits. For example, if you have 1000","chapter-2","Mathematical Preliminaries"
"codes to store, you will require at least dlog2 1000e = 10 bits to have 1000","chapter-2","Mathematical Preliminaries"
"different codes (10 bits provide 1024 distinct code values).","chapter-2","Mathematical Preliminaries"
"Example 2.7 Consider the binary search algorithm for finding a given","chapter-2","Mathematical Preliminaries"
"value within an array sorted by value from lowest to highest. Binary search","chapter-2","Mathematical Preliminaries"
"first looks at the middle element and determines if the value being searched","chapter-2","Mathematical Preliminaries"
"for is in the upper half or the lower half of the array. The algorithm then","chapter-2","Mathematical Preliminaries"
"continues splitting the appropriate subarray in half until the desired value","chapter-2","Mathematical Preliminaries"
"is found. (Binary search is described in more detail in Section 3.5.) How","chapter-2","Mathematical Preliminaries"
"many times can an array of size n be split in half until only one element","chapter-2","Mathematical Preliminaries"
"remains in the final subarray? The answer is dlog2 ne times.","chapter-2","Mathematical Preliminaries"
"In this book, nearly all logarithms used have a base of two. This is because","chapter-2","Mathematical Preliminaries"
"data structures and algorithms most often divide things in half, or store codes with","chapter-2","Mathematical Preliminaries"
"binary bits. Whenever you see the notation log n in this book, either log2 n is meant","chapter-2","Mathematical Preliminaries"
"or else the term is being used asymptotically and so the actual base does not matter.","chapter-2","Mathematical Preliminaries"
"Logarithms using any base other than two will show the base explicitly.","chapter-2","Mathematical Preliminaries"
"Logarithms have the following properties, for any positive values of m, n, and","chapter-2","Mathematical Preliminaries"
"r, and any positive integers a and b.","chapter-2","Mathematical Preliminaries"
"1. log(nm) = log n + log m.","chapter-2","Mathematical Preliminaries"
"2. log(n/m) = log n − log m.","chapter-2","Mathematical Preliminaries"
"3. log(n","chapter-2","Mathematical Preliminaries"
"r","chapter-2","Mathematical Preliminaries"
") = r log n.","chapter-2","Mathematical Preliminaries"
"4. loga n = logb n/ logb a.","chapter-2","Mathematical Preliminaries"
"30 Chap. 2 Mathematical Preliminaries","chapter-2","Mathematical Preliminaries"
"The first two properties state that the logarithm of two numbers multiplied (or","chapter-2","Mathematical Preliminaries"
"divided) can be found by adding (or subtracting) the logarithms of the two num-","chapter-2","Mathematical Preliminaries"
"bers.4 Property (3) is simply an extension of property (1). Property (4) tells us that,","chapter-2","Mathematical Preliminaries"
"for variable n and any two integer constants a and b, loga n and logb n differ by","chapter-2","Mathematical Preliminaries"
"the constant factor logb a, regardless of the value of n. Most runtime analyses in","chapter-2","Mathematical Preliminaries"
"this book are of a type that ignores constant factors in costs. Property (4) says that","chapter-2","Mathematical Preliminaries"
"such analyses need not be concerned with the base of the logarithm, because this","chapter-2","Mathematical Preliminaries"
"can change the total cost only by a constant factor. Note that 2","chapter-2","Mathematical Preliminaries"
"log n = n.","chapter-2","Mathematical Preliminaries"
"When discussing logarithms, exponents often lead to confusion. Property (3)","chapter-2","Mathematical Preliminaries"
"tells us that log n","chapter-2","Mathematical Preliminaries"
"2 = 2 log n. How do we indicate the square of the logarithm","chapter-2","Mathematical Preliminaries"
"(as opposed to the logarithm of n","chapter-2","Mathematical Preliminaries"
"2","chapter-2","Mathematical Preliminaries"
")? This could be written as (log n)","chapter-2","Mathematical Preliminaries"
"2","chapter-2","Mathematical Preliminaries"
", but it is","chapter-2","Mathematical Preliminaries"
"traditional to use log2 n. On the other hand, we might want to take the logarithm of","chapter-2","Mathematical Preliminaries"
"the logarithm of n. This is written log log n.","chapter-2","Mathematical Preliminaries"
"A special notation is used in the rare case when we need to know how many","chapter-2","Mathematical Preliminaries"
"times we must take the log of a number before we reach a value ≤ 1. This quantity","chapter-2","Mathematical Preliminaries"
"is written log∗ n. For example, log∗","chapter-2","Mathematical Preliminaries"
"1024 = 4 because log 1024 = 10, log 10 ≈","chapter-2","Mathematical Preliminaries"
"3.33, log 3.33 ≈ 1.74, and log 1.74 < 1, which is a total of 4 log operations.","chapter-2","Mathematical Preliminaries"
"2.4 Summations and Recurrences","chapter-2","Mathematical Preliminaries"
"Most programs contain loop constructs. When analyzing running time costs for","chapter-2","Mathematical Preliminaries"
"programs with loops, we need to add up the costs for each time the loop is executed.","chapter-2","Mathematical Preliminaries"
"This is an example of a summation. Summations are simply the sum of costs for","chapter-2","Mathematical Preliminaries"
"some function applied to a range of parameter values. Summations are typically","chapter-2","Mathematical Preliminaries"
"written with the following “Sigma” notation:","chapter-2","Mathematical Preliminaries"
"Xn","chapter-2","Mathematical Preliminaries"
"i=1","chapter-2","Mathematical Preliminaries"
"f(i).","chapter-2","Mathematical Preliminaries"
"This notation indicates that we are summing the value of f(i) over some range of","chapter-2","Mathematical Preliminaries"
"(integer) values. The parameter to the expression and its initial value are indicated","chapter-2","Mathematical Preliminaries"
"below the P symbol. Here, the notation i = 1 indicates that the parameter is i and","chapter-2","Mathematical Preliminaries"
"that it begins with the value 1. At the top of the P symbol is the expression n. This","chapter-2","Mathematical Preliminaries"
"indicates the maximum value for the parameter i. Thus, this notation means to sum","chapter-2","Mathematical Preliminaries"
"the values of f(i) as i ranges across the integers from 1 through n. This can also be","chapter-2","Mathematical Preliminaries"
"4 These properties are the idea behind the slide rule. Adding two numbers can be viewed as","chapter-2","Mathematical Preliminaries"
"joining two lengths together and measuring their combined length. Multiplication is not so easily","chapter-2","Mathematical Preliminaries"
"done. However, if the numbers are first converted to the lengths of their logarithms, then those lengths","chapter-2","Mathematical Preliminaries"
"can be added and the inverse logarithm of the resulting length gives the answer for the multiplication","chapter-2","Mathematical Preliminaries"
"(this is simply logarithm property (1)). A slide rule measures the length of the logarithm for the","chapter-2","Mathematical Preliminaries"
"numbers, lets you slide bars representing these lengths to add up the total length, and finally converts","chapter-2","Mathematical Preliminaries"
"this total length to the correct numeric answer by taking the inverse of the logarithm for the result.","chapter-2","Mathematical Preliminaries"
"Sec. 2.4 Summations and Recurrences 31","chapter-2","Mathematical Preliminaries"
"written f(1) + f(2) + · · · + f(n − 1) + f(n). Within a sentence, Sigma notation","chapter-2","Mathematical Preliminaries"
"is typeset as Pn","chapter-2","Mathematical Preliminaries"
"i=1 f(i).","chapter-2","Mathematical Preliminaries"
"Given a summation, you often wish to replace it with an algebraic equation","chapter-2","Mathematical Preliminaries"
"with the same value as the summation. This is known as a closed-form solution,","chapter-2","Mathematical Preliminaries"
"and the process of replacing the summation with its closed-form solution is known","chapter-2","Mathematical Preliminaries"
"as solving the summation. For example, the summation Pn","chapter-2","Mathematical Preliminaries"
"i=1 1 is simply the ex-","chapter-2","Mathematical Preliminaries"
"pression “1” summed n times (remember that i ranges from 1 to n). Because the","chapter-2","Mathematical Preliminaries"
"sum of n 1s is n, the closed-form solution is n. The following is a list of useful","chapter-2","Mathematical Preliminaries"
"summations, along with their closed-form solutions.","chapter-2","Mathematical Preliminaries"
"Xn","chapter-2","Mathematical Preliminaries"
"i=1","chapter-2","Mathematical Preliminaries"
"i =","chapter-2","Mathematical Preliminaries"
"n(n + 1)","chapter-2","Mathematical Preliminaries"
"2","chapter-2","Mathematical Preliminaries"
". (2.1)","chapter-2","Mathematical Preliminaries"
"Xn","chapter-2","Mathematical Preliminaries"
"i=1","chapter-2","Mathematical Preliminaries"
"i","chapter-2","Mathematical Preliminaries"
"2 =","chapter-2","Mathematical Preliminaries"
"2n","chapter-2","Mathematical Preliminaries"
"3 + 3n","chapter-2","Mathematical Preliminaries"
"2 + n","chapter-2","Mathematical Preliminaries"
"6","chapter-2","Mathematical Preliminaries"
"=","chapter-2","Mathematical Preliminaries"
"n(2n + 1)(n + 1)","chapter-2","Mathematical Preliminaries"
"6","chapter-2","Mathematical Preliminaries"
". (2.2)","chapter-2","Mathematical Preliminaries"
"log","chapter-2","Mathematical Preliminaries"
"Xn","chapter-2","Mathematical Preliminaries"
"i=1","chapter-2","Mathematical Preliminaries"
"n = n log n. (2.3)","chapter-2","Mathematical Preliminaries"
"X∞","chapter-2","Mathematical Preliminaries"
"i=0","chapter-2","Mathematical Preliminaries"
"a","chapter-2","Mathematical Preliminaries"
"i =","chapter-2","Mathematical Preliminaries"
"1","chapter-2","Mathematical Preliminaries"
"1 − a","chapter-2","Mathematical Preliminaries"
"for 0 < a < 1. (2.4)","chapter-2","Mathematical Preliminaries"
"Xn","chapter-2","Mathematical Preliminaries"
"i=0","chapter-2","Mathematical Preliminaries"
"a","chapter-2","Mathematical Preliminaries"
"i =","chapter-2","Mathematical Preliminaries"
"a","chapter-2","Mathematical Preliminaries"
"n+1 − 1","chapter-2","Mathematical Preliminaries"
"a − 1","chapter-2","Mathematical Preliminaries"
"for a 6= 1. (2.5)","chapter-2","Mathematical Preliminaries"
"As special cases to Equation 2.5,","chapter-2","Mathematical Preliminaries"
"Xn","chapter-2","Mathematical Preliminaries"
"i=1","chapter-2","Mathematical Preliminaries"
"1","chapter-2","Mathematical Preliminaries"
"2","chapter-2","Mathematical Preliminaries"
"i","chapter-2","Mathematical Preliminaries"
"= 1 −","chapter-2","Mathematical Preliminaries"
"1","chapter-2","Mathematical Preliminaries"
"2","chapter-2","Mathematical Preliminaries"
"n","chapter-2","Mathematical Preliminaries"
", (2.6)","chapter-2","Mathematical Preliminaries"
"and","chapter-2","Mathematical Preliminaries"
"Xn","chapter-2","Mathematical Preliminaries"
"i=0","chapter-2","Mathematical Preliminaries"
"2","chapter-2","Mathematical Preliminaries"
"i = 2n+1 − 1. (2.7)","chapter-2","Mathematical Preliminaries"
"As a corollary to Equation 2.7,","chapter-2","Mathematical Preliminaries"
"log","chapter-2","Mathematical Preliminaries"
"Xn","chapter-2","Mathematical Preliminaries"
"i=0","chapter-2","Mathematical Preliminaries"
"2","chapter-2","Mathematical Preliminaries"
"i = 2log n+1 − 1 = 2n − 1. (2.8)","chapter-2","Mathematical Preliminaries"
"Finally,","chapter-2","Mathematical Preliminaries"
"Xn","chapter-2","Mathematical Preliminaries"
"i=1","chapter-2","Mathematical Preliminaries"
"i","chapter-2","Mathematical Preliminaries"
"2","chapter-2","Mathematical Preliminaries"
"i","chapter-2","Mathematical Preliminaries"
"= 2 −","chapter-2","Mathematical Preliminaries"
"n + 2","chapter-2","Mathematical Preliminaries"
"2","chapter-2","Mathematical Preliminaries"
"n","chapter-2","Mathematical Preliminaries"
". (2.9)","chapter-2","Mathematical Preliminaries"
"The sum of reciprocals from 1 to n, called the Harmonic Series and written","chapter-2","Mathematical Preliminaries"
"Hn, has a value between loge n and loge n+ 1. To be more precise, as n grows, the","chapter-2","Mathematical Preliminaries"
"32 Chap. 2 Mathematical Preliminaries","chapter-2","Mathematical Preliminaries"
"summation grows closer to","chapter-2","Mathematical Preliminaries"
"Hn ≈ loge n + γ +","chapter-2","Mathematical Preliminaries"
"1","chapter-2","Mathematical Preliminaries"
"2n","chapter-2","Mathematical Preliminaries"
", (2.10)","chapter-2","Mathematical Preliminaries"
"where γ is Euler’s constant and has the value 0.5772...","chapter-2","Mathematical Preliminaries"
"Most of these equalities can be proved easily by mathematical induction (see","chapter-2","Mathematical Preliminaries"
"Section 2.6.3). Unfortunately, induction does not help us derive a closed-form solu-","chapter-2","Mathematical Preliminaries"
"tion. It only confirms when a proposed closed-form solution is correct. Techniques","chapter-2","Mathematical Preliminaries"
"for deriving closed-form solutions are discussed in Section 14.1.","chapter-2","Mathematical Preliminaries"
"The running time for a recursive algorithm is most easily expressed by a recur-","chapter-2","Mathematical Preliminaries"
"sive expression because the total time for the recursive algorithm includes the time","chapter-2","Mathematical Preliminaries"
"to run the recursive call(s). A recurrence relation defines a function by means","chapter-2","Mathematical Preliminaries"
"of an expression that includes one or more (smaller) instances of itself. A classic","chapter-2","Mathematical Preliminaries"
"example is the recursive definition for the factorial function:","chapter-2","Mathematical Preliminaries"
"n! = (n − 1)! · n for n > 1; 1! = 0! = 1.","chapter-2","Mathematical Preliminaries"
"Another standard example of a recurrence is the Fibonacci sequence:","chapter-2","Mathematical Preliminaries"
"Fib(n) = Fib(n − 1) + Fib(n − 2) for n > 2; Fib(1) = Fib(2) = 1.","chapter-2","Mathematical Preliminaries"
"From this definition, the first seven numbers of the Fibonacci sequence are","chapter-2","Mathematical Preliminaries"
"1, 1, 2, 3, 5, 8, and 13.","chapter-2","Mathematical Preliminaries"
"Notice that this definition contains two parts: the general definition for Fib(n) and","chapter-2","Mathematical Preliminaries"
"the base cases for Fib(1) and Fib(2). Likewise, the definition for factorial contains","chapter-2","Mathematical Preliminaries"
"a recursive part and base cases.","chapter-2","Mathematical Preliminaries"
"Recurrence relations are often used to model the cost of recursive functions. For","chapter-2","Mathematical Preliminaries"
"example, the number of multiplications required by function fact of Section 2.5","chapter-2","Mathematical Preliminaries"
"for an input of size n will be zero when n = 0 or n = 1 (the base cases), and it will","chapter-2","Mathematical Preliminaries"
"be one plus the cost of calling fact on a value of n − 1. This can be defined using","chapter-2","Mathematical Preliminaries"
"the following recurrence:","chapter-2","Mathematical Preliminaries"
"T(n) = T(n − 1) + 1 for n > 1; T(0) = T(1) = 0.","chapter-2","Mathematical Preliminaries"
"As with summations, we typically wish to replace the recurrence relation with","chapter-2","Mathematical Preliminaries"
"a closed-form solution. One approach is to expand the recurrence by replacing any","chapter-2","Mathematical Preliminaries"
"occurrences of T on the right-hand side with its definition.","chapter-2","Mathematical Preliminaries"
"Example 2.8 If we expand the recurrence T(n) = T(n − 1) + 1, we get","chapter-2","Mathematical Preliminaries"
"T(n) = T(n − 1) + 1","chapter-2","Mathematical Preliminaries"
"= (T(n − 2) + 1) + 1.","chapter-2","Mathematical Preliminaries"
"Sec. 2.4 Summations and Recurrences 33","chapter-2","Mathematical Preliminaries"
"We can expand the recurrence as many steps as we like, but the goal is","chapter-2","Mathematical Preliminaries"
"to detect some pattern that will permit us to rewrite the recurrence in terms","chapter-2","Mathematical Preliminaries"
"of a summation. In this example, we might notice that","chapter-2","Mathematical Preliminaries"
"(T(n − 2) + 1) + 1 = T(n − 2) + 2","chapter-2","Mathematical Preliminaries"
"and if we expand the recurrence again, we get","chapter-2","Mathematical Preliminaries"
"T(n) = T(n − 2) + 2 = T(n − 3) + 1 + 2 = T(n − 3) + 3","chapter-2","Mathematical Preliminaries"
"which generalizes to the pattern T(n) = T(n − i) + i. We might conclude","chapter-2","Mathematical Preliminaries"
"that","chapter-2","Mathematical Preliminaries"
"T(n) = T(n − (n − 1)) + (n − 1)","chapter-2","Mathematical Preliminaries"
"= T(1) + n − 1","chapter-2","Mathematical Preliminaries"
"= n − 1.","chapter-2","Mathematical Preliminaries"
"Because we have merely guessed at a pattern and not actually proved","chapter-2","Mathematical Preliminaries"
"that this is the correct closed form solution, we should use an induction","chapter-2","Mathematical Preliminaries"
"proof to complete the process (see Example 2.13).","chapter-2","Mathematical Preliminaries"
"Example 2.9 A slightly more complicated recurrence is","chapter-2","Mathematical Preliminaries"
"T(n) = T(n − 1) + n; T(1) = 1.","chapter-2","Mathematical Preliminaries"
"Expanding this recurrence a few steps, we get","chapter-2","Mathematical Preliminaries"
"T(n) = T(n − 1) + n","chapter-2","Mathematical Preliminaries"
"= T(n − 2) + (n − 1) + n","chapter-2","Mathematical Preliminaries"
"= T(n − 3) + (n − 2) + (n − 1) + n.","chapter-2","Mathematical Preliminaries"
"We should then observe that this recurrence appears to have a pattern that","chapter-2","Mathematical Preliminaries"
"leads to","chapter-2","Mathematical Preliminaries"
"T(n) = T(n − (n − 1)) + (n − (n − 2)) + · · · + (n − 1) + n","chapter-2","Mathematical Preliminaries"
"= 1 + 2 + · · · + (n − 1) + n.","chapter-2","Mathematical Preliminaries"
"This is equivalent to the summation Pn","chapter-2","Mathematical Preliminaries"
"i=1 i, for which we already know the","chapter-2","Mathematical Preliminaries"
"closed-form solution.","chapter-2","Mathematical Preliminaries"
"Techniques to find closed-form solutions for recurrence relations are discussed","chapter-2","Mathematical Preliminaries"
"in Section 14.2. Prior to Chapter 14, recurrence relations are used infrequently in","chapter-2","Mathematical Preliminaries"
"this book, and the corresponding closed-form solution and an explanation for how","chapter-2","Mathematical Preliminaries"
"it was derived will be supplied at the time of use.","chapter-2","Mathematical Preliminaries"
"34 Chap. 2 Mathematical Preliminaries","chapter-2","Mathematical Preliminaries"
"2.5 Recursion","chapter-2","Mathematical Preliminaries"
"An algorithm is recursive if it calls itself to do part of its work. For this approach","chapter-2","Mathematical Preliminaries"
"to be successful, the “call to itself” must be on a smaller problem then the one","chapter-2","Mathematical Preliminaries"
"originally attempted. In general, a recursive algorithm must have two parts: the","chapter-2","Mathematical Preliminaries"
"base case, which handles a simple input that can be solved without resorting to","chapter-2","Mathematical Preliminaries"
"a recursive call, and the recursive part which contains one or more recursive calls","chapter-2","Mathematical Preliminaries"
"to the algorithm where the parameters are in some sense “closer” to the base case","chapter-2","Mathematical Preliminaries"
"than those of the original call. Here is a recursive Java function to compute the","chapter-2","Mathematical Preliminaries"
"factorial of n. A trace of fact’s execution for a small value of n is presented in","chapter-2","Mathematical Preliminaries"
"Section 4.2.4.","chapter-2","Mathematical Preliminaries"
"/** Recursively compute and return n! */","chapter-2","Mathematical Preliminaries"
"static long fact(int n) {","chapter-2","Mathematical Preliminaries"
"// fact(20) is the largest value that fits in a long","chapter-2","Mathematical Preliminaries"
"assert (n >= 0) && (n <= 20) : "n out of range";","chapter-2","Mathematical Preliminaries"
"if (n <= 1) return 1; // Base case: return base solution","chapter-2","Mathematical Preliminaries"
"return n * fact(n-1); // Recursive call for n > 1","chapter-2","Mathematical Preliminaries"
"}","chapter-2","Mathematical Preliminaries"
"The first two lines of the function constitute the base cases. If n ≤ 1, then one","chapter-2","Mathematical Preliminaries"
"of the base cases computes a solution for the problem. If n > 1, then fact calls","chapter-2","Mathematical Preliminaries"
"a function that knows how to find the factorial of n − 1. Of course, the function","chapter-2","Mathematical Preliminaries"
"that knows how to compute the factorial of n − 1 happens to be fact itself. But","chapter-2","Mathematical Preliminaries"
"we should not think too hard about this while writing the algorithm. The design","chapter-2","Mathematical Preliminaries"
"for recursive algorithms can always be approached in this way. First write the base","chapter-2","Mathematical Preliminaries"
"cases. Then think about solving the problem by combining the results of one or","chapter-2","Mathematical Preliminaries"
"more smaller — but similar — subproblems. If the algorithm you write is correct,","chapter-2","Mathematical Preliminaries"
"then certainly you can rely on it (recursively) to solve the smaller subproblems.","chapter-2","Mathematical Preliminaries"
"The secret to success is: Do not worry about how the recursive call solves the","chapter-2","Mathematical Preliminaries"
"subproblem. Simply accept that it will solve it correctly, and use this result to in","chapter-2","Mathematical Preliminaries"
"turn correctly solve the original problem. What could be simpler?","chapter-2","Mathematical Preliminaries"
"Recursion has no counterpart in everyday, physical-world problem solving. The","chapter-2","Mathematical Preliminaries"
"concept can be difficult to grasp because it requires you to think about problems in","chapter-2","Mathematical Preliminaries"
"a new way. To use recursion effectively, it is necessary to train yourself to stop","chapter-2","Mathematical Preliminaries"
"analyzing the recursive process beyond the recursive call. The subproblems will","chapter-2","Mathematical Preliminaries"
"take care of themselves. You just worry about the base cases and how to recombine","chapter-2","Mathematical Preliminaries"
"the subproblems.","chapter-2","Mathematical Preliminaries"
"The recursive version of the factorial function might seem unnecessarily com-","chapter-2","Mathematical Preliminaries"
"plicated to you because the same effect can be achieved by using a while loop.","chapter-2","Mathematical Preliminaries"
"Here is another example of recursion, based on a famous puzzle called “Towers of","chapter-2","Mathematical Preliminaries"
"Hanoi.” The natural algorithm to solve this problem has multiple recursive calls. It","chapter-2","Mathematical Preliminaries"
"cannot be rewritten easily using while loops.","chapter-2","Mathematical Preliminaries"
"Sec. 2.5 Recursion 35","chapter-2","Mathematical Preliminaries"
"(a) (b)","chapter-2","Mathematical Preliminaries"
"Figure 2.2 Towers of Hanoi example. (a) The initial conditions for a problem","chapter-2","Mathematical Preliminaries"
"with six rings. (b) A necessary intermediate step on the road to a solution.","chapter-2","Mathematical Preliminaries"
"The Towers of Hanoi puzzle begins with three poles and n rings, where all rings","chapter-2","Mathematical Preliminaries"
"start on the leftmost pole (labeled Pole 1). The rings each have a different size, and","chapter-2","Mathematical Preliminaries"
"are stacked in order of decreasing size with the largest ring at the bottom, as shown","chapter-2","Mathematical Preliminaries"
"in Figure 2.2(a). The problem is to move the rings from the leftmost pole to the","chapter-2","Mathematical Preliminaries"
"rightmost pole (labeled Pole 3) in a series of steps. At each step the top ring on","chapter-2","Mathematical Preliminaries"
"some pole is moved to another pole. There is one limitation on where rings may be","chapter-2","Mathematical Preliminaries"
"moved: A ring can never be moved on top of a smaller ring.","chapter-2","Mathematical Preliminaries"
"How can you solve this problem? It is easy if you don’t think too hard about","chapter-2","Mathematical Preliminaries"
"the details. Instead, consider that all rings are to be moved from Pole 1 to Pole 3.","chapter-2","Mathematical Preliminaries"
"It is not possible to do this without first moving the bottom (largest) ring to Pole 3.","chapter-2","Mathematical Preliminaries"
"To do that, Pole 3 must be empty, and only the bottom ring can be on Pole 1.","chapter-2","Mathematical Preliminaries"
"The remaining n − 1 rings must be stacked up in order on Pole 2, as shown in","chapter-2","Mathematical Preliminaries"
"Figure 2.2(b). How can you do this? Assume that a function X is available to","chapter-2","Mathematical Preliminaries"
"solve the problem of moving the top n − 1 rings from Pole 1 to Pole 2. Then move","chapter-2","Mathematical Preliminaries"
"the bottom ring from Pole 1 to Pole 3. Finally, again use function X to move the","chapter-2","Mathematical Preliminaries"
"remaining n − 1 rings from Pole 2 to Pole 3. In both cases, “function X” is simply","chapter-2","Mathematical Preliminaries"
"the Towers of Hanoi function called on a smaller version of the problem.","chapter-2","Mathematical Preliminaries"
"The secret to success is relying on the Towers of Hanoi algorithm to do the","chapter-2","Mathematical Preliminaries"
"work for you. You need not be concerned about the gory details of how the Towers","chapter-2","Mathematical Preliminaries"
"of Hanoi subproblem will be solved. That will take care of itself provided that two","chapter-2","Mathematical Preliminaries"
"things are done. First, there must be a base case (what to do if there is only one","chapter-2","Mathematical Preliminaries"
"ring) so that the recursive process will not go on forever. Second, the recursive call","chapter-2","Mathematical Preliminaries"
"to Towers of Hanoi can only be used to solve a smaller problem, and then only one","chapter-2","Mathematical Preliminaries"
"of the proper form (one that meets the original definition for the Towers of Hanoi","chapter-2","Mathematical Preliminaries"
"problem, assuming appropriate renaming of the poles).","chapter-2","Mathematical Preliminaries"
"Here is an implementation for the recursive Towers of Hanoi algorithm. Func-","chapter-2","Mathematical Preliminaries"
"tion move(start, goal) takes the top ring from Pole start and moves it to","chapter-2","Mathematical Preliminaries"
"Pole goal. If move were to print the values of its parameters, then the result of","chapter-2","Mathematical Preliminaries"
"calling TOH would be a list of ring-moving instructions that solves the problem.","chapter-2","Mathematical Preliminaries"
"36 Chap. 2 Mathematical Preliminaries","chapter-2","Mathematical Preliminaries"
"/** Compute the moves to solve a Tower of Hanoi puzzle.","chapter-2","Mathematical Preliminaries"
"Function move does (or prints) the actual move of a disk","chapter-2","Mathematical Preliminaries"
"from one pole to another.","chapter-2","Mathematical Preliminaries"
"@param n The number of disks","chapter-2","Mathematical Preliminaries"
"@param start The start pole","chapter-2","Mathematical Preliminaries"
"@param goal The goal pole","chapter-2","Mathematical Preliminaries"
"@param temp The other pole */","chapter-2","Mathematical Preliminaries"
"static void TOH(int n, Pole start, Pole goal, Pole temp) {","chapter-2","Mathematical Preliminaries"
"if (n == 0) return; // Base case","chapter-2","Mathematical Preliminaries"
"TOH(n-1, start, temp, goal); // Recursive call: n-1 rings","chapter-2","Mathematical Preliminaries"
"move(start, goal); // Move bottom disk to goal","chapter-2","Mathematical Preliminaries"
"TOH(n-1, temp, goal, start); // Recursive call: n-1 rings","chapter-2","Mathematical Preliminaries"
"}","chapter-2","Mathematical Preliminaries"
"Those who are unfamiliar with recursion might find it hard to accept that it is","chapter-2","Mathematical Preliminaries"
"used primarily as a tool for simplifying the design and description of algorithms.","chapter-2","Mathematical Preliminaries"
"A recursive algorithm usually does not yield the most efficient computer program","chapter-2","Mathematical Preliminaries"
"for solving the problem because recursion involves function calls, which are typi-","chapter-2","Mathematical Preliminaries"
"cally more expensive than other alternatives such as a while loop. However, the","chapter-2","Mathematical Preliminaries"
"recursive approach usually provides an algorithm that is reasonably efficient in the","chapter-2","Mathematical Preliminaries"
"sense discussed in Chapter 3. (But not always! See Exercise 2.11.) If necessary,","chapter-2","Mathematical Preliminaries"
"the clear, recursive solution can later be modified to yield a faster implementation,","chapter-2","Mathematical Preliminaries"
"as described in Section 4.2.4.","chapter-2","Mathematical Preliminaries"
"Many data structures are naturally recursive, in that they can be defined as be-","chapter-2","Mathematical Preliminaries"
"ing made up of self-similar parts. Tree structures are an example of this. Thus,","chapter-2","Mathematical Preliminaries"
"the algorithms to manipulate such data structures are often presented recursively.","chapter-2","Mathematical Preliminaries"
"Many searching and sorting algorithms are based on a strategy of divide and con-","chapter-2","Mathematical Preliminaries"
"quer. That is, a solution is found by breaking the problem into smaller (similar)","chapter-2","Mathematical Preliminaries"
"subproblems, solving the subproblems, then combining the subproblem solutions","chapter-2","Mathematical Preliminaries"
"to form the solution to the original problem. This process is often implemented","chapter-2","Mathematical Preliminaries"
"using recursion. Thus, recursion plays an important role throughout this book, and","chapter-2","Mathematical Preliminaries"
"many more examples of recursive functions will be given.","chapter-2","Mathematical Preliminaries"
"2.6 Mathematical Proof Techniques","chapter-2","Mathematical Preliminaries"
"Solving any problem has two distinct parts: the investigation and the argument.","chapter-2","Mathematical Preliminaries"
"Students are too used to seeing only the argument in their textbooks and lectures.","chapter-2","Mathematical Preliminaries"
"But to be successful in school (and in life after school), one needs to be good at","chapter-2","Mathematical Preliminaries"
"both, and to understand the differences between these two phases of the process.","chapter-2","Mathematical Preliminaries"
"To solve the problem, you must investigate successfully. That means engaging the","chapter-2","Mathematical Preliminaries"
"problem, and working through until you find a solution. Then, to give the answer","chapter-2","Mathematical Preliminaries"
"to your client (whether that “client” be your instructor when writing answers on","chapter-2","Mathematical Preliminaries"
"a homework assignment or exam, or a written report to your boss), you need to","chapter-2","Mathematical Preliminaries"
"be able to make the argument in a way that gets the solution across clearly and","chapter-2","Mathematical Preliminaries"
"Sec. 2.6 Mathematical Proof Techniques 37","chapter-2","Mathematical Preliminaries"
"succinctly. The argument phase involves good technical writing skills — the ability","chapter-2","Mathematical Preliminaries"
"to make a clear, logical argument.","chapter-2","Mathematical Preliminaries"
"Being conversant with standard proof techniques can help you in this process.","chapter-2","Mathematical Preliminaries"
"Knowing how to write a good proof helps in many ways. First, it clarifies your","chapter-2","Mathematical Preliminaries"
"thought process, which in turn clarifies your explanations. Second, if you use one of","chapter-2","Mathematical Preliminaries"
"the standard proof structures such as proof by contradiction or an induction proof,","chapter-2","Mathematical Preliminaries"
"then both you and your reader are working from a shared understanding of that","chapter-2","Mathematical Preliminaries"
"structure. That makes for less complexity to your reader to understand your proof,","chapter-2","Mathematical Preliminaries"
"because the reader need not decode the structure of your argument from scratch.","chapter-2","Mathematical Preliminaries"
"This section briefly introduces three commonly used proof techniques: (i) de-","chapter-2","Mathematical Preliminaries"
"duction, or direct proof; (ii) proof by contradiction, and (iii) proof by mathematical","chapter-2","Mathematical Preliminaries"
"induction.","chapter-2","Mathematical Preliminaries"
"2.6.1 Direct Proof","chapter-2","Mathematical Preliminaries"
"In general, a direct proof is just a “logical explanation.” A direct proof is some-","chapter-2","Mathematical Preliminaries"
"times referred to as an argument by deduction. This is simply an argument in terms","chapter-2","Mathematical Preliminaries"
"of logic. Often written in English with words such as “if ... then,” it could also","chapter-2","Mathematical Preliminaries"
"be written with logic notation such as “P ⇒ Q.” Even if we don’t wish to use","chapter-2","Mathematical Preliminaries"
"symbolic logic notation, we can still take advantage of fundamental theorems of","chapter-2","Mathematical Preliminaries"
"logic to structure our arguments. For example, if we want to prove that P and Q","chapter-2","Mathematical Preliminaries"
"are equivalent, we can first prove P ⇒ Q and then prove Q ⇒ P.","chapter-2","Mathematical Preliminaries"
"In some domains, proofs are essentially a series of state changes from a start","chapter-2","Mathematical Preliminaries"
"state to an end state. Formal predicate logic can be viewed in this way, with the vari-","chapter-2","Mathematical Preliminaries"
"ous “rules of logic” being used to make the changes from one formula or combining","chapter-2","Mathematical Preliminaries"
"a couple of formulas to make a new formula on the route to the destination. Sym-","chapter-2","Mathematical Preliminaries"
"bolic manipulations to solve integration problems in introductory calculus classes","chapter-2","Mathematical Preliminaries"
"are similar in spirit, as are high school geometry proofs.","chapter-2","Mathematical Preliminaries"
"2.6.2 Proof by Contradiction","chapter-2","Mathematical Preliminaries"
"The simplest way to disprove a theorem or statement is to find a counterexample","chapter-2","Mathematical Preliminaries"
"to the theorem. Unfortunately, no number of examples supporting a theorem is","chapter-2","Mathematical Preliminaries"
"sufficient to prove that the theorem is correct. However, there is an approach that","chapter-2","Mathematical Preliminaries"
"is vaguely similar to disproving by counterexample, called Proof by Contradiction.","chapter-2","Mathematical Preliminaries"
"To prove a theorem by contradiction, we first assume that the theorem is false. We","chapter-2","Mathematical Preliminaries"
"then find a logical contradiction stemming from this assumption. If the logic used","chapter-2","Mathematical Preliminaries"
"to find the contradiction is correct, then the only way to resolve the contradiction is","chapter-2","Mathematical Preliminaries"
"to recognize that the assumption that the theorem is false must be incorrect. That","chapter-2","Mathematical Preliminaries"
"is, we conclude that the theorem must be true.","chapter-2","Mathematical Preliminaries"
"Example 2.10 Here is a simple proof by contradiction.","chapter-2","Mathematical Preliminaries"
"38 Chap. 2 Mathematical Preliminaries","chapter-2","Mathematical Preliminaries"
"Theorem 2.1 There is no largest integer.","chapter-2","Mathematical Preliminaries"
"Proof: Proof by contradiction.","chapter-2","Mathematical Preliminaries"
"Step 1. Contrary assumption: Assume that there is a largest integer.","chapter-2","Mathematical Preliminaries"
"Call it B (for “biggest”).","chapter-2","Mathematical Preliminaries"
"Step 2. Show this assumption leads to a contradiction: Consider","chapter-2","Mathematical Preliminaries"
"C = B + 1. C is an integer because it is the sum of two integers. Also,","chapter-2","Mathematical Preliminaries"
"C > B, which means that B is not the largest integer after all. Thus, we","chapter-2","Mathematical Preliminaries"
"have reached a contradiction. The only flaw in our reasoning is the initial","chapter-2","Mathematical Preliminaries"
"assumption that the theorem is false. Thus, we conclude that the theorem is","chapter-2","Mathematical Preliminaries"
"correct. ✷","chapter-2","Mathematical Preliminaries"
"A related proof technique is proving the contrapositive. We can prove that","chapter-2","Mathematical Preliminaries"
"P ⇒ Q by proving (not Q) ⇒ (not P).","chapter-2","Mathematical Preliminaries"
"2.6.3 Proof by Mathematical Induction","chapter-2","Mathematical Preliminaries"
"Mathematical induction can be used to prove a wide variety of theorems. Induction","chapter-2","Mathematical Preliminaries"
"also provides a useful way to think about algorithm design, because it encourages","chapter-2","Mathematical Preliminaries"
"you to think about solving a problem by building up from simple subproblems.","chapter-2","Mathematical Preliminaries"
"Induction can help to prove that a recursive function produces the correct result..","chapter-2","Mathematical Preliminaries"
"Understanding recursion is a big step toward understanding induction, and vice","chapter-2","Mathematical Preliminaries"
"versa, since they work by essentially the same process.","chapter-2","Mathematical Preliminaries"
"Within the context of algorithm analysis, one of the most important uses for","chapter-2","Mathematical Preliminaries"
"mathematical induction is as a method to test a hypothesis. As explained in Sec-","chapter-2","Mathematical Preliminaries"
"tion 2.4, when seeking a closed-form solution for a summation or recurrence we","chapter-2","Mathematical Preliminaries"
"might first guess or otherwise acquire evidence that a particular formula is the cor-","chapter-2","Mathematical Preliminaries"
"rect solution. If the formula is indeed correct, it is often an easy matter to prove","chapter-2","Mathematical Preliminaries"
"that fact with an induction proof.","chapter-2","Mathematical Preliminaries"
"Let Thrm be a theorem to prove, and express Thrm in terms of a positive","chapter-2","Mathematical Preliminaries"
"integer parameter n. Mathematical induction states that Thrm is true for any value","chapter-2","Mathematical Preliminaries"
"of parameter n (for n ≥ c, where c is some constant) if the following two conditions","chapter-2","Mathematical Preliminaries"
"are true:","chapter-2","Mathematical Preliminaries"
"1. Base Case: Thrm holds for n = c, and","chapter-2","Mathematical Preliminaries"
"2. Induction Step: If Thrm holds for n − 1, then Thrm holds for n.","chapter-2","Mathematical Preliminaries"
"Proving the base case is usually easy, typically requiring that some small value","chapter-2","Mathematical Preliminaries"
"such as 1 be substituted for n in the theorem and applying simple algebra or logic","chapter-2","Mathematical Preliminaries"
"as necessary to verify the theorem. Proving the induction step is sometimes easy,","chapter-2","Mathematical Preliminaries"
"and sometimes difficult. An alternative formulation of the induction step is known","chapter-2","Mathematical Preliminaries"
"as strong induction. The induction step for strong induction is:","chapter-2","Mathematical Preliminaries"
"2a. Induction Step: If Thrm holds for all k, c ≤ k < n, then Thrm holds for n.","chapter-2","Mathematical Preliminaries"
"Sec. 2.6 Mathematical Proof Techniques 39","chapter-2","Mathematical Preliminaries"
"Proving either variant of the induction step (in conjunction with verifying the base","chapter-2","Mathematical Preliminaries"
"case) yields a satisfactory proof by mathematical induction.","chapter-2","Mathematical Preliminaries"
"The two conditions that make up the induction proof combine to demonstrate","chapter-2","Mathematical Preliminaries"
"that Thrm holds for n = 2 as an extension of the fact that Thrm holds for n = 1.","chapter-2","Mathematical Preliminaries"
"This fact, combined again with condition (2) or (2a), indicates that Thrm also holds","chapter-2","Mathematical Preliminaries"
"for n = 3, and so on. Thus, Thrm holds for all values of n (larger than the base","chapter-2","Mathematical Preliminaries"
"cases) once the two conditions have been proved.","chapter-2","Mathematical Preliminaries"
"What makes mathematical induction so powerful (and so mystifying to most","chapter-2","Mathematical Preliminaries"
"people at first) is that we can take advantage of the assumption that Thrm holds","chapter-2","Mathematical Preliminaries"
"for all values less than n as a tool to help us prove that Thrm holds for n. This is","chapter-2","Mathematical Preliminaries"
"known as the induction hypothesis. Having this assumption to work with makes","chapter-2","Mathematical Preliminaries"
"the induction step easier to prove than tackling the original theorem itself. Being","chapter-2","Mathematical Preliminaries"
"able to rely on the induction hypothesis provides extra information that we can","chapter-2","Mathematical Preliminaries"
"bring to bear on the problem.","chapter-2","Mathematical Preliminaries"
"Recursion and induction have many similarities. Both are anchored on one or","chapter-2","Mathematical Preliminaries"
"more base cases. A recursive function relies on the ability to call itself to get the","chapter-2","Mathematical Preliminaries"
"answer for smaller instances of the problem. Likewise, induction proofs rely on the","chapter-2","Mathematical Preliminaries"
"truth of the induction hypothesis to prove the theorem. The induction hypothesis","chapter-2","Mathematical Preliminaries"
"does not come out of thin air. It is true if and only if the theorem itself is true, and","chapter-2","Mathematical Preliminaries"
"therefore is reliable within the proof context. Using the induction hypothesis it do","chapter-2","Mathematical Preliminaries"
"work is exactly the same as using a recursive call to do work.","chapter-2","Mathematical Preliminaries"
"Example 2.11 Here is a sample proof by mathematical induction. Call","chapter-2","Mathematical Preliminaries"
"the sum of the first n positive integers S(n).","chapter-2","Mathematical Preliminaries"
"Theorem 2.2 S(n) = n(n + 1)/2.","chapter-2","Mathematical Preliminaries"
"Proof: The proof is by mathematical induction.","chapter-2","Mathematical Preliminaries"
"1. Check the base case. For n = 1, verify that S(1) = 1(1 + 1)/2. S(1)","chapter-2","Mathematical Preliminaries"
"is simply the sum of the first positive number, which is 1. Because","chapter-2","Mathematical Preliminaries"
"1(1 + 1)/2 = 1, the formula is correct for the base case.","chapter-2","Mathematical Preliminaries"
"2. State the induction hypothesis. The induction hypothesis is","chapter-2","Mathematical Preliminaries"
"S(n − 1) =","chapter-2","Mathematical Preliminaries"
"nX−1","chapter-2","Mathematical Preliminaries"
"i=1","chapter-2","Mathematical Preliminaries"
"i =","chapter-2","Mathematical Preliminaries"
"(n − 1)((n − 1) + 1)","chapter-2","Mathematical Preliminaries"
"2","chapter-2","Mathematical Preliminaries"
"=","chapter-2","Mathematical Preliminaries"
"(n − 1)(n)","chapter-2","Mathematical Preliminaries"
"2","chapter-2","Mathematical Preliminaries"
".","chapter-2","Mathematical Preliminaries"
"3. Use the assumption from the induction hypothesis for n − 1 to","chapter-2","Mathematical Preliminaries"
"show that the result is true for n. The induction hypothesis states","chapter-2","Mathematical Preliminaries"
"that S(n − 1) = (n − 1)(n)/2, and because S(n) = S(n − 1) + n,","chapter-2","Mathematical Preliminaries"
"we can substitute for S(n − 1) to get","chapter-2","Mathematical Preliminaries"
"Xn","chapter-2","Mathematical Preliminaries"
"i=1","chapter-2","Mathematical Preliminaries"
"i =","chapter-2","Mathematical Preliminaries"
"nX−1","chapter-2","Mathematical Preliminaries"
"i=1","chapter-2","Mathematical Preliminaries"
"i","chapter-2","Mathematical Preliminaries"
"!","chapter-2","Mathematical Preliminaries"
"+ n =","chapter-2","Mathematical Preliminaries"
"(n − 1)(n)","chapter-2","Mathematical Preliminaries"
"2","chapter-2","Mathematical Preliminaries"
"+ n","chapter-2","Mathematical Preliminaries"
"40 Chap. 2 Mathematical Preliminaries","chapter-2","Mathematical Preliminaries"
"=","chapter-2","Mathematical Preliminaries"
"n","chapter-2","Mathematical Preliminaries"
"2 − n + 2n","chapter-2","Mathematical Preliminaries"
"2","chapter-2","Mathematical Preliminaries"
"=","chapter-2","Mathematical Preliminaries"
"n(n + 1)","chapter-2","Mathematical Preliminaries"
"2","chapter-2","Mathematical Preliminaries"
".","chapter-2","Mathematical Preliminaries"
"Thus, by mathematical induction,","chapter-2","Mathematical Preliminaries"
"S(n) = Xn","chapter-2","Mathematical Preliminaries"
"i=1","chapter-2","Mathematical Preliminaries"
"i = n(n + 1)/2.","chapter-2","Mathematical Preliminaries"
"✷","chapter-2","Mathematical Preliminaries"
"Note carefully what took place in this example. First we cast S(n) in terms","chapter-2","Mathematical Preliminaries"
"of a smaller occurrence of the problem: S(n) = S(n − 1) + n. This is important","chapter-2","Mathematical Preliminaries"
"because once S(n − 1) comes into the picture, we can use the induction hypothesis","chapter-2","Mathematical Preliminaries"
"to replace S(n − 1) with (n − 1)(n)/2. From here, it is simple algebra to prove","chapter-2","Mathematical Preliminaries"
"that S(n − 1) + n equals the right-hand side of the original theorem.","chapter-2","Mathematical Preliminaries"
"Example 2.12 Here is another simple proof by induction that illustrates","chapter-2","Mathematical Preliminaries"
"choosing the proper variable for induction. We wish to prove by induction","chapter-2","Mathematical Preliminaries"
"that the sum of the first n positive odd numbers is n","chapter-2","Mathematical Preliminaries"
"2","chapter-2","Mathematical Preliminaries"
". First we need a way","chapter-2","Mathematical Preliminaries"
"to describe the nth odd number, which is simply 2n − 1. This also allows","chapter-2","Mathematical Preliminaries"
"us to cast the theorem as a summation.","chapter-2","Mathematical Preliminaries"
"Theorem 2.3 Pn","chapter-2","Mathematical Preliminaries"
"i=1(2i − 1) = n","chapter-2","Mathematical Preliminaries"
"2","chapter-2","Mathematical Preliminaries"
".","chapter-2","Mathematical Preliminaries"
"Proof: The base case of n = 1 yields 1 = 12","chapter-2","Mathematical Preliminaries"
", which is true. The induction","chapter-2","Mathematical Preliminaries"
"hypothesis is","chapter-2","Mathematical Preliminaries"
"nX−1","chapter-2","Mathematical Preliminaries"
"i=1","chapter-2","Mathematical Preliminaries"
"(2i − 1) = (n − 1)2","chapter-2","Mathematical Preliminaries"
".","chapter-2","Mathematical Preliminaries"
"We now use the induction hypothesis to show that the theorem holds true","chapter-2","Mathematical Preliminaries"
"for n. The sum of the first n odd numbers is simply the sum of the first","chapter-2","Mathematical Preliminaries"
"n − 1 odd numbers plus the nth odd number. In the second line below, we","chapter-2","Mathematical Preliminaries"
"will use the induction hypothesis to replace the partial summation (shown","chapter-2","Mathematical Preliminaries"
"in brackets in the first line) with its closed-form solution. After that, algebra","chapter-2","Mathematical Preliminaries"
"takes care of the rest.","chapter-2","Mathematical Preliminaries"
"Xn","chapter-2","Mathematical Preliminaries"
"i=1","chapter-2","Mathematical Preliminaries"
"(2i − 1) = "","chapter-2","Mathematical Preliminaries"
"nX−1","chapter-2","Mathematical Preliminaries"
"i=1","chapter-2","Mathematical Preliminaries"
"(2i − 1)#","chapter-2","Mathematical Preliminaries"
"+ 2n − 1","chapter-2","Mathematical Preliminaries"
"= [(n − 1)2","chapter-2","Mathematical Preliminaries"
"] + 2n − 1","chapter-2","Mathematical Preliminaries"
"= n","chapter-2","Mathematical Preliminaries"
"2 − 2n + 1 + 2n − 1","chapter-2","Mathematical Preliminaries"
"= n","chapter-2","Mathematical Preliminaries"
"2","chapter-2","Mathematical Preliminaries"
".","chapter-2","Mathematical Preliminaries"
"Thus, by mathematical induction, Pn","chapter-2","Mathematical Preliminaries"
"i=1(2i − 1) = n","chapter-2","Mathematical Preliminaries"
"2","chapter-2","Mathematical Preliminaries"
". ✷","chapter-2","Mathematical Preliminaries"
"Sec. 2.6 Mathematical Proof Techniques 41","chapter-2","Mathematical Preliminaries"
"Example 2.13 This example shows how we can use induction to prove","chapter-2","Mathematical Preliminaries"
"that a proposed closed-form solution for a recurrence relation is correct.","chapter-2","Mathematical Preliminaries"
"Theorem 2.4 The recurrence relation T(n) = T(n−1)+1; T(1) = 0","chapter-2","Mathematical Preliminaries"
"has closed-form solution T(n) = n − 1.","chapter-2","Mathematical Preliminaries"
"Proof: To prove the base case, we observe that T(1) = 1 − 1 = 0. The","chapter-2","Mathematical Preliminaries"
"induction hypothesis is that T(n − 1) = n − 2. Combining the definition","chapter-2","Mathematical Preliminaries"
"of the recurrence with the induction hypothesis, we see immediately that","chapter-2","Mathematical Preliminaries"
"T(n) = T(n − 1) + 1 = n − 2 + 1 = n − 1","chapter-2","Mathematical Preliminaries"
"for n > 1. Thus, we have proved the theorem correct by mathematical","chapter-2","Mathematical Preliminaries"
"induction. ✷","chapter-2","Mathematical Preliminaries"
"Example 2.14 This example uses induction without involving summa-","chapter-2","Mathematical Preliminaries"
"tions or other equations. It also illustrates a more flexible use of base cases.","chapter-2","Mathematical Preliminaries"
"Theorem 2.5 2¢ and 5¢ stamps can be used to form any value (for values","chapter-2","Mathematical Preliminaries"
"≥ 4).","chapter-2","Mathematical Preliminaries"
"Proof: The theorem defines the problem for values ≥ 4 because it does","chapter-2","Mathematical Preliminaries"
"not hold for the values 1 and 3. Using 4 as the base case, a value of 4¢","chapter-2","Mathematical Preliminaries"
"can be made from two 2¢ stamps. The induction hypothesis is that a value","chapter-2","Mathematical Preliminaries"
"of n − 1 can be made from some combination of 2¢ and 5¢ stamps. We","chapter-2","Mathematical Preliminaries"
"now use the induction hypothesis to show how to get the value n from 2¢","chapter-2","Mathematical Preliminaries"
"and 5¢ stamps. Either the makeup for value n − 1 includes a 5¢ stamp, or","chapter-2","Mathematical Preliminaries"
"it does not. If so, then replace a 5¢ stamp with three 2¢ stamps. If not,","chapter-2","Mathematical Preliminaries"
"then the makeup must have included at least two 2¢ stamps (because it is","chapter-2","Mathematical Preliminaries"
"at least of size 4 and contains only 2¢ stamps). In this case, replace two of","chapter-2","Mathematical Preliminaries"
"the 2¢ stamps with a single 5¢ stamp. In either case, we now have a value","chapter-2","Mathematical Preliminaries"
"of n made up of 2¢ and 5¢ stamps. Thus, by mathematical induction, the","chapter-2","Mathematical Preliminaries"
"theorem is correct. ✷","chapter-2","Mathematical Preliminaries"
"Example 2.15 Here is an example using strong induction.","chapter-2","Mathematical Preliminaries"
"Theorem 2.6 For n > 1, n is divisible by some prime number.","chapter-2","Mathematical Preliminaries"
"Proof: For the base case, choose n = 2. 2 is divisible by the prime num-","chapter-2","Mathematical Preliminaries"
"ber 2. The induction hypothesis is that any value a, 2 ≤ a < n, is divisible","chapter-2","Mathematical Preliminaries"
"by some prime number. There are now two cases to consider when proving","chapter-2","Mathematical Preliminaries"
"the theorem for n. If n is a prime number, then n is divisible by itself. If n","chapter-2","Mathematical Preliminaries"
"is not a prime number, then n = a × b for a and b, both integers less than","chapter-2","Mathematical Preliminaries"
"42 Chap. 2 Mathematical Preliminaries","chapter-2","Mathematical Preliminaries"
"Figure 2.3 A two-coloring for the regions formed by three lines in the plane.","chapter-2","Mathematical Preliminaries"
"n but greater than 1. The induction hypothesis tells us that a is divisible by","chapter-2","Mathematical Preliminaries"
"some prime number. That same prime number must also divide n. Thus,","chapter-2","Mathematical Preliminaries"
"by mathematical induction, the theorem is correct. ✷","chapter-2","Mathematical Preliminaries"
"Our next example of mathematical induction proves a theorem from geometry.","chapter-2","Mathematical Preliminaries"
"It also illustrates a standard technique of induction proof where we take n objects","chapter-2","Mathematical Preliminaries"
"and remove some object to use the induction hypothesis.","chapter-2","Mathematical Preliminaries"
"Example 2.16 Define a two-coloring for a set of regions as a way of as-","chapter-2","Mathematical Preliminaries"
"signing one of two colors to each region such that no two regions sharing a","chapter-2","Mathematical Preliminaries"
"side have the same color. For example, a chessboard is two-colored. Fig-","chapter-2","Mathematical Preliminaries"
"ure 2.3 shows a two-coloring for the plane with three lines. We will assume","chapter-2","Mathematical Preliminaries"
"that the two colors to be used are black and white.","chapter-2","Mathematical Preliminaries"
"Theorem 2.7 The set of regions formed by n infinite lines in the plane can","chapter-2","Mathematical Preliminaries"
"be two-colored.","chapter-2","Mathematical Preliminaries"
"Proof: Consider the base case of a single infinite line in the plane. This line","chapter-2","Mathematical Preliminaries"
"splits the plane into two regions. One region can be colored black and the","chapter-2","Mathematical Preliminaries"
"other white to get a valid two-coloring. The induction hypothesis is that the","chapter-2","Mathematical Preliminaries"
"set of regions formed by n − 1 infinite lines can be two-colored. To prove","chapter-2","Mathematical Preliminaries"
"the theorem for n, consider the set of regions formed by the n − 1 lines","chapter-2","Mathematical Preliminaries"
"remaining when any one of the n lines is removed. By the induction hy-","chapter-2","Mathematical Preliminaries"
"pothesis, this set of regions can be two-colored. Now, put the nth line back.","chapter-2","Mathematical Preliminaries"
"This splits the plane into two half-planes, each of which (independently)","chapter-2","Mathematical Preliminaries"
"has a valid two-coloring inherited from the two-coloring of the plane with","chapter-2","Mathematical Preliminaries"
"n − 1 lines. Unfortunately, the regions newly split by the nth line violate","chapter-2","Mathematical Preliminaries"
"the rule for a two-coloring. Take all regions on one side of the nth line and","chapter-2","Mathematical Preliminaries"
"reverse their coloring (after doing so, this half-plane is still two-colored).","chapter-2","Mathematical Preliminaries"
"Those regions split by the nth line are now properly two-colored, because","chapter-2","Mathematical Preliminaries"
"Sec. 2.6 Mathematical Proof Techniques 43","chapter-2","Mathematical Preliminaries"
"the part of the region to one side of the line is now black and the region","chapter-2","Mathematical Preliminaries"
"to the other side is now white. Thus, by mathematical induction, the entire","chapter-2","Mathematical Preliminaries"
"plane is two-colored. ✷","chapter-2","Mathematical Preliminaries"
"Compare the proof of Theorem 2.7 with that of Theorem 2.5. For Theorem 2.5,","chapter-2","Mathematical Preliminaries"
"we took a collection of stamps of size n − 1 (which, by the induction hypothesis,","chapter-2","Mathematical Preliminaries"
"must have the desired property) and from that “built” a collection of size n that","chapter-2","Mathematical Preliminaries"
"has the desired property. We therefore proved the existence of some collection of","chapter-2","Mathematical Preliminaries"
"stamps of size n with the desired property.","chapter-2","Mathematical Preliminaries"
"For Theorem 2.7 we must prove that any collection of n lines has the desired","chapter-2","Mathematical Preliminaries"
"property. Thus, our strategy is to take an arbitrary collection of n lines, and “re-","chapter-2","Mathematical Preliminaries"
"duce” it so that we have a set of lines that must have the desired property because","chapter-2","Mathematical Preliminaries"
"it matches the induction hypothesis. From there, we merely need to show that re-","chapter-2","Mathematical Preliminaries"
"versing the original reduction process preserves the desired property.","chapter-2","Mathematical Preliminaries"
"In contrast, consider what is required if we attempt to “build” from a set of lines","chapter-2","Mathematical Preliminaries"
"of size n − 1 to one of size n. We would have great difficulty justifying that all","chapter-2","Mathematical Preliminaries"
"possible collections of n lines are covered by our building process. By reducing","chapter-2","Mathematical Preliminaries"
"from an arbitrary collection of n lines to something less, we avoid this problem.","chapter-2","Mathematical Preliminaries"
"This section’s final example shows how induction can be used to prove that a","chapter-2","Mathematical Preliminaries"
"recursive function produces the correct result.","chapter-2","Mathematical Preliminaries"
"Example 2.17 We would like to prove that function fact does indeed","chapter-2","Mathematical Preliminaries"
"compute the factorial function. There are two distinct steps to such a proof.","chapter-2","Mathematical Preliminaries"
"The first is to prove that the function always terminates. The second is to","chapter-2","Mathematical Preliminaries"
"prove that the function returns the correct value.","chapter-2","Mathematical Preliminaries"
"Theorem 2.8 Function fact will terminate for any value of n.","chapter-2","Mathematical Preliminaries"
"Proof: For the base case, we observe that fact will terminate directly","chapter-2","Mathematical Preliminaries"
"whenever n ≤ 0. The induction hypothesis is that fact will terminate for","chapter-2","Mathematical Preliminaries"
"n − 1. For n, we have two possibilities. One possibility is that n ≥ 12.","chapter-2","Mathematical Preliminaries"
"In that case, fact will terminate directly because it will fail its assertion","chapter-2","Mathematical Preliminaries"
"test. Otherwise, fact will make a recursive call to fact(n-1). By the","chapter-2","Mathematical Preliminaries"
"induction hypothesis, fact(n-1) must terminate. ✷","chapter-2","Mathematical Preliminaries"
"Theorem 2.9 Function fact does compute the factorial function for any","chapter-2","Mathematical Preliminaries"
"value in the range 0 to 12.","chapter-2","Mathematical Preliminaries"
"Proof: To prove the base case, observe that when n = 0 or n = 1,","chapter-2","Mathematical Preliminaries"
"fact(n) returns the correct value of 1. The induction hypothesis is that","chapter-2","Mathematical Preliminaries"
"fact(n-1) returns the correct value of (n − 1)!. For any value n within","chapter-2","Mathematical Preliminaries"
"the legal range, fact(n) returns n ∗ fact(n-1). By the induction hy-","chapter-2","Mathematical Preliminaries"
"pothesis, fact(n-1) = (n−1)!, and because n ∗ (n−1)! = n!, we have","chapter-2","Mathematical Preliminaries"
"proved that fact(n) produces the correct result. ✷","chapter-2","Mathematical Preliminaries"
"44 Chap. 2 Mathematical Preliminaries","chapter-2","Mathematical Preliminaries"
"We can use a similar process to prove many recursive programs correct. The","chapter-2","Mathematical Preliminaries"
"general form is to show that the base cases perform correctly, and then to use the","chapter-2","Mathematical Preliminaries"
"induction hypothesis to show that the recursive step also produces the correct result.","chapter-2","Mathematical Preliminaries"
"Prior to this, we must prove that the function always terminates, which might also","chapter-2","Mathematical Preliminaries"
"be done using an induction proof.","chapter-2","Mathematical Preliminaries"
"2.7 Estimation","chapter-2","Mathematical Preliminaries"
"One of the most useful life skills that you can gain from your computer science","chapter-2","Mathematical Preliminaries"
"training is the ability to perform quick estimates. This is sometimes known as “back","chapter-2","Mathematical Preliminaries"
"of the napkin” or “back of the envelope” calculation. Both nicknames suggest","chapter-2","Mathematical Preliminaries"
"that only a rough estimate is produced. Estimation techniques are a standard part","chapter-2","Mathematical Preliminaries"
"of engineering curricula but are often neglected in computer science. Estimation","chapter-2","Mathematical Preliminaries"
"is no substitute for rigorous, detailed analysis of a problem, but it can serve to","chapter-2","Mathematical Preliminaries"
"indicate when a rigorous analysis is warranted: If the initial estimate indicates that","chapter-2","Mathematical Preliminaries"
"the solution is unworkable, then further analysis is probably unnecessary.","chapter-2","Mathematical Preliminaries"
"Estimation can be formalized by the following three-step process:","chapter-2","Mathematical Preliminaries"
"1. Determine the major parameters that affect the problem.","chapter-2","Mathematical Preliminaries"
"2. Derive an equation that relates the parameters to the problem.","chapter-2","Mathematical Preliminaries"
"3. Select values for the parameters, and apply the equation to yield an estimated","chapter-2","Mathematical Preliminaries"
"solution.","chapter-2","Mathematical Preliminaries"
"When doing estimations, a good way to reassure yourself that the estimate is","chapter-2","Mathematical Preliminaries"
"reasonable is to do it in two different ways. In general, if you want to know what","chapter-2","Mathematical Preliminaries"
"comes out of a system, you can either try to estimate that directly, or you can","chapter-2","Mathematical Preliminaries"
"estimate what goes into the system (assuming that what goes in must later come","chapter-2","Mathematical Preliminaries"
"out). If both approaches (independently) give similar answers, then this should","chapter-2","Mathematical Preliminaries"
"build confidence in the estimate.","chapter-2","Mathematical Preliminaries"
"When calculating, be sure that your units match. For example, do not add feet","chapter-2","Mathematical Preliminaries"
"and pounds. Verify that the result is in the correct units. Always keep in mind that","chapter-2","Mathematical Preliminaries"
"the output of a calculation is only as good as its input. The more uncertain your","chapter-2","Mathematical Preliminaries"
"valuation for the input parameters in Step 3, the more uncertain the output value.","chapter-2","Mathematical Preliminaries"
"However, back of the envelope calculations are often meant only to get an answer","chapter-2","Mathematical Preliminaries"
"within an order of magnitude, or perhaps within a factor of two. Before doing an","chapter-2","Mathematical Preliminaries"
"estimate, you should decide on acceptable error bounds, such as within 25%, within","chapter-2","Mathematical Preliminaries"
"a factor of two, and so forth. Once you are confident that an estimate falls within","chapter-2","Mathematical Preliminaries"
"your error bounds, leave it alone! Do not try to get a more precise estimate than","chapter-2","Mathematical Preliminaries"
"necessary for your purpose.","chapter-2","Mathematical Preliminaries"
"Example 2.18 How many library bookcases does it take to store books","chapter-2","Mathematical Preliminaries"
"containing one million pages? I estimate that a 500-page book requires","chapter-2","Mathematical Preliminaries"
"Sec. 2.8 Further Reading 45","chapter-2","Mathematical Preliminaries"
"one inch on the library shelf (it will help to look at the size of any handy","chapter-2","Mathematical Preliminaries"
"book), yielding about 200 feet of shelf space for one million pages. If a","chapter-2","Mathematical Preliminaries"
"shelf is 4 feet wide, then 50 shelves are required. If a bookcase contains","chapter-2","Mathematical Preliminaries"
"5 shelves, this yields about 10 library bookcases. To reach this conclusion,","chapter-2","Mathematical Preliminaries"
"I estimated the number of pages per inch, the width of a shelf, and the","chapter-2","Mathematical Preliminaries"
"number of shelves in a bookcase. None of my estimates are likely to be","chapter-2","Mathematical Preliminaries"
"precise, but I feel confident that my answer is correct to within a factor of","chapter-2","Mathematical Preliminaries"
"two. (After writing this, I went to Virginia Tech’s library and looked at","chapter-2","Mathematical Preliminaries"
"some real bookcases. They were only about 3 feet wide, but typically had","chapter-2","Mathematical Preliminaries"
"7 shelves for a total of 21 shelf-feet. So I was correct to within 10% on","chapter-2","Mathematical Preliminaries"
"bookcase capacity, far better than I expected or needed. One of my selected","chapter-2","Mathematical Preliminaries"
"values was too high, and the other too low, which canceled out the errors.)","chapter-2","Mathematical Preliminaries"
"Example 2.19 Is it more economical to buy a car that gets 20 miles per","chapter-2","Mathematical Preliminaries"
"gallon, or one that gets 30 miles per gallon but costs $3000 more? The","chapter-2","Mathematical Preliminaries"
"typical car is driven about 12,000 miles per year. If gasoline costs $3/gallon,","chapter-2","Mathematical Preliminaries"
"then the yearly gas bill is $1800 for the less efficient car and $1200 for the","chapter-2","Mathematical Preliminaries"
"more efficient car. If we ignore issues such as the payback that would be","chapter-2","Mathematical Preliminaries"
"received if we invested $3000 in a bank, it would take 5 years to make","chapter-2","Mathematical Preliminaries"
"up the difference in price. At this point, the buyer must decide if price is","chapter-2","Mathematical Preliminaries"
"the only criterion and if a 5-year payback time is acceptable. Naturally,","chapter-2","Mathematical Preliminaries"
"a person who drives more will make up the difference more quickly, and","chapter-2","Mathematical Preliminaries"
"changes in gasoline prices will also greatly affect the outcome.","chapter-2","Mathematical Preliminaries"
"Example 2.20 When at the supermarket doing the week’s shopping, can","chapter-2","Mathematical Preliminaries"
"you estimate about how much you will have to pay at the checkout? One","chapter-2","Mathematical Preliminaries"
"simple way is to round the price of each item to the nearest dollar, and add","chapter-2","Mathematical Preliminaries"
"this value to a mental running total as you put the item in your shopping","chapter-2","Mathematical Preliminaries"
"cart. This will likely give an answer within a couple of dollars of the true","chapter-2","Mathematical Preliminaries"
"total.","chapter-2","Mathematical Preliminaries"
"2.8 Further Reading","chapter-2","Mathematical Preliminaries"
"Most of the topics covered in this chapter are considered part of Discrete Math-","chapter-2","Mathematical Preliminaries"
"ematics. An introduction to this field is Discrete Mathematics with Applications","chapter-2","Mathematical Preliminaries"
"by Susanna S. Epp [Epp10]. An advanced treatment of many mathematical topics","chapter-2","Mathematical Preliminaries"
"useful to computer scientists is Concrete Mathematics: A Foundation for Computer","chapter-2","Mathematical Preliminaries"
"Science by Graham, Knuth, and Patashnik [GKP94].","chapter-2","Mathematical Preliminaries"
"46 Chap. 2 Mathematical Preliminaries","chapter-2","Mathematical Preliminaries"
"See “Technically Speaking” from the February 1995 issue of IEEE Spectrum","chapter-2","Mathematical Preliminaries"
"[Sel95] for a discussion on the standard for indicating units of computer storage","chapter-2","Mathematical Preliminaries"
"used in this book.","chapter-2","Mathematical Preliminaries"
"Introduction to Algorithms by Udi Manber [Man89] makes extensive use of","chapter-2","Mathematical Preliminaries"
"mathematical induction as a technique for developing algorithms.","chapter-2","Mathematical Preliminaries"
"For more information on recursion, see Thinking Recursively by Eric S. Roberts","chapter-2","Mathematical Preliminaries"
"[Rob86]. To learn recursion properly, it is worth your while to learn the program-","chapter-2","Mathematical Preliminaries"
"ming languages LISP or Scheme, even if you never intend to write a program in","chapter-2","Mathematical Preliminaries"
"either language. In particular, Friedman and Felleisen’s “Little” books (including","chapter-2","Mathematical Preliminaries"
"The Little LISPer[FF89] and The Little Schemer[FFBS95]) are designed to teach","chapter-2","Mathematical Preliminaries"
"you how to think recursively as well as teach you the language. These books are","chapter-2","Mathematical Preliminaries"
"entertaining reading as well.","chapter-2","Mathematical Preliminaries"
"A good book on writing mathematical proofs is Daniel Solow’s How to Read","chapter-2","Mathematical Preliminaries"
"and Do Proofs [Sol09]. To improve your general mathematical problem-solving","chapter-2","Mathematical Preliminaries"
"abilities, see The Art and Craft of Problem Solving by Paul Zeitz [Zei07]. Zeitz","chapter-2","Mathematical Preliminaries"
"also discusses the three proof techniques presented in Section 2.6, and the roles of","chapter-2","Mathematical Preliminaries"
"investigation and argument in problem solving.","chapter-2","Mathematical Preliminaries"
"For more about estimation techniques, see two Programming Pearls by John","chapter-2","Mathematical Preliminaries"
"Louis Bentley entitled The Back of the Envelope and The Envelope is Back [Ben84,","chapter-2","Mathematical Preliminaries"
"Ben00, Ben86, Ben88]. Genius: The Life and Science of Richard Feynman by","chapter-2","Mathematical Preliminaries"
"James Gleick [Gle92] gives insight into how important back of the envelope calcu-","chapter-2","Mathematical Preliminaries"
"lation was to the developers of the atomic bomb, and to modern theoretical physics","chapter-2","Mathematical Preliminaries"
"in general.","chapter-2","Mathematical Preliminaries"
"2.9 Exercises","chapter-2","Mathematical Preliminaries"
"2.1 For each relation below, explain why the relation does or does not satisfy","chapter-2","Mathematical Preliminaries"
"each of the properties reflexive, symmetric, antisymmetric, and transitive.","chapter-2","Mathematical Preliminaries"
"(a) “isBrotherOf” on the set of people.","chapter-2","Mathematical Preliminaries"
"(b) “isFatherOf” on the set of people.","chapter-2","Mathematical Preliminaries"
"(c) The relation R = {hx, yi | x","chapter-2","Mathematical Preliminaries"
"2 + y","chapter-2","Mathematical Preliminaries"
"2 = 1} for real numbers x and y.","chapter-2","Mathematical Preliminaries"
"(d) The relation R = {hx, yi | x","chapter-2","Mathematical Preliminaries"
"2 = y","chapter-2","Mathematical Preliminaries"
"2} for real numbers x and y.","chapter-2","Mathematical Preliminaries"
"(e) The relation R = {hx, yi | x mod y = 0} for x, y ∈ {1, 2, 3, 4}.","chapter-2","Mathematical Preliminaries"
"(f) The empty relation ∅ (i.e., the relation with no ordered pairs for which","chapter-2","Mathematical Preliminaries"
"it is true) on the set of integers.","chapter-2","Mathematical Preliminaries"
"(g) The empty relation ∅ (i.e., the relation with no ordered pairs for which","chapter-2","Mathematical Preliminaries"
"it is true) on the empty set.","chapter-2","Mathematical Preliminaries"
"2.2 For each of the following relations, either prove that it is an equivalence","chapter-2","Mathematical Preliminaries"
"relation or prove that it is not an equivalence relation.","chapter-2","Mathematical Preliminaries"
"(a) For integers a and b, a ≡ b if and only if a + b is even.","chapter-2","Mathematical Preliminaries"
"(b) For integers a and b, a ≡ b if and only if a + b is odd.","chapter-2","Mathematical Preliminaries"
"Sec. 2.9 Exercises 47","chapter-2","Mathematical Preliminaries"
"(c) For nonzero rational numbers a and b, a ≡ b if and only if a × b > 0.","chapter-2","Mathematical Preliminaries"
"(d) For nonzero rational numbers a and b, a ≡ b if and only if a/b is an","chapter-2","Mathematical Preliminaries"
"integer.","chapter-2","Mathematical Preliminaries"
"(e) For rational numbers a and b, a ≡ b if and only if a − b is an integer.","chapter-2","Mathematical Preliminaries"
"(f) For rational numbers a and b, a ≡ b if and only if |a − b| ≤ 2.","chapter-2","Mathematical Preliminaries"
"2.3 State whether each of the following relations is a partial ordering, and explain","chapter-2","Mathematical Preliminaries"
"why or why not.","chapter-2","Mathematical Preliminaries"
"(a) “isFatherOf” on the set of people.","chapter-2","Mathematical Preliminaries"
"(b) “isAncestorOf” on the set of people.","chapter-2","Mathematical Preliminaries"
"(c) “isOlderThan” on the set of people.","chapter-2","Mathematical Preliminaries"
"(d) “isSisterOf” on the set of people.","chapter-2","Mathematical Preliminaries"
"(e) {ha, bi,ha, ai,hb, ai} on the set {a, b}.","chapter-2","Mathematical Preliminaries"
"(f) {h2, 1i,h1, 3i,h2, 3i} on the set {1, 2, 3}.","chapter-2","Mathematical Preliminaries"
"2.4 How many total orderings can be defined on a set with n elements? Explain","chapter-2","Mathematical Preliminaries"
"your answer.","chapter-2","Mathematical Preliminaries"
"2.5 Define an ADT for a set of integers (remember that a set has no concept of","chapter-2","Mathematical Preliminaries"
"duplicate elements, and has no concept of order). Your ADT should consist","chapter-2","Mathematical Preliminaries"
"of the functions that can be performed on a set to control its membership,","chapter-2","Mathematical Preliminaries"
"check the size, check if a given element is in the set, and so on. Each function","chapter-2","Mathematical Preliminaries"
"should be defined in terms of its input and output.","chapter-2","Mathematical Preliminaries"
"2.6 Define an ADT for a bag of integers (remember that a bag may contain du-","chapter-2","Mathematical Preliminaries"
"plicates, and has no concept of order). Your ADT should consist of the func-","chapter-2","Mathematical Preliminaries"
"tions that can be performed on a bag to control its membership, check the","chapter-2","Mathematical Preliminaries"
"size, check if a given element is in the set, and so on. Each function should","chapter-2","Mathematical Preliminaries"
"be defined in terms of its input and output.","chapter-2","Mathematical Preliminaries"
"2.7 Define an ADT for a sequence of integers (remember that a sequence may","chapter-2","Mathematical Preliminaries"
"contain duplicates, and supports the concept of position for its elements).","chapter-2","Mathematical Preliminaries"
"Your ADT should consist of the functions that can be performed on a se-","chapter-2","Mathematical Preliminaries"
"quence to control its membership, check the size, check if a given element is","chapter-2","Mathematical Preliminaries"
"in the set, and so on. Each function should be defined in terms of its input","chapter-2","Mathematical Preliminaries"
"and output.","chapter-2","Mathematical Preliminaries"
"2.8 An investor places $30,000 into a stock fund. 10 years later the account has","chapter-2","Mathematical Preliminaries"
"a value of $69,000. Using logarithms and anti-logarithms, present a formula","chapter-2","Mathematical Preliminaries"
"for calculating the average annual rate of increase. Then use your formula to","chapter-2","Mathematical Preliminaries"
"determine the average annual growth rate for this fund.","chapter-2","Mathematical Preliminaries"
"2.9 Rewrite the factorial function of Section 2.5 without using recursion.","chapter-2","Mathematical Preliminaries"
"2.10 Rewrite the for loop for the random permutation generator of Section 2.2","chapter-2","Mathematical Preliminaries"
"as a recursive function.","chapter-2","Mathematical Preliminaries"
"2.11 Here is a simple recursive function to compute the Fibonacci sequence:","chapter-2","Mathematical Preliminaries"
"48 Chap. 2 Mathematical Preliminaries","chapter-2","Mathematical Preliminaries"
"/** Recursively generate and return the n’th Fibonacci","chapter-2","Mathematical Preliminaries"
"number */","chapter-2","Mathematical Preliminaries"
"static long fibr(int n) {","chapter-2","Mathematical Preliminaries"
"// fibr(91) is the largest value that fits in a long","chapter-2","Mathematical Preliminaries"
"assert (n > 0) && (n <= 91) : "n out of range";","chapter-2","Mathematical Preliminaries"
"if ((n == 1) || (n == 2)) return 1; // Base case","chapter-2","Mathematical Preliminaries"
"return fibr(n-1) + fibr(n-2); // Recursive call","chapter-2","Mathematical Preliminaries"
"}","chapter-2","Mathematical Preliminaries"
"This algorithm turns out to be very slow, calling Fibr a total of Fib(n) times.","chapter-2","Mathematical Preliminaries"
"Contrast this with the following iterative algorithm:","chapter-2","Mathematical Preliminaries"
"/** Iteratively generate and return the n’th Fibonacci","chapter-2","Mathematical Preliminaries"
"number */","chapter-2","Mathematical Preliminaries"
"static long fibi(int n) {","chapter-2","Mathematical Preliminaries"
"// fibr(91) is the largest value that fits in a long","chapter-2","Mathematical Preliminaries"
"assert (n > 0) && (n <= 91) : "n out of range";","chapter-2","Mathematical Preliminaries"
"long curr, prev, past;","chapter-2","Mathematical Preliminaries"
"if ((n == 1) || (n == 2)) return 1;","chapter-2","Mathematical Preliminaries"
"curr = prev = 1; // curr holds current Fib value","chapter-2","Mathematical Preliminaries"
"for (int i=3; i<=n; i++) { // Compute next value","chapter-2","Mathematical Preliminaries"
"past = prev; // past holds fibi(i-2)","chapter-2","Mathematical Preliminaries"
"prev = curr; // prev holds fibi(i-1)","chapter-2","Mathematical Preliminaries"
"curr = past + prev; // curr now holds fibi(i)","chapter-2","Mathematical Preliminaries"
"}","chapter-2","Mathematical Preliminaries"
"return curr;","chapter-2","Mathematical Preliminaries"
"}","chapter-2","Mathematical Preliminaries"
"Function Fibi executes the for loop n − 2 times.","chapter-2","Mathematical Preliminaries"
"(a) Which version is easier to understand? Why?","chapter-2","Mathematical Preliminaries"
"(b) Explain why Fibr is so much slower than Fibi.","chapter-2","Mathematical Preliminaries"
"2.12 Write a recursive function to solve a generalization of the Towers of Hanoi","chapter-2","Mathematical Preliminaries"
"problem where each ring may begin on any pole so long as no ring sits on","chapter-2","Mathematical Preliminaries"
"top of a smaller ring.","chapter-2","Mathematical Preliminaries"
"2.13 Revise the recursive implementation for Towers of Hanoi from Section 2.5","chapter-2","Mathematical Preliminaries"
"to return the list of moves needed to solve the problem.","chapter-2","Mathematical Preliminaries"
"2.14 Consider the following function:","chapter-2","Mathematical Preliminaries"
"static void foo (double val) {","chapter-2","Mathematical Preliminaries"
"if (val != 0.0)","chapter-2","Mathematical Preliminaries"
"foo(val/2.0);","chapter-2","Mathematical Preliminaries"
"}","chapter-2","Mathematical Preliminaries"
"This function makes progress towards the base case on every recursive call.","chapter-2","Mathematical Preliminaries"
"In theory (that is, if double variables acted like true real numbers), would","chapter-2","Mathematical Preliminaries"
"this function ever terminate for input val a nonzero number? In practice (an","chapter-2","Mathematical Preliminaries"
"actual computer implementation), will it terminate?","chapter-2","Mathematical Preliminaries"
"2.15 Write a function to print all of the permutations for the elements of an array","chapter-2","Mathematical Preliminaries"
"containing n distinct integer values.","chapter-2","Mathematical Preliminaries"
"Sec. 2.9 Exercises 49","chapter-2","Mathematical Preliminaries"
"2.16 Write a recursive algorithm to print all of the subsets for the set of the first n","chapter-2","Mathematical Preliminaries"
"positive integers.","chapter-2","Mathematical Preliminaries"
"2.17 The Largest Common Factor (LCF) for two positive integers n and m is","chapter-2","Mathematical Preliminaries"
"the largest integer that divides both n and m evenly. LCF(n, m) is at least","chapter-2","Mathematical Preliminaries"
"one, and at most m, assuming that n ≥ m. Over two thousand years ago,","chapter-2","Mathematical Preliminaries"
"Euclid provided an efficient algorithm based on the observation that, when","chapter-2","Mathematical Preliminaries"
"n mod m 6= 0, LCF(n, m) = LCF(m, n mod m). Use this fact to write two","chapter-2","Mathematical Preliminaries"
"algorithms to find the LCF for two positive integers. The first version should","chapter-2","Mathematical Preliminaries"
"compute the value iteratively. The second version should compute the value","chapter-2","Mathematical Preliminaries"
"using recursion.","chapter-2","Mathematical Preliminaries"
"2.18 Prove by contradiction that the number of primes is infinite.","chapter-2","Mathematical Preliminaries"
"2.19 (a) Use induction to show that n","chapter-2","Mathematical Preliminaries"
"2 − n is always even.","chapter-2","Mathematical Preliminaries"
"(b) Give a direct proof in one or two sentences that n","chapter-2","Mathematical Preliminaries"
"2 − n is always even.","chapter-2","Mathematical Preliminaries"
"(c) Show that n","chapter-2","Mathematical Preliminaries"
"3 − n is always divisible by three.","chapter-2","Mathematical Preliminaries"
"(d) Is n","chapter-2","Mathematical Preliminaries"
"5 − n aways divisible by 5? Explain your answer.","chapter-2","Mathematical Preliminaries"
"2.20 Prove that √","chapter-2","Mathematical Preliminaries"
"2 is irrational.","chapter-2","Mathematical Preliminaries"
"2.21 Explain why","chapter-2","Mathematical Preliminaries"
"Xn","chapter-2","Mathematical Preliminaries"
"i=1","chapter-2","Mathematical Preliminaries"
"i =","chapter-2","Mathematical Preliminaries"
"Xn","chapter-2","Mathematical Preliminaries"
"i=1","chapter-2","Mathematical Preliminaries"
"(n − i + 1) =","chapter-2","Mathematical Preliminaries"
"nX−1","chapter-2","Mathematical Preliminaries"
"i=0","chapter-2","Mathematical Preliminaries"
"(n − i).","chapter-2","Mathematical Preliminaries"
"2.22 Prove Equation 2.2 using mathematical induction.","chapter-2","Mathematical Preliminaries"
"2.23 Prove Equation 2.6 using mathematical induction.","chapter-2","Mathematical Preliminaries"
"2.24 Prove Equation 2.7 using mathematical induction.","chapter-2","Mathematical Preliminaries"
"2.25 Find a closed-form solution and prove (using induction) that your solution is","chapter-2","Mathematical Preliminaries"
"correct for the summation","chapter-2","Mathematical Preliminaries"
"Xn","chapter-2","Mathematical Preliminaries"
"i=1","chapter-2","Mathematical Preliminaries"
"3","chapter-2","Mathematical Preliminaries"
"i","chapter-2","Mathematical Preliminaries"
".","chapter-2","Mathematical Preliminaries"
"2.26 Prove that the sum of the first n even numbers is n","chapter-2","Mathematical Preliminaries"
"2 + n","chapter-2","Mathematical Preliminaries"
"(a) by assuming that the sum of the first n odd numbers is n","chapter-2","Mathematical Preliminaries"
"2","chapter-2","Mathematical Preliminaries"
".","chapter-2","Mathematical Preliminaries"
"(b) by mathematical induction.","chapter-2","Mathematical Preliminaries"
"2.27 Give a closed-form formula for the summation Pn","chapter-2","Mathematical Preliminaries"
"i=a","chapter-2","Mathematical Preliminaries"
"i where a is an integer","chapter-2","Mathematical Preliminaries"
"between 1 and n.","chapter-2","Mathematical Preliminaries"
"2.28 Prove that Fib(n) < (","chapter-2","Mathematical Preliminaries"
"5","chapter-2","Mathematical Preliminaries"
"3","chapter-2","Mathematical Preliminaries"
")","chapter-2","Mathematical Preliminaries"
"n","chapter-2","Mathematical Preliminaries"
".","chapter-2","Mathematical Preliminaries"
"2.29 Prove, for n ≥ 1, that","chapter-2","Mathematical Preliminaries"
"Xn","chapter-2","Mathematical Preliminaries"
"i=1","chapter-2","Mathematical Preliminaries"
"i","chapter-2","Mathematical Preliminaries"
"3 =","chapter-2","Mathematical Preliminaries"
"n","chapter-2","Mathematical Preliminaries"
"2","chapter-2","Mathematical Preliminaries"
"(n + 1)2","chapter-2","Mathematical Preliminaries"
"4","chapter-2","Mathematical Preliminaries"
".","chapter-2","Mathematical Preliminaries"
"2.30 The following theorem is called the Pigeonhole Principle.","chapter-2","Mathematical Preliminaries"
"Theorem 2.10 When n + 1 pigeons roost in n holes, there must be some","chapter-2","Mathematical Preliminaries"
"hole containing at least two pigeons.","chapter-2","Mathematical Preliminaries"
"50 Chap. 2 Mathematical Preliminaries","chapter-2","Mathematical Preliminaries"
"(a) Prove the Pigeonhole Principle using proof by contradiction.","chapter-2","Mathematical Preliminaries"
"(b) Prove the Pigeonhole Principle using mathematical induction.","chapter-2","Mathematical Preliminaries"
"2.31 For this problem, you will consider arrangements of infinite lines in the plane","chapter-2","Mathematical Preliminaries"
"such that three or more lines never intersect at a single point and no two lines","chapter-2","Mathematical Preliminaries"
"are parallel.","chapter-2","Mathematical Preliminaries"
"(a) Give a recurrence relation that expresses the number of regions formed","chapter-2","Mathematical Preliminaries"
"by n lines, and explain why your recurrence is correct.","chapter-2","Mathematical Preliminaries"
"(b) Give the summation that results from expanding your recurrence.","chapter-2","Mathematical Preliminaries"
"(c) Give a closed-form solution for the summation.","chapter-2","Mathematical Preliminaries"
"2.32 Prove (using induction) that the recurrence T(n) = T(n − 1) +n; T(1) = 1","chapter-2","Mathematical Preliminaries"
"has as its closed-form solution T(n) = n(n + 1)/2.","chapter-2","Mathematical Preliminaries"
"2.33 Expand the following recurrence to help you find a closed-form solution, and","chapter-2","Mathematical Preliminaries"
"then use induction to prove your answer is correct.","chapter-2","Mathematical Preliminaries"
"T(n) = 2T(n − 1) + 1 for n > 0; T(0) = 0.","chapter-2","Mathematical Preliminaries"
"2.34 Expand the following recurrence to help you find a closed-form solution, and","chapter-2","Mathematical Preliminaries"
"then use induction to prove your answer is correct.","chapter-2","Mathematical Preliminaries"
"T(n) = T(n − 1) + 3n + 1 for n > 0; T(0) = 1.","chapter-2","Mathematical Preliminaries"
"2.35 Assume that an n-bit integer (represented by standard binary notation) takes","chapter-2","Mathematical Preliminaries"
"any value in the range 0 to 2","chapter-2","Mathematical Preliminaries"
"n − 1 with equal probability.","chapter-2","Mathematical Preliminaries"
"(a) For each bit position, what is the probability of its value being 1 and","chapter-2","Mathematical Preliminaries"
"what is the probability of its value being 0?","chapter-2","Mathematical Preliminaries"
"(b) What is the average number of “1” bits for an n-bit random number?","chapter-2","Mathematical Preliminaries"
"(c) What is the expected value for the position of the leftmost “1” bit? In","chapter-2","Mathematical Preliminaries"
"other words, how many positions on average must we examine when","chapter-2","Mathematical Preliminaries"
"moving from left to right before encountering a “1” bit? Show the","chapter-2","Mathematical Preliminaries"
"appropriate summation.","chapter-2","Mathematical Preliminaries"
"2.36 What is the total volume of your body in liters (or, if you prefer, gallons)?","chapter-2","Mathematical Preliminaries"
"2.37 An art historian has a database of 20,000 full-screen color images.","chapter-2","Mathematical Preliminaries"
"(a) About how much space will this require? How many CDs would be","chapter-2","Mathematical Preliminaries"
"required to store the database? (A CD holds about 600MB of data). Be","chapter-2","Mathematical Preliminaries"
"sure to explain all assumptions you made to derive your answer.","chapter-2","Mathematical Preliminaries"
"(b) Now, assume that you have access to a good image compression tech-","chapter-2","Mathematical Preliminaries"
"nique that can store the images in only 1/10 of the space required for","chapter-2","Mathematical Preliminaries"
"an uncompressed image. Will the entire database fit onto a single CD","chapter-2","Mathematical Preliminaries"
"if the images are compressed?","chapter-2","Mathematical Preliminaries"
"Sec. 2.9 Exercises 51","chapter-2","Mathematical Preliminaries"
"2.38 How many cubic miles of water flow out of the mouth of the Mississippi","chapter-2","Mathematical Preliminaries"
"River each day? DO NOT look up the answer or any supplemental facts. Be","chapter-2","Mathematical Preliminaries"
"sure to describe all assumptions made in arriving at your answer.","chapter-2","Mathematical Preliminaries"
"2.39 When buying a home mortgage, you often have the option of paying some","chapter-2","Mathematical Preliminaries"
"money in advance (called “discount points”) to get a lower interest rate. As-","chapter-2","Mathematical Preliminaries"
"sume that you have the choice between two 15-year fixed-rate mortgages:","chapter-2","Mathematical Preliminaries"
"one at 8% with no up-front charge, and the other at 7","chapter-2","Mathematical Preliminaries"
"3","chapter-2","Mathematical Preliminaries"
"4% with an up-front","chapter-2","Mathematical Preliminaries"
"charge of 1% of the mortgage value. How long would it take to recover the","chapter-2","Mathematical Preliminaries"
"1% charge when you take the mortgage at the lower rate? As a second, more","chapter-2","Mathematical Preliminaries"
"precise estimate, how long would it take to recover the charge plus the in-","chapter-2","Mathematical Preliminaries"
"terest you would have received if you had invested the equivalent of the 1%","chapter-2","Mathematical Preliminaries"
"charge in the bank at 5% interest while paying the higher rate? DO NOT use","chapter-2","Mathematical Preliminaries"
"a calculator to help you answer this question.","chapter-2","Mathematical Preliminaries"
"2.40 When you build a new house, you sometimes get a “construction loan” which","chapter-2","Mathematical Preliminaries"
"is a temporary line of credit out of which you pay construction costs as they","chapter-2","Mathematical Preliminaries"
"occur. At the end of the construction period, you then replace the construc-","chapter-2","Mathematical Preliminaries"
"tion loan with a regular mortgage on the house. During the construction loan,","chapter-2","Mathematical Preliminaries"
"you only pay each month for the interest charged against the actual amount","chapter-2","Mathematical Preliminaries"
"borrowed so far. Assume that your house construction project starts at the","chapter-2","Mathematical Preliminaries"
"beginning of April, and is complete at the end of six months. Assume that","chapter-2","Mathematical Preliminaries"
"the total construction cost will be $300,000 with the costs occurring at the be-","chapter-2","Mathematical Preliminaries"
"ginning of each month in $50,000 increments. The construction loan charges","chapter-2","Mathematical Preliminaries"
"6% interest. Estimate the total interest payments that must be paid over the","chapter-2","Mathematical Preliminaries"
"life of the construction loan.","chapter-2","Mathematical Preliminaries"
"2.41 Here are some questions that test your working knowledge of how fast com-","chapter-2","Mathematical Preliminaries"
"puters operate. Is disk drive access time normally measured in milliseconds","chapter-2","Mathematical Preliminaries"
"(thousandths of a second) or microseconds (millionths of a second)? Does","chapter-2","Mathematical Preliminaries"
"your RAM memory access a word in more or less than one microsecond?","chapter-2","Mathematical Preliminaries"
"How many instructions can your CPU execute in one year if the machine is","chapter-2","Mathematical Preliminaries"
"left running at full speed all the time? DO NOT use paper or a calculator to","chapter-2","Mathematical Preliminaries"
"derive your answers.","chapter-2","Mathematical Preliminaries"
"2.42 Does your home contain enough books to total one million pages? How","chapter-2","Mathematical Preliminaries"
"many total pages are stored in your school library building? Explain how","chapter-2","Mathematical Preliminaries"
"you got your answer.","chapter-2","Mathematical Preliminaries"
"2.43 How many words are in this book? Explain how you got your answer.","chapter-2","Mathematical Preliminaries"
"2.44 How many hours are one million seconds? How many days? Answer these","chapter-2","Mathematical Preliminaries"
"questions doing all arithmetic in your head. Explain how you got your an-","chapter-2","Mathematical Preliminaries"
"swer.","chapter-2","Mathematical Preliminaries"
"2.45 How many cities and towns are there in the United States? Explain how you","chapter-2","Mathematical Preliminaries"
"got your answer.","chapter-2","Mathematical Preliminaries"
"2.46 How many steps would it take to walk from Boston to San Francisco? Ex-","chapter-2","Mathematical Preliminaries"
"plain how you got your answer.","chapter-2","Mathematical Preliminaries"
"52 Chap. 2 Mathematical Preliminaries","chapter-2","Mathematical Preliminaries"
"2.47 A man begins a car trip to visit his in-laws. The total distance is 60 miles,","chapter-2","Mathematical Preliminaries"
"and he starts off at a speed of 60 miles per hour. After driving exactly 1 mile,","chapter-2","Mathematical Preliminaries"
"he loses some of his enthusiasm for the journey, and (instantaneously) slows","chapter-2","Mathematical Preliminaries"
"down to 59 miles per hour. After traveling another mile, he again slows to","chapter-2","Mathematical Preliminaries"
"58 miles per hour. This continues, progressively slowing by 1 mile per hour","chapter-2","Mathematical Preliminaries"
"for each mile traveled until the trip is complete.","chapter-2","Mathematical Preliminaries"
"(a) How long does it take the man to reach his in-laws?","chapter-2","Mathematical Preliminaries"
"(b) How long would the trip take in the continuous case where the speed","chapter-2","Mathematical Preliminaries"
"smoothly diminishes with the distance yet to travel?","chapter-2","Mathematical Preliminaries"
"How long will it take to process the company payroll once we complete our planned","chapter-3","Algorithm Analysis"
"merger? Should I buy a new payroll program from vendor X or vendor Y? If a","chapter-3","Algorithm Analysis"
"particular program is slow, is it badly implemented or is it solving a hard problem?","chapter-3","Algorithm Analysis"
"Questions like these ask us to consider the difficulty of a problem, or the relative","chapter-3","Algorithm Analysis"
"efficiency of two or more approaches to solving a problem.","chapter-3","Algorithm Analysis"
"This chapter introduces the motivation, basic notation, and fundamental tech-","chapter-3","Algorithm Analysis"
"niques of algorithm analysis. We focus on a methodology known as asymptotic","chapter-3","Algorithm Analysis"
"algorithm analysis, or simply asymptotic analysis. Asymptotic analysis attempts","chapter-3","Algorithm Analysis"
"to estimate the resource consumption of an algorithm. It allows us to compare the","chapter-3","Algorithm Analysis"
"relative costs of two or more algorithms for solving the same problem. Asymptotic","chapter-3","Algorithm Analysis"
"analysis also gives algorithm designers a tool for estimating whether a proposed","chapter-3","Algorithm Analysis"
"solution is likely to meet the resource constraints for a problem before they imple-","chapter-3","Algorithm Analysis"
"ment an actual program. After reading this chapter, you should understand","chapter-3","Algorithm Analysis"
"• the concept of a growth rate, the rate at which the cost of an algorithm grows","chapter-3","Algorithm Analysis"
"as the size of its input grows;","chapter-3","Algorithm Analysis"
"• the concept of upper and lower bounds for a growth rate, and how to estimate","chapter-3","Algorithm Analysis"
"these bounds for a simple program, algorithm, or problem; and","chapter-3","Algorithm Analysis"
"• the difference between the cost of an algorithm (or program) and the cost of","chapter-3","Algorithm Analysis"
"a problem.","chapter-3","Algorithm Analysis"
"The chapter concludes with a brief discussion of the practical difficulties encoun-","chapter-3","Algorithm Analysis"
"tered when empirically measuring the cost of a program, and some principles for","chapter-3","Algorithm Analysis"
"code tuning to improve program efficiency.","chapter-3","Algorithm Analysis"
"3.1 Introduction","chapter-3","Algorithm Analysis"
"How do you compare two algorithms for solving some problem in terms of effi-","chapter-3","Algorithm Analysis"
"ciency? We could implement both algorithms as computer programs and then run","chapter-3","Algorithm Analysis"
"53","chapter-3","Algorithm Analysis"
"54 Chap. 3 Algorithm Analysis","chapter-3","Algorithm Analysis"
"them on a suitable range of inputs, measuring how much of the resources in ques-","chapter-3","Algorithm Analysis"
"tion each program uses. This approach is often unsatisfactory for four reasons.","chapter-3","Algorithm Analysis"
"First, there is the effort involved in programming and testing two algorithms when","chapter-3","Algorithm Analysis"
"at best you want to keep only one. Second, when empirically comparing two al-","chapter-3","Algorithm Analysis"
"gorithms there is always the chance that one of the programs was “better written”","chapter-3","Algorithm Analysis"
"than the other, and therefor the relative qualities of the underlying algorithms are","chapter-3","Algorithm Analysis"
"not truly represented by their implementations. This can easily occur when the","chapter-3","Algorithm Analysis"
"programmer has a bias regarding the algorithms. Third, the choice of empirical","chapter-3","Algorithm Analysis"
"test cases might unfairly favor one algorithm. Fourth, you could find that even the","chapter-3","Algorithm Analysis"
"better of the two algorithms does not fall within your resource budget. In that case","chapter-3","Algorithm Analysis"
"you must begin the entire process again with yet another program implementing a","chapter-3","Algorithm Analysis"
"new algorithm. But, how would you know if any algorithm can meet the resource","chapter-3","Algorithm Analysis"
"budget? Perhaps the problem is simply too difficult for any implementation to be","chapter-3","Algorithm Analysis"
"within budget.","chapter-3","Algorithm Analysis"
"These problems can often be avoided by using asymptotic analysis. Asymp-","chapter-3","Algorithm Analysis"
"totic analysis measures the efficiency of an algorithm, or its implementation as a","chapter-3","Algorithm Analysis"
"program, as the input size becomes large. It is actually an estimating technique and","chapter-3","Algorithm Analysis"
"does not tell us anything about the relative merits of two programs where one is","chapter-3","Algorithm Analysis"
"always “slightly faster” than the other. However, asymptotic analysis has proved","chapter-3","Algorithm Analysis"
"useful to computer scientists who must determine if a particular algorithm is worth","chapter-3","Algorithm Analysis"
"considering for implementation.","chapter-3","Algorithm Analysis"
"The critical resource for a program is most often its running time. However,","chapter-3","Algorithm Analysis"
"you cannot pay attention to running time alone. You must also be concerned with","chapter-3","Algorithm Analysis"
"other factors such as the space required to run the program (both main memory and","chapter-3","Algorithm Analysis"
"disk space). Typically you will analyze the time required for an algorithm (or the","chapter-3","Algorithm Analysis"
"instantiation of an algorithm in the form of a program), and the space required for","chapter-3","Algorithm Analysis"
"a data structure.","chapter-3","Algorithm Analysis"
"Many factors affect the running time of a program. Some relate to the environ-","chapter-3","Algorithm Analysis"
"ment in which the program is compiled and run. Such factors include the speed of","chapter-3","Algorithm Analysis"
"the computer’s CPU, bus, and peripheral hardware. Competition with other users","chapter-3","Algorithm Analysis"
"for the computer’s (or the network’s) resources can make a program slow to a crawl.","chapter-3","Algorithm Analysis"
"The programming language and the quality of code generated by a particular com-","chapter-3","Algorithm Analysis"
"piler can have a significant effect. The “coding efficiency” of the programmer who","chapter-3","Algorithm Analysis"
"converts the algorithm to a program can have a tremendous impact as well.","chapter-3","Algorithm Analysis"
"If you need to get a program working within time and space constraints on a","chapter-3","Algorithm Analysis"
"particular computer, all of these factors can be relevant. Yet, none of these factors","chapter-3","Algorithm Analysis"
"address the differences between two algorithms or data structures. To be fair, pro-","chapter-3","Algorithm Analysis"
"grams derived from two algorithms for solving the same problem should both be","chapter-3","Algorithm Analysis"
"compiled with the same compiler and run on the same computer under the same","chapter-3","Algorithm Analysis"
"conditions. As much as possible, the same amount of care should be taken in the","chapter-3","Algorithm Analysis"
"programming effort devoted to each program to make the implementations “equally","chapter-3","Algorithm Analysis"
"Sec. 3.1 Introduction 55","chapter-3","Algorithm Analysis"
"efficient.” In this sense, all of the factors mentioned above should cancel out of the","chapter-3","Algorithm Analysis"
"comparison because they apply to both algorithms equally.","chapter-3","Algorithm Analysis"
"If you truly wish to understand the running time of an algorithm, there are other","chapter-3","Algorithm Analysis"
"factors that are more appropriate to consider than machine speed, programming","chapter-3","Algorithm Analysis"
"language, compiler, and so forth. Ideally we would measure the running time of","chapter-3","Algorithm Analysis"
"the algorithm under standard benchmark conditions. However, we have no way","chapter-3","Algorithm Analysis"
"to calculate the running time reliably other than to run an implementation of the","chapter-3","Algorithm Analysis"
"algorithm on some computer. The only alternative is to use some other measure as","chapter-3","Algorithm Analysis"
"a surrogate for running time.","chapter-3","Algorithm Analysis"
"Of primary consideration when estimating an algorithm’s performance is the","chapter-3","Algorithm Analysis"
"number of basic operations required by the algorithm to process an input of a","chapter-3","Algorithm Analysis"
"certain size. The terms “basic operations” and “size” are both rather vague and","chapter-3","Algorithm Analysis"
"depend on the algorithm being analyzed. Size is often the number of inputs pro-","chapter-3","Algorithm Analysis"
"cessed. For example, when comparing sorting algorithms, the size of the problem","chapter-3","Algorithm Analysis"
"is typically measured by the number of records to be sorted. A basic operation","chapter-3","Algorithm Analysis"
"must have the property that its time to complete does not depend on the particular","chapter-3","Algorithm Analysis"
"values of its operands. Adding or comparing two integer variables are examples","chapter-3","Algorithm Analysis"
"of basic operations in most programming languages. Summing the contents of an","chapter-3","Algorithm Analysis"
"array containing n integers is not, because the cost depends on the value of n (i.e.,","chapter-3","Algorithm Analysis"
"the size of the input).","chapter-3","Algorithm Analysis"
"Example 3.1 Consider a simple algorithm to solve the problem of finding","chapter-3","Algorithm Analysis"
"the largest value in an array of n integers. The algorithm looks at each","chapter-3","Algorithm Analysis"
"integer in turn, saving the position of the largest value seen so far. This","chapter-3","Algorithm Analysis"
"algorithm is called the largest-value sequential search and is illustrated by","chapter-3","Algorithm Analysis"
"the following function:","chapter-3","Algorithm Analysis"
"/** @return Position of largest value in array A */","chapter-3","Algorithm Analysis"
"static int largest(int[] A) {","chapter-3","Algorithm Analysis"
"int currlarge = 0; // Holds largest element position","chapter-3","Algorithm Analysis"
"for (int i=1; i<A.length; i++) // For each element","chapter-3","Algorithm Analysis"
"if (A[currlarge] < A[i]) // if A[i] is larger","chapter-3","Algorithm Analysis"
"currlarge = i; // remember its position","chapter-3","Algorithm Analysis"
"return currlarge; // Return largest position","chapter-3","Algorithm Analysis"
"}","chapter-3","Algorithm Analysis"
"Here, the size of the problem is A.length, the number of integers stored","chapter-3","Algorithm Analysis"
"in array A. The basic operation is to compare an integer’s value to that of","chapter-3","Algorithm Analysis"
"the largest value seen so far. It is reasonable to assume that it takes a fixed","chapter-3","Algorithm Analysis"
"amount of time to do one such comparison, regardless of the value of the","chapter-3","Algorithm Analysis"
"two integers or their positions in the array.","chapter-3","Algorithm Analysis"
"Because the most important factor affecting running time is normally","chapter-3","Algorithm Analysis"
"size of the input, for a given input size n we often express the time T to run","chapter-3","Algorithm Analysis"
"56 Chap. 3 Algorithm Analysis","chapter-3","Algorithm Analysis"
"the algorithm as a function of n, written as T(n). We will always assume","chapter-3","Algorithm Analysis"
"T(n) is a non-negative value.","chapter-3","Algorithm Analysis"
"Let us call c the amount of time required to compare two integers in","chapter-3","Algorithm Analysis"
"function largest. We do not care right now what the precise value of c","chapter-3","Algorithm Analysis"
"might be. Nor are we concerned with the time required to increment vari-","chapter-3","Algorithm Analysis"
"able i because this must be done for each value in the array, or the time","chapter-3","Algorithm Analysis"
"for the actual assignment when a larger value is found, or the little bit of","chapter-3","Algorithm Analysis"
"extra time taken to initialize currlarge. We just want a reasonable ap-","chapter-3","Algorithm Analysis"
"proximation for the time taken to execute the algorithm. The total time","chapter-3","Algorithm Analysis"
"to run largest is therefore approximately cn, because we must make n","chapter-3","Algorithm Analysis"
"comparisons, with each comparison costing c time. We say that function","chapter-3","Algorithm Analysis"
"largest (and by extension ,the largest-value sequential search algorithm","chapter-3","Algorithm Analysis"
"for any typical implementation) has a running time expressed by the equa-","chapter-3","Algorithm Analysis"
"tion","chapter-3","Algorithm Analysis"
"T(n) = cn.","chapter-3","Algorithm Analysis"
"This equation describes the growth rate for the running time of the largest-","chapter-3","Algorithm Analysis"
"value sequential search algorithm.","chapter-3","Algorithm Analysis"
"Example 3.2 The running time of a statement that assigns the first value","chapter-3","Algorithm Analysis"
"of an integer array to a variable is simply the time required to copy the value","chapter-3","Algorithm Analysis"
"of the first array value. We can assume this assignment takes a constant","chapter-3","Algorithm Analysis"
"amount of time regardless of the value. Let us call c1 the amount of time","chapter-3","Algorithm Analysis"
"necessary to copy an integer. No matter how large the array on a typical","chapter-3","Algorithm Analysis"
"computer (given reasonable conditions for memory and array size), the time","chapter-3","Algorithm Analysis"
"to copy the value from the first position of the array is always c1. Thus, the","chapter-3","Algorithm Analysis"
"equation for this algorithm is simply","chapter-3","Algorithm Analysis"
"T(n) = c1,","chapter-3","Algorithm Analysis"
"indicating that the size of the input n has no effect on the running time.","chapter-3","Algorithm Analysis"
"This is called a constant running time.","chapter-3","Algorithm Analysis"
"Example 3.3 Consider the following code:","chapter-3","Algorithm Analysis"
"sum = 0;","chapter-3","Algorithm Analysis"
"for (i=1; i<=n; i++)","chapter-3","Algorithm Analysis"
"for (j=1; j<=n; j++)","chapter-3","Algorithm Analysis"
"sum++;","chapter-3","Algorithm Analysis"
"What is the running time for this code fragment? Clearly it takes longer","chapter-3","Algorithm Analysis"
"to run when n is larger. The basic operation in this example is the increment","chapter-3","Algorithm Analysis"
"Sec. 3.1 Introduction 57","chapter-3","Algorithm Analysis"
"0","chapter-3","Algorithm Analysis"
"100","chapter-3","Algorithm Analysis"
"200","chapter-3","Algorithm Analysis"
"300","chapter-3","Algorithm Analysis"
"400","chapter-3","Algorithm Analysis"
"10n","chapter-3","Algorithm Analysis"
"20n","chapter-3","Algorithm Analysis"
"2n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"5n log n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"n n!","chapter-3","Algorithm Analysis"
"0 5 10 15","chapter-3","Algorithm Analysis"
"0 10 20 30 40 50","chapter-3","Algorithm Analysis"
"Input size n","chapter-3","Algorithm Analysis"
"10n","chapter-3","Algorithm Analysis"
"20n","chapter-3","Algorithm Analysis"
"2n 5n log n","chapter-3","Algorithm Analysis"
"2 2","chapter-3","Algorithm Analysis"
"n n!","chapter-3","Algorithm Analysis"
"0","chapter-3","Algorithm Analysis"
"200","chapter-3","Algorithm Analysis"
"400","chapter-3","Algorithm Analysis"
"600","chapter-3","Algorithm Analysis"
"800","chapter-3","Algorithm Analysis"
"1000","chapter-3","Algorithm Analysis"
"1200","chapter-3","Algorithm Analysis"
"1400","chapter-3","Algorithm Analysis"
"Figure 3.1 Two views of a graph illustrating the growth rates for six equations.","chapter-3","Algorithm Analysis"
"The bottom view shows in detail the lower-left portion of the top view. The hor-","chapter-3","Algorithm Analysis"
"izontal axis represents input size. The vertical axis can represent time, space, or","chapter-3","Algorithm Analysis"
"any other measure of cost.","chapter-3","Algorithm Analysis"
"operation for variable sum. We can assume that incrementing takes constant","chapter-3","Algorithm Analysis"
"time; call this time c2. (We can ignore the time required to initialize sum,","chapter-3","Algorithm Analysis"
"and to increment the loop counters i and j. In practice, these costs can","chapter-3","Algorithm Analysis"
"safely be bundled into time c2.) The total number of increment operations","chapter-3","Algorithm Analysis"
"is n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
". Thus, we say that the running time is T(n) = c2n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
".","chapter-3","Algorithm Analysis"
"58 Chap. 3 Algorithm Analysis","chapter-3","Algorithm Analysis"
"n log log n log n n n log n n","chapter-3","Algorithm Analysis"
"2 n","chapter-3","Algorithm Analysis"
"3 2","chapter-3","Algorithm Analysis"
"n","chapter-3","Algorithm Analysis"
"16 2 4 2","chapter-3","Algorithm Analysis"
"4 4 · 2","chapter-3","Algorithm Analysis"
"4 = 2","chapter-3","Algorithm Analysis"
"6 2","chapter-3","Algorithm Analysis"
"8 2","chapter-3","Algorithm Analysis"
"12 2","chapter-3","Algorithm Analysis"
"16","chapter-3","Algorithm Analysis"
"256 3 8 2","chapter-3","Algorithm Analysis"
"8 8 · 2","chapter-3","Algorithm Analysis"
"8 = 2","chapter-3","Algorithm Analysis"
"11 2","chapter-3","Algorithm Analysis"
"16 2","chapter-3","Algorithm Analysis"
"24 2","chapter-3","Algorithm Analysis"
"256","chapter-3","Algorithm Analysis"
"1024 ≈ 3.3 10 2","chapter-3","Algorithm Analysis"
"10 10 · 2","chapter-3","Algorithm Analysis"
"10 ≈ 2","chapter-3","Algorithm Analysis"
"13 2","chapter-3","Algorithm Analysis"
"20 2","chapter-3","Algorithm Analysis"
"30 2","chapter-3","Algorithm Analysis"
"1024","chapter-3","Algorithm Analysis"
"64K 4 16 2","chapter-3","Algorithm Analysis"
"16 16 · 2","chapter-3","Algorithm Analysis"
"16 = 2","chapter-3","Algorithm Analysis"
"20 2","chapter-3","Algorithm Analysis"
"32 2","chapter-3","Algorithm Analysis"
"48 2","chapter-3","Algorithm Analysis"
"64K","chapter-3","Algorithm Analysis"
"1M ≈ 4.3 20 2","chapter-3","Algorithm Analysis"
"20 20 · 2","chapter-3","Algorithm Analysis"
"20 ≈ 2","chapter-3","Algorithm Analysis"
"24 2","chapter-3","Algorithm Analysis"
"40 2","chapter-3","Algorithm Analysis"
"60 2","chapter-3","Algorithm Analysis"
"1M","chapter-3","Algorithm Analysis"
"1G ≈ 4.9 30 2","chapter-3","Algorithm Analysis"
"30 30 · 2","chapter-3","Algorithm Analysis"
"30 ≈ 2","chapter-3","Algorithm Analysis"
"35 2","chapter-3","Algorithm Analysis"
"60 2","chapter-3","Algorithm Analysis"
"90 2","chapter-3","Algorithm Analysis"
"1G","chapter-3","Algorithm Analysis"
"Figure 3.2 Costs for growth rates representative of most computer algorithms.","chapter-3","Algorithm Analysis"
"The growth rate for an algorithm is the rate at which the cost of the algorithm","chapter-3","Algorithm Analysis"
"grows as the size of its input grows. Figure 3.1 shows a graph for six equations, each","chapter-3","Algorithm Analysis"
"meant to describe the running time for a particular program or algorithm. A variety","chapter-3","Algorithm Analysis"
"of growth rates representative of typical algorithms are shown. The two equations","chapter-3","Algorithm Analysis"
"labeled 10n and 20n are graphed by straight lines. A growth rate of cn (for c any","chapter-3","Algorithm Analysis"
"positive constant) is often referred to as a linear growth rate or running time. This","chapter-3","Algorithm Analysis"
"means that as the value of n grows, the running time of the algorithm grows in the","chapter-3","Algorithm Analysis"
"same proportion. Doubling the value of n roughly doubles the running time. An","chapter-3","Algorithm Analysis"
"algorithm whose running-time equation has a highest-order term containing a factor","chapter-3","Algorithm Analysis"
"of n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"is said to have a quadratic growth rate. In Figure 3.1, the line labeled 2n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"represents a quadratic growth rate. The line labeled 2","chapter-3","Algorithm Analysis"
"n","chapter-3","Algorithm Analysis"
"represents an exponential","chapter-3","Algorithm Analysis"
"growth rate. This name comes from the fact that n appears in the exponent. The","chapter-3","Algorithm Analysis"
"line labeled n! is also growing exponentially.","chapter-3","Algorithm Analysis"
"As you can see from Figure 3.1, the difference between an algorithm whose","chapter-3","Algorithm Analysis"
"running time has cost T(n) = 10n and another with cost T(n) = 2n","chapter-3","Algorithm Analysis"
"2 becomes","chapter-3","Algorithm Analysis"
"tremendous as n grows. For n > 5, the algorithm with running time T(n) = 2n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"is","chapter-3","Algorithm Analysis"
"already much slower. This is despite the fact that 10n has a greater constant factor","chapter-3","Algorithm Analysis"
"than 2n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
". Comparing the two curves marked 20n and 2n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"shows that changing the","chapter-3","Algorithm Analysis"
"constant factor for one of the equations only shifts the point at which the two curves","chapter-3","Algorithm Analysis"
"cross. For n > 10, the algorithm with cost T(n) = 2n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"is slower than the algorithm","chapter-3","Algorithm Analysis"
"with cost T(n) = 20n. This graph also shows that the equation T(n) = 5n log n","chapter-3","Algorithm Analysis"
"grows somewhat more quickly than both T(n) = 10n and T(n) = 20n, but not","chapter-3","Algorithm Analysis"
"nearly so quickly as the equation T(n) = 2n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
". For constants a, b > 1, n","chapter-3","Algorithm Analysis"
"a grows","chapter-3","Algorithm Analysis"
"faster than either logb n or log n","chapter-3","Algorithm Analysis"
"b","chapter-3","Algorithm Analysis"
". Finally, algorithms with cost T(n) = 2n or","chapter-3","Algorithm Analysis"
"T(n) = n! are prohibitively expensive for even modest values of n. Note that for","chapter-3","Algorithm Analysis"
"constants a, b ≥ 1, a","chapter-3","Algorithm Analysis"
"n grows faster than n","chapter-3","Algorithm Analysis"
"b","chapter-3","Algorithm Analysis"
".","chapter-3","Algorithm Analysis"
"We can get some further insight into relative growth rates for various algorithms","chapter-3","Algorithm Analysis"
"from Figure 3.2. Most of the growth rates that appear in typical algorithms are","chapter-3","Algorithm Analysis"
"shown, along with some representative input sizes. Once again, we see that the","chapter-3","Algorithm Analysis"
"growth rate has a tremendous effect on the resources consumed by an algorithm.","chapter-3","Algorithm Analysis"
"Sec. 3.2 Best, Worst, and Average Cases 59","chapter-3","Algorithm Analysis"
"3.2 Best, Worst, and Average Cases","chapter-3","Algorithm Analysis"
"Consider the problem of finding the factorial of n. For this problem, there is only","chapter-3","Algorithm Analysis"
"one input of a given “size” (that is, there is only a single instance for each size of","chapter-3","Algorithm Analysis"
"n). Now consider our largest-value sequential search algorithm of Example 3.1,","chapter-3","Algorithm Analysis"
"which always examines every array value. This algorithm works on many inputs of","chapter-3","Algorithm Analysis"
"a given size n. That is, there are many possible arrays of any given size. However,","chapter-3","Algorithm Analysis"
"no matter what array of size n that the algorithm looks at, its cost will always be","chapter-3","Algorithm Analysis"
"the same in that it always looks at every element in the array one time.","chapter-3","Algorithm Analysis"
"For some algorithms, different inputs of a given size require different amounts","chapter-3","Algorithm Analysis"
"of time. For example, consider the problem of searching an array containing n","chapter-3","Algorithm Analysis"
"integers to find the one with a particular value K (assume that K appears exactly","chapter-3","Algorithm Analysis"
"once in the array). The sequential search algorithm begins at the first position in","chapter-3","Algorithm Analysis"
"the array and looks at each value in turn until K is found. Once K is found, the","chapter-3","Algorithm Analysis"
"algorithm stops. This is different from the largest-value sequential search algorithm","chapter-3","Algorithm Analysis"
"of Example 3.1, which always examines every array value.","chapter-3","Algorithm Analysis"
"There is a wide range of possible running times for the sequential search alg-","chapter-3","Algorithm Analysis"
"orithm. The first integer in the array could have value K, and so only one integer","chapter-3","Algorithm Analysis"
"is examined. In this case the running time is short. This is the best case for this","chapter-3","Algorithm Analysis"
"algorithm, because it is not possible for sequential search to look at less than one","chapter-3","Algorithm Analysis"
"value. Alternatively, if the last position in the array contains K, then the running","chapter-3","Algorithm Analysis"
"time is relatively long, because the algorithm must examine n values. This is the","chapter-3","Algorithm Analysis"
"worst case for this algorithm, because sequential search never looks at more than","chapter-3","Algorithm Analysis"
"n values. If we implement sequential search as a program and run it many times","chapter-3","Algorithm Analysis"
"on many different arrays of size n, or search for many different values of K within","chapter-3","Algorithm Analysis"
"the same array, we expect the algorithm on average to go halfway through the array","chapter-3","Algorithm Analysis"
"before finding the value we seek. On average, the algorithm examines about n/2","chapter-3","Algorithm Analysis"
"values. We call this the average case for this algorithm.","chapter-3","Algorithm Analysis"
"When analyzing an algorithm, should we study the best, worst, or average case?","chapter-3","Algorithm Analysis"
"Normally we are not interested in the best case, because this might happen only","chapter-3","Algorithm Analysis"
"rarely and generally is too optimistic for a fair characterization of the algorithm’s","chapter-3","Algorithm Analysis"
"running time. In other words, analysis based on the best case is not likely to be","chapter-3","Algorithm Analysis"
"representative of the behavior of the algorithm. However, there are rare instances","chapter-3","Algorithm Analysis"
"where a best-case analysis is useful — in particular, when the best case has high","chapter-3","Algorithm Analysis"
"probability of occurring. In Chapter 7 you will see some examples where taking","chapter-3","Algorithm Analysis"
"advantage of the best-case running time for one sorting algorithm makes a second","chapter-3","Algorithm Analysis"
"more efficient.","chapter-3","Algorithm Analysis"
"How about the worst case? The advantage to analyzing the worst case is that","chapter-3","Algorithm Analysis"
"you know for certain that the algorithm must perform at least that well. This is es-","chapter-3","Algorithm Analysis"
"pecially important for real-time applications, such as for the computers that monitor","chapter-3","Algorithm Analysis"
"an air traffic control system. Here, it would not be acceptable to use an algorithm","chapter-3","Algorithm Analysis"
"60 Chap. 3 Algorithm Analysis","chapter-3","Algorithm Analysis"
"that can handle n airplanes quickly enough most of the time, but which fails to","chapter-3","Algorithm Analysis"
"perform quickly enough when all n airplanes are coming from the same direction.","chapter-3","Algorithm Analysis"
"For other applications — particularly when we wish to aggregate the cost of","chapter-3","Algorithm Analysis"
"running the program many times on many different inputs — worst-case analy-","chapter-3","Algorithm Analysis"
"sis might not be a representative measure of the algorithm’s performance. Often","chapter-3","Algorithm Analysis"
"we prefer to know the average-case running time. This means that we would like","chapter-3","Algorithm Analysis"
"to know the typical behavior of the algorithm on inputs of size n. Unfortunately,","chapter-3","Algorithm Analysis"
"average-case analysis is not always possible. Average-case analysis first requires","chapter-3","Algorithm Analysis"
"that we understand how the actual inputs to the program (and their costs) are dis-","chapter-3","Algorithm Analysis"
"tributed with respect to the set of all possible inputs to the program. For example, it","chapter-3","Algorithm Analysis"
"was stated previously that the sequential search algorithm on average examines half","chapter-3","Algorithm Analysis"
"of the array values. This is only true if the element with value K is equally likely","chapter-3","Algorithm Analysis"
"to appear in any position in the array. If this assumption is not correct, then the","chapter-3","Algorithm Analysis"
"algorithm does not necessarily examine half of the array values in the average case.","chapter-3","Algorithm Analysis"
"See Section 9.2 for further discussion regarding the effects of data distribution on","chapter-3","Algorithm Analysis"
"the sequential search algorithm.","chapter-3","Algorithm Analysis"
"The characteristics of a data distribution have a significant effect on many","chapter-3","Algorithm Analysis"
"search algorithms, such as those based on hashing (Section 9.4) and search trees","chapter-3","Algorithm Analysis"
"(e.g., see Section 5.4). Incorrect assumptions about data distribution can have dis-","chapter-3","Algorithm Analysis"
"astrous consequences on a program’s space or time performance. Unusual data","chapter-3","Algorithm Analysis"
"distributions can also be used to advantage, as shown in Section 9.2.","chapter-3","Algorithm Analysis"
"In summary, for real-time applications we are likely to prefer a worst-case anal-","chapter-3","Algorithm Analysis"
"ysis of an algorithm. Otherwise, we often desire an average-case analysis if we","chapter-3","Algorithm Analysis"
"know enough about the distribution of our input to compute the average case. If","chapter-3","Algorithm Analysis"
"not, then we must resort to worst-case analysis.","chapter-3","Algorithm Analysis"
"3.3 A Faster Computer, or a Faster Algorithm?","chapter-3","Algorithm Analysis"
"Imagine that you have a problem to solve, and you know of an algorithm whose","chapter-3","Algorithm Analysis"
"running time is proportional to n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
". Unfortunately, the resulting program takes ten","chapter-3","Algorithm Analysis"
"times too long to run. If you replace your current computer with a new one that","chapter-3","Algorithm Analysis"
"is ten times faster, will the n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"algorithm become acceptable? If the problem size","chapter-3","Algorithm Analysis"
"remains the same, then perhaps the faster computer will allow you to get your work","chapter-3","Algorithm Analysis"
"done quickly enough even with an algorithm having a high growth rate. But a funny","chapter-3","Algorithm Analysis"
"thing happens to most people who get a faster computer. They don’t run the same","chapter-3","Algorithm Analysis"
"problem faster. They run a bigger problem! Say that on your old computer you","chapter-3","Algorithm Analysis"
"were content to sort 10,000 records because that could be done by the computer","chapter-3","Algorithm Analysis"
"during your lunch break. On your new computer you might hope to sort 100,000","chapter-3","Algorithm Analysis"
"records in the same time. You won’t be back from lunch any sooner, so you are","chapter-3","Algorithm Analysis"
"better off solving a larger problem. And because the new machine is ten times","chapter-3","Algorithm Analysis"
"faster, you would like to sort ten times as many records.","chapter-3","Algorithm Analysis"
"Sec. 3.3 A Faster Computer, or a Faster Algorithm? 61","chapter-3","Algorithm Analysis"
"f(n) n n","chapter-3","Algorithm Analysis"
"0 Change n","chapter-3","Algorithm Analysis"
"0","chapter-3","Algorithm Analysis"
"/n","chapter-3","Algorithm Analysis"
"10n 1000 10, 000 n","chapter-3","Algorithm Analysis"
"0 = 10n 10","chapter-3","Algorithm Analysis"
"20n 500 5000 n","chapter-3","Algorithm Analysis"
"0 = 10n 10","chapter-3","Algorithm Analysis"
"5n log n 250 1842 √","chapter-3","Algorithm Analysis"
"10n < n","chapter-3","Algorithm Analysis"
"0 < 10n 7.37","chapter-3","Algorithm Analysis"
"2n2 70 223 n","chapter-3","Algorithm Analysis"
"0 =","chapter-3","Algorithm Analysis"
"√","chapter-3","Algorithm Analysis"
"10n 3.16","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"n 13 16 n","chapter-3","Algorithm Analysis"
"0 = n + 3 −−","chapter-3","Algorithm Analysis"
"Figure 3.3 The increase in problem size that can be run in a fixed period of time","chapter-3","Algorithm Analysis"
"on a computer that is ten times faster. The first column lists the right-hand sides","chapter-3","Algorithm Analysis"
"for each of five growth rate equations from Figure 3.1. For the purpose of this","chapter-3","Algorithm Analysis"
"example, arbitrarily assume that the old machine can run 10,000 basic operations","chapter-3","Algorithm Analysis"
"in one hour. The second column shows the maximum value for n that can be run","chapter-3","Algorithm Analysis"
"in 10,000 basic operations on the old machine. The third column shows the value","chapter-3","Algorithm Analysis"
"for n","chapter-3","Algorithm Analysis"
"0","chapter-3","Algorithm Analysis"
", the new maximum size for the problem that can be run in the same time","chapter-3","Algorithm Analysis"
"on the new machine that is ten times faster. Variable n","chapter-3","Algorithm Analysis"
"0","chapter-3","Algorithm Analysis"
"is the greatest size for the","chapter-3","Algorithm Analysis"
"problem that can run in 100,000 basic operations. The fourth column shows how","chapter-3","Algorithm Analysis"
"the size of n changed to become n","chapter-3","Algorithm Analysis"
"0 on the new machine. The fifth column shows","chapter-3","Algorithm Analysis"
"the increase in the problem size as the ratio of n","chapter-3","Algorithm Analysis"
"0","chapter-3","Algorithm Analysis"
"to n.","chapter-3","Algorithm Analysis"
"If your algorithm’s growth rate is linear (i.e., if the equation that describes the","chapter-3","Algorithm Analysis"
"running time on input size n is T(n) = cn for some constant c), then 100,000","chapter-3","Algorithm Analysis"
"records on the new machine will be sorted in the same time as 10,000 records on","chapter-3","Algorithm Analysis"
"the old machine. If the algorithm’s growth rate is greater than cn, such as c1n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
",","chapter-3","Algorithm Analysis"
"then you will not be able to do a problem ten times the size in the same amount of","chapter-3","Algorithm Analysis"
"time on a machine that is ten times faster.","chapter-3","Algorithm Analysis"
"How much larger a problem can be solved in a given amount of time by a faster","chapter-3","Algorithm Analysis"
"computer? Assume that the new machine is ten times faster than the old. Say that","chapter-3","Algorithm Analysis"
"the old machine could solve a problem of size n in an hour. What is the largest","chapter-3","Algorithm Analysis"
"problem that the new machine can solve in one hour? Figure 3.3 shows how large","chapter-3","Algorithm Analysis"
"a problem can be solved on the two machines for five of the running-time functions","chapter-3","Algorithm Analysis"
"from Figure 3.1.","chapter-3","Algorithm Analysis"
"This table illustrates many important points. The first two equations are both","chapter-3","Algorithm Analysis"
"linear; only the value of the constant factor has changed. In both cases, the machine","chapter-3","Algorithm Analysis"
"that is ten times faster gives an increase in problem size by a factor of ten. In other","chapter-3","Algorithm Analysis"
"words, while the value of the constant does affect the absolute size of the problem","chapter-3","Algorithm Analysis"
"that can be solved in a fixed amount of time, it does not affect the improvement in","chapter-3","Algorithm Analysis"
"problem size (as a proportion to the original size) gained by a faster computer. This","chapter-3","Algorithm Analysis"
"relationship holds true regardless of the algorithm’s growth rate: Constant factors","chapter-3","Algorithm Analysis"
"never affect the relative improvement gained by a faster computer.","chapter-3","Algorithm Analysis"
"An algorithm with time equation T(n) = 2n","chapter-3","Algorithm Analysis"
"2 does not receive nearly as great","chapter-3","Algorithm Analysis"
"an improvement from the faster machine as an algorithm with linear growth rate.","chapter-3","Algorithm Analysis"
"Instead of an improvement by a factor of ten, the improvement is only the square","chapter-3","Algorithm Analysis"
"62 Chap. 3 Algorithm Analysis","chapter-3","Algorithm Analysis"
"root of that: √","chapter-3","Algorithm Analysis"
"10 ≈ 3.16. Thus, the algorithm with higher growth rate not only","chapter-3","Algorithm Analysis"
"solves a smaller problem in a given time in the first place, it also receives less of","chapter-3","Algorithm Analysis"
"a speedup from a faster computer. As computers get ever faster, the disparity in","chapter-3","Algorithm Analysis"
"problem sizes becomes ever greater.","chapter-3","Algorithm Analysis"
"The algorithm with growth rate T(n) = 5n log n improves by a greater amount","chapter-3","Algorithm Analysis"
"than the one with quadratic growth rate, but not by as great an amount as the algo-","chapter-3","Algorithm Analysis"
"rithms with linear growth rates.","chapter-3","Algorithm Analysis"
"Note that something special happens in the case of the algorithm whose running","chapter-3","Algorithm Analysis"
"time grows exponentially. In Figure 3.1, the curve for the algorithm whose time is","chapter-3","Algorithm Analysis"
"proportional to 2","chapter-3","Algorithm Analysis"
"n goes up very quickly. In Figure 3.3, the increase in problem","chapter-3","Algorithm Analysis"
"size on the machine ten times as fast is shown to be about n + 3 (to be precise,","chapter-3","Algorithm Analysis"
"it is n + log2 10). The increase in problem size for an algorithm with exponential","chapter-3","Algorithm Analysis"
"growth rate is by a constant addition, not by a multiplicative factor. Because the","chapter-3","Algorithm Analysis"
"old value of n was 13, the new problem size is 16. If next year you buy another","chapter-3","Algorithm Analysis"
"computer ten times faster yet, then the new computer (100 times faster than the","chapter-3","Algorithm Analysis"
"original computer) will only run a problem of size 19. If you had a second program","chapter-3","Algorithm Analysis"
"whose growth rate is 2","chapter-3","Algorithm Analysis"
"n","chapter-3","Algorithm Analysis"
"and for which the original computer could run a problem","chapter-3","Algorithm Analysis"
"of size 1000 in an hour, than a machine ten times faster can run a problem only of","chapter-3","Algorithm Analysis"
"size 1003 in an hour! Thus, an exponential growth rate is radically different than","chapter-3","Algorithm Analysis"
"the other growth rates shown in Figure 3.3. The significance of this difference is","chapter-3","Algorithm Analysis"
"explored in Chapter 17.","chapter-3","Algorithm Analysis"
"Instead of buying a faster computer, consider what happens if you replace an","chapter-3","Algorithm Analysis"
"algorithm whose running time is proportional to n","chapter-3","Algorithm Analysis"
"2 with a new algorithm whose","chapter-3","Algorithm Analysis"
"running time is proportional to n log n. In the graph of Figure 3.1, a fixed amount of","chapter-3","Algorithm Analysis"
"time would appear as a horizontal line. If the line for the amount of time available","chapter-3","Algorithm Analysis"
"to solve your problem is above the point at which the curves for the two growth","chapter-3","Algorithm Analysis"
"rates in question meet, then the algorithm whose running time grows less quickly","chapter-3","Algorithm Analysis"
"is faster. An algorithm with running time T(n) = n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"requires 1024 × 1024 =","chapter-3","Algorithm Analysis"
"1, 048, 576 time steps for an input of size n = 1024. An algorithm with running","chapter-3","Algorithm Analysis"
"time T(n) = n log n requires 1024 × 10 = 10, 240 time steps for an input of","chapter-3","Algorithm Analysis"
"size n = 1024, which is an improvement of much more than a factor of ten when","chapter-3","Algorithm Analysis"
"compared to the algorithm with running time T(n) = n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
". Because n","chapter-3","Algorithm Analysis"
"2 > 10n log n","chapter-3","Algorithm Analysis"
"whenever n > 58, if the typical problem size is larger than 58 for this example, then","chapter-3","Algorithm Analysis"
"you would be much better off changing algorithms instead of buying a computer","chapter-3","Algorithm Analysis"
"ten times faster. Furthermore, when you do buy a faster computer, an algorithm","chapter-3","Algorithm Analysis"
"with a slower growth rate provides a greater benefit in terms of larger problem size","chapter-3","Algorithm Analysis"
"that can run in a certain time on the new computer.","chapter-3","Algorithm Analysis"
"Sec. 3.4 Asymptotic Analysis 63","chapter-3","Algorithm Analysis"
"3.4 Asymptotic Analysis","chapter-3","Algorithm Analysis"
"Despite the larger constant for the curve labeled 10n in Figure 3.1, 2n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"crosses","chapter-3","Algorithm Analysis"
"it at the relatively small value of n = 5. What if we double the value of the","chapter-3","Algorithm Analysis"
"constant in front of the linear equation? As shown in the graph, 20n is surpassed","chapter-3","Algorithm Analysis"
"by 2n","chapter-3","Algorithm Analysis"
"2 once n = 10. The additional factor of two for the linear growth rate does","chapter-3","Algorithm Analysis"
"not much matter. It only doubles the x-coordinate for the intersection point. In","chapter-3","Algorithm Analysis"
"general, changes to a constant factor in either equation only shift where the two","chapter-3","Algorithm Analysis"
"curves cross, not whether the two curves cross.","chapter-3","Algorithm Analysis"
"When you buy a faster computer or a faster compiler, the new problem size","chapter-3","Algorithm Analysis"
"that can be run in a given amount of time for a given growth rate is larger by the","chapter-3","Algorithm Analysis"
"same factor, regardless of the constant on the running-time equation. The time","chapter-3","Algorithm Analysis"
"curves for two algorithms with different growth rates still cross, regardless of their","chapter-3","Algorithm Analysis"
"running-time equation constants. For these reasons, we usually ignore the con-","chapter-3","Algorithm Analysis"
"stants when we want an estimate of the growth rate for the running time or other","chapter-3","Algorithm Analysis"
"resource requirements of an algorithm. This simplifies the analysis and keeps us","chapter-3","Algorithm Analysis"
"thinking about the most important aspect: the growth rate. This is called asymp-","chapter-3","Algorithm Analysis"
"totic algorithm analysis. To be precise, asymptotic analysis refers to the study of","chapter-3","Algorithm Analysis"
"an algorithm as the input size “gets big” or reaches a limit (in the calculus sense).","chapter-3","Algorithm Analysis"
"However, it has proved to be so useful to ignore all constant factors that asymptotic","chapter-3","Algorithm Analysis"
"analysis is used for most algorithm comparisons.","chapter-3","Algorithm Analysis"
"It is not always reasonable to ignore the constants. When comparing algorithms","chapter-3","Algorithm Analysis"
"meant to run on small values of n, the constant can have a large effect. For exam-","chapter-3","Algorithm Analysis"
"ple, if the problem is to sort a collection of exactly five records, then an algorithm","chapter-3","Algorithm Analysis"
"designed for sorting thousands of records is probably not appropriate, even if its","chapter-3","Algorithm Analysis"
"asymptotic analysis indicates good performance. There are rare cases where the","chapter-3","Algorithm Analysis"
"constants for two algorithms under comparison can differ by a factor of 1000 or","chapter-3","Algorithm Analysis"
"more, making the one with lower growth rate impractical for most purposes due to","chapter-3","Algorithm Analysis"
"its large constant. Asymptotic analysis is a form of “back of the envelope” esti-","chapter-3","Algorithm Analysis"
"mation for algorithm resource consumption. It provides a simplified model of the","chapter-3","Algorithm Analysis"
"running time or other resource needs of an algorithm. This simplification usually","chapter-3","Algorithm Analysis"
"helps you understand the behavior of your algorithms. Just be aware of the limi-","chapter-3","Algorithm Analysis"
"tations to asymptotic analysis in the rare situation where the constant is important.","chapter-3","Algorithm Analysis"
"3.4.1 Upper Bounds","chapter-3","Algorithm Analysis"
"Several terms are used to describe the running-time equation for an algorithm.","chapter-3","Algorithm Analysis"
"These terms — and their associated symbols — indicate precisely what aspect of","chapter-3","Algorithm Analysis"
"the algorithm’s behavior is being described. One is the upper bound for the growth","chapter-3","Algorithm Analysis"
"of the algorithm’s running time. It indicates the upper or highest growth rate that","chapter-3","Algorithm Analysis"
"the algorithm can have.","chapter-3","Algorithm Analysis"
"64 Chap. 3 Algorithm Analysis","chapter-3","Algorithm Analysis"
"Because the phrase “has an upper bound to its growth rate of f(n)” is long and","chapter-3","Algorithm Analysis"
"often used when discussing algorithms, we adopt a special notation, called big-Oh","chapter-3","Algorithm Analysis"
"notation. If the upper bound for an algorithm’s growth rate (for, say, the worst","chapter-3","Algorithm Analysis"
"case) is f(n), then we would write that this algorithm is “in the set O(f(n))in the","chapter-3","Algorithm Analysis"
"worst case” (or just “in O(f(n))in the worst case”). For example, if n","chapter-3","Algorithm Analysis"
"2 grows as","chapter-3","Algorithm Analysis"
"fast as T(n) (the running time of our algorithm) for the worst-case input, we would","chapter-3","Algorithm Analysis"
"say the algorithm is “in O(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
") in the worst case.”","chapter-3","Algorithm Analysis"
"The following is a precise definition for an upper bound. T(n) represents the","chapter-3","Algorithm Analysis"
"true running time of the algorithm. f(n) is some expression for the upper bound.","chapter-3","Algorithm Analysis"
"For T(n) a non-negatively valued function, T(n) is in set O(f(n))","chapter-3","Algorithm Analysis"
"if there exist two positive constants c and n0 such that T(n) ≤ cf(n)","chapter-3","Algorithm Analysis"
"for all n > n0.","chapter-3","Algorithm Analysis"
"Constant n0 is the smallest value of n for which the claim of an upper bound holds","chapter-3","Algorithm Analysis"
"true. Usually n0 is small, such as 1, but does not need to be. You must also be","chapter-3","Algorithm Analysis"
"able to pick some constant c, but it is irrelevant what the value for c actually is.","chapter-3","Algorithm Analysis"
"In other words, the definition says that for all inputs of the type in question (such","chapter-3","Algorithm Analysis"
"as the worst case for all inputs of size n) that are large enough (i.e., n > n0), the","chapter-3","Algorithm Analysis"
"algorithm always executes in less than cf(n) steps for some constant c.","chapter-3","Algorithm Analysis"
"Example 3.4 Consider the sequential search algorithm for finding a spec-","chapter-3","Algorithm Analysis"
"ified value in an array of integers. If visiting and examining one value in","chapter-3","Algorithm Analysis"
"the array requires cs steps where cs is a positive number, and if the value","chapter-3","Algorithm Analysis"
"we search for has equal probability of appearing in any position in the ar-","chapter-3","Algorithm Analysis"
"ray, then in the average case T(n) = csn/2. For all values of n > 1,","chapter-3","Algorithm Analysis"
"csn/2 ≤ csn. Therefore, by the definition, T(n) is in O(n) for n0 = 1 and","chapter-3","Algorithm Analysis"
"c = cs.","chapter-3","Algorithm Analysis"
"Example 3.5 For a particular algorithm, T(n) = c1n","chapter-3","Algorithm Analysis"
"2 + c2n in the av-","chapter-3","Algorithm Analysis"
"erage case where c1 and c2 are positive numbers. Then, c1n","chapter-3","Algorithm Analysis"
"2 + c2n ≤","chapter-3","Algorithm Analysis"
"c1n","chapter-3","Algorithm Analysis"
"2 + c2n","chapter-3","Algorithm Analysis"
"2 ≤ (c1 + c2)n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"for all n > 1. So, T(n) ≤ cn2","chapter-3","Algorithm Analysis"
"for c = c1 + c2,","chapter-3","Algorithm Analysis"
"and n0 = 1. Therefore, T(n) is in O(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
") by the second definition.","chapter-3","Algorithm Analysis"
"Example 3.6 Assigning the value from the first position of an array to","chapter-3","Algorithm Analysis"
"a variable takes constant time regardless of the size of the array. Thus,","chapter-3","Algorithm Analysis"
"T(n) = c (for the best, worst, and average cases). We could say in this","chapter-3","Algorithm Analysis"
"case that T(n) is in O(c). However, it is traditional to say that an algorithm","chapter-3","Algorithm Analysis"
"whose running time has a constant upper bound is in O(1).","chapter-3","Algorithm Analysis"
"Sec. 3.4 Asymptotic Analysis 65","chapter-3","Algorithm Analysis"
"If someone asked you out of the blue “Who is the best?” your natural reaction","chapter-3","Algorithm Analysis"
"should be to reply “Best at what?” In the same way, if you are asked “What is","chapter-3","Algorithm Analysis"
"the growth rate of this algorithm,” you would need to ask “When? Best case?","chapter-3","Algorithm Analysis"
"Average case? Or worst case?” Some algorithms have the same behavior no matter","chapter-3","Algorithm Analysis"
"which input instance they receive. An example is finding the maximum in an array","chapter-3","Algorithm Analysis"
"of integers. But for many algorithms, it makes a big difference, such as when","chapter-3","Algorithm Analysis"
"searching an unsorted array for a particular value. So any statement about the","chapter-3","Algorithm Analysis"
"upper bound of an algorithm must be in the context of some class of inputs of size","chapter-3","Algorithm Analysis"
"n. We measure this upper bound nearly always on the best-case, average-case, or","chapter-3","Algorithm Analysis"
"worst-case inputs. Thus, we cannot say, “this algorithm has an upper bound to","chapter-3","Algorithm Analysis"
"its growth rate of n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
".” We must say something like, “this algorithm has an upper","chapter-3","Algorithm Analysis"
"bound to its growth rate of n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"in the average case.”","chapter-3","Algorithm Analysis"
"Knowing that something is in O(f(n)) says only how bad things can be. Per-","chapter-3","Algorithm Analysis"
"haps things are not nearly so bad. Because sequential search is in O(n) in the worst","chapter-3","Algorithm Analysis"
"case, it is also true to say that sequential search is in O(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"). But sequential search","chapter-3","Algorithm Analysis"
"is practical for large n, in a way that is not true for some other algorithms in O(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
").","chapter-3","Algorithm Analysis"
"We always seek to define the running time of an algorithm with the tightest (low-","chapter-3","Algorithm Analysis"
"est) possible upper bound. Thus, we prefer to say that sequential search is in O(n).","chapter-3","Algorithm Analysis"
"This also explains why the phrase “is in O(f(n))” or the notation “∈ O(f(n))” is","chapter-3","Algorithm Analysis"
"used instead of “is O(f(n))” or “= O(f(n)).” There is no strict equality to the use","chapter-3","Algorithm Analysis"
"of big-Oh notation. O(n) is in O(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"), but O(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
") is not in O(n).","chapter-3","Algorithm Analysis"
"3.4.2 Lower Bounds","chapter-3","Algorithm Analysis"
"Big-Oh notation describes an upper bound. In other words, big-Oh notation states","chapter-3","Algorithm Analysis"
"a claim about the greatest amount of some resource (usually time) that is required","chapter-3","Algorithm Analysis"
"by an algorithm for some class of inputs of size n (typically the worst such input,","chapter-3","Algorithm Analysis"
"the average of all possible inputs, or the best such input).","chapter-3","Algorithm Analysis"
"Similar notation is used to describe the least amount of a resource that an alg-","chapter-3","Algorithm Analysis"
"orithm needs for some class of input. Like big-Oh notation, this is a measure of the","chapter-3","Algorithm Analysis"
"algorithm’s growth rate. Like big-Oh notation, it works for any resource, but we","chapter-3","Algorithm Analysis"
"most often measure the least amount of time required. And again, like big-Oh no-","chapter-3","Algorithm Analysis"
"tation, we are measuring the resource required for some particular class of inputs:","chapter-3","Algorithm Analysis"
"the worst-, average-, or best-case input of size n.","chapter-3","Algorithm Analysis"
"The lower bound for an algorithm (or a problem, as explained later) is denoted","chapter-3","Algorithm Analysis"
"by the symbol Ω, pronounced “big-Omega” or just “Omega.” The following defi-","chapter-3","Algorithm Analysis"
"nition for Ω is symmetric with the definition of big-Oh.","chapter-3","Algorithm Analysis"
"For T(n) a non-negatively valued function, T(n) is in set Ω(g(n))","chapter-3","Algorithm Analysis"
"if there exist two positive constants c and n0 such that T(n) ≥ cg(n)","chapter-3","Algorithm Analysis"
"for all n > n0.","chapter-3","Algorithm Analysis"
"1","chapter-3","Algorithm Analysis"
"1 An alternate (non-equivalent) definition for Ω is","chapter-3","Algorithm Analysis"
"66 Chap. 3 Algorithm Analysis","chapter-3","Algorithm Analysis"
"Example 3.7 Assume T(n) = c1n","chapter-3","Algorithm Analysis"
"2 + c2n for c1 and c2 > 0. Then,","chapter-3","Algorithm Analysis"
"c1n","chapter-3","Algorithm Analysis"
"2 + c2n ≥ c1n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"for all n > 1. So, T(n) ≥ cn2","chapter-3","Algorithm Analysis"
"for c = c1 and n0 = 1. Therefore, T(n) is","chapter-3","Algorithm Analysis"
"in Ω(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
") by the definition.","chapter-3","Algorithm Analysis"
"It is also true that the equation of Example 3.7 is in Ω(n). However, as with","chapter-3","Algorithm Analysis"
"big-Oh notation, we wish to get the “tightest” (for Ω notation, the largest) bound","chapter-3","Algorithm Analysis"
"possible. Thus, we prefer to say that this running time is in Ω(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
").","chapter-3","Algorithm Analysis"
"Recall the sequential search algorithm to find a value K within an array of","chapter-3","Algorithm Analysis"
"integers. In the average and worst cases this algorithm is in Ω(n), because in both","chapter-3","Algorithm Analysis"
"the average and worst cases we must examine at least cn values (where c is 1/2 in","chapter-3","Algorithm Analysis"
"the average case and 1 in the worst case).","chapter-3","Algorithm Analysis"
"3.4.3 Θ Notation","chapter-3","Algorithm Analysis"
"The definitions for big-Oh and Ω give us ways to describe the upper bound for an","chapter-3","Algorithm Analysis"
"algorithm (if we can find an equation for the maximum cost of a particular class of","chapter-3","Algorithm Analysis"
"inputs of size n) and the lower bound for an algorithm (if we can find an equation","chapter-3","Algorithm Analysis"
"for the minimum cost for a particular class of inputs of size n). When the upper","chapter-3","Algorithm Analysis"
"and lower bounds are the same within a constant factor, we indicate this by using","chapter-3","Algorithm Analysis"
"Θ (big-Theta) notation. An algorithm is said to be Θ(h(n)) if it is in O(h(n)) and","chapter-3","Algorithm Analysis"
"T(n) is in the set Ω(g(n)) if there exists a positive constant c such that T(n) ≥","chapter-3","Algorithm Analysis"
"cg(n) for an infinite number of values for n.","chapter-3","Algorithm Analysis"
"This definition says that for an “interesting” number of cases, the algorithm takes at least cg(n)","chapter-3","Algorithm Analysis"
"time. Note that this definition is not symmetric with the definition of big-Oh. For g(n) to be a lower","chapter-3","Algorithm Analysis"
"bound, this definition does not require that T(n) ≥ cg(n) for all values of n greater than some","chapter-3","Algorithm Analysis"
"constant. It only requires that this happen often enough, in particular that it happen for an infinite","chapter-3","Algorithm Analysis"
"number of values for n. Motivation for this alternate definition can be found in the following example.","chapter-3","Algorithm Analysis"
"Assume a particular algorithm has the following behavior:","chapter-3","Algorithm Analysis"
"T(n) = ","chapter-3","Algorithm Analysis"
"n for all odd n ≥ 1","chapter-3","Algorithm Analysis"
"n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"/100 for all even n ≥ 0","chapter-3","Algorithm Analysis"
"From this definition, n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"/100 ≥","chapter-3","Algorithm Analysis"
"1","chapter-3","Algorithm Analysis"
"100n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"for all even n ≥ 0. So, T(n) ≥ cn2","chapter-3","Algorithm Analysis"
"for an infinite number","chapter-3","Algorithm Analysis"
"of values of n (i.e., for all even n) for c = 1/100. Therefore, T(n) is in Ω(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
") by the definition.","chapter-3","Algorithm Analysis"
"For this equation for T(n), it is true that all inputs of size n take at least cn time. But an infinite","chapter-3","Algorithm Analysis"
"number of inputs of size n take cn2","chapter-3","Algorithm Analysis"
"time, so we would like to say that the algorithm is in Ω(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
").","chapter-3","Algorithm Analysis"
"Unfortunately, using our first definition will yield a lower bound of Ω(n) because it is not possible to","chapter-3","Algorithm Analysis"
"pick constants c and n0 such that T(n) ≥ cn2","chapter-3","Algorithm Analysis"
"for all n > n0. The alternative definition does result","chapter-3","Algorithm Analysis"
"in a lower bound of Ω(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
") for this algorithm, which seems to fit common sense more closely. Fortu-","chapter-3","Algorithm Analysis"
"nately, few real algorithms or computer programs display the pathological behavior of this example.","chapter-3","Algorithm Analysis"
"Our first definition for Ω generally yields the expected result.","chapter-3","Algorithm Analysis"
"As you can see from this discussion, asymptotic bounds notation is not a law of nature. It is merely","chapter-3","Algorithm Analysis"
"a powerful modeling tool used to describe the behavior of algorithms.","chapter-3","Algorithm Analysis"
"Sec. 3.4 Asymptotic Analysis 67","chapter-3","Algorithm Analysis"
"it is in Ω(h(n)). Note that we drop the word “in” for Θ notation, because there","chapter-3","Algorithm Analysis"
"is a strict equality for two equations with the same Θ. In other words, if f(n) is","chapter-3","Algorithm Analysis"
"Θ(g(n)), then g(n) is Θ(f(n)).","chapter-3","Algorithm Analysis"
"Because the sequential search algorithm is both in O(n) and in Ω(n) in the","chapter-3","Algorithm Analysis"
"average case, we say it is Θ(n) in the average case.","chapter-3","Algorithm Analysis"
"Given an algebraic equation describing the time requirement for an algorithm,","chapter-3","Algorithm Analysis"
"the upper and lower bounds always meet. That is because in some sense we have","chapter-3","Algorithm Analysis"
"a perfect analysis for the algorithm, embodied by the running-time equation. For","chapter-3","Algorithm Analysis"
"many algorithms (or their instantiations as programs), it is easy to come up with","chapter-3","Algorithm Analysis"
"the equation that defines their runtime behavior. Most algorithms presented in this","chapter-3","Algorithm Analysis"
"book are well understood and we can almost always give a Θ analysis for them.","chapter-3","Algorithm Analysis"
"However, Chapter 17 discusses a whole class of algorithms for which we have no","chapter-3","Algorithm Analysis"
"Θ analysis, just some unsatisfying big-Oh and Ω analyses. Exercise 3.14 presents","chapter-3","Algorithm Analysis"
"a short, simple program fragment for which nobody currently knows the true upper","chapter-3","Algorithm Analysis"
"or lower bounds.","chapter-3","Algorithm Analysis"
"While some textbooks and programmers will casually say that an algorithm is","chapter-3","Algorithm Analysis"
"“order of” or “big-Oh” of some cost function, it is generally better to use Θ notation","chapter-3","Algorithm Analysis"
"rather than big-Oh notation whenever we have sufficient knowledge about an alg-","chapter-3","Algorithm Analysis"
"orithm to be sure that the upper and lower bounds indeed match. Throughout this","chapter-3","Algorithm Analysis"
"book, Θ notation will be used in preference to big-Oh notation whenever our state","chapter-3","Algorithm Analysis"
"of knowledge makes that possible. Limitations on our ability to analyze certain","chapter-3","Algorithm Analysis"
"algorithms may require use of big-Oh or Ω notations. In rare occasions when the","chapter-3","Algorithm Analysis"
"discussion is explicitly about the upper or lower bound of a problem or algorithm,","chapter-3","Algorithm Analysis"
"the corresponding notation will be used in preference to Θ notation.","chapter-3","Algorithm Analysis"
"3.4.4 Simplifying Rules","chapter-3","Algorithm Analysis"
"Once you determine the running-time equation for an algorithm, it really is a simple","chapter-3","Algorithm Analysis"
"matter to derive the big-Oh, Ω, and Θ expressions from the equation. You do not","chapter-3","Algorithm Analysis"
"need to resort to the formal definitions of asymptotic analysis. Instead, you can use","chapter-3","Algorithm Analysis"
"the following rules to determine the simplest form.","chapter-3","Algorithm Analysis"
"1. If f(n) is in O(g(n)) and g(n) is in O(h(n)), then f(n) is in O(h(n)).","chapter-3","Algorithm Analysis"
"2. If f(n) is in O(kg(n)) for any constant k > 0, then f(n) is in O(g(n)).","chapter-3","Algorithm Analysis"
"3. If f1(n) is in O(g1(n)) and f2(n) is in O(g2(n)), then f1(n) + f2(n) is in","chapter-3","Algorithm Analysis"
"O(max(g1(n), g2(n))).","chapter-3","Algorithm Analysis"
"4. If f1(n) is in O(g1(n)) and f2(n) is in O(g2(n)), then f1(n)f2(n) is in","chapter-3","Algorithm Analysis"
"O(g1(n)g2(n)).","chapter-3","Algorithm Analysis"
"The first rule says that if some function g(n) is an upper bound for your cost","chapter-3","Algorithm Analysis"
"function, then any upper bound for g(n) is also an upper bound for your cost func-","chapter-3","Algorithm Analysis"
"tion. A similar property holds true for Ω notation: If g(n) is a lower bound for your","chapter-3","Algorithm Analysis"
"68 Chap. 3 Algorithm Analysis","chapter-3","Algorithm Analysis"
"cost function, then any lower bound for g(n) is also a lower bound for your cost","chapter-3","Algorithm Analysis"
"function. Likewise for Θ notation.","chapter-3","Algorithm Analysis"
"The significance of rule (2) is that you can ignore any multiplicative constants","chapter-3","Algorithm Analysis"
"in your equations when using big-Oh notation. This rule also holds true for Ω and","chapter-3","Algorithm Analysis"
"Θ notations.","chapter-3","Algorithm Analysis"
"Rule (3) says that given two parts of a program run in sequence (whether two","chapter-3","Algorithm Analysis"
"statements or two sections of code), you need consider only the more expensive","chapter-3","Algorithm Analysis"
"part. This rule applies to Ω and Θ notations as well: For both, you need consider","chapter-3","Algorithm Analysis"
"only the more expensive part.","chapter-3","Algorithm Analysis"
"Rule (4) is used to analyze simple loops in programs. If some action is repeated","chapter-3","Algorithm Analysis"
"some number of times, and each repetition has the same cost, then the total cost is","chapter-3","Algorithm Analysis"
"the cost of the action multiplied by the number of times that the action takes place.","chapter-3","Algorithm Analysis"
"This rule applies to Ω and Θ notations as well.","chapter-3","Algorithm Analysis"
"Taking the first three rules collectively, you can ignore all constants and all","chapter-3","Algorithm Analysis"
"lower-order terms to determine the asymptotic growth rate for any cost function.","chapter-3","Algorithm Analysis"
"The advantages and dangers of ignoring constants were discussed near the begin-","chapter-3","Algorithm Analysis"
"ning of this section. Ignoring lower-order terms is reasonable when performing an","chapter-3","Algorithm Analysis"
"asymptotic analysis. The higher-order terms soon swamp the lower-order terms in","chapter-3","Algorithm Analysis"
"their contribution to the total cost as n becomes larger. Thus, if T(n) = 3n","chapter-3","Algorithm Analysis"
"4 + 5n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
",","chapter-3","Algorithm Analysis"
"then T(n) is in O(n","chapter-3","Algorithm Analysis"
"4","chapter-3","Algorithm Analysis"
"). The n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"term contributes relatively little to the total cost for","chapter-3","Algorithm Analysis"
"large n.","chapter-3","Algorithm Analysis"
"Throughout the rest of this book, these simplifying rules are used when dis-","chapter-3","Algorithm Analysis"
"cussing the cost for a program or algorithm.","chapter-3","Algorithm Analysis"
"3.4.5 Classifying Functions","chapter-3","Algorithm Analysis"
"Given functions f(n) and g(n) whose growth rates are expressed as algebraic equa-","chapter-3","Algorithm Analysis"
"tions, we might like to determine if one grows faster than the other. The best way","chapter-3","Algorithm Analysis"
"to do this is to take the limit of the two functions as n grows towards infinity,","chapter-3","Algorithm Analysis"
"limn→∞","chapter-3","Algorithm Analysis"
"f(n)","chapter-3","Algorithm Analysis"
"g(n)","chapter-3","Algorithm Analysis"
".","chapter-3","Algorithm Analysis"
"If the limit goes to ∞, then f(n) is in Ω(g(n)) because f(n) grows faster. If the","chapter-3","Algorithm Analysis"
"limit goes to zero, then f(n) is in O(g(n)) because g(n) grows faster. If the limit","chapter-3","Algorithm Analysis"
"goes to some constant other than zero, then f(n) = Θ(g(n)) because both grow at","chapter-3","Algorithm Analysis"
"the same rate.","chapter-3","Algorithm Analysis"
"Example 3.8 If f(n) = 2n log n and g(n) = n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
", is f(n) in O(g(n)),","chapter-3","Algorithm Analysis"
"Ω(g(n)), or Θ(g(n))? Because","chapter-3","Algorithm Analysis"
"n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"2n log n","chapter-3","Algorithm Analysis"
"=","chapter-3","Algorithm Analysis"
"n","chapter-3","Algorithm Analysis"
"2 log n","chapter-3","Algorithm Analysis"
",","chapter-3","Algorithm Analysis"
"Sec. 3.5 Calculating the Running Time for a Program 69","chapter-3","Algorithm Analysis"
"we easily see that","chapter-3","Algorithm Analysis"
"limn→∞","chapter-3","Algorithm Analysis"
"n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"2n log n","chapter-3","Algorithm Analysis"
"= ∞","chapter-3","Algorithm Analysis"
"because n grows faster than 2 log n. Thus, n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"is in Ω(2n log n).","chapter-3","Algorithm Analysis"
"3.5 Calculating the Running Time for a Program","chapter-3","Algorithm Analysis"
"This section presents the analysis for several simple code fragments.","chapter-3","Algorithm Analysis"
"Example 3.9 We begin with an analysis of a simple assignment to an","chapter-3","Algorithm Analysis"
"integer variable.","chapter-3","Algorithm Analysis"
"a = b;","chapter-3","Algorithm Analysis"
"Because the assignment statement takes constant time, it is Θ(1).","chapter-3","Algorithm Analysis"
"Example 3.10 Consider a simple for loop.","chapter-3","Algorithm Analysis"
"sum = 0;","chapter-3","Algorithm Analysis"
"for (i=1; i<=n; i++)","chapter-3","Algorithm Analysis"
"sum += n;","chapter-3","Algorithm Analysis"
"The first line is Θ(1). The for loop is repeated n times. The third","chapter-3","Algorithm Analysis"
"line takes constant time so, by simplifying rule (4) of Section 3.4.4, the","chapter-3","Algorithm Analysis"
"total cost for executing the two lines making up the for loop is Θ(n). By","chapter-3","Algorithm Analysis"
"rule (3), the cost of the entire code fragment is also Θ(n).","chapter-3","Algorithm Analysis"
"Example 3.11 We now analyze a code fragment with several for loops,","chapter-3","Algorithm Analysis"
"some of which are nested.","chapter-3","Algorithm Analysis"
"sum = 0;","chapter-3","Algorithm Analysis"
"for (j=1; j<=n; j++) // First for loop","chapter-3","Algorithm Analysis"
"for (i=1; i<=j; i++) // is a double loop","chapter-3","Algorithm Analysis"
"sum++;","chapter-3","Algorithm Analysis"
"for (k=0; k<n; k++) // Second for loop","chapter-3","Algorithm Analysis"
"A[k] = k;","chapter-3","Algorithm Analysis"
"This code fragment has three separate statements: the first assignment","chapter-3","Algorithm Analysis"
"statement and the two for loops. Again the assignment statement takes","chapter-3","Algorithm Analysis"
"constant time; call it c1. The second for loop is just like the one in Exam-","chapter-3","Algorithm Analysis"
"ple 3.10 and takes c2n = Θ(n) time.","chapter-3","Algorithm Analysis"
"The first for loop is a double loop and requires a special technique. We","chapter-3","Algorithm Analysis"
"work from the inside of the loop outward. The expression sum++ requires","chapter-3","Algorithm Analysis"
"constant time; call it c3. Because the inner for loop is executed i times, by","chapter-3","Algorithm Analysis"
"70 Chap. 3 Algorithm Analysis","chapter-3","Algorithm Analysis"
"simplifying rule (4) it has cost c3i. The outer for loop is executed n times,","chapter-3","Algorithm Analysis"
"but each time the cost of the inner loop is different because it costs c3i with","chapter-3","Algorithm Analysis"
"i changing each time. You should see that for the first execution of the outer","chapter-3","Algorithm Analysis"
"loop, i is 1. For the second execution of the outer loop, i is 2. Each time","chapter-3","Algorithm Analysis"
"through the outer loop, i becomes one greater, until the last time through","chapter-3","Algorithm Analysis"
"the loop when i = n. Thus, the total cost of the loop is c3 times the sum of","chapter-3","Algorithm Analysis"
"the integers 1 through n. From Equation 2.1, we know that","chapter-3","Algorithm Analysis"
"Xn","chapter-3","Algorithm Analysis"
"i=1","chapter-3","Algorithm Analysis"
"i =","chapter-3","Algorithm Analysis"
"n(n + 1)","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
",","chapter-3","Algorithm Analysis"
"which is Θ(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"). By simplifying rule (3), Θ(c1 + c2n + c3n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
") is simply","chapter-3","Algorithm Analysis"
"Θ(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
").","chapter-3","Algorithm Analysis"
"Example 3.12 Compare the asymptotic analysis for the following two","chapter-3","Algorithm Analysis"
"code fragments:","chapter-3","Algorithm Analysis"
"sum1 = 0;","chapter-3","Algorithm Analysis"
"for (i=1; i<=n; i++) // First double loop","chapter-3","Algorithm Analysis"
"for (j=1; j<=n; j++) // do n times","chapter-3","Algorithm Analysis"
"sum1++;","chapter-3","Algorithm Analysis"
"sum2 = 0;","chapter-3","Algorithm Analysis"
"for (i=1; i<=n; i++) // Second double loop","chapter-3","Algorithm Analysis"
"for (j=1; j<=i; j++) // do i times","chapter-3","Algorithm Analysis"
"sum2++;","chapter-3","Algorithm Analysis"
"In the first double loop, the inner for loop always executes n times.","chapter-3","Algorithm Analysis"
"Because the outer loop executes n times, it should be obvious that the state-","chapter-3","Algorithm Analysis"
"ment sum1++ is executed precisely n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"times. The second loop is similar","chapter-3","Algorithm Analysis"
"to the one analyzed in the previous example, with cost Pn","chapter-3","Algorithm Analysis"
"j=1 j. This is ap-","chapter-3","Algorithm Analysis"
"proximately 1","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
". Thus, both double loops cost Θ(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"), though the second","chapter-3","Algorithm Analysis"
"requires about half the time of the first.","chapter-3","Algorithm Analysis"
"Example 3.13 Not all doubly nested for loops are Θ(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"). The follow-","chapter-3","Algorithm Analysis"
"ing pair of nested loops illustrates this fact.","chapter-3","Algorithm Analysis"
"sum1 = 0;","chapter-3","Algorithm Analysis"
"for (k=1; k<=n; k*=2) // Do log n times","chapter-3","Algorithm Analysis"
"for (j=1; j<=n; j++) // Do n times","chapter-3","Algorithm Analysis"
"sum1++;","chapter-3","Algorithm Analysis"
"sum2 = 0;","chapter-3","Algorithm Analysis"
"for (k=1; k<=n; k*=2) // Do log n times","chapter-3","Algorithm Analysis"
"for (j=1; j<=k; j++) // Do k times","chapter-3","Algorithm Analysis"
"sum2++;","chapter-3","Algorithm Analysis"
"Sec. 3.5 Calculating the Running Time for a Program 71","chapter-3","Algorithm Analysis"
"When analyzing these two code fragments, we will assume that n is","chapter-3","Algorithm Analysis"
"a power of two. The first code fragment has its outer for loop executed","chapter-3","Algorithm Analysis"
"log n + 1 times because on each iteration k is multiplied by two until it","chapter-3","Algorithm Analysis"
"reaches n. Because the inner loop always executes n times, the total cost for","chapter-3","Algorithm Analysis"
"the first code fragment can be expressed as Plog n","chapter-3","Algorithm Analysis"
"i=0 n. Note that a variable","chapter-3","Algorithm Analysis"
"substitution takes place here to create the summation, with k = 2i","chapter-3","Algorithm Analysis"
". From","chapter-3","Algorithm Analysis"
"Equation 2.3, the solution for this summation is Θ(n log n). In the second","chapter-3","Algorithm Analysis"
"code fragment, the outer loop is also executed log n + 1 times. The inner","chapter-3","Algorithm Analysis"
"loop has cost k, which doubles each time. The summation can be expressed","chapter-3","Algorithm Analysis"
"as Plog n","chapter-3","Algorithm Analysis"
"i=0 2","chapter-3","Algorithm Analysis"
"i where n is assumed to be a power of two and again k = 2i","chapter-3","Algorithm Analysis"
".","chapter-3","Algorithm Analysis"
"From Equation 2.8, we know that this summation is simply Θ(n).","chapter-3","Algorithm Analysis"
"What about other control statements? While loops are analyzed in a manner","chapter-3","Algorithm Analysis"
"similar to for loops. The cost of an if statement in the worst case is the greater","chapter-3","Algorithm Analysis"
"of the costs for the then and else clauses. This is also true for the average case,","chapter-3","Algorithm Analysis"
"assuming that the size of n does not affect the probability of executing one of the","chapter-3","Algorithm Analysis"
"clauses (which is usually, but not necessarily, true). For switch statements, the","chapter-3","Algorithm Analysis"
"worst-case cost is that of the most expensive branch. For subroutine calls, simply","chapter-3","Algorithm Analysis"
"add the cost of executing the subroutine.","chapter-3","Algorithm Analysis"
"There are rare situations in which the probability for executing the various","chapter-3","Algorithm Analysis"
"branches of an if or switch statement are functions of the input size. For exam-","chapter-3","Algorithm Analysis"
"ple, for input of size n, the then clause of an if statement might be executed with","chapter-3","Algorithm Analysis"
"probability 1/n. An example would be an if statement that executes the then","chapter-3","Algorithm Analysis"
"clause only for the smallest of n values. To perform an average-case analysis for","chapter-3","Algorithm Analysis"
"such programs, we cannot simply count the cost of the if statement as being the","chapter-3","Algorithm Analysis"
"cost of the more expensive branch. In such situations, the technique of amortized","chapter-3","Algorithm Analysis"
"analysis (see Section 14.3) can come to the rescue.","chapter-3","Algorithm Analysis"
"Determining the execution time of a recursive subroutine can be difficult. The","chapter-3","Algorithm Analysis"
"running time for a recursive subroutine is typically best expressed by a recurrence","chapter-3","Algorithm Analysis"
"relation. For example, the recursive factorial function fact of Section 2.5 calls","chapter-3","Algorithm Analysis"
"itself with a value one less than its input value. The result of this recursive call is","chapter-3","Algorithm Analysis"
"then multiplied by the input value, which takes constant time. Thus, the cost of","chapter-3","Algorithm Analysis"
"the factorial function, if we wish to measure cost in terms of the number of multi-","chapter-3","Algorithm Analysis"
"plication operations, is one more than the number of multiplications made by the","chapter-3","Algorithm Analysis"
"recursive call on the smaller input. Because the base case does no multiplications,","chapter-3","Algorithm Analysis"
"its cost is zero. Thus, the running time for this function can be expressed as","chapter-3","Algorithm Analysis"
"T(n) = T(n − 1) + 1 for n > 1; T(1) = 0.","chapter-3","Algorithm Analysis"
"We know from Examples 2.8 and 2.13 that the closed-form solution for this recur-","chapter-3","Algorithm Analysis"
"rence relation is Θ(n).","chapter-3","Algorithm Analysis"
"72 Chap. 3 Algorithm Analysis","chapter-3","Algorithm Analysis"
"Key","chapter-3","Algorithm Analysis"
"Position 0 2 3 4 5 6 7 8","chapter-3","Algorithm Analysis"
"26 29 36","chapter-3","Algorithm Analysis"
"10 11 12 13 14 15","chapter-3","Algorithm Analysis"
"11 13 21 41 45 51 54","chapter-3","Algorithm Analysis"
"1","chapter-3","Algorithm Analysis"
"56 65 72 77","chapter-3","Algorithm Analysis"
"9","chapter-3","Algorithm Analysis"
"40 83","chapter-3","Algorithm Analysis"
"Figure 3.4 An illustration of binary search on a sorted array of 16 positions.","chapter-3","Algorithm Analysis"
"Consider a search for the position with value K = 45. Binary search first checks","chapter-3","Algorithm Analysis"
"the value at position 7. Because 41 < K, the desired value cannot appear in any","chapter-3","Algorithm Analysis"
"position below 7 in the array. Next, binary search checks the value at position 11.","chapter-3","Algorithm Analysis"
"Because 56 > K, the desired value (if it exists) must be between positions 7","chapter-3","Algorithm Analysis"
"and 11. Position 9 is checked next. Again, its value is too great. The final search","chapter-3","Algorithm Analysis"
"is at position 8, which contains the desired value. Thus, function binary returns","chapter-3","Algorithm Analysis"
"position 8. Alternatively, if K were 44, then the same series of record accesses","chapter-3","Algorithm Analysis"
"would be made. After checking position 8, binary would return a value of n,","chapter-3","Algorithm Analysis"
"indicating that the search is unsuccessful.","chapter-3","Algorithm Analysis"
"The final example of algorithm analysis for this section will compare two algo-","chapter-3","Algorithm Analysis"
"rithms for performing search in an array. Earlier, we determined that the running","chapter-3","Algorithm Analysis"
"time for sequential search on an array where the search value K is equally likely","chapter-3","Algorithm Analysis"
"to appear in any location is Θ(n) in both the average and worst cases. We would","chapter-3","Algorithm Analysis"
"like to compare this running time to that required to perform a binary search on","chapter-3","Algorithm Analysis"
"an array whose values are stored in order from lowest to highest.","chapter-3","Algorithm Analysis"
"Binary search begins by examining the value in the middle position of the ar-","chapter-3","Algorithm Analysis"
"ray; call this position mid and the corresponding value kmid. If kmid = K, then","chapter-3","Algorithm Analysis"
"processing can stop immediately. This is unlikely to be the case, however. Fortu-","chapter-3","Algorithm Analysis"
"nately, knowing the middle value provides useful information that can help guide","chapter-3","Algorithm Analysis"
"the search process. In particular, if kmid > K, then you know that the value K","chapter-3","Algorithm Analysis"
"cannot appear in the array at any position greater than mid. Thus, you can elim-","chapter-3","Algorithm Analysis"
"inate future search in the upper half of the array. Conversely, if kmid < K, then","chapter-3","Algorithm Analysis"
"you know that you can ignore all positions in the array less than mid. Either way,","chapter-3","Algorithm Analysis"
"half of the positions are eliminated from further consideration. Binary search next","chapter-3","Algorithm Analysis"
"looks at the middle position in that part of the array where value K may exist. The","chapter-3","Algorithm Analysis"
"value at this position again allows us to eliminate half of the remaining positions","chapter-3","Algorithm Analysis"
"from consideration. This process repeats until either the desired value is found, or","chapter-3","Algorithm Analysis"
"there are no positions remaining in the array that might contain the value K. Fig-","chapter-3","Algorithm Analysis"
"ure 3.4 illustrates the binary search method. Figure 3.5 shows an implementation","chapter-3","Algorithm Analysis"
"for binary search.","chapter-3","Algorithm Analysis"
"To find the cost of this algorithm in the worst case, we can model the running","chapter-3","Algorithm Analysis"
"time as a recurrence and then find the closed-form solution. Each recursive call","chapter-3","Algorithm Analysis"
"to binary cuts the size of the array approximately in half, so we can model the","chapter-3","Algorithm Analysis"
"worst-case cost as follows, assuming for simplicity that n is a power of two.","chapter-3","Algorithm Analysis"
"T(n) = T(n/2) + 1 for n > 1; T(1) = 1.","chapter-3","Algorithm Analysis"
"Sec. 3.5 Calculating the Running Time for a Program 73","chapter-3","Algorithm Analysis"
"/** @return The position of an element in sorted array A","chapter-3","Algorithm Analysis"
"with value k. If k is not in A, return A.length. */","chapter-3","Algorithm Analysis"
"static int binary(int[] A, int k) {","chapter-3","Algorithm Analysis"
"int l = -1;","chapter-3","Algorithm Analysis"
"int r = A.length; // l and r are beyond array bounds","chapter-3","Algorithm Analysis"
"while (l+1 != r) { // Stop when l and r meet","chapter-3","Algorithm Analysis"
"int i = (l+r)/2; // Check middle of remaining subarray","chapter-3","Algorithm Analysis"
"if (k < A[i]) r = i; // In left half","chapter-3","Algorithm Analysis"
"if (k == A[i]) return i; // Found it","chapter-3","Algorithm Analysis"
"if (k > A[i]) l = i; // In right half","chapter-3","Algorithm Analysis"
"}","chapter-3","Algorithm Analysis"
"return A.length; // Search value not in A","chapter-3","Algorithm Analysis"
"}","chapter-3","Algorithm Analysis"
"Figure 3.5 Implementation for binary search.","chapter-3","Algorithm Analysis"
"If we expand the recurrence, we find that we can do so only log n times before","chapter-3","Algorithm Analysis"
"we reach the base case, and each expansion adds one to the cost. Thus, the closed-","chapter-3","Algorithm Analysis"
"form solution for the recurrence is T(n) = log n.","chapter-3","Algorithm Analysis"
"Function binary is designed to find the (single) occurrence of K and return","chapter-3","Algorithm Analysis"
"its position. A special value is returned if K does not appear in the array. This","chapter-3","Algorithm Analysis"
"algorithm can be modified to implement variations such as returning the position","chapter-3","Algorithm Analysis"
"of the first occurrence of K in the array if multiple occurrences are allowed, and","chapter-3","Algorithm Analysis"
"returning the position of the greatest value less than K when K is not in the array.","chapter-3","Algorithm Analysis"
"Comparing sequential search to binary search, we see that as n grows, the Θ(n)","chapter-3","Algorithm Analysis"
"running time for sequential search in the average and worst cases quickly becomes","chapter-3","Algorithm Analysis"
"much greater than the Θ(log n) running time for binary search. Taken in isolation,","chapter-3","Algorithm Analysis"
"binary search appears to be much more efficient than sequential search. This is","chapter-3","Algorithm Analysis"
"despite the fact that the constant factor for binary search is greater than that for","chapter-3","Algorithm Analysis"
"sequential search, because the calculation for the next search position in binary","chapter-3","Algorithm Analysis"
"search is more expensive than just incrementing the current position, as sequential","chapter-3","Algorithm Analysis"
"search does.","chapter-3","Algorithm Analysis"
"Note however that the running time for sequential search will be roughly the","chapter-3","Algorithm Analysis"
"same regardless of whether or not the array values are stored in order. In contrast,","chapter-3","Algorithm Analysis"
"binary search requires that the array values be ordered from lowest to highest. De-","chapter-3","Algorithm Analysis"
"pending on the context in which binary search is to be used, this requirement for a","chapter-3","Algorithm Analysis"
"sorted array could be detrimental to the running time of a complete program, be-","chapter-3","Algorithm Analysis"
"cause maintaining the values in sorted order requires to greater cost when inserting","chapter-3","Algorithm Analysis"
"new elements into the array. This is an example of a tradeoff between the advan-","chapter-3","Algorithm Analysis"
"tage of binary search during search and the disadvantage related to maintaining a","chapter-3","Algorithm Analysis"
"sorted array. Only in the context of the complete problem to be solved can we know","chapter-3","Algorithm Analysis"
"whether the advantage outweighs the disadvantage.","chapter-3","Algorithm Analysis"
"74 Chap. 3 Algorithm Analysis","chapter-3","Algorithm Analysis"
"3.6 Analyzing Problems","chapter-3","Algorithm Analysis"
"You most often use the techniques of “algorithm” analysis to analyze an algorithm,","chapter-3","Algorithm Analysis"
"or the instantiation of an algorithm as a program. You can also use these same","chapter-3","Algorithm Analysis"
"techniques to analyze the cost of a problem. It should make sense to you to say that","chapter-3","Algorithm Analysis"
"the upper bound for a problem cannot be worse than the upper bound for the best","chapter-3","Algorithm Analysis"
"algorithm that we know for that problem. But what does it mean to give a lower","chapter-3","Algorithm Analysis"
"bound for a problem?","chapter-3","Algorithm Analysis"
"Consider a graph of cost over all inputs of a given size n for some algorithm","chapter-3","Algorithm Analysis"
"for a given problem. Define A to be the collection of all algorithms that solve","chapter-3","Algorithm Analysis"
"the problem (theoretically, there are an infinite number of such algorithms). Now,","chapter-3","Algorithm Analysis"
"consider the collection of all the graphs for all of the (infinitely many) algorithms","chapter-3","Algorithm Analysis"
"in A. The worst case lower bound is the least of all the highest points on all the","chapter-3","Algorithm Analysis"
"graphs.","chapter-3","Algorithm Analysis"
"It is much easier to show that an algorithm (or program) is in Ω(f(n)) than it","chapter-3","Algorithm Analysis"
"is to show that a problem is in Ω(f(n)). For a problem to be in Ω(f(n)) means","chapter-3","Algorithm Analysis"
"that every algorithm that solves the problem is in Ω(f(n)), even algorithms that we","chapter-3","Algorithm Analysis"
"have not thought of!","chapter-3","Algorithm Analysis"
"So far all of our examples of algorithm analysis give “obvious” results, with","chapter-3","Algorithm Analysis"
"big-Oh always matching Ω. To understand how big-Oh, Ω, and Θ notations are","chapter-3","Algorithm Analysis"
"properly used to describe our understanding of a problem or an algorithm, it is best","chapter-3","Algorithm Analysis"
"to consider an example where you do not already know a lot about the problem.","chapter-3","Algorithm Analysis"
"Let us look ahead to analyzing the problem of sorting to see how this process","chapter-3","Algorithm Analysis"
"works. What is the least possible cost for any sorting algorithm in the worst case?","chapter-3","Algorithm Analysis"
"The algorithm must at least look at every element in the input, just to determine","chapter-3","Algorithm Analysis"
"that the input is truly sorted. Thus, any sorting algorithm must take at least cn time.","chapter-3","Algorithm Analysis"
"For many problems, this observation that each of the n inputs must be looked at","chapter-3","Algorithm Analysis"
"leads to an easy Ω(n) lower bound.","chapter-3","Algorithm Analysis"
"In your previous study of computer science, you have probably seen an example","chapter-3","Algorithm Analysis"
"of a sorting algorithm whose running time is in O(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
") in the worst case. The simple","chapter-3","Algorithm Analysis"
"Bubble Sort and Insertion Sort algorithms typically given as examples in a first year","chapter-3","Algorithm Analysis"
"programming course have worst case running times in O(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"). Thus, the problem","chapter-3","Algorithm Analysis"
"of sorting can be said to have an upper bound in O(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"). How do we close the","chapter-3","Algorithm Analysis"
"gap between Ω(n) and O(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
")? Can there be a better sorting algorithm? If you can","chapter-3","Algorithm Analysis"
"think of no algorithm whose worst-case growth rate is better than O(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"), and if you","chapter-3","Algorithm Analysis"
"have discovered no analysis technique to show that the least cost for the problem","chapter-3","Algorithm Analysis"
"of sorting in the worst case is greater than Ω(n), then you cannot know for sure","chapter-3","Algorithm Analysis"
"whether or not there is a better algorithm.","chapter-3","Algorithm Analysis"
"Chapter 7 presents sorting algorithms whose running time is in O(n log n) for","chapter-3","Algorithm Analysis"
"the worst case. This greatly narrows the gap. With this new knowledge, we now","chapter-3","Algorithm Analysis"
"have a lower bound in Ω(n) and an upper bound in O(n log n). Should we search","chapter-3","Algorithm Analysis"
"Sec. 3.7 Common Misunderstandings 75","chapter-3","Algorithm Analysis"
"for a faster algorithm? Many have tried, without success. Fortunately (or perhaps","chapter-3","Algorithm Analysis"
"unfortunately?), Chapter 7 also includes a proof that any sorting algorithm must","chapter-3","Algorithm Analysis"
"have running time in Ω(n log n) in the worst case.2 This proof is one of the most","chapter-3","Algorithm Analysis"
"important results in the field of algorithm analysis, and it means that no sorting","chapter-3","Algorithm Analysis"
"algorithm can possibly run faster than cn log n for the worst-case input of size n.","chapter-3","Algorithm Analysis"
"Thus, we can conclude that the problem of sorting is Θ(n log n) in the worst case,","chapter-3","Algorithm Analysis"
"because the upper and lower bounds have met.","chapter-3","Algorithm Analysis"
"Knowing the lower bound for a problem does not give you a good algorithm.","chapter-3","Algorithm Analysis"
"But it does help you to know when to stop looking. If the lower bound for the","chapter-3","Algorithm Analysis"
"problem matches the upper bound for the algorithm (within a constant factor), then","chapter-3","Algorithm Analysis"
"we know that we can find an algorithm that is better only by a constant factor.","chapter-3","Algorithm Analysis"
"3.7 Common Misunderstandings","chapter-3","Algorithm Analysis"
"Asymptotic analysis is one of the most intellectually difficult topics that undergrad-","chapter-3","Algorithm Analysis"
"uate computer science majors are confronted with. Most people find growth rates","chapter-3","Algorithm Analysis"
"and asymptotic analysis confusing and so develop misconceptions about either the","chapter-3","Algorithm Analysis"
"concepts or the terminology. It helps to know what the standard points of confusion","chapter-3","Algorithm Analysis"
"are, in hopes of avoiding them.","chapter-3","Algorithm Analysis"
"One problem with differentiating the concepts of upper and lower bounds is","chapter-3","Algorithm Analysis"
"that, for most algorithms that you will encounter, it is easy to recognize the true","chapter-3","Algorithm Analysis"
"growth rate for that algorithm. Given complete knowledge about a cost function,","chapter-3","Algorithm Analysis"
"the upper and lower bound for that cost function are always the same. Thus, the","chapter-3","Algorithm Analysis"
"distinction between an upper and a lower bound is only worthwhile when you have","chapter-3","Algorithm Analysis"
"incomplete knowledge about the thing being measured. If this distinction is still not","chapter-3","Algorithm Analysis"
"clear, reread Section 3.6. We use Θ-notation to indicate that there is no meaningful","chapter-3","Algorithm Analysis"
"difference between what we know about the growth rates of the upper and lower","chapter-3","Algorithm Analysis"
"bound (which is usually the case for simple algorithms).","chapter-3","Algorithm Analysis"
"It is a common mistake to confuse the concepts of upper bound or lower bound","chapter-3","Algorithm Analysis"
"on the one hand, and worst case or best case on the other. The best, worst, or","chapter-3","Algorithm Analysis"
"average cases each give us a concrete input instance (or concrete set of instances)","chapter-3","Algorithm Analysis"
"that we can apply to an algorithm description to get a cost measure. The upper and","chapter-3","Algorithm Analysis"
"lower bounds describe our understanding of the growth rate for that cost measure.","chapter-3","Algorithm Analysis"
"So to define the growth rate for an algorithm or problem, we need to determine","chapter-3","Algorithm Analysis"
"what we are measuring (the best, worst, or average case) and also our description","chapter-3","Algorithm Analysis"
"for what we know about the growth rate of that cost measure (big-Oh, Ω, or Θ).","chapter-3","Algorithm Analysis"
"The upper bound for an algorithm is not the same as the worst case for that","chapter-3","Algorithm Analysis"
"algorithm for a given input of size n. What is being bounded is not the actual cost","chapter-3","Algorithm Analysis"
"(which you can determine for a given value of n), but rather the growth rate for the","chapter-3","Algorithm Analysis"
"2While it is fortunate to know the truth, it is unfortunate that sorting is Θ(n log n) rather than","chapter-3","Algorithm Analysis"
"Θ(n)!","chapter-3","Algorithm Analysis"
"76 Chap. 3 Algorithm Analysis","chapter-3","Algorithm Analysis"
"cost. There cannot be a growth rate for a single point, such as a particular value","chapter-3","Algorithm Analysis"
"of n. The growth rate applies to the change in cost as a change in input size occurs.","chapter-3","Algorithm Analysis"
"Likewise, the lower bound is not the same as the best case for a given size n.","chapter-3","Algorithm Analysis"
"Another common misconception is thinking that the best case for an algorithm","chapter-3","Algorithm Analysis"
"occurs when the input size is as small as possible, or that the worst case occurs","chapter-3","Algorithm Analysis"
"when the input size is as large as possible. What is correct is that best- and worse-","chapter-3","Algorithm Analysis"
"case instances exist for each possible size of input. That is, for all inputs of a given","chapter-3","Algorithm Analysis"
"size, say i, one (or more) of the inputs of size i is the best and one (or more) of the","chapter-3","Algorithm Analysis"
"inputs of size i is the worst. Often (but not always!), we can characterize the best","chapter-3","Algorithm Analysis"
"input case for an arbitrary size, and we can characterize the worst input case for an","chapter-3","Algorithm Analysis"
"arbitrary size. Ideally, we can determine the growth rate for the characterized best,","chapter-3","Algorithm Analysis"
"worst, and average cases as the input size grows.","chapter-3","Algorithm Analysis"
"Example 3.14 What is the growth rate of the best case for sequential","chapter-3","Algorithm Analysis"
"search? For any array of size n, the best case occurs when the value we","chapter-3","Algorithm Analysis"
"are looking for appears in the first position of the array. This is true regard-","chapter-3","Algorithm Analysis"
"less of the size of the array. Thus, the best case (for arbitrary size n) occurs","chapter-3","Algorithm Analysis"
"when the desired value is in the first of n positions, and its cost is 1. It is","chapter-3","Algorithm Analysis"
"not correct to say that the best case occurs when n = 1.","chapter-3","Algorithm Analysis"
"Example 3.15 Imagine drawing a graph to show the cost of finding the","chapter-3","Algorithm Analysis"
"maximum value among n values, as n grows. That is, the x axis would","chapter-3","Algorithm Analysis"
"be n, and the y value would be the cost. Of course, this is a diagonal line","chapter-3","Algorithm Analysis"
"going up to the right, as n increases (you might want to sketch this graph","chapter-3","Algorithm Analysis"
"for yourself before reading further).","chapter-3","Algorithm Analysis"
"Now, imagine the graph showing the cost for each instance of the prob-","chapter-3","Algorithm Analysis"
"lem of finding the maximum value among (say) 20 elements in an array.","chapter-3","Algorithm Analysis"
"The first position along the x axis of the graph might correspond to having","chapter-3","Algorithm Analysis"
"the maximum element in the first position of the array. The second position","chapter-3","Algorithm Analysis"
"along the x axis of the graph might correspond to having the maximum el-","chapter-3","Algorithm Analysis"
"ement in the second position of the array, and so on. Of course, the cost is","chapter-3","Algorithm Analysis"
"always 20. Therefore, the graph would be a horizontal line with value 20.","chapter-3","Algorithm Analysis"
"You should sketch this graph for yourself.","chapter-3","Algorithm Analysis"
"Now, let us switch to the problem of doing a sequential search for a","chapter-3","Algorithm Analysis"
"given value in an array. Think about the graph showing all the problem","chapter-3","Algorithm Analysis"
"instances of size 20. The first problem instance might be when the value","chapter-3","Algorithm Analysis"
"we search for is in the first position of the array. This has cost 1. The second","chapter-3","Algorithm Analysis"
"problem instance might be when the value we search for is in the second","chapter-3","Algorithm Analysis"
"position of the array. This has cost 2. And so on. If we arrange the problem","chapter-3","Algorithm Analysis"
"instances of size 20 from least expensive on the left to most expensive on","chapter-3","Algorithm Analysis"
"Sec. 3.8 Multiple Parameters 77","chapter-3","Algorithm Analysis"
"the right, we see that the graph forms a diagonal line from lower left (with","chapter-3","Algorithm Analysis"
"value 0) to upper right (with value 20). Sketch this graph for yourself.","chapter-3","Algorithm Analysis"
"Finally, let us consider the cost for performing sequential search as the","chapter-3","Algorithm Analysis"
"size of the array n gets bigger. What will this graph look like? Unfortu-","chapter-3","Algorithm Analysis"
"nately, there’s not one simple answer, as there was for finding the maximum","chapter-3","Algorithm Analysis"
"value. The shape of this graph depends on whether we are considering the","chapter-3","Algorithm Analysis"
"best case cost (that would be a horizontal line with value 1), the worst case","chapter-3","Algorithm Analysis"
"cost (that would be a diagonal line with value i at position i along the x","chapter-3","Algorithm Analysis"
"axis), or the average cost (that would be a a diagonal line with value i/2 at","chapter-3","Algorithm Analysis"
"position i along the x axis). This is why we must always say that function","chapter-3","Algorithm Analysis"
"f(n) is in O(g(n)) in the best, average, or worst case! If we leave off which","chapter-3","Algorithm Analysis"
"class of inputs we are discussing, we cannot know which cost measure we","chapter-3","Algorithm Analysis"
"are referring to for most algorithms.","chapter-3","Algorithm Analysis"
"3.8 Multiple Parameters","chapter-3","Algorithm Analysis"
"Sometimes the proper analysis for an algorithm requires multiple parameters to de-","chapter-3","Algorithm Analysis"
"scribe the cost. To illustrate the concept, consider an algorithm to compute the rank","chapter-3","Algorithm Analysis"
"ordering for counts of all pixel values in a picture. Pictures are often represented by","chapter-3","Algorithm Analysis"
"a two-dimensional array, and a pixel is one cell in the array. The value of a pixel is","chapter-3","Algorithm Analysis"
"either the code value for the color, or a value for the intensity of the picture at that","chapter-3","Algorithm Analysis"
"pixel. Assume that each pixel can take any integer value in the range 0 to C − 1.","chapter-3","Algorithm Analysis"
"The problem is to find the number of pixels of each color value and then sort the","chapter-3","Algorithm Analysis"
"color values with respect to the number of times each value appears in the picture.","chapter-3","Algorithm Analysis"
"Assume that the picture is a rectangle with P pixels. A pseudocode algorithm to","chapter-3","Algorithm Analysis"
"solve the problem follows.","chapter-3","Algorithm Analysis"
"for (i=0; i<C; i++) // Initialize count","chapter-3","Algorithm Analysis"
"count[i] = 0;","chapter-3","Algorithm Analysis"
"for (i=0; i<P; i++) // Look at all of the pixels","chapter-3","Algorithm Analysis"
"count[value(i)]++; // Increment a pixel value count","chapter-3","Algorithm Analysis"
"sort(count); // Sort pixel value counts","chapter-3","Algorithm Analysis"
"In this example, count is an array of size C that stores the number of pixels for","chapter-3","Algorithm Analysis"
"each color value. Function value(i) returns the color value for pixel i.","chapter-3","Algorithm Analysis"
"The time for the first for loop (which initializes count) is based on the num-","chapter-3","Algorithm Analysis"
"ber of colors, C. The time for the second loop (which determines the number of","chapter-3","Algorithm Analysis"
"pixels with each color) is Θ(P). The time for the final line, the call to sort, de-","chapter-3","Algorithm Analysis"
"pends on the cost of the sorting algorithm used. From the discussion of Section 3.6,","chapter-3","Algorithm Analysis"
"we can assume that the sorting algorithm has cost Θ(P log P) if P items are sorted,","chapter-3","Algorithm Analysis"
"thus yielding Θ(P log P) as the total algorithm cost.","chapter-3","Algorithm Analysis"
"78 Chap. 3 Algorithm Analysis","chapter-3","Algorithm Analysis"
"Is this a good representation for the cost of this algorithm? What is actu-","chapter-3","Algorithm Analysis"
"ally being sorted? It is not the pixels, but rather the colors. What if C is much","chapter-3","Algorithm Analysis"
"smaller than P? Then the estimate of Θ(P log P) is pessimistic, because much","chapter-3","Algorithm Analysis"
"fewer than P items are being sorted. Instead, we should use P as our analysis vari-","chapter-3","Algorithm Analysis"
"able for steps that look at each pixel, and C as our analysis variable for steps that","chapter-3","Algorithm Analysis"
"look at colors. Then we get Θ(C) for the initialization loop, Θ(P) for the pixel","chapter-3","Algorithm Analysis"
"count loop, and Θ(C log C) for the sorting operation. This yields a total cost of","chapter-3","Algorithm Analysis"
"Θ(P + C log C).","chapter-3","Algorithm Analysis"
"Why can we not simply use the value of C for input size and say that the cost","chapter-3","Algorithm Analysis"
"of the algorithm is Θ(C log C)? Because, C is typically much less than P. For","chapter-3","Algorithm Analysis"
"example, a picture might have 1000 × 1000 pixels and a range of 256 possible","chapter-3","Algorithm Analysis"
"colors. So, P is one million, which is much larger than C log C. But, if P is","chapter-3","Algorithm Analysis"
"smaller, or C larger (even if it is still less than P), then C log C can become the","chapter-3","Algorithm Analysis"
"larger quantity. Thus, neither variable should be ignored.","chapter-3","Algorithm Analysis"
"3.9 Space Bounds","chapter-3","Algorithm Analysis"
"Besides time, space is the other computing resource that is commonly of concern","chapter-3","Algorithm Analysis"
"to programmers. Just as computers have become much faster over the years, they","chapter-3","Algorithm Analysis"
"have also received greater allotments of memory. Even so, the amount of available","chapter-3","Algorithm Analysis"
"disk space or main memory can be significant constraints for algorithm designers.","chapter-3","Algorithm Analysis"
"The analysis techniques used to measure space requirements are similar to those","chapter-3","Algorithm Analysis"
"used to measure time requirements. However, while time requirements are nor-","chapter-3","Algorithm Analysis"
"mally measured for an algorithm that manipulates a particular data structure, space","chapter-3","Algorithm Analysis"
"requirements are normally determined for the data structure itself. The concepts of","chapter-3","Algorithm Analysis"
"asymptotic analysis for growth rates on input size apply completely to measuring","chapter-3","Algorithm Analysis"
"space requirements.","chapter-3","Algorithm Analysis"
"Example 3.16 What are the space requirements for an array of n inte-","chapter-3","Algorithm Analysis"
"gers? If each integer requires c bytes, then the array requires cn bytes,","chapter-3","Algorithm Analysis"
"which is Θ(n).","chapter-3","Algorithm Analysis"
"Example 3.17 Imagine that we want to keep track of friendships between","chapter-3","Algorithm Analysis"
"n people. We can do this with an array of size n × n. Each row of the array","chapter-3","Algorithm Analysis"
"represents the friends of an individual, with the columns indicating who has","chapter-3","Algorithm Analysis"
"that individual as a friend. For example, if person j is a friend of person","chapter-3","Algorithm Analysis"
"i, then we place a mark in column j of row i in the array. Likewise, we","chapter-3","Algorithm Analysis"
"should also place a mark in column i of row j if we assume that friendship","chapter-3","Algorithm Analysis"
"works both ways. For n people, the total size of the array is Θ(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
").","chapter-3","Algorithm Analysis"
"Sec. 3.9 Space Bounds 79","chapter-3","Algorithm Analysis"
"A data structure’s primary purpose is to store data in a way that allows efficient","chapter-3","Algorithm Analysis"
"access to those data. To provide efficient access, it may be necessary to store addi-","chapter-3","Algorithm Analysis"
"tional information about where the data are within the data structure. For example,","chapter-3","Algorithm Analysis"
"each node of a linked list must store a pointer to the next value on the list. All such","chapter-3","Algorithm Analysis"
"information stored in addition to the actual data values is referred to as overhead.","chapter-3","Algorithm Analysis"
"Ideally, overhead should be kept to a minimum while allowing maximum access.","chapter-3","Algorithm Analysis"
"The need to maintain a balance between these opposing goals is what makes the","chapter-3","Algorithm Analysis"
"study of data structures so interesting.","chapter-3","Algorithm Analysis"
"One important aspect of algorithm design is referred to as the space/time trade-","chapter-3","Algorithm Analysis"
"off principle. The space/time tradeoff principle says that one can often achieve a","chapter-3","Algorithm Analysis"
"reduction in time if one is willing to sacrifice space or vice versa. Many programs","chapter-3","Algorithm Analysis"
"can be modified to reduce storage requirements by “packing” or encoding informa-","chapter-3","Algorithm Analysis"
"tion. “Unpacking” or decoding the information requires additional time. Thus, the","chapter-3","Algorithm Analysis"
"resulting program uses less space but runs slower. Conversely, many programs can","chapter-3","Algorithm Analysis"
"be modified to pre-store results or reorganize information to allow faster running","chapter-3","Algorithm Analysis"
"time at the expense of greater storage requirements. Typically, such changes in time","chapter-3","Algorithm Analysis"
"and space are both by a constant factor.","chapter-3","Algorithm Analysis"
"A classic example of a space/time tradeoff is the lookup table. A lookup table","chapter-3","Algorithm Analysis"
"pre-stores the value of a function that would otherwise be computed each time it is","chapter-3","Algorithm Analysis"
"needed. For example, 12! is the greatest value for the factorial function that can be","chapter-3","Algorithm Analysis"
"stored in a 32-bit int variable. If you are writing a program that often computes","chapter-3","Algorithm Analysis"
"factorials, it is likely to be much more time efficient to simply pre-compute and","chapter-3","Algorithm Analysis"
"store the 12 values in a table. Whenever the program needs the value of n! it can","chapter-3","Algorithm Analysis"
"simply check the lookup table. (If n > 12, the value is too large to store as an int","chapter-3","Algorithm Analysis"
"variable anyway.) Compared to the time required to compute factorials, it may be","chapter-3","Algorithm Analysis"
"well worth the small amount of additional space needed to store the lookup table.","chapter-3","Algorithm Analysis"
"Lookup tables can also store approximations for an expensive function such as","chapter-3","Algorithm Analysis"
"sine or cosine. If you compute this function only for exact degrees or are willing","chapter-3","Algorithm Analysis"
"to approximate the answer with the value for the nearest degree, then a lookup","chapter-3","Algorithm Analysis"
"table storing the computation for exact degrees can be used instead of repeatedly","chapter-3","Algorithm Analysis"
"computing the sine function. Note that initially building the lookup table requires","chapter-3","Algorithm Analysis"
"a certain amount of time. Your application must use the lookup table often enough","chapter-3","Algorithm Analysis"
"to make this initialization worthwhile.","chapter-3","Algorithm Analysis"
"Another example of the space/time tradeoff is typical of what a programmer","chapter-3","Algorithm Analysis"
"might encounter when trying to optimize space. Here is a simple code fragment for","chapter-3","Algorithm Analysis"
"sorting an array of integers. We assume that this is a special case where there are n","chapter-3","Algorithm Analysis"
"integers whose values are a permutation of the integers from 0 to n − 1. This is an","chapter-3","Algorithm Analysis"
"example of a Binsort, which is discussed in Section 7.7. Binsort assigns each value","chapter-3","Algorithm Analysis"
"to an array position corresponding to its value.","chapter-3","Algorithm Analysis"
"for (i=0; i<n; i++)","chapter-3","Algorithm Analysis"
"B[A[i]] = A[i];","chapter-3","Algorithm Analysis"
"80 Chap. 3 Algorithm Analysis","chapter-3","Algorithm Analysis"
"This is efficient and requires Θ(n) time. However, it also requires two arrays","chapter-3","Algorithm Analysis"
"of size n. Next is a code fragment that places the permutation in order but does so","chapter-3","Algorithm Analysis"
"within the same array (thus it is an example of an “in place” sort).","chapter-3","Algorithm Analysis"
"for (i=0; i<n; i++)","chapter-3","Algorithm Analysis"
"while (A[i] != i) // Swap element A[i] with A[A[i]]","chapter-3","Algorithm Analysis"
"DSutil.swap(A, i, A[i]);","chapter-3","Algorithm Analysis"
"Function swap(A, i, j) exchanges elements i and j in array A. It may","chapter-3","Algorithm Analysis"
"not be obvious that the second code fragment actually sorts the array. To see that","chapter-3","Algorithm Analysis"
"this does work, notice that each pass through the for loop will at least move the","chapter-3","Algorithm Analysis"
"integer with value i to its correct position in the array, and that during this iteration,","chapter-3","Algorithm Analysis"
"the value of A[i] must be greater than or equal to i. A total of at most n swap","chapter-3","Algorithm Analysis"
"operations take place, because an integer cannot be moved out of its correct position","chapter-3","Algorithm Analysis"
"once it has been placed there, and each swap operation places at least one integer in","chapter-3","Algorithm Analysis"
"its correct position. Thus, this code fragment has cost Θ(n). However, it requires","chapter-3","Algorithm Analysis"
"more time to run than the first code fragment. On my computer the second version","chapter-3","Algorithm Analysis"
"takes nearly twice as long to run as the first, but it only requires half the space.","chapter-3","Algorithm Analysis"
"A second principle for the relationship between a program’s space and time","chapter-3","Algorithm Analysis"
"requirements applies to programs that process information stored on disk, as dis-","chapter-3","Algorithm Analysis"
"cussed in Chapter 8 and thereafter. Strangely enough, the disk-based space/time","chapter-3","Algorithm Analysis"
"tradeoff principle is almost the reverse of the space/time tradeoff principle for pro-","chapter-3","Algorithm Analysis"
"grams using main memory.","chapter-3","Algorithm Analysis"
"The disk-based space/time tradeoff principle states that the smaller you can","chapter-3","Algorithm Analysis"
"make your disk storage requirements, the faster your program will run. This is be-","chapter-3","Algorithm Analysis"
"cause the time to read information from disk is enormous compared to computation","chapter-3","Algorithm Analysis"
"time, so almost any amount of additional computation needed to unpack the data is","chapter-3","Algorithm Analysis"
"going to be less than the disk-reading time saved by reducing the storage require-","chapter-3","Algorithm Analysis"
"ments. Naturally this principle does not hold true in all cases, but it is good to keep","chapter-3","Algorithm Analysis"
"in mind when designing programs that process information stored on disk.","chapter-3","Algorithm Analysis"
"3.10 Speeding Up Your Programs","chapter-3","Algorithm Analysis"
"In practice, there is not such a big difference in running time between an algorithm","chapter-3","Algorithm Analysis"
"with growth rate Θ(n) and another with growth rate Θ(n log n). There is, however,","chapter-3","Algorithm Analysis"
"an enormous difference in running time between algorithms with growth rates of","chapter-3","Algorithm Analysis"
"Θ(n log n) and Θ(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"). As you shall see during the course of your study of common","chapter-3","Algorithm Analysis"
"data structures and algorithms, it is not unusual that a problem whose obvious solu-","chapter-3","Algorithm Analysis"
"tion requires Θ(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
") time also has a solution requiring Θ(n log n) time. Examples","chapter-3","Algorithm Analysis"
"include sorting and searching, two of the most important computer problems.","chapter-3","Algorithm Analysis"
"Example 3.18 The following is a true story. A few years ago, one of","chapter-3","Algorithm Analysis"
"my graduate students had a big problem. His thesis work involved several","chapter-3","Algorithm Analysis"
"Sec. 3.10 Speeding Up Your Programs 81","chapter-3","Algorithm Analysis"
"intricate operations on a large database. He was now working on the final","chapter-3","Algorithm Analysis"
"step. “Dr. Shaffer,” he said, “I am running this program and it seems to","chapter-3","Algorithm Analysis"
"be taking a long time.” After examining the algorithm we realized that its","chapter-3","Algorithm Analysis"
"running time was Θ(n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"), and that it would likely take one to two weeks","chapter-3","Algorithm Analysis"
"to complete. Even if we could keep the computer running uninterrupted","chapter-3","Algorithm Analysis"
"for that long, he was hoping to complete his thesis and graduate before","chapter-3","Algorithm Analysis"
"then. Fortunately, we realized that there was a fairly easy way to convert","chapter-3","Algorithm Analysis"
"the algorithm so that its running time was Θ(n log n). By the next day he","chapter-3","Algorithm Analysis"
"had modified the program. It ran in only a few hours, and he finished his","chapter-3","Algorithm Analysis"
"thesis on time.","chapter-3","Algorithm Analysis"
"While not nearly so important as changing an algorithm to reduce its growth","chapter-3","Algorithm Analysis"
"rate, “code tuning” can also lead to dramatic improvements in running time. Code","chapter-3","Algorithm Analysis"
"tuning is the art of hand-optimizing a program to run faster or require less storage.","chapter-3","Algorithm Analysis"
"For many programs, code tuning can reduce running time by a factor of ten, or","chapter-3","Algorithm Analysis"
"cut the storage requirements by a factor of two or more. I once tuned a critical","chapter-3","Algorithm Analysis"
"function in a program — without changing its basic algorithm — to achieve a factor","chapter-3","Algorithm Analysis"
"of 200 speedup. To get this speedup, however, I did make major changes in the","chapter-3","Algorithm Analysis"
"representation of the information, converting from a symbolic coding scheme to a","chapter-3","Algorithm Analysis"
"numeric coding scheme on which I was able to do direct computation.","chapter-3","Algorithm Analysis"
"Here are some suggestions for ways to speed up your programs by code tuning.","chapter-3","Algorithm Analysis"
"The most important thing to realize is that most statements in a program do not","chapter-3","Algorithm Analysis"
"have much effect on the running time of that program. There are normally just a","chapter-3","Algorithm Analysis"
"few key subroutines, possibly even key lines of code within the key subroutines,","chapter-3","Algorithm Analysis"
"that account for most of the running time. There is little point to cutting in half the","chapter-3","Algorithm Analysis"
"running time of a subroutine that accounts for only 1% of the total running time.","chapter-3","Algorithm Analysis"
"Focus your attention on those parts of the program that have the most impact.","chapter-3","Algorithm Analysis"
"When tuning code, it is important to gather good timing statistics. Many com-","chapter-3","Algorithm Analysis"
"pilers and operating systems include profilers and other special tools to help gather","chapter-3","Algorithm Analysis"
"information on both time and space use. These are invaluable when trying to make","chapter-3","Algorithm Analysis"
"a program more efficient, because they can tell you where to invest your effort.","chapter-3","Algorithm Analysis"
"A lot of code tuning is based on the principle of avoiding work rather than","chapter-3","Algorithm Analysis"
"speeding up work. A common situation occurs when we can test for a condition","chapter-3","Algorithm Analysis"
"that lets us skip some work. However, such a test is never completely free. Care","chapter-3","Algorithm Analysis"
"must be taken that the cost of the test does not exceed the amount of work saved.","chapter-3","Algorithm Analysis"
"While one test might be cheaper than the work potentially saved, the test must","chapter-3","Algorithm Analysis"
"always be made and the work can be avoided only some fraction of the time.","chapter-3","Algorithm Analysis"
"Example 3.19 A common operation in computer graphics applications is","chapter-3","Algorithm Analysis"
"to find which among a set of complex objects contains a given point in","chapter-3","Algorithm Analysis"
"space. Many useful data structures and algorithms have been developed to","chapter-3","Algorithm Analysis"
"82 Chap. 3 Algorithm Analysis","chapter-3","Algorithm Analysis"
"deal with variations of this problem. Most such implementations involve","chapter-3","Algorithm Analysis"
"the following tuning step. Directly testing whether a given complex ob-","chapter-3","Algorithm Analysis"
"ject contains the point in question is relatively expensive. Instead, we can","chapter-3","Algorithm Analysis"
"screen for whether the point is contained within a bounding box for the","chapter-3","Algorithm Analysis"
"object. The bounding box is simply the smallest rectangle (usually defined","chapter-3","Algorithm Analysis"
"to have sides perpendicular to the x and y axes) that contains the object.","chapter-3","Algorithm Analysis"
"If the point is not in the bounding box, then it cannot be in the object. If","chapter-3","Algorithm Analysis"
"the point is in the bounding box, only then would we conduct the full com-","chapter-3","Algorithm Analysis"
"parison of the object versus the point. Note that if the point is outside the","chapter-3","Algorithm Analysis"
"bounding box, we saved time because the bounding box test is cheaper than","chapter-3","Algorithm Analysis"
"the comparison of the full object versus the point. But if the point is inside","chapter-3","Algorithm Analysis"
"the bounding box, then that test is redundant because we still have to com-","chapter-3","Algorithm Analysis"
"pare the point against the object. Typically the amount of work avoided by","chapter-3","Algorithm Analysis"
"making this test is greater than the cost of making the test on every object.","chapter-3","Algorithm Analysis"
"Example 3.20 Section 7.2.3 presents a sorting algorithm named Selec-","chapter-3","Algorithm Analysis"
"tion Sort. The chief distinguishing characteristic of this algorithm is that","chapter-3","Algorithm Analysis"
"it requires relatively few swaps of records stored in the array to be sorted.","chapter-3","Algorithm Analysis"
"However, it sometimes performs an unnecessary swap operation where it","chapter-3","Algorithm Analysis"
"tries to swap a record with itself. This work could be avoided by testing","chapter-3","Algorithm Analysis"
"whether the two indices being swapped are the same. However, this event","chapter-3","Algorithm Analysis"
"does not occurr often. Because the cost of the test is high enough compared","chapter-3","Algorithm Analysis"
"to the work saved when the test is successful, adding the test typically will","chapter-3","Algorithm Analysis"
"slow down the program rather than speed it up.","chapter-3","Algorithm Analysis"
"Be careful not to use tricks that make the program unreadable. Most code tun-","chapter-3","Algorithm Analysis"
"ing is simply cleaning up a carelessly written program, not taking a clear program","chapter-3","Algorithm Analysis"
"and adding tricks. In particular, you should develop an appreciation for the capa-","chapter-3","Algorithm Analysis"
"bilities of modern compilers to make extremely good optimizations of expressions.","chapter-3","Algorithm Analysis"
"“Optimization of expressions” here means a rearrangement of arithmetic or logical","chapter-3","Algorithm Analysis"
"expressions to run more efficiently. Be careful not to damage the compiler’s ability","chapter-3","Algorithm Analysis"
"to do such optimizations for you in an effort to optimize the expression yourself.","chapter-3","Algorithm Analysis"
"Always check that your “optimizations” really do improve the program by running","chapter-3","Algorithm Analysis"
"the program before and after the change on a suitable benchmark set of input. Many","chapter-3","Algorithm Analysis"
"times I have been wrong about the positive effects of code tuning in my own pro-","chapter-3","Algorithm Analysis"
"grams. Most often I am wrong when I try to optimize an expression. It is hard to","chapter-3","Algorithm Analysis"
"do better than the compiler.","chapter-3","Algorithm Analysis"
"The greatest time and space improvements come from a better data structure or","chapter-3","Algorithm Analysis"
"algorithm. The final thought for this section is","chapter-3","Algorithm Analysis"
"First tune the algorithm, then tune the code.","chapter-3","Algorithm Analysis"
"Sec. 3.11 Empirical Analysis 83","chapter-3","Algorithm Analysis"
"3.11 Empirical Analysis","chapter-3","Algorithm Analysis"
"This chapter has focused on asymptotic analysis. This is an analytic tool, whereby","chapter-3","Algorithm Analysis"
"we model the key aspects of an algorithm to determine the growth rate of the alg-","chapter-3","Algorithm Analysis"
"orithm as the input size grows. As pointed out previously, there are many limita-","chapter-3","Algorithm Analysis"
"tions to this approach. These include the effects at small problem size, determining","chapter-3","Algorithm Analysis"
"the finer distinctions between algorithms with the same growth rate, and the inher-","chapter-3","Algorithm Analysis"
"ent difficulty of doing mathematical modeling for more complex problems.","chapter-3","Algorithm Analysis"
"An alternative to analytical approaches are empirical ones. The most obvious","chapter-3","Algorithm Analysis"
"empirical approach is simply to run two competitors and see which performs better.","chapter-3","Algorithm Analysis"
"In this way we might overcome the deficiencies of analytical approaches.","chapter-3","Algorithm Analysis"
"Be warned that comparative timing of programs is a difficult business, often","chapter-3","Algorithm Analysis"
"subject to experimental errors arising from uncontrolled factors (system load, the","chapter-3","Algorithm Analysis"
"language or compiler used, etc.). The most important point is not to be biased in","chapter-3","Algorithm Analysis"
"favor of one of the programs. If you are biased, this is certain to be reflected in","chapter-3","Algorithm Analysis"
"the timings. One look at competing software or hardware vendors’ advertisements","chapter-3","Algorithm Analysis"
"should convince you of this. The most common pitfall when writing two programs","chapter-3","Algorithm Analysis"
"to compare their performance is that one receives more code-tuning effort than the","chapter-3","Algorithm Analysis"
"other. As mentioned in Section 3.10, code tuning can often reduce running time by","chapter-3","Algorithm Analysis"
"a factor of ten. If the running times for two programs differ by a constant factor","chapter-3","Algorithm Analysis"
"regardless of input size (i.e., their growth rates are the same), then differences in","chapter-3","Algorithm Analysis"
"code tuning might account for any difference in running time. Be suspicious of","chapter-3","Algorithm Analysis"
"empirical comparisons in this situation.","chapter-3","Algorithm Analysis"
"Another approach to analysis is simulation. The idea of simulation is to model","chapter-3","Algorithm Analysis"
"the problem with a computer program and then run it to get a result. In the con-","chapter-3","Algorithm Analysis"
"text of algorithm analysis, simulation is distinct from empirical comparison of two","chapter-3","Algorithm Analysis"
"competitors because the purpose of the simulation is to perform analysis that might","chapter-3","Algorithm Analysis"
"otherwise be too difficult. A good example of this appears in Figure 9.10. This","chapter-3","Algorithm Analysis"
"figure shows the cost for inserting or deleting a record from a hash table under two","chapter-3","Algorithm Analysis"
"different assumptions for the policy used to find a free slot in the table. The y axes","chapter-3","Algorithm Analysis"
"is the cost in number of hash table slots evaluated, and the x axes is the percentage","chapter-3","Algorithm Analysis"
"of slots in the table that are full. The mathematical equations for these curves can","chapter-3","Algorithm Analysis"
"be determined, but this is not so easy. A reasonable alternative is to write simple","chapter-3","Algorithm Analysis"
"variations on hashing. By timing the cost of the program for various loading con-","chapter-3","Algorithm Analysis"
"ditions, it is not difficult to construct a plot similar to Figure 9.10. The purpose of","chapter-3","Algorithm Analysis"
"this analysis is not to determine which approach to hashing is most efficient, so we","chapter-3","Algorithm Analysis"
"are not doing empirical comparison of hashing alternatives. Instead, the purpose","chapter-3","Algorithm Analysis"
"is to analyze the proper loading factor that would be used in an efficient hashing","chapter-3","Algorithm Analysis"
"system to balance time cost versus hash table size (space cost).","chapter-3","Algorithm Analysis"
"84 Chap. 3 Algorithm Analysis","chapter-3","Algorithm Analysis"
"3.12 Further Reading","chapter-3","Algorithm Analysis"
"Pioneering works on algorithm analysis include The Art of Computer Programming","chapter-3","Algorithm Analysis"
"by Donald E. Knuth [Knu97, Knu98], and The Design and Analysis of Computer","chapter-3","Algorithm Analysis"
"Algorithms by Aho, Hopcroft, and Ullman [AHU74]. The alternate definition for","chapter-3","Algorithm Analysis"
"Ω comes from [AHU83]. The use of the notation “T(n) is in O(f(n))” rather","chapter-3","Algorithm Analysis"
"than the more commonly used “T(n) = O(f(n))” I derive from Brassard and","chapter-3","Algorithm Analysis"
"Bratley [BB96], though certainly this use predates them. A good book to read for","chapter-3","Algorithm Analysis"
"further information on algorithm analysis techniques is Compared to What? by","chapter-3","Algorithm Analysis"
"Gregory J.E. Rawlins [Raw92].","chapter-3","Algorithm Analysis"
"Bentley [Ben88] describes one problem in numerical analysis for which, be-","chapter-3","Algorithm Analysis"
"tween 1945 and 1988, the complexity of the best known algorithm had decreased","chapter-3","Algorithm Analysis"
"from O(n","chapter-3","Algorithm Analysis"
"7","chapter-3","Algorithm Analysis"
") to O(n","chapter-3","Algorithm Analysis"
"3","chapter-3","Algorithm Analysis"
"). For a problem of size n = 64, this is roughly equivalent","chapter-3","Algorithm Analysis"
"to the speedup achieved from all advances in computer hardware during the same","chapter-3","Algorithm Analysis"
"time period.","chapter-3","Algorithm Analysis"
"While the most important aspect of program efficiency is the algorithm, much","chapter-3","Algorithm Analysis"
"improvement can be gained from efficient coding of a program. As cited by Freder-","chapter-3","Algorithm Analysis"
"ick P. Brooks in The Mythical Man-Month [Bro95], an efficient programmer can of-","chapter-3","Algorithm Analysis"
"ten produce programs that run five times faster than an inefficient programmer, even","chapter-3","Algorithm Analysis"
"when neither takes special efforts to speed up their code. For excellent and enjoy-","chapter-3","Algorithm Analysis"
"able essays on improving your coding efficiency, and ways to speed up your code","chapter-3","Algorithm Analysis"
"when it really matters, see the books by Jon Bentley [Ben82, Ben00, Ben88]. The","chapter-3","Algorithm Analysis"
"situation described in Example 3.18 arose when we were working on the project","chapter-3","Algorithm Analysis"
"reported on in [SU92].","chapter-3","Algorithm Analysis"
"As an interesting aside, writing a correct binary search algorithm is not easy.","chapter-3","Algorithm Analysis"
"Knuth [Knu98] notes that while the first binary search was published in 1946, the","chapter-3","Algorithm Analysis"
"first bug-free algorithm was not published until 1962! Bentley (“Writing Correct","chapter-3","Algorithm Analysis"
"Programs” in [Ben00]) has found that 90% of the computer professionals he tested","chapter-3","Algorithm Analysis"
"could not write a bug-free binary search in two hours.","chapter-3","Algorithm Analysis"
"Sec. 3.13 Exercises 85","chapter-3","Algorithm Analysis"
"3.13 Exercises","chapter-3","Algorithm Analysis"
"3.1 For each of the six expressions of Figure 3.1, give the range of values of n","chapter-3","Algorithm Analysis"
"for which that expression is most efficient.","chapter-3","Algorithm Analysis"
"3.2 Graph the following expressions. For each expression, state the range of","chapter-3","Algorithm Analysis"
"values of n for which that expression is the most efficient.","chapter-3","Algorithm Analysis"
"4n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"log3 n 3","chapter-3","Algorithm Analysis"
"n","chapter-3","Algorithm Analysis"
"20n 2 log2 n n2/3","chapter-3","Algorithm Analysis"
"3.3 Arrange the following expressions by growth rate from slowest to fastest.","chapter-3","Algorithm Analysis"
"4n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"log3 n n! 3n","chapter-3","Algorithm Analysis"
"20n 2 log2 n n2/3","chapter-3","Algorithm Analysis"
"See Stirling’s approximation in Section 2.2 for help in classifying n!.","chapter-3","Algorithm Analysis"
"3.4 (a) Suppose that a particular algorithm has time complexity T(n) = 3 ×","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"n","chapter-3","Algorithm Analysis"
", and that executing an implementation of it on a particular machine","chapter-3","Algorithm Analysis"
"takes t seconds for n inputs. Now suppose that we are presented with a","chapter-3","Algorithm Analysis"
"machine that is 64 times as fast. How many inputs could we process on","chapter-3","Algorithm Analysis"
"the new machine in t seconds?","chapter-3","Algorithm Analysis"
"(b) Suppose that another algorithm has time complexity T(n) = n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
", and","chapter-3","Algorithm Analysis"
"that executing an implementation of it on a particular machine takes","chapter-3","Algorithm Analysis"
"t seconds for n inputs. Now suppose that we are presented with a ma-","chapter-3","Algorithm Analysis"
"chine that is 64 times as fast. How many inputs could we process on","chapter-3","Algorithm Analysis"
"the new machine in t seconds?","chapter-3","Algorithm Analysis"
"(c) A third algorithm has time complexity T(n) = 8n. Executing an im-","chapter-3","Algorithm Analysis"
"plementation of it on a particular machine takes t seconds for n inputs.","chapter-3","Algorithm Analysis"
"Given a new machine that is 64 times as fast, how many inputs could","chapter-3","Algorithm Analysis"
"we process in t seconds?","chapter-3","Algorithm Analysis"
"3.5 Hardware vendor XYZ Corp. claims that their latest computer will run 100","chapter-3","Algorithm Analysis"
"times faster than that of their competitor, Prunes, Inc. If the Prunes, Inc.","chapter-3","Algorithm Analysis"
"computer can execute a program on input of size n in one hour, what size","chapter-3","Algorithm Analysis"
"input can XYZ’s computer execute in one hour for each algorithm with the","chapter-3","Algorithm Analysis"
"following growth rate equations?","chapter-3","Algorithm Analysis"
"n n2 n","chapter-3","Algorithm Analysis"
"3","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"n","chapter-3","Algorithm Analysis"
"3.6 (a) Find a growth rate that squares the run time when we double the input","chapter-3","Algorithm Analysis"
"size. That is, if T(n) = X, then T(2n) = x","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"(b) Find a growth rate that cubes the run time when we double the input","chapter-3","Algorithm Analysis"
"size. That is, if T(n) = X, then T(2n) = x","chapter-3","Algorithm Analysis"
"3","chapter-3","Algorithm Analysis"
"3.7 Using the definition of big-Oh, show that 1 is in O(1) and that 1 is in O(n).","chapter-3","Algorithm Analysis"
"3.8 Using the definitions of big-Oh and Ω, find the upper and lower bounds for","chapter-3","Algorithm Analysis"
"the following expressions. Be sure to state appropriate values for c and n0.","chapter-3","Algorithm Analysis"
"86 Chap. 3 Algorithm Analysis","chapter-3","Algorithm Analysis"
"(a) c1n","chapter-3","Algorithm Analysis"
"(b) c2n","chapter-3","Algorithm Analysis"
"3 + c3","chapter-3","Algorithm Analysis"
"(c) c4n log n + c5n","chapter-3","Algorithm Analysis"
"(d) c62","chapter-3","Algorithm Analysis"
"n + c7n","chapter-3","Algorithm Analysis"
"6","chapter-3","Algorithm Analysis"
"3.9 (a) What is the smallest integer k such that √","chapter-3","Algorithm Analysis"
"n = O(n","chapter-3","Algorithm Analysis"
"k","chapter-3","Algorithm Analysis"
")?","chapter-3","Algorithm Analysis"
"(b) What is the smallest integer k such that n log n = O(n","chapter-3","Algorithm Analysis"
"k","chapter-3","Algorithm Analysis"
")?","chapter-3","Algorithm Analysis"
"3.10 (a) Is 2n = Θ(3n)? Explain why or why not.","chapter-3","Algorithm Analysis"
"(b) Is 2","chapter-3","Algorithm Analysis"
"n = Θ(3n","chapter-3","Algorithm Analysis"
")? Explain why or why not.","chapter-3","Algorithm Analysis"
"3.11 For each of the following pairs of functions, either f(n) is in O(g(n)), f(n)","chapter-3","Algorithm Analysis"
"is in Ω(g(n)), or f(n) = Θ(g(n)). For each pair, determine which relation-","chapter-3","Algorithm Analysis"
"ship is correct. Justify your answer, using the method of limits discussed in","chapter-3","Algorithm Analysis"
"Section 3.4.5.","chapter-3","Algorithm Analysis"
"(a) f(n) = log n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"; g(n) = log n + 5.","chapter-3","Algorithm Analysis"
"(b) f(n) = √","chapter-3","Algorithm Analysis"
"n; g(n) = log n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
".","chapter-3","Algorithm Analysis"
"(c) f(n) = log2 n; g(n) = log n.","chapter-3","Algorithm Analysis"
"(d) f(n) = n; g(n) = log2n.","chapter-3","Algorithm Analysis"
"(e) f(n) = n log n + n; g(n) = log n.","chapter-3","Algorithm Analysis"
"(f) f(n) = log n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
"; g(n) = (log n)","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
".","chapter-3","Algorithm Analysis"
"(g) f(n) = 10; g(n) = log 10.","chapter-3","Algorithm Analysis"
"(h) f(n) = 2n","chapter-3","Algorithm Analysis"
"; g(n) = 10n","chapter-3","Algorithm Analysis"
"2","chapter-3","Algorithm Analysis"
".","chapter-3","Algorithm Analysis"
"(i) f(n) = 2n","chapter-3","Algorithm Analysis"
"; g(n) = n log n.","chapter-3","Algorithm Analysis"
"(j) f(n) = 2n","chapter-3","Algorithm Analysis"
"; g(n) = 3n","chapter-3","Algorithm Analysis"
".","chapter-3","Algorithm Analysis"
"(k) f(n) = 2n","chapter-3","Algorithm Analysis"
"; g(n) = n","chapter-3","Algorithm Analysis"
"n","chapter-3","Algorithm Analysis"
".","chapter-3","Algorithm Analysis"
"3.12 Determine Θ for the following code fragments in the average case. Assume","chapter-3","Algorithm Analysis"
"that all variables are of type int.","chapter-3","Algorithm Analysis"
"(a) a = b + c;","chapter-3","Algorithm Analysis"
"d = a + e;","chapter-3","Algorithm Analysis"
"(b) sum = 0;","chapter-3","Algorithm Analysis"
"for (i=0; i<3; i++)","chapter-3","Algorithm Analysis"
"for (j=0; j<n; j++)","chapter-3","Algorithm Analysis"
"sum++;","chapter-3","Algorithm Analysis"
"(c) sum=0;","chapter-3","Algorithm Analysis"
"for (i=0; i<n*n; i++)","chapter-3","Algorithm Analysis"
"sum++;","chapter-3","Algorithm Analysis"
"(d) for (i=0; i < n-1; i++)","chapter-3","Algorithm Analysis"
"for (j=i+1; j < n; j++) {","chapter-3","Algorithm Analysis"
"tmp = AA[i][j];","chapter-3","Algorithm Analysis"
"AA[i][j] = AA[j][i];","chapter-3","Algorithm Analysis"
"AA[j][i] = tmp;","chapter-3","Algorithm Analysis"
"}","chapter-3","Algorithm Analysis"
"(e) sum = 0;","chapter-3","Algorithm Analysis"
"for (i=1; i<=n; i++)","chapter-3","Algorithm Analysis"
"for (j=1; j<=n; j*=2)","chapter-3","Algorithm Analysis"
"sum++;","chapter-3","Algorithm Analysis"
"Sec. 3.13 Exercises 87","chapter-3","Algorithm Analysis"
"(f) sum = 0;","chapter-3","Algorithm Analysis"
"for (i=1; i<=n; i*=2)","chapter-3","Algorithm Analysis"
"for (j=1; j<=n; j++)","chapter-3","Algorithm Analysis"
"sum++;","chapter-3","Algorithm Analysis"
"(g) Assume that array A contains n values, Random takes constant time,","chapter-3","Algorithm Analysis"
"and sort takes n log n steps.","chapter-3","Algorithm Analysis"
"for (i=0; i<n; i++) {","chapter-3","Algorithm Analysis"
"for (j=0; j<n; j++)","chapter-3","Algorithm Analysis"
"A[j] = DSutil.random(n);","chapter-3","Algorithm Analysis"
"sort(A);","chapter-3","Algorithm Analysis"
"}","chapter-3","Algorithm Analysis"
"(h) Assume array A contains a random permutation of the values from 0 to","chapter-3","Algorithm Analysis"
"n − 1.","chapter-3","Algorithm Analysis"
"sum = 0;","chapter-3","Algorithm Analysis"
"for (i=0; i<n; i++)","chapter-3","Algorithm Analysis"
"for (j=0; A[j]!=i; j++)","chapter-3","Algorithm Analysis"
"sum++;","chapter-3","Algorithm Analysis"
"(i) sum = 0;","chapter-3","Algorithm Analysis"
"if (EVEN(n))","chapter-3","Algorithm Analysis"
"for (i=0; i<n; i++)","chapter-3","Algorithm Analysis"
"sum++;","chapter-3","Algorithm Analysis"
"else","chapter-3","Algorithm Analysis"
"sum = sum + n;","chapter-3","Algorithm Analysis"
"3.13 Show that big-Theta notation (Θ) defines an equivalence relation on the set","chapter-3","Algorithm Analysis"
"of functions.","chapter-3","Algorithm Analysis"
"3.14 Give the best lower bound that you can for the following code fragment, as a","chapter-3","Algorithm Analysis"
"function of the initial value of n.","chapter-3","Algorithm Analysis"
"while (n > 1)","chapter-3","Algorithm Analysis"
"if (ODD(n))","chapter-3","Algorithm Analysis"
"n = 3 * n + 1;","chapter-3","Algorithm Analysis"
"else","chapter-3","Algorithm Analysis"
"n = n / 2;","chapter-3","Algorithm Analysis"
"Do you think that the upper bound is likely to be the same as the answer you","chapter-3","Algorithm Analysis"
"gave for the lower bound?","chapter-3","Algorithm Analysis"
"3.15 Does every algorithm have a Θ running-time equation? In other words, are","chapter-3","Algorithm Analysis"
"the upper and lower bounds for the running time (on any specified class of","chapter-3","Algorithm Analysis"
"inputs) always the same?","chapter-3","Algorithm Analysis"
"3.16 Does every problem for which there exists some algorithm have a Θ running-","chapter-3","Algorithm Analysis"
"time equation? In other words, for every problem, and for any specified","chapter-3","Algorithm Analysis"
"class of inputs, is there some algorithm whose upper bound is equal to the","chapter-3","Algorithm Analysis"
"problem’s lower bound?","chapter-3","Algorithm Analysis"
"3.17 Given an array storing integers ordered by value, modify the binary search","chapter-3","Algorithm Analysis"
"routine to return the position of the first integer with value K in the situation","chapter-3","Algorithm Analysis"
"where K can appear multiple times in the array. Be sure that your algorithm","chapter-3","Algorithm Analysis"
"88 Chap. 3 Algorithm Analysis","chapter-3","Algorithm Analysis"
"is Θ(log n), that is, do not resort to sequential search once an occurrence of","chapter-3","Algorithm Analysis"
"K is found.","chapter-3","Algorithm Analysis"
"3.18 Given an array storing integers ordered by value, modify the binary search","chapter-3","Algorithm Analysis"
"routine to return the position of the integer with the greatest value less than","chapter-3","Algorithm Analysis"
"K when K itself does not appear in the array. Return ERROR if the least","chapter-3","Algorithm Analysis"
"value in the array is greater than K.","chapter-3","Algorithm Analysis"
"3.19 Modify the binary search routine to support search in an array of infinite","chapter-3","Algorithm Analysis"
"size. In particular, you are given as input a sorted array and a key value","chapter-3","Algorithm Analysis"
"K to search for. Call n the position of the smallest value in the array that","chapter-3","Algorithm Analysis"
"is equal to or larger than X. Provide an algorithm that can determine n in","chapter-3","Algorithm Analysis"
"O(log n) comparisons in the worst case. Explain why your algorithm meets","chapter-3","Algorithm Analysis"
"the required time bound.","chapter-3","Algorithm Analysis"
"3.20 It is possible to change the way that we pick the dividing point in a binary","chapter-3","Algorithm Analysis"
"search, and still get a working search routine. However, where we pick the","chapter-3","Algorithm Analysis"
"dividing point could affect the performance of the algorithm.","chapter-3","Algorithm Analysis"
"(a) If we change the dividing point computation in function binary from","chapter-3","Algorithm Analysis"
"i = (l + r)/2 to i = (l + ((r − l)/3)), what will the worst-case run-","chapter-3","Algorithm Analysis"
"ning time be in asymptotic terms? If the difference is only a constant","chapter-3","Algorithm Analysis"
"time factor, how much slower or faster will the modified program be","chapter-3","Algorithm Analysis"
"compared to the original version of binary?","chapter-3","Algorithm Analysis"
"(b) If we change the dividing point computation in function binary from","chapter-3","Algorithm Analysis"
"i = (l + r)/2 to i = r − 2, what will the worst-case running time be in","chapter-3","Algorithm Analysis"
"asymptotic terms? If the difference is only a constant time factor, how","chapter-3","Algorithm Analysis"
"much slower or faster will the modified program be compared to the","chapter-3","Algorithm Analysis"
"original version of binary?","chapter-3","Algorithm Analysis"
"3.21 Design an algorithm to assemble a jigsaw puzzle. Assume that each piece","chapter-3","Algorithm Analysis"
"has four sides, and that each piece’s final orientation is known (top, bottom,","chapter-3","Algorithm Analysis"
"etc.). Assume that you have available a function","chapter-3","Algorithm Analysis"
"boolean compare(Piece a, Piece b, Side ad)","chapter-3","Algorithm Analysis"
"that can tell, in constant time, whether piece a connects to piece b on a’s","chapter-3","Algorithm Analysis"
"side ad and b’s opposite side bd. The input to your algorithm should consist","chapter-3","Algorithm Analysis"
"of an n × m array of random pieces, along with dimensions n and m. The","chapter-3","Algorithm Analysis"
"algorithm should put the pieces in their correct positions in the array. Your","chapter-3","Algorithm Analysis"
"algorithm should be as efficient as possible in the asymptotic sense. Write","chapter-3","Algorithm Analysis"
"a summation for the running time of your algorithm on n pieces, and then","chapter-3","Algorithm Analysis"
"derive a closed-form solution for the summation.","chapter-3","Algorithm Analysis"
"3.22 Can the average case cost for an algorithm be worse than the worst case cost?","chapter-3","Algorithm Analysis"
"Can it be better than the best case cost? Explain why or why not.","chapter-3","Algorithm Analysis"
"3.23 Prove that if an algorithm is Θ(f(n)) in the average case, then it is Ω(f(n))","chapter-3","Algorithm Analysis"
"in the worst case.","chapter-3","Algorithm Analysis"
"Sec. 3.14 Projects 89","chapter-3","Algorithm Analysis"
"3.24 Prove that if an algorithm is Θ(f(n)) in the average case, then it is O(f(n))","chapter-3","Algorithm Analysis"
"in the best case.","chapter-3","Algorithm Analysis"
"3.14 Projects","chapter-3","Algorithm Analysis"
"3.1 Imagine that you are trying to store 32 Boolean values, and must access","chapter-3","Algorithm Analysis"
"them frequently. Compare the time required to access Boolean values stored","chapter-3","Algorithm Analysis"
"alternatively as a single bit field, a character, a short integer, or a long integer.","chapter-3","Algorithm Analysis"
"There are two things to be careful of when writing your program. First, be","chapter-3","Algorithm Analysis"
"sure that your program does enough variable accesses to make meaningful","chapter-3","Algorithm Analysis"
"measurements. A single access takes much less time than a single unit of","chapter-3","Algorithm Analysis"
"measurement (typically milliseconds) for all four methods. Second, be sure","chapter-3","Algorithm Analysis"
"that your program spends as much time as possible doing variable accesses","chapter-3","Algorithm Analysis"
"rather than other things such as calling timing functions or incrementing for","chapter-3","Algorithm Analysis"
"loop counters.","chapter-3","Algorithm Analysis"
"3.2 Implement sequential search and binary search algorithms on your computer.","chapter-3","Algorithm Analysis"
"Run timings for each algorithm on arrays of size n = 10i","chapter-3","Algorithm Analysis"
"for i ranging from","chapter-3","Algorithm Analysis"
"1 to as large a value as your computer’s memory and compiler will allow. For","chapter-3","Algorithm Analysis"
"both algorithms, store the values 0 through n − 1 in order in the array, and","chapter-3","Algorithm Analysis"
"use a variety of random search values in the range 0 to n − 1 on each size","chapter-3","Algorithm Analysis"
"n. Graph the resulting times. When is sequential search faster than binary","chapter-3","Algorithm Analysis"
"search for a sorted array?","chapter-3","Algorithm Analysis"
"3.3 Implement a program that runs and gives timings for the two Fibonacci se-","chapter-3","Algorithm Analysis"
"quence functions provided in Exercise 2.11. Graph the resulting running","chapter-3","Algorithm Analysis"
"times for as many values of n as your computer can handle.","chapter-3","Algorithm Analysis"
"If your program needs to store a few things — numbers, payroll records, or job de-","chapter-4","Lists, Stacks, and Queues"
"scriptions for example — the simplest and most effective approach might be to put","chapter-4","Lists, Stacks, and Queues"
"them in a list. Only when you have to organize and search through a large number","chapter-4","Lists, Stacks, and Queues"
"of things do more sophisticated data structures usually become necessary. (We will","chapter-4","Lists, Stacks, and Queues"
"study how to organize and search through medium amounts of data in Chapters 5, 7,","chapter-4","Lists, Stacks, and Queues"
"and 9, and discuss how to deal with large amounts of data in Chapters 8–10.) Many","chapter-4","Lists, Stacks, and Queues"
"applications don’t require any form of search, and they do not require that any or-","chapter-4","Lists, Stacks, and Queues"
"dering be placed on the objects being stored. Some applications require processing","chapter-4","Lists, Stacks, and Queues"
"in a strict chronological order, processing objects in the order that they arrived, or","chapter-4","Lists, Stacks, and Queues"
"perhaps processing objects in the reverse of the order that they arrived. For all these","chapter-4","Lists, Stacks, and Queues"
"situations, a simple list structure is appropriate.","chapter-4","Lists, Stacks, and Queues"
"This chapter describes representations for lists in general, as well as two impor-","chapter-4","Lists, Stacks, and Queues"
"tant list-like structures called the stack and the queue. Along with presenting these","chapter-4","Lists, Stacks, and Queues"
"fundamental data structures, the other goals of the chapter are to: (1) Give examples","chapter-4","Lists, Stacks, and Queues"
"of separating a logical representation in the form of an ADT from a physical im-","chapter-4","Lists, Stacks, and Queues"
"plementation for a data structure. (2) Illustrate the use of asymptotic analysis in the","chapter-4","Lists, Stacks, and Queues"
"context of some simple operations that you might already be familiar with. In this","chapter-4","Lists, Stacks, and Queues"
"way you can begin to see how asymptotic analysis works, without the complica-","chapter-4","Lists, Stacks, and Queues"
"tions that arise when analyzing more sophisticated algorithms and data structures.","chapter-4","Lists, Stacks, and Queues"
"(3) Introduce the concept and use of dictionaries.","chapter-4","Lists, Stacks, and Queues"
"We begin by defining an ADT for lists in Section 4.1. Two implementations for","chapter-4","Lists, Stacks, and Queues"
"the list ADT — the array-based list and the linked list — are covered in detail and","chapter-4","Lists, Stacks, and Queues"
"their relative merits discussed. Sections 4.2 and 4.3 cover stacks and queues, re-","chapter-4","Lists, Stacks, and Queues"
"spectively. Sample implementations for each of these data structures are presented.","chapter-4","Lists, Stacks, and Queues"
"Section 4.4 presents the Dictionary ADT for storing and retrieving data, which sets","chapter-4","Lists, Stacks, and Queues"
"a context for implementing search structures such as the Binary Search Tree of","chapter-4","Lists, Stacks, and Queues"
"Section 5.4.","chapter-4","Lists, Stacks, and Queues"
"93","chapter-4","Lists, Stacks, and Queues"
"94 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"4.1 Lists","chapter-4","Lists, Stacks, and Queues"
"We all have an intuitive understanding of what we mean by a “list.” Our first step is","chapter-4","Lists, Stacks, and Queues"
"to define precisely what is meant so that this intuitive understanding can eventually","chapter-4","Lists, Stacks, and Queues"
"be converted into a concrete data structure and its operations. The most important","chapter-4","Lists, Stacks, and Queues"
"concept related to lists is that of position. In other words, we perceive that there","chapter-4","Lists, Stacks, and Queues"
"is a first element in the list, a second element, and so on. We should view a list as","chapter-4","Lists, Stacks, and Queues"
"embodying the mathematical concepts of a sequence, as defined in Section 2.1.","chapter-4","Lists, Stacks, and Queues"
"We define a list to be a finite, ordered sequence of data items known as ele-","chapter-4","Lists, Stacks, and Queues"
"ments. “Ordered” in this definition means that each element has a position in the","chapter-4","Lists, Stacks, and Queues"
"list. (We will not use “ordered” in this context to mean that the list elements are","chapter-4","Lists, Stacks, and Queues"
"sorted by value.) Each list element has a data type. In the simple list implemen-","chapter-4","Lists, Stacks, and Queues"
"tations discussed in this chapter, all elements of the list have the same data type,","chapter-4","Lists, Stacks, and Queues"
"although there is no conceptual objection to lists whose elements have differing","chapter-4","Lists, Stacks, and Queues"
"data types if the application requires it (see Section 12.1). The operations defined","chapter-4","Lists, Stacks, and Queues"
"as part of the list ADT do not depend on the elemental data type. For example, the","chapter-4","Lists, Stacks, and Queues"
"list ADT can be used for lists of integers, lists of characters, lists of payroll records,","chapter-4","Lists, Stacks, and Queues"
"even lists of lists.","chapter-4","Lists, Stacks, and Queues"
"A list is said to be empty when it contains no elements. The number of ele-","chapter-4","Lists, Stacks, and Queues"
"ments currently stored is called the length of the list. The beginning of the list is","chapter-4","Lists, Stacks, and Queues"
"called the head, the end of the list is called the tail. There might or might not be","chapter-4","Lists, Stacks, and Queues"
"some relationship between the value of an element and its position in the list. For","chapter-4","Lists, Stacks, and Queues"
"example, sorted lists have their elements positioned in ascending order of value,","chapter-4","Lists, Stacks, and Queues"
"while unsorted lists have no particular relationship between element values and","chapter-4","Lists, Stacks, and Queues"
"positions. This section will consider only unsorted lists. Chapters 7 and 9 treat the","chapter-4","Lists, Stacks, and Queues"
"problems of how to create and search sorted lists efficiently.","chapter-4","Lists, Stacks, and Queues"
"When presenting the contents of a list, we use the same notation as was in-","chapter-4","Lists, Stacks, and Queues"
"troduced for sequences in Section 2.1. To be consistent with Java array indexing,","chapter-4","Lists, Stacks, and Queues"
"the first position on the list is denoted as 0. Thus, if there are n elements in the","chapter-4","Lists, Stacks, and Queues"
"list, they are given positions 0 through n − 1 as ha0, a1, ..., an−1i. The subscript","chapter-4","Lists, Stacks, and Queues"
"indicates an element’s position within the list. Using this notation, the empty list","chapter-4","Lists, Stacks, and Queues"
"would appear as hi.","chapter-4","Lists, Stacks, and Queues"
"Before selecting a list implementation, a program designer should first consider","chapter-4","Lists, Stacks, and Queues"
"what basic operations the implementation must support. Our common intuition","chapter-4","Lists, Stacks, and Queues"
"about lists tells us that a list should be able to grow and shrink in size as we insert","chapter-4","Lists, Stacks, and Queues"
"and remove elements. We should be able to insert and remove elements from any-","chapter-4","Lists, Stacks, and Queues"
"where in the list. We should be able to gain access to any element’s value, either to","chapter-4","Lists, Stacks, and Queues"
"read it or to change it. We must be able to create and clear (or reinitialize) lists. It","chapter-4","Lists, Stacks, and Queues"
"is also convenient to access the next or previous element from the “current” one.","chapter-4","Lists, Stacks, and Queues"
"The next step is to define the ADT for a list object in terms of a set of operations","chapter-4","Lists, Stacks, and Queues"
"on that object. We will use the Java notation of an interface to formally define the","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.1 Lists 95","chapter-4","Lists, Stacks, and Queues"
"list ADT. Interface List defines the member functions that any list implementa-","chapter-4","Lists, Stacks, and Queues"
"tion inheriting from it must support, along with their parameters and return types.","chapter-4","Lists, Stacks, and Queues"
"We increase the flexibility of the list ADT by writing it as a Java generic.","chapter-4","Lists, Stacks, and Queues"
"True to the notion of an ADT, an interface does not specify how operations","chapter-4","Lists, Stacks, and Queues"
"are implemented. Two complete implementations are presented later in this sec-","chapter-4","Lists, Stacks, and Queues"
"tion, both of which use the same list ADT to define their operations, but they are","chapter-4","Lists, Stacks, and Queues"
"considerably different in approaches and in their space/time tradeoffs.","chapter-4","Lists, Stacks, and Queues"
"Figure 4.1 presents our list ADT. Class List is a generic of one parameter,","chapter-4","Lists, Stacks, and Queues"
"named E for “element”. E serves as a placeholder for whatever element type the","chapter-4","Lists, Stacks, and Queues"
"user would like to store in a list. The comments given in Figure 4.1 describe pre-","chapter-4","Lists, Stacks, and Queues"
"cisely what each member function is intended to do. However, some explanation","chapter-4","Lists, Stacks, and Queues"
"of the basic design is in order. Given that we wish to support the concept of a se-","chapter-4","Lists, Stacks, and Queues"
"quence, with access to any position in the list, the need for many of the member","chapter-4","Lists, Stacks, and Queues"
"functions such as insert and moveToPos is clear. The key design decision em-","chapter-4","Lists, Stacks, and Queues"
"bodied in this ADT is support for the concept of a current position. For example,","chapter-4","Lists, Stacks, and Queues"
"member moveToStart sets the current position to be the first element on the list,","chapter-4","Lists, Stacks, and Queues"
"while methods next and prev move the current position to the next and previ-","chapter-4","Lists, Stacks, and Queues"
"ous elements, respectively. The intention is that any implementation for this ADT","chapter-4","Lists, Stacks, and Queues"
"support the concept of a current position. The current position is where any action","chapter-4","Lists, Stacks, and Queues"
"such as insertion or deletion will take place.","chapter-4","Lists, Stacks, and Queues"
"Since insertions take place at the current position, and since we want to be able","chapter-4","Lists, Stacks, and Queues"
"to insert to the front or the back of the list as well as anywhere in between, there are","chapter-4","Lists, Stacks, and Queues"
"actually n + 1 possible “current positions” when there are n elements in the list.","chapter-4","Lists, Stacks, and Queues"
"It is helpful to modify our list display notation to show the position of the","chapter-4","Lists, Stacks, and Queues"
"current element. I will use a vertical bar, such as h20, 23 | 12, 15i to indicate","chapter-4","Lists, Stacks, and Queues"
"the list of four elements, with the current position being to the right of the bar at","chapter-4","Lists, Stacks, and Queues"
"element 12. Given this configuration, calling insert with value 10 will change","chapter-4","Lists, Stacks, and Queues"
"the list to be h20, 23 | 10, 12, 15i.","chapter-4","Lists, Stacks, and Queues"
"If you examine Figure 4.1, you should find that the list member functions pro-","chapter-4","Lists, Stacks, and Queues"
"vided allow you to build a list with elements in any desired order, and to access","chapter-4","Lists, Stacks, and Queues"
"any desired position in the list. You might notice that the clear method is not","chapter-4","Lists, Stacks, and Queues"
"necessary, in that it could be implemented by means of the other member functions","chapter-4","Lists, Stacks, and Queues"
"in the same asymptotic time. It is included merely for convenience.","chapter-4","Lists, Stacks, and Queues"
"Method getValue returns a reference to the current element. It is considered","chapter-4","Lists, Stacks, and Queues"
"a violation of getValue’s preconditions to ask for the value of a non-existent ele-","chapter-4","Lists, Stacks, and Queues"
"ment (i.e., there must be something to the right of the vertical bar). In our concrete","chapter-4","Lists, Stacks, and Queues"
"list implementations, assertions are used to enforce such preconditions. In a com-","chapter-4","Lists, Stacks, and Queues"
"mercial implementation, such violations would be best implemented by the Java","chapter-4","Lists, Stacks, and Queues"
"exception mechanism.","chapter-4","Lists, Stacks, and Queues"
"A list can be iterated through as shown in the following code fragment.","chapter-4","Lists, Stacks, and Queues"
"96 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"/** List ADT */","chapter-4","Lists, Stacks, and Queues"
"public interface List<E> {","chapter-4","Lists, Stacks, and Queues"
"/** Remove all contents from the list, so it is once again","chapter-4","Lists, Stacks, and Queues"
"empty. Client is responsible for reclaiming storage","chapter-4","Lists, Stacks, and Queues"
"used by the list elements. */","chapter-4","Lists, Stacks, and Queues"
"public void clear();","chapter-4","Lists, Stacks, and Queues"
"/** Insert an element at the current location. The client","chapter-4","Lists, Stacks, and Queues"
"must ensure that the list’s capacity is not exceeded.","chapter-4","Lists, Stacks, and Queues"
"@param item The element to be inserted. */","chapter-4","Lists, Stacks, and Queues"
"public void insert(E item);","chapter-4","Lists, Stacks, and Queues"
"/** Append an element at the end of the list. The client","chapter-4","Lists, Stacks, and Queues"
"must ensure that the list’s capacity is not exceeded.","chapter-4","Lists, Stacks, and Queues"
"@param item The element to be appended. */","chapter-4","Lists, Stacks, and Queues"
"public void append(E item);","chapter-4","Lists, Stacks, and Queues"
"/** Remove and return the current element.","chapter-4","Lists, Stacks, and Queues"
"@return The element that was removed. */","chapter-4","Lists, Stacks, and Queues"
"public E remove();","chapter-4","Lists, Stacks, and Queues"
"/** Set the current position to the start of the list */","chapter-4","Lists, Stacks, and Queues"
"public void moveToStart();","chapter-4","Lists, Stacks, and Queues"
"/** Set the current position to the end of the list */","chapter-4","Lists, Stacks, and Queues"
"public void moveToEnd();","chapter-4","Lists, Stacks, and Queues"
"/** Move the current position one step left. No change","chapter-4","Lists, Stacks, and Queues"
"if already at beginning. */","chapter-4","Lists, Stacks, and Queues"
"public void prev();","chapter-4","Lists, Stacks, and Queues"
"/** Move the current position one step right. No change","chapter-4","Lists, Stacks, and Queues"
"if already at end. */","chapter-4","Lists, Stacks, and Queues"
"public void next();","chapter-4","Lists, Stacks, and Queues"
"/** @return The number of elements in the list. */","chapter-4","Lists, Stacks, and Queues"
"public int length();","chapter-4","Lists, Stacks, and Queues"
"/** @return The position of the current element. */","chapter-4","Lists, Stacks, and Queues"
"public int currPos();","chapter-4","Lists, Stacks, and Queues"
"/** Set current position.","chapter-4","Lists, Stacks, and Queues"
"@param pos The position to make current. */","chapter-4","Lists, Stacks, and Queues"
"public void moveToPos(int pos);","chapter-4","Lists, Stacks, and Queues"
"/** @return The current element. */","chapter-4","Lists, Stacks, and Queues"
"public E getValue();","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"Figure 4.1 The ADT for a list.","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.1 Lists 97","chapter-4","Lists, Stacks, and Queues"
"for (L.moveToStart(); L.currPos()<L.length(); L.next()) {","chapter-4","Lists, Stacks, and Queues"
"it = L.getValue();","chapter-4","Lists, Stacks, and Queues"
"doSomething(it);","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"In this example, each element of the list in turn is stored in it, and passed to the","chapter-4","Lists, Stacks, and Queues"
"doSomething function. The loop terminates when the current position reaches","chapter-4","Lists, Stacks, and Queues"
"the end of the list.","chapter-4","Lists, Stacks, and Queues"
"The list class declaration presented here is just one of many possible interpreta-","chapter-4","Lists, Stacks, and Queues"
"tions for lists. Figure 4.1 provides most of the operations that one naturally expects","chapter-4","Lists, Stacks, and Queues"
"to perform on lists and serves to illustrate the issues relevant to implementing the","chapter-4","Lists, Stacks, and Queues"
"list data structure. As an example of using the list ADT, we can create a function","chapter-4","Lists, Stacks, and Queues"
"to return true if there is an occurrence of a given integer in the list, and false","chapter-4","Lists, Stacks, and Queues"
"otherwise. The find method needs no knowledge about the specific list imple-","chapter-4","Lists, Stacks, and Queues"
"mentation, just the list ADT.","chapter-4","Lists, Stacks, and Queues"
"/** @return True if k is in list L, false otherwise */","chapter-4","Lists, Stacks, and Queues"
"public static boolean find(List<Integer> L, int k) {","chapter-4","Lists, Stacks, and Queues"
"for (L.moveToStart(); L.currPos()<L.length(); L.next())","chapter-4","Lists, Stacks, and Queues"
"if (k == L.getValue()) return true; // Found k","chapter-4","Lists, Stacks, and Queues"
"return false; // k not found","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"While this implementation for find could be written as a generic with respect","chapter-4","Lists, Stacks, and Queues"
"to the element type, it would still be limited in its ability to handle different data","chapter-4","Lists, Stacks, and Queues"
"types stored on the list. In particular, it only works when the description for the","chapter-4","Lists, Stacks, and Queues"
"object being searched for (k in the function) is of the same type as the objects","chapter-4","Lists, Stacks, and Queues"
"themselves, and that can meaningfully be compared when using the == comparison","chapter-4","Lists, Stacks, and Queues"
"operator. A more typical situation is that we are searching for a record that contains","chapter-4","Lists, Stacks, and Queues"
"a key field who’s value matches k. Similar functions to find and return a composite","chapter-4","Lists, Stacks, and Queues"
"element based on a key value can be created using the list implementation, but to","chapter-4","Lists, Stacks, and Queues"
"do so requires some agreement between the list ADT and the find function on the","chapter-4","Lists, Stacks, and Queues"
"concept of a key, and on how keys may be compared. This topic will be discussed","chapter-4","Lists, Stacks, and Queues"
"in Section 4.4.","chapter-4","Lists, Stacks, and Queues"
"4.1.1 Array-Based List Implementation","chapter-4","Lists, Stacks, and Queues"
"There are two standard approaches to implementing lists, the array-based list, and","chapter-4","Lists, Stacks, and Queues"
"the linked list. This section discusses the array-based approach. The linked list is","chapter-4","Lists, Stacks, and Queues"
"presented in Section 4.1.2. Time and space efficiency comparisons for the two are","chapter-4","Lists, Stacks, and Queues"
"discussed in Section 4.1.3.","chapter-4","Lists, Stacks, and Queues"
"Figure 4.2 shows the array-based list implementation, named AList. AList","chapter-4","Lists, Stacks, and Queues"
"inherits from abstract class List and so must implement all of the member func-","chapter-4","Lists, Stacks, and Queues"
"tions of List.","chapter-4","Lists, Stacks, and Queues"
"Class AList’s private portion contains the data members for the array-based","chapter-4","Lists, Stacks, and Queues"
"list. These include listArray, the array which holds the list elements. Because","chapter-4","Lists, Stacks, and Queues"
"98 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"/** Array-based list implementation */","chapter-4","Lists, Stacks, and Queues"
"class AList<E> implements List<E> {","chapter-4","Lists, Stacks, and Queues"
"private static final int defaultSize = 10; // Default size","chapter-4","Lists, Stacks, and Queues"
"private int maxSize; // Maximum size of list","chapter-4","Lists, Stacks, and Queues"
"private int listSize; // Current # of list items","chapter-4","Lists, Stacks, and Queues"
"private int curr; // Position of current element","chapter-4","Lists, Stacks, and Queues"
"private E[] listArray; // Array holding list elements","chapter-4","Lists, Stacks, and Queues"
"/** Constructors */","chapter-4","Lists, Stacks, and Queues"
"/** Create a list with the default capacity. */","chapter-4","Lists, Stacks, and Queues"
"AList() { this(defaultSize); }","chapter-4","Lists, Stacks, and Queues"
"/** Create a new list object.","chapter-4","Lists, Stacks, and Queues"
"@param size Max # of elements list can contain. */","chapter-4","Lists, Stacks, and Queues"
"@SuppressWarnings("unchecked") // Generic array allocation","chapter-4","Lists, Stacks, and Queues"
"AList(int size) {","chapter-4","Lists, Stacks, and Queues"
"maxSize = size;","chapter-4","Lists, Stacks, and Queues"
"listSize = curr = 0;","chapter-4","Lists, Stacks, and Queues"
"listArray = (E[])new Object[size]; // Create listArray","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"public void clear() // Reinitialize the list","chapter-4","Lists, Stacks, and Queues"
"{ listSize = curr = 0; } // Simply reinitialize values","chapter-4","Lists, Stacks, and Queues"
"/** Insert "it" at current position */","chapter-4","Lists, Stacks, and Queues"
"public void insert(E it) {","chapter-4","Lists, Stacks, and Queues"
"assert listSize < maxSize : "List capacity exceeded";","chapter-4","Lists, Stacks, and Queues"
"for (int i=listSize; i>curr; i--) // Shift elements up","chapter-4","Lists, Stacks, and Queues"
"listArray[i] = listArray[i-1]; // to make room","chapter-4","Lists, Stacks, and Queues"
"listArray[curr] = it;","chapter-4","Lists, Stacks, and Queues"
"listSize++; // Increment list size","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Append "it" to list */","chapter-4","Lists, Stacks, and Queues"
"public void append(E it) {","chapter-4","Lists, Stacks, and Queues"
"assert listSize < maxSize : "List capacity exceeded";","chapter-4","Lists, Stacks, and Queues"
"listArray[listSize++] = it;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Remove and return the current element */","chapter-4","Lists, Stacks, and Queues"
"public E remove() {","chapter-4","Lists, Stacks, and Queues"
"if ((curr<0) || (curr>=listSize)) // No current element","chapter-4","Lists, Stacks, and Queues"
"return null;","chapter-4","Lists, Stacks, and Queues"
"E it = listArray[curr]; // Copy the element","chapter-4","Lists, Stacks, and Queues"
"for(int i=curr; i<listSize-1; i++) // Shift them down","chapter-4","Lists, Stacks, and Queues"
"listArray[i] = listArray[i+1];","chapter-4","Lists, Stacks, and Queues"
"listSize--; // Decrement size","chapter-4","Lists, Stacks, and Queues"
"return it;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"Figure 4.2 An array-based list implementation.","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.1 Lists 99","chapter-4","Lists, Stacks, and Queues"
"public void moveToStart() { curr = 0; } // Set to front","chapter-4","Lists, Stacks, and Queues"
"public void moveToEnd() { curr = listSize; } // Set at end","chapter-4","Lists, Stacks, and Queues"
"public void prev() { if (curr != 0) curr--; } // Back up","chapter-4","Lists, Stacks, and Queues"
"public void next() { if (curr < listSize) curr++; }","chapter-4","Lists, Stacks, and Queues"
"/** @return List size */","chapter-4","Lists, Stacks, and Queues"
"public int length() { return listSize; }","chapter-4","Lists, Stacks, and Queues"
"/** @return Current position */","chapter-4","Lists, Stacks, and Queues"
"public int currPos() { return curr; }","chapter-4","Lists, Stacks, and Queues"
"/** Set current list position to "pos" */","chapter-4","Lists, Stacks, and Queues"
"public void moveToPos(int pos) {","chapter-4","Lists, Stacks, and Queues"
"assert (pos>=0) && (pos<=listSize) : "Pos out of range";","chapter-4","Lists, Stacks, and Queues"
"curr = pos;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** @return Current element */","chapter-4","Lists, Stacks, and Queues"
"public E getValue() {","chapter-4","Lists, Stacks, and Queues"
"assert (curr>=0) && (curr<listSize) :","chapter-4","Lists, Stacks, and Queues"
""No current element";","chapter-4","Lists, Stacks, and Queues"
"return listArray[curr];","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"Figure 4.2 (continued)","chapter-4","Lists, Stacks, and Queues"
"listArray must be allocated at some fixed size, the size of the array must be","chapter-4","Lists, Stacks, and Queues"
"known when the list object is created. Note that an optional parameter is declared","chapter-4","Lists, Stacks, and Queues"
"for the AList constructor. With this parameter, the user can indicate the maximum","chapter-4","Lists, Stacks, and Queues"
"number of elements permitted in the list. If no parameter is given, then it takes the","chapter-4","Lists, Stacks, and Queues"
"value defaultSize, which is assumed to be a suitably defined constant value.","chapter-4","Lists, Stacks, and Queues"
"Because each list can have a differently sized array, each list must remember","chapter-4","Lists, Stacks, and Queues"
"its maximum permitted size. Data member maxSize serves this purpose. At any","chapter-4","Lists, Stacks, and Queues"
"given time the list actually holds some number of elements that can be less than the","chapter-4","Lists, Stacks, and Queues"
"maximum allowed by the array. This value is stored in listSize. Data member","chapter-4","Lists, Stacks, and Queues"
"curr stores the current position. Because listArray, maxSize, listSize,","chapter-4","Lists, Stacks, and Queues"
"and curr are all declared to be private, they may only be accessed by methods","chapter-4","Lists, Stacks, and Queues"
"of Class AList.","chapter-4","Lists, Stacks, and Queues"
"Class AList stores the list elements in the first listSize contiguous array","chapter-4","Lists, Stacks, and Queues"
"positions. Array positions correspond to list positions. In other words, the element","chapter-4","Lists, Stacks, and Queues"
"at position i in the list is stored at array cell i. The head of the list is always at","chapter-4","Lists, Stacks, and Queues"
"position 0. This makes random access to any element in the list quite easy. Given","chapter-4","Lists, Stacks, and Queues"
"some position in the list, the value of the element in that position can be accessed","chapter-4","Lists, Stacks, and Queues"
"directly. Thus, access to any element using the moveToPos method followed by","chapter-4","Lists, Stacks, and Queues"
"the getValue method takes Θ(1) time.","chapter-4","Lists, Stacks, and Queues"
"Because the array-based list implementation is defined to store list elements in","chapter-4","Lists, Stacks, and Queues"
"contiguous cells of the array, the insert, append, and remove methods must","chapter-4","Lists, Stacks, and Queues"
"100 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"Insert 23:","chapter-4","Lists, Stacks, and Queues"
"12 20 3 8 13 12 20 8 3","chapter-4","Lists, Stacks, and Queues"
"23 13 12 20 8 3","chapter-4","Lists, Stacks, and Queues"
"13","chapter-4","Lists, Stacks, and Queues"
"(a) (b)","chapter-4","Lists, Stacks, and Queues"
"(c)","chapter-4","Lists, Stacks, and Queues"
"0 1 2 4 4 0 1 2 3 5","chapter-4","Lists, Stacks, and Queues"
"1 2 3 4 5","chapter-4","Lists, Stacks, and Queues"
"5","chapter-4","Lists, Stacks, and Queues"
"0","chapter-4","Lists, Stacks, and Queues"
"3","chapter-4","Lists, Stacks, and Queues"
"Figure 4.3 Inserting an element at the head of an array-based list requires shift-","chapter-4","Lists, Stacks, and Queues"
"ing all existing elements in the array by one position toward the tail. (a) A list","chapter-4","Lists, Stacks, and Queues"
"containing five elements before inserting an element with value 23. (b) The list","chapter-4","Lists, Stacks, and Queues"
"after shifting all existing elements one position to the right. (c) The list after 23","chapter-4","Lists, Stacks, and Queues"
"has been inserted in array position 0. Shading indicates the unused part of the","chapter-4","Lists, Stacks, and Queues"
"array.","chapter-4","Lists, Stacks, and Queues"
"maintain this property. Inserting or removing elements at the tail of the list is easy,","chapter-4","Lists, Stacks, and Queues"
"so the append operation takes Θ(1) time. But if we wish to insert an element at","chapter-4","Lists, Stacks, and Queues"
"the head of the list, all elements currently in the list must shift one position toward","chapter-4","Lists, Stacks, and Queues"
"the tail to make room, as illustrated by Figure 4.3. This process takes Θ(n) time","chapter-4","Lists, Stacks, and Queues"
"if there are n elements already in the list. If we wish to insert at position i within","chapter-4","Lists, Stacks, and Queues"
"a list of n elements, then n − i elements must shift toward the tail. Removing an","chapter-4","Lists, Stacks, and Queues"
"element from the head of the list is similar in that all remaining elements must shift","chapter-4","Lists, Stacks, and Queues"
"toward the head by one position to fill in the gap. To remove the element at position","chapter-4","Lists, Stacks, and Queues"
"i, n − i − 1 elements must shift toward the head. In the average case, insertion or","chapter-4","Lists, Stacks, and Queues"
"removal requires moving half of the elements, which is Θ(n).","chapter-4","Lists, Stacks, and Queues"
"Most of the other member functions for Class AList simply access the current","chapter-4","Lists, Stacks, and Queues"
"list element or move the current position. Such operations all require Θ(1) time.","chapter-4","Lists, Stacks, and Queues"
"Aside from insert and remove, the only other operations that might require","chapter-4","Lists, Stacks, and Queues"
"more than constant time are the constructor, the destructor, and clear. These","chapter-4","Lists, Stacks, and Queues"
"three member functions each make use of the system free-store operation new. As","chapter-4","Lists, Stacks, and Queues"
"discussed further in Section 4.1.2, system free-store operations can be expensive.","chapter-4","Lists, Stacks, and Queues"
"4.1.2 Linked Lists","chapter-4","Lists, Stacks, and Queues"
"The second traditional approach to implementing lists makes use of pointers and is","chapter-4","Lists, Stacks, and Queues"
"usually called a linked list. The linked list uses dynamic memory allocation, that","chapter-4","Lists, Stacks, and Queues"
"is, it allocates memory for new list elements as needed.","chapter-4","Lists, Stacks, and Queues"
"A linked list is made up of a series of objects, called the nodes of the list.","chapter-4","Lists, Stacks, and Queues"
"Because a list node is a distinct object (as opposed to simply a cell in an array), it is","chapter-4","Lists, Stacks, and Queues"
"good practice to make a separate list node class. An additional benefit to creating a","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.1 Lists 101","chapter-4","Lists, Stacks, and Queues"
"/** Singly linked list node */","chapter-4","Lists, Stacks, and Queues"
"class Link<E> {","chapter-4","Lists, Stacks, and Queues"
"private E element; // Value for this node","chapter-4","Lists, Stacks, and Queues"
"private Link<E> next; // Pointer to next node in list","chapter-4","Lists, Stacks, and Queues"
"// Constructors","chapter-4","Lists, Stacks, and Queues"
"Link(E it, Link<E> nextval)","chapter-4","Lists, Stacks, and Queues"
"{ element = it; next = nextval; }","chapter-4","Lists, Stacks, and Queues"
"Link(Link<E> nextval) { next = nextval; }","chapter-4","Lists, Stacks, and Queues"
"Link<E> next() { return next; } // Return next field","chapter-4","Lists, Stacks, and Queues"
"Link<E> setNext(Link<E> nextval) // Set next field","chapter-4","Lists, Stacks, and Queues"
"{ return next = nextval; } // Return element field","chapter-4","Lists, Stacks, and Queues"
"E element() { return element; } // Set element field","chapter-4","Lists, Stacks, and Queues"
"E setElement(E it) { return element = it; }","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"Figure 4.4 A simple singly linked list node implementation.","chapter-4","Lists, Stacks, and Queues"
"list node class is that it can be reused by the linked implementations for the stack","chapter-4","Lists, Stacks, and Queues"
"and queue data structures presented later in this chapter. Figure 4.4 shows the","chapter-4","Lists, Stacks, and Queues"
"implementation for list nodes, called the Link class. Objects in the Link class","chapter-4","Lists, Stacks, and Queues"
"contain an element field to store the element value, and a next field to store a","chapter-4","Lists, Stacks, and Queues"
"pointer to the next node on the list. The list built from such nodes is called a singly","chapter-4","Lists, Stacks, and Queues"
"linked list, or a one-way list, because each list node has a single pointer to the next","chapter-4","Lists, Stacks, and Queues"
"node on the list.","chapter-4","Lists, Stacks, and Queues"
"The Link class is quite simple. There are two forms for its constructor, one","chapter-4","Lists, Stacks, and Queues"
"with an initial element value and one without. Member functions allow the link","chapter-4","Lists, Stacks, and Queues"
"user to get or set the element and link fields.","chapter-4","Lists, Stacks, and Queues"
"Figure 4.5(a) shows a graphical depiction for a linked list storing four integers.","chapter-4","Lists, Stacks, and Queues"
"The value stored in a pointer variable is indicated by an arrow “pointing” to some-","chapter-4","Lists, Stacks, and Queues"
"thing. Java uses the special symbol null for a pointer value that points nowhere,","chapter-4","Lists, Stacks, and Queues"
"such as for the last list node’s next field. A null pointer is indicated graphically","chapter-4","Lists, Stacks, and Queues"
"by a diagonal slash through a pointer variable’s box. The vertical line between the","chapter-4","Lists, Stacks, and Queues"
"nodes labeled 23 and 12 in Figure 4.5(a) indicates the current position (immediately","chapter-4","Lists, Stacks, and Queues"
"to the right of this line).","chapter-4","Lists, Stacks, and Queues"
"The list’s first node is accessed from a pointer named head. To speed access","chapter-4","Lists, Stacks, and Queues"
"to the end of the list, and to allow the append method to be performed in constant","chapter-4","Lists, Stacks, and Queues"
"time, a pointer named tail is also kept to the last link of the list. The position of","chapter-4","Lists, Stacks, and Queues"
"the current element is indicated by another pointer, named curr. Finally, because","chapter-4","Lists, Stacks, and Queues"
"there is no simple way to compute the length of the list simply from these three","chapter-4","Lists, Stacks, and Queues"
"pointers, the list length must be stored explicitly, and updated by every operation","chapter-4","Lists, Stacks, and Queues"
"that modifies the list size. The value cnt stores the length of the list.","chapter-4","Lists, Stacks, and Queues"
"Note that LList’s constructor maintains the optional parameter for minimum","chapter-4","Lists, Stacks, and Queues"
"list size introduced for Class AList. This is done simply to keep the calls to the","chapter-4","Lists, Stacks, and Queues"
"102 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"head","chapter-4","Lists, Stacks, and Queues"
"20 23 15","chapter-4","Lists, Stacks, and Queues"
"(a)","chapter-4","Lists, Stacks, and Queues"
"head tail","chapter-4","Lists, Stacks, and Queues"
"20 23 10 12 15","chapter-4","Lists, Stacks, and Queues"
"(b)","chapter-4","Lists, Stacks, and Queues"
"curr","chapter-4","Lists, Stacks, and Queues"
"curr","chapter-4","Lists, Stacks, and Queues"
"tail","chapter-4","Lists, Stacks, and Queues"
"12","chapter-4","Lists, Stacks, and Queues"
"Figure 4.5 Illustration of a faulty linked-list implementation where curr points","chapter-4","Lists, Stacks, and Queues"
"directly to the current node. (a) Linked list prior to inserting element with","chapter-4","Lists, Stacks, and Queues"
"value 10. (b) Desired effect of inserting element with value 10.","chapter-4","Lists, Stacks, and Queues"
"constructor the same for both variants. Because the linked list class does not need","chapter-4","Lists, Stacks, and Queues"
"to declare a fixed-size array when the list is created, this parameter is unnecessary","chapter-4","Lists, Stacks, and Queues"
"for linked lists. It is ignored by the implementation.","chapter-4","Lists, Stacks, and Queues"
"A key design decision for the linked list implementation is how to represent","chapter-4","Lists, Stacks, and Queues"
"the current position. The most reasonable choices appear to be a pointer to the","chapter-4","Lists, Stacks, and Queues"
"current element. But there is a big advantage to making curr point to the element","chapter-4","Lists, Stacks, and Queues"
"preceding the current element.","chapter-4","Lists, Stacks, and Queues"
"Figure 4.5(a) shows the list’s curr pointer pointing to the current element. The","chapter-4","Lists, Stacks, and Queues"
"vertical line between the nodes containing 23 and 12 indicates the logical position","chapter-4","Lists, Stacks, and Queues"
"of the current element. Consider what happens if we wish to insert a new node with","chapter-4","Lists, Stacks, and Queues"
"value 10 into the list. The result should be as shown in Figure 4.5(b). However,","chapter-4","Lists, Stacks, and Queues"
"there is a problem. To “splice” the list node containing the new element into the","chapter-4","Lists, Stacks, and Queues"
"list, the list node storing 23 must have its next pointer changed to point to the new","chapter-4","Lists, Stacks, and Queues"
"node. Unfortunately, there is no convenient access to the node preceding the one","chapter-4","Lists, Stacks, and Queues"
"pointed to by curr.","chapter-4","Lists, Stacks, and Queues"
"There is an easy solution to this problem. If we set curr to point directly to","chapter-4","Lists, Stacks, and Queues"
"the preceding element, there is no difficulty in adding a new element after curr.","chapter-4","Lists, Stacks, and Queues"
"Figure 4.6 shows how the list looks when pointer variable curr is set to point to the","chapter-4","Lists, Stacks, and Queues"
"node preceding the physical current node. See Exercise 4.5 for further discussion","chapter-4","Lists, Stacks, and Queues"
"of why making curr point directly to the current element fails.","chapter-4","Lists, Stacks, and Queues"
"We encounter a number of potential special cases when the list is empty, or","chapter-4","Lists, Stacks, and Queues"
"when the current position is at an end of the list. In particular, when the list is empty","chapter-4","Lists, Stacks, and Queues"
"we have no element for head, tail, and curr to point to. Implementing special","chapter-4","Lists, Stacks, and Queues"
"cases for insert and remove increases code complexity, making it harder to","chapter-4","Lists, Stacks, and Queues"
"understand, and thus increases the chance of introducing a programming bug.","chapter-4","Lists, Stacks, and Queues"
"These special cases can be eliminated by implementing linked lists with an","chapter-4","Lists, Stacks, and Queues"
"additional header node as the first node of the list. This header node is a link","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.1 Lists 103","chapter-4","Lists, Stacks, and Queues"
"head curr tail","chapter-4","Lists, Stacks, and Queues"
"20 23 12 15","chapter-4","Lists, Stacks, and Queues"
"(a)","chapter-4","Lists, Stacks, and Queues"
"head tail","chapter-4","Lists, Stacks, and Queues"
"20 23 10 12","chapter-4","Lists, Stacks, and Queues"
"(b)","chapter-4","Lists, Stacks, and Queues"
"15","chapter-4","Lists, Stacks, and Queues"
"curr","chapter-4","Lists, Stacks, and Queues"
"Figure 4.6 Insertion using a header node, with curr pointing one node head of","chapter-4","Lists, Stacks, and Queues"
"the current element. (a) Linked list before insertion. The current node contains 12.","chapter-4","Lists, Stacks, and Queues"
"(b) Linked list after inserting the node containing 10.","chapter-4","Lists, Stacks, and Queues"
"tail","chapter-4","Lists, Stacks, and Queues"
"head","chapter-4","Lists, Stacks, and Queues"
"curr","chapter-4","Lists, Stacks, and Queues"
"Figure 4.7 Initial state of a linked list when using a header node.","chapter-4","Lists, Stacks, and Queues"
"node like any other, but its value is ignored and it is not considered to be an actual","chapter-4","Lists, Stacks, and Queues"
"element of the list. The header node saves coding effort because we no longer need","chapter-4","Lists, Stacks, and Queues"
"to consider special cases for empty lists or when the current position is at one end","chapter-4","Lists, Stacks, and Queues"
"of the list. The cost of this simplification is the space for the header node. However,","chapter-4","Lists, Stacks, and Queues"
"there are space savings due to smaller code size, because statements to handle the","chapter-4","Lists, Stacks, and Queues"
"special cases are omitted. In practice, this reduction in code size typically saves","chapter-4","Lists, Stacks, and Queues"
"more space than that required for the header node, depending on the number of","chapter-4","Lists, Stacks, and Queues"
"lists created. Figure 4.7 shows the state of an initialized or empty list when using a","chapter-4","Lists, Stacks, and Queues"
"header node.","chapter-4","Lists, Stacks, and Queues"
"Figure 4.8 shows the definition for the linked list class, named LList. Class","chapter-4","Lists, Stacks, and Queues"
"LList inherits from the abstract list class and thus must implement all of Class","chapter-4","Lists, Stacks, and Queues"
"List’s member functions.","chapter-4","Lists, Stacks, and Queues"
"Implementations for most member functions of the list class are straightfor-","chapter-4","Lists, Stacks, and Queues"
"ward. However, insert and remove should be studied carefully.","chapter-4","Lists, Stacks, and Queues"
"Inserting a new element is a three-step process. First, the new list node is","chapter-4","Lists, Stacks, and Queues"
"created and the new element is stored into it. Second, the next field of the new","chapter-4","Lists, Stacks, and Queues"
"list node is assigned to point to the current node (the one after the node that curr","chapter-4","Lists, Stacks, and Queues"
"points to). Third, the next field of node pointed to by curr is assigned to point to","chapter-4","Lists, Stacks, and Queues"
"the newly inserted node. The following line in the insert method of Figure 4.8","chapter-4","Lists, Stacks, and Queues"
"does all three of these steps.","chapter-4","Lists, Stacks, and Queues"
"curr.setNext(new Link<E>(it, curr.next()));","chapter-4","Lists, Stacks, and Queues"
"104 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"/** Linked list implementation */","chapter-4","Lists, Stacks, and Queues"
"class LList<E> implements List<E> {","chapter-4","Lists, Stacks, and Queues"
"private Link<E> head; // Pointer to list header","chapter-4","Lists, Stacks, and Queues"
"private Link<E> tail; // Pointer to last element","chapter-4","Lists, Stacks, and Queues"
"protected Link<E> curr; // Access to current element","chapter-4","Lists, Stacks, and Queues"
"private int cnt; // Size of list","chapter-4","Lists, Stacks, and Queues"
"/** Constructors */","chapter-4","Lists, Stacks, and Queues"
"LList(int size) { this(); } // Constructor -- Ignore size","chapter-4","Lists, Stacks, and Queues"
"LList() {","chapter-4","Lists, Stacks, and Queues"
"curr = tail = head = new Link<E>(null); // Create header","chapter-4","Lists, Stacks, and Queues"
"cnt = 0;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Remove all elements */","chapter-4","Lists, Stacks, and Queues"
"public void clear() {","chapter-4","Lists, Stacks, and Queues"
"head.setNext(null); // Drop access to links","chapter-4","Lists, Stacks, and Queues"
"curr = tail = head = new Link<E>(null); // Create header","chapter-4","Lists, Stacks, and Queues"
"cnt = 0;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Insert "it" at current position */","chapter-4","Lists, Stacks, and Queues"
"public void insert(E it) {","chapter-4","Lists, Stacks, and Queues"
"curr.setNext(new Link<E>(it, curr.next()));","chapter-4","Lists, Stacks, and Queues"
"if (tail == curr) tail = curr.next(); // New tail","chapter-4","Lists, Stacks, and Queues"
"cnt++;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Append "it" to list */","chapter-4","Lists, Stacks, and Queues"
"public void append(E it) {","chapter-4","Lists, Stacks, and Queues"
"tail = tail.setNext(new Link<E>(it, null));","chapter-4","Lists, Stacks, and Queues"
"cnt++;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Remove and return current element */","chapter-4","Lists, Stacks, and Queues"
"public E remove() {","chapter-4","Lists, Stacks, and Queues"
"if (curr.next() == null) return null; // Nothing to remove","chapter-4","Lists, Stacks, and Queues"
"E it = curr.next().element(); // Remember value","chapter-4","Lists, Stacks, and Queues"
"if (tail == curr.next()) tail = curr; // Removed last","chapter-4","Lists, Stacks, and Queues"
"curr.setNext(curr.next().next()); // Remove from list","chapter-4","Lists, Stacks, and Queues"
"cnt--; // Decrement count","chapter-4","Lists, Stacks, and Queues"
"return it; // Return value","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Set curr at list start */","chapter-4","Lists, Stacks, and Queues"
"public void moveToStart()","chapter-4","Lists, Stacks, and Queues"
"{ curr = head; }","chapter-4","Lists, Stacks, and Queues"
"Figure 4.8 A linked list implementation.","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.1 Lists 105","chapter-4","Lists, Stacks, and Queues"
"/** Set curr at list end */","chapter-4","Lists, Stacks, and Queues"
"public void moveToEnd()","chapter-4","Lists, Stacks, and Queues"
"{ curr = tail; }","chapter-4","Lists, Stacks, and Queues"
"/** Move curr one step left; no change if now at front */","chapter-4","Lists, Stacks, and Queues"
"public void prev() {","chapter-4","Lists, Stacks, and Queues"
"if (curr == head) return; // No previous element","chapter-4","Lists, Stacks, and Queues"
"Link<E> temp = head;","chapter-4","Lists, Stacks, and Queues"
"// March down list until we find the previous element","chapter-4","Lists, Stacks, and Queues"
"while (temp.next() != curr) temp = temp.next();","chapter-4","Lists, Stacks, and Queues"
"curr = temp;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Move curr one step right; no change if now at end */","chapter-4","Lists, Stacks, and Queues"
"public void next()","chapter-4","Lists, Stacks, and Queues"
"{ if (curr != tail) curr = curr.next(); }","chapter-4","Lists, Stacks, and Queues"
"/** @return List length */","chapter-4","Lists, Stacks, and Queues"
"public int length() { return cnt; }","chapter-4","Lists, Stacks, and Queues"
"/** @return The position of the current element */","chapter-4","Lists, Stacks, and Queues"
"public int currPos() {","chapter-4","Lists, Stacks, and Queues"
"Link<E> temp = head;","chapter-4","Lists, Stacks, and Queues"
"int i;","chapter-4","Lists, Stacks, and Queues"
"for (i=0; curr != temp; i++)","chapter-4","Lists, Stacks, and Queues"
"temp = temp.next();","chapter-4","Lists, Stacks, and Queues"
"return i;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Move down list to "pos" position */","chapter-4","Lists, Stacks, and Queues"
"public void moveToPos(int pos) {","chapter-4","Lists, Stacks, and Queues"
"assert (pos>=0) && (pos<=cnt) : "Position out of range";","chapter-4","Lists, Stacks, and Queues"
"curr = head;","chapter-4","Lists, Stacks, and Queues"
"for(int i=0; i<pos; i++) curr = curr.next();","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** @return Current element value */","chapter-4","Lists, Stacks, and Queues"
"public E getValue() {","chapter-4","Lists, Stacks, and Queues"
"if(curr.next() == null) return null;","chapter-4","Lists, Stacks, and Queues"
"return curr.next().element();","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"Figure 4.8 (continued)","chapter-4","Lists, Stacks, and Queues"
"106 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"... ...","chapter-4","Lists, Stacks, and Queues"
"(a)","chapter-4","Lists, Stacks, and Queues"
"... ...","chapter-4","Lists, Stacks, and Queues"
"(b)","chapter-4","Lists, Stacks, and Queues"
"curr","chapter-4","Lists, Stacks, and Queues"
"curr","chapter-4","Lists, Stacks, and Queues"
"23 12","chapter-4","Lists, Stacks, and Queues"
"Insert 10: 10","chapter-4","Lists, Stacks, and Queues"
"23 12","chapter-4","Lists, Stacks, and Queues"
"10","chapter-4","Lists, Stacks, and Queues"
"1 2","chapter-4","Lists, Stacks, and Queues"
"3","chapter-4","Lists, Stacks, and Queues"
"Figure 4.9 The linked list insertion process. (a) The linked list before insertion.","chapter-4","Lists, Stacks, and Queues"
"(b) The linked list after insertion. 1 marks the element field of the new link","chapter-4","Lists, Stacks, and Queues"
"node. 2 marks the next field of the new link node, which is set to point to what","chapter-4","Lists, Stacks, and Queues"
"used to be the current node (the node with value 12). 3 marks the next field of","chapter-4","Lists, Stacks, and Queues"
"the node preceding the current position. It used to point to the node containing 12;","chapter-4","Lists, Stacks, and Queues"
"now it points to the new node containing 10.","chapter-4","Lists, Stacks, and Queues"
"Operator new creates the new link node and calls the Link class constructor, which","chapter-4","Lists, Stacks, and Queues"
"takes two parameters. The first is the element. The second is the value to be placed","chapter-4","Lists, Stacks, and Queues"
"in the list node’s next field, in this case “curr.next.” Method setNext does","chapter-4","Lists, Stacks, and Queues"
"the assignment to curr’s next field. Figure 4.9 illustrates this three-step process.","chapter-4","Lists, Stacks, and Queues"
"Once the new node is added, tail is pushed forward if the new element was added","chapter-4","Lists, Stacks, and Queues"
"to the end of the list. Insertion requires Θ(1) time.","chapter-4","Lists, Stacks, and Queues"
"Removing a node from the linked list requires only that the appropriate pointer","chapter-4","Lists, Stacks, and Queues"
"be redirected around the node to be deleted. The following lines from the remove","chapter-4","Lists, Stacks, and Queues"
"method of Figure 4.8 do precisely this.","chapter-4","Lists, Stacks, and Queues"
"E it = curr.next().element(); // Remember value","chapter-4","Lists, Stacks, and Queues"
"curr.setNext(curr.next().next()); // Remove from list","chapter-4","Lists, Stacks, and Queues"
"Memory for the link will eventually be reclaimed by the garbage collector. Fig-","chapter-4","Lists, Stacks, and Queues"
"ure 4.10 illustrates the remove method. Removing an element requires Θ(1) time.","chapter-4","Lists, Stacks, and Queues"
"Method next simply moves curr one position toward the tail of the list,","chapter-4","Lists, Stacks, and Queues"
"which takes Θ(1) time. Method prev moves curr one position toward the head","chapter-4","Lists, Stacks, and Queues"
"of the list, but its implementation is more difficult. In a singly linked list, there is","chapter-4","Lists, Stacks, and Queues"
"no pointer to the previous node. Thus, the only alternative is to march down the list","chapter-4","Lists, Stacks, and Queues"
"from the beginning until we reach the current node (being sure always to remember","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.1 Lists 107","chapter-4","Lists, Stacks, and Queues"
"...","chapter-4","Lists, Stacks, and Queues"
"... ...","chapter-4","Lists, Stacks, and Queues"
"...","chapter-4","Lists, Stacks, and Queues"
"(a)","chapter-4","Lists, Stacks, and Queues"
"(b)","chapter-4","Lists, Stacks, and Queues"
"it","chapter-4","Lists, Stacks, and Queues"
"curr","chapter-4","Lists, Stacks, and Queues"
"23 12","chapter-4","Lists, Stacks, and Queues"
"10 12","chapter-4","Lists, Stacks, and Queues"
"10","chapter-4","Lists, Stacks, and Queues"
"23","chapter-4","Lists, Stacks, and Queues"
"curr","chapter-4","Lists, Stacks, and Queues"
"2","chapter-4","Lists, Stacks, and Queues"
"1","chapter-4","Lists, Stacks, and Queues"
"Figure 4.10 The linked list removal process. (a) The linked list before removing","chapter-4","Lists, Stacks, and Queues"
"the node with value 10. (b) The linked list after removal. 1 marks the list node","chapter-4","Lists, Stacks, and Queues"
"being removed. it is set to point to the element. 2 marks the next field of","chapter-4","Lists, Stacks, and Queues"
"the preceding list node, which is set to point to the node following the one being","chapter-4","Lists, Stacks, and Queues"
"deleted.","chapter-4","Lists, Stacks, and Queues"
"the node before it, because that is what we really want). This takes Θ(n) time in","chapter-4","Lists, Stacks, and Queues"
"the average and worst cases. Implementation of method moveToPos is similar in","chapter-4","Lists, Stacks, and Queues"
"that finding the ith position requires marching down i positions from the head of","chapter-4","Lists, Stacks, and Queues"
"the list, taking Θ(i) time.","chapter-4","Lists, Stacks, and Queues"
"Implementations for the remaining operations each require Θ(1) time.","chapter-4","Lists, Stacks, and Queues"
"Freelists","chapter-4","Lists, Stacks, and Queues"
"The new operator is relatively expensive to use. Garbage collection is also expen-","chapter-4","Lists, Stacks, and Queues"
"sive. Section 12.3 discusses how general-purpose memory managers are imple-","chapter-4","Lists, Stacks, and Queues"
"mented. The expense comes from the fact that free-store routines must be capable","chapter-4","Lists, Stacks, and Queues"
"of handling requests to and from free store with no particular pattern, as well as","chapter-4","Lists, Stacks, and Queues"
"memory requests of vastly different sizes. This, combined with unpredictable free-","chapter-4","Lists, Stacks, and Queues"
"ing of space by the garbage collector, makes them inefficient compared to what","chapter-4","Lists, Stacks, and Queues"
"might be implemented for more controlled patterns of memory access.","chapter-4","Lists, Stacks, and Queues"
"List nodes are created and deleted in a linked list implementation in a way","chapter-4","Lists, Stacks, and Queues"
"that allows the Link class programmer to provide simple but efficient memory","chapter-4","Lists, Stacks, and Queues"
"management routines. Instead of making repeated calls to new, the Link class","chapter-4","Lists, Stacks, and Queues"
"can handle its own freelist. A freelist holds those list nodes that are not currently","chapter-4","Lists, Stacks, and Queues"
"being used. When a node is deleted from a linked list, it is placed at the head of the","chapter-4","Lists, Stacks, and Queues"
"freelist. When a new element is to be added to a linked list, the freelist is checked","chapter-4","Lists, Stacks, and Queues"
"to see if a list node is available. If so, the node is taken from the freelist. If the","chapter-4","Lists, Stacks, and Queues"
"freelist is empty, the standard new operator must then be called.","chapter-4","Lists, Stacks, and Queues"
"Freelists are particularly useful for linked lists that periodically grow and then","chapter-4","Lists, Stacks, and Queues"
"shrink. The freelist will never grow larger than the largest size yet reached by the","chapter-4","Lists, Stacks, and Queues"
"108 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"linked list. Requests for new nodes (after the list has shrunk) can be handled by","chapter-4","Lists, Stacks, and Queues"
"the freelist. Another good opportunity to use a freelist occurs when a program uses","chapter-4","Lists, Stacks, and Queues"
"multiple lists. So long as they do not all grow and shrink together, the free list can","chapter-4","Lists, Stacks, and Queues"
"let link nodes move between the lists.","chapter-4","Lists, Stacks, and Queues"
"In the implementation shown here, the link class is augmented with methods","chapter-4","Lists, Stacks, and Queues"
"get and release. Figure 4.11 shows the reimplementation for the Link class to","chapter-4","Lists, Stacks, and Queues"
"support these methods. Note how simple they are, because they need only remove","chapter-4","Lists, Stacks, and Queues"
"and add an element to the front of the freelist, respectively. The freelist methods","chapter-4","Lists, Stacks, and Queues"
"get and release both run in Θ(1) time, except in the case where the freelist is","chapter-4","Lists, Stacks, and Queues"
"exhausted and the new operation must be called. Figure 4.12 shows the necessary","chapter-4","Lists, Stacks, and Queues"
"modifications to members of the linked list class to make use of the freelist version","chapter-4","Lists, Stacks, and Queues"
"of the link class.","chapter-4","Lists, Stacks, and Queues"
"The freelist variable declaration uses the keyword static. This creates a","chapter-4","Lists, Stacks, and Queues"
"single variable shared among all instances of the Link nodes. In this way, a single","chapter-4","Lists, Stacks, and Queues"
"freelist shared by all Link nodes.","chapter-4","Lists, Stacks, and Queues"
"4.1.3 Comparison of List Implementations","chapter-4","Lists, Stacks, and Queues"
"Now that you have seen two substantially different implementations for lists, it is","chapter-4","Lists, Stacks, and Queues"
"natural to ask which is better. In particular, if you must implement a list for some","chapter-4","Lists, Stacks, and Queues"
"task, which implementation should you choose?","chapter-4","Lists, Stacks, and Queues"
"Array-based lists have the disadvantage that their size must be predetermined","chapter-4","Lists, Stacks, and Queues"
"before the array can be allocated. Array-based lists cannot grow beyond their pre-","chapter-4","Lists, Stacks, and Queues"
"determined size. Whenever the list contains only a few elements, a substantial","chapter-4","Lists, Stacks, and Queues"
"amount of space might be tied up in a largely empty array. Linked lists have the","chapter-4","Lists, Stacks, and Queues"
"advantage that they only need space for the objects actually on the list. There is","chapter-4","Lists, Stacks, and Queues"
"no limit to the number of elements on a linked list, as long as there is free-store","chapter-4","Lists, Stacks, and Queues"
"memory available. The amount of space required by a linked list is Θ(n), while the","chapter-4","Lists, Stacks, and Queues"
"space required by the array-based list implementation is Ω(n), but can be greater.","chapter-4","Lists, Stacks, and Queues"
"Array-based lists have the advantage that there is no wasted space for an in-","chapter-4","Lists, Stacks, and Queues"
"dividual element. Linked lists require that an extra pointer be added to every list","chapter-4","Lists, Stacks, and Queues"
"node. If the element size is small, then the overhead for links can be a significant","chapter-4","Lists, Stacks, and Queues"
"fraction of the total storage. When the array for the array-based list is completely","chapter-4","Lists, Stacks, and Queues"
"filled, there is no storage overhead. The array-based list will then be more space","chapter-4","Lists, Stacks, and Queues"
"efficient, by a constant factor, than the linked implementation.","chapter-4","Lists, Stacks, and Queues"
"A simple formula can be used to determine whether the array-based list or","chapter-4","Lists, Stacks, and Queues"
"linked list implementation will be more space efficient in a particular situation.","chapter-4","Lists, Stacks, and Queues"
"Call n the number of elements currently in the list, P the size of a pointer in stor-","chapter-4","Lists, Stacks, and Queues"
"age units (typically four bytes), E the size of a data element in storage units (this","chapter-4","Lists, Stacks, and Queues"
"could be anything, from one bit for a Boolean variable on up to thousands of bytes","chapter-4","Lists, Stacks, and Queues"
"or more for complex records), and D the maximum number of list elements that","chapter-4","Lists, Stacks, and Queues"
"can be stored in the array. The amount of space required for the array-based list is","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.1 Lists 109","chapter-4","Lists, Stacks, and Queues"
"/** Singly linked list node with freelist support */","chapter-4","Lists, Stacks, and Queues"
"class Link<E> {","chapter-4","Lists, Stacks, and Queues"
"private E element; // Value for this node","chapter-4","Lists, Stacks, and Queues"
"private Link<E> next; // Point to next node in list","chapter-4","Lists, Stacks, and Queues"
"/** Constructors */","chapter-4","Lists, Stacks, and Queues"
"Link(E it, Link<E> nextval)","chapter-4","Lists, Stacks, and Queues"
"{ element = it; next = nextval; }","chapter-4","Lists, Stacks, and Queues"
"Link(Link<E> nextval) { next = nextval; }","chapter-4","Lists, Stacks, and Queues"
"/** Get and set methods */","chapter-4","Lists, Stacks, and Queues"
"Link<E> next() { return next; }","chapter-4","Lists, Stacks, and Queues"
"Link<E> setNext(Link<E> nxtval) { return next = nxtval; }","chapter-4","Lists, Stacks, and Queues"
"E element() { return element; }","chapter-4","Lists, Stacks, and Queues"
"E setElement(E it) { return element = it; }","chapter-4","Lists, Stacks, and Queues"
"/** Extensions to support freelists */","chapter-4","Lists, Stacks, and Queues"
"static Link freelist = null; // Freelist for the class","chapter-4","Lists, Stacks, and Queues"
"/** @return A new link */","chapter-4","Lists, Stacks, and Queues"
"static <E> Link<E> get(E it, Link<E> nextval) {","chapter-4","Lists, Stacks, and Queues"
"if (freelist == null)","chapter-4","Lists, Stacks, and Queues"
"return new Link<E>(it, nextval); // Get from "new"","chapter-4","Lists, Stacks, and Queues"
"Link<E> temp = freelist; // Get from freelist","chapter-4","Lists, Stacks, and Queues"
"freelist = freelist.next();","chapter-4","Lists, Stacks, and Queues"
"temp.setElement(it);","chapter-4","Lists, Stacks, and Queues"
"temp.setNext(nextval);","chapter-4","Lists, Stacks, and Queues"
"return temp;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Return a link to the freelist */","chapter-4","Lists, Stacks, and Queues"
"void release() {","chapter-4","Lists, Stacks, and Queues"
"element = null; // Drop reference to the element","chapter-4","Lists, Stacks, and Queues"
"next = freelist;","chapter-4","Lists, Stacks, and Queues"
"freelist = this;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"Figure 4.11 Implementation for the Link class with a freelist. The static","chapter-4","Lists, Stacks, and Queues"
"declaration for member freelist means that all Link class objects share the","chapter-4","Lists, Stacks, and Queues"
"same freelist pointer variable instead of each object storing its own copy.","chapter-4","Lists, Stacks, and Queues"
"110 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"/** Insert "it" at current position */","chapter-4","Lists, Stacks, and Queues"
"public void insert(E it) {","chapter-4","Lists, Stacks, and Queues"
"curr.setNext(Link.get(it, curr.next())); // Get link","chapter-4","Lists, Stacks, and Queues"
"if (tail == curr) tail = curr.next(); // New tail","chapter-4","Lists, Stacks, and Queues"
"cnt++;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Append "it" to list */","chapter-4","Lists, Stacks, and Queues"
"public void append(E it) {","chapter-4","Lists, Stacks, and Queues"
"tail = tail.setNext(Link.get(it, null));","chapter-4","Lists, Stacks, and Queues"
"cnt++;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Remove and return current element */","chapter-4","Lists, Stacks, and Queues"
"public E remove() {","chapter-4","Lists, Stacks, and Queues"
"if (curr.next() == null) return null; // Nothing to remove","chapter-4","Lists, Stacks, and Queues"
"E it = curr.next().element(); // Remember value","chapter-4","Lists, Stacks, and Queues"
"if (tail == curr.next()) tail = curr; // Removed last","chapter-4","Lists, Stacks, and Queues"
"Link<E> tempptr = curr.next(); // Remember link","chapter-4","Lists, Stacks, and Queues"
"curr.setNext(curr.next().next()); // Remove from list","chapter-4","Lists, Stacks, and Queues"
"tempptr.release(); // Release link","chapter-4","Lists, Stacks, and Queues"
"cnt--; // Decrement count","chapter-4","Lists, Stacks, and Queues"
"return it; // Return removed","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"Figure 4.12 Linked-list class members that are modified to use the freelist ver-","chapter-4","Lists, Stacks, and Queues"
"sion of the link class in Figure 4.11.","chapter-4","Lists, Stacks, and Queues"
"DE, regardless of the number of elements actually stored in the list at any given","chapter-4","Lists, Stacks, and Queues"
"time. The amount of space required for the linked list is n(P + E). The smaller","chapter-4","Lists, Stacks, and Queues"
"of these expressions for a given value n determines the more space-efficient imple-","chapter-4","Lists, Stacks, and Queues"
"mentation for n elements. In general, the linked implementation requires less space","chapter-4","Lists, Stacks, and Queues"
"than the array-based implementation when relatively few elements are in the list.","chapter-4","Lists, Stacks, and Queues"
"Conversely, the array-based implementation becomes more space efficient when","chapter-4","Lists, Stacks, and Queues"
"the array is close to full. Using the equation, we can solve for n to determine","chapter-4","Lists, Stacks, and Queues"
"the break-even point beyond which the array-based implementation is more space","chapter-4","Lists, Stacks, and Queues"
"efficient in any particular situation. This occurs when","chapter-4","Lists, Stacks, and Queues"
"n > DE/(P + E).","chapter-4","Lists, Stacks, and Queues"
"If P = E, then the break-even point is at D/2. This would happen if the element","chapter-4","Lists, Stacks, and Queues"
"field is either a four-byte int value or a pointer, and the next field is a typical four-","chapter-4","Lists, Stacks, and Queues"
"byte pointer. That is, the array-based implementation would be more efficient (if","chapter-4","Lists, Stacks, and Queues"
"the link field and the element field are the same size) whenever the array is more","chapter-4","Lists, Stacks, and Queues"
"than half full.","chapter-4","Lists, Stacks, and Queues"
"As a rule of thumb, linked lists are more space efficient when implementing","chapter-4","Lists, Stacks, and Queues"
"lists whose number of elements varies widely or is unknown. Array-based lists are","chapter-4","Lists, Stacks, and Queues"
"generally more space efficient when the user knows in advance approximately how","chapter-4","Lists, Stacks, and Queues"
"large the list will become.","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.1 Lists 111","chapter-4","Lists, Stacks, and Queues"
"Array-based lists are faster for random access by position. Positions can easily","chapter-4","Lists, Stacks, and Queues"
"be adjusted forwards or backwards by the next and prev methods. These opera-","chapter-4","Lists, Stacks, and Queues"
"tions always take Θ(1) time. In contrast, singly linked lists have no explicit access","chapter-4","Lists, Stacks, and Queues"
"to the previous element, and access by position requires that we march down the","chapter-4","Lists, Stacks, and Queues"
"list from the front (or the current position) to the specified position. Both of these","chapter-4","Lists, Stacks, and Queues"
"operations require Θ(n) time in the average and worst cases, if we assume that","chapter-4","Lists, Stacks, and Queues"
"each position on the list is equally likely to be accessed on any call to prev or","chapter-4","Lists, Stacks, and Queues"
"moveToPos.","chapter-4","Lists, Stacks, and Queues"
"Given a pointer to a suitable location in the list, the insert and remove","chapter-4","Lists, Stacks, and Queues"
"methods for linked lists require only Θ(1) time. Array-based lists must shift the re-","chapter-4","Lists, Stacks, and Queues"
"mainder of the list up or down within the array. This requires Θ(n) time in the aver-","chapter-4","Lists, Stacks, and Queues"
"age and worst cases. For many applications, the time to insert and delete elements","chapter-4","Lists, Stacks, and Queues"
"dominates all other operations. For this reason, linked lists are often preferred to","chapter-4","Lists, Stacks, and Queues"
"array-based lists.","chapter-4","Lists, Stacks, and Queues"
"When implementing the array-based list, an implementor could allow the size","chapter-4","Lists, Stacks, and Queues"
"of the array to grow and shrink depending on the number of elements that are","chapter-4","Lists, Stacks, and Queues"
"actually stored. This data structure is known as a dynamic array. Both the Java and","chapter-4","Lists, Stacks, and Queues"
"C++/STL Vector classes implement a dynamic array. Dynamic arrays allow the","chapter-4","Lists, Stacks, and Queues"
"programmer to get around the limitation on the standard array that its size cannot","chapter-4","Lists, Stacks, and Queues"
"be changed once the array has been created. This also means that space need not","chapter-4","Lists, Stacks, and Queues"
"be allocated to the dynamic array until it is to be used. The disadvantage of this","chapter-4","Lists, Stacks, and Queues"
"approach is that it takes time to deal with space adjustments on the array. Each time","chapter-4","Lists, Stacks, and Queues"
"the array grows in size, its contents must be copied. A good implementation of the","chapter-4","Lists, Stacks, and Queues"
"dynamic array will grow and shrink the array in such a way as to keep the overall","chapter-4","Lists, Stacks, and Queues"
"cost for a series of insert/delete operations relatively inexpensive, even though an","chapter-4","Lists, Stacks, and Queues"
"occasional insert/delete operation might be expensive. A simple rule of thumb is","chapter-4","Lists, Stacks, and Queues"
"to double the size of the array when it becomes full, and to cut the array size in","chapter-4","Lists, Stacks, and Queues"
"half when it becomes one quarter full. To analyze the overall cost of dynamic array","chapter-4","Lists, Stacks, and Queues"
"operations over time, we need to use a technique known as amortized analysis,","chapter-4","Lists, Stacks, and Queues"
"which is discussed in Section 14.3.","chapter-4","Lists, Stacks, and Queues"
"4.1.4 Element Implementations","chapter-4","Lists, Stacks, and Queues"
"List users must decide whether they wish to store a copy of any given element","chapter-4","Lists, Stacks, and Queues"
"on each list that contains it. For small elements such as an integer, this makes","chapter-4","Lists, Stacks, and Queues"
"sense. If the elements are payroll records, it might be desirable for the list node","chapter-4","Lists, Stacks, and Queues"
"to store a reference to the record rather than store a copy of the record itself. This","chapter-4","Lists, Stacks, and Queues"
"change would allow multiple list nodes (or other data structures) to point to the","chapter-4","Lists, Stacks, and Queues"
"same record, rather than make repeated copies of the record. Not only might this","chapter-4","Lists, Stacks, and Queues"
"save space, but it also means that a modification to an element’s value is automati-","chapter-4","Lists, Stacks, and Queues"
"cally reflected at all locations where it is referenced. The disadvantage of storing a","chapter-4","Lists, Stacks, and Queues"
"pointer to each element is that the pointer requires space of its own. If elements are","chapter-4","Lists, Stacks, and Queues"
"112 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"never duplicated, then this additional space adds unnecessary overhead. Java most","chapter-4","Lists, Stacks, and Queues"
"naturally stores references to objects, meaning that only a single copy of an object","chapter-4","Lists, Stacks, and Queues"
"such as a payroll record will be maintained, even if it is on multiple lists.","chapter-4","Lists, Stacks, and Queues"
"Whether it is more advantageous to use references to shared elements or sepa-","chapter-4","Lists, Stacks, and Queues"
"rate copies depends on the intended application. In general, the larger the elements","chapter-4","Lists, Stacks, and Queues"
"and the more they are duplicated, the more likely that references to shared elements","chapter-4","Lists, Stacks, and Queues"
"is the better approach.","chapter-4","Lists, Stacks, and Queues"
"A second issue faced by implementors of a list class (or any other data structure","chapter-4","Lists, Stacks, and Queues"
"that stores a collection of user-defined data elements) is whether the elements stored","chapter-4","Lists, Stacks, and Queues"
"are all required to be of the same type. This is known as homogeneity in a data","chapter-4","Lists, Stacks, and Queues"
"structure. In some applications, the user would like to define the class of the data","chapter-4","Lists, Stacks, and Queues"
"element that is stored on a given list, and then never permit objects of a different","chapter-4","Lists, Stacks, and Queues"
"class to be stored on that same list. In other applications, the user would like to","chapter-4","Lists, Stacks, and Queues"
"permit the objects stored on a single list to be of differing types.","chapter-4","Lists, Stacks, and Queues"
"For the list implementations presented in this section, the compiler requires that","chapter-4","Lists, Stacks, and Queues"
"all objects stored on the list be of the same type. Besides Java generics, there are","chapter-4","Lists, Stacks, and Queues"
"other techniques that implementors of a list class can use to ensure that the element","chapter-4","Lists, Stacks, and Queues"
"type for a given list remains fixed, while still permitting different lists to store","chapter-4","Lists, Stacks, and Queues"
"different element types. One approach is to store an object of the appropriate type","chapter-4","Lists, Stacks, and Queues"
"in the header node of the list (perhaps an object of the appropriate type is supplied","chapter-4","Lists, Stacks, and Queues"
"as a parameter to the list constructor), and then check that all insert operations on","chapter-4","Lists, Stacks, and Queues"
"that list use the same element type.","chapter-4","Lists, Stacks, and Queues"
"The third issue that users of the list implementations must face is primarily of","chapter-4","Lists, Stacks, and Queues"
"concern when programming in languages that do not support automatic garbage","chapter-4","Lists, Stacks, and Queues"
"collection. That is how to deal with the memory of the objects stored on the list","chapter-4","Lists, Stacks, and Queues"
"when the list is deleted or the clear method is called. The list destructor and the","chapter-4","Lists, Stacks, and Queues"
"clear method are problematic in that there is a potential that they will be misused.","chapter-4","Lists, Stacks, and Queues"
"Deleting listArray in the array-based implementation, or deleting a link node","chapter-4","Lists, Stacks, and Queues"
"in the linked list implementation, might remove the only reference to an object,","chapter-4","Lists, Stacks, and Queues"
"leaving its memory space inaccessible. Unfortunately, there is no way for the list","chapter-4","Lists, Stacks, and Queues"
"implementation to know whether a given object is pointed to in another part of the","chapter-4","Lists, Stacks, and Queues"
"program or not. Thus, the user of the list must be responsible for deleting these","chapter-4","Lists, Stacks, and Queues"
"objects when that is appropriate.","chapter-4","Lists, Stacks, and Queues"
"4.1.5 Doubly Linked Lists","chapter-4","Lists, Stacks, and Queues"
"The singly linked list presented in Section 4.1.2 allows for direct access from a","chapter-4","Lists, Stacks, and Queues"
"list node only to the next node in the list. A doubly linked list allows convenient","chapter-4","Lists, Stacks, and Queues"
"access from a list node to the next node and also to the preceding node on the list.","chapter-4","Lists, Stacks, and Queues"
"The doubly linked list node accomplishes this in the obvious way by storing two","chapter-4","Lists, Stacks, and Queues"
"pointers: one to the node following it (as in the singly linked list), and a second","chapter-4","Lists, Stacks, and Queues"
"pointer to the node preceding it. The most common reason to use a doubly linked","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.1 Lists 113","chapter-4","Lists, Stacks, and Queues"
"head","chapter-4","Lists, Stacks, and Queues"
"20 23","chapter-4","Lists, Stacks, and Queues"
"curr","chapter-4","Lists, Stacks, and Queues"
"12 15","chapter-4","Lists, Stacks, and Queues"
"tail","chapter-4","Lists, Stacks, and Queues"
"Figure 4.13 A doubly linked list.","chapter-4","Lists, Stacks, and Queues"
"list is because it is easier to implement than a singly linked list. While the code for","chapter-4","Lists, Stacks, and Queues"
"the doubly linked implementation is a little longer than for the singly linked version,","chapter-4","Lists, Stacks, and Queues"
"it tends to be a bit more “obvious” in its intention, and so easier to implement","chapter-4","Lists, Stacks, and Queues"
"and debug. Figure 4.13 illustrates the doubly linked list concept. Whether a list","chapter-4","Lists, Stacks, and Queues"
"implementation is doubly or singly linked should be hidden from the List class","chapter-4","Lists, Stacks, and Queues"
"user.","chapter-4","Lists, Stacks, and Queues"
"Like our singly linked list implementation, the doubly linked list implementa-","chapter-4","Lists, Stacks, and Queues"
"tion makes use of a header node. We also add a tailer node to the end of the list.","chapter-4","Lists, Stacks, and Queues"
"The tailer is similar to the header, in that it is a node that contains no value, and it","chapter-4","Lists, Stacks, and Queues"
"always exists. When the doubly linked list is initialized, the header and tailer nodes","chapter-4","Lists, Stacks, and Queues"
"are created. Data member head points to the header node, and tail points to","chapter-4","Lists, Stacks, and Queues"
"the tailer node. The purpose of these nodes is to simplify the insert, append,","chapter-4","Lists, Stacks, and Queues"
"and remove methods by eliminating all need for special-case code when the list","chapter-4","Lists, Stacks, and Queues"
"is empty, or when we insert at the head or tail of the list.","chapter-4","Lists, Stacks, and Queues"
"For singly linked lists we set curr to point to the node preceding the node that","chapter-4","Lists, Stacks, and Queues"
"contained the actual current element, due to lack of access to the previous node","chapter-4","Lists, Stacks, and Queues"
"during insertion and deletion. Since we do have access to the previous node in a","chapter-4","Lists, Stacks, and Queues"
"doubly linked list, this is no longer necessary. We could set curr to point directly","chapter-4","Lists, Stacks, and Queues"
"to the node containing the current element. However, I have chosen to keep the","chapter-4","Lists, Stacks, and Queues"
"same convention for the curr pointer as we set up for singly linked lists, purely","chapter-4","Lists, Stacks, and Queues"
"for the sake of consistency.","chapter-4","Lists, Stacks, and Queues"
"Figure 4.14 shows the complete implementation for a Link class to be used","chapter-4","Lists, Stacks, and Queues"
"with doubly linked lists. This code is a little longer than that for the singly linked list","chapter-4","Lists, Stacks, and Queues"
"node implementation since the doubly linked list nodes have an extra data member.","chapter-4","Lists, Stacks, and Queues"
"Figure 4.15 shows the implementation for the insert, append, remove,","chapter-4","Lists, Stacks, and Queues"
"and prev doubly linked list methods. The class declaration and the remaining","chapter-4","Lists, Stacks, and Queues"
"member functions for the doubly linked list class are nearly identical to the singly","chapter-4","Lists, Stacks, and Queues"
"linked list version.","chapter-4","Lists, Stacks, and Queues"
"The insert method is especially simple for our doubly linked list implemen-","chapter-4","Lists, Stacks, and Queues"
"tation, because most of the work is done by the node’s constructor. Figure 4.16","chapter-4","Lists, Stacks, and Queues"
"shows the list before and after insertion of a node with value 10.","chapter-4","Lists, Stacks, and Queues"
"The three parameters to the new operator allow the list node class constructor","chapter-4","Lists, Stacks, and Queues"
"to set the element, prev, and next fields, respectively, for the new link node.","chapter-4","Lists, Stacks, and Queues"
"The new operator returns a pointer to the newly created node. The nodes to either","chapter-4","Lists, Stacks, and Queues"
"side have their pointers updated to point to the newly created node. The existence","chapter-4","Lists, Stacks, and Queues"
"114 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"/** Doubly linked list node */","chapter-4","Lists, Stacks, and Queues"
"class DLink<E> {","chapter-4","Lists, Stacks, and Queues"
"private E element; // Value for this node","chapter-4","Lists, Stacks, and Queues"
"private DLink<E> next; // Pointer to next node in list","chapter-4","Lists, Stacks, and Queues"
"private DLink<E> prev; // Pointer to previous node","chapter-4","Lists, Stacks, and Queues"
"/** Constructors */","chapter-4","Lists, Stacks, and Queues"
"DLink(E it, DLink<E> p, DLink<E> n)","chapter-4","Lists, Stacks, and Queues"
"{ element = it; prev = p; next = n; }","chapter-4","Lists, Stacks, and Queues"
"DLink(DLink<E> p, DLink<E> n) { prev = p; next = n; }","chapter-4","Lists, Stacks, and Queues"
"/** Get and set methods for the data members */","chapter-4","Lists, Stacks, and Queues"
"DLink<E> next() { return next; }","chapter-4","Lists, Stacks, and Queues"
"DLink<E> setNext(DLink<E> nextval)","chapter-4","Lists, Stacks, and Queues"
"{ return next = nextval; }","chapter-4","Lists, Stacks, and Queues"
"DLink<E> prev() { return prev; }","chapter-4","Lists, Stacks, and Queues"
"DLink<E> setPrev(DLink<E> prevval)","chapter-4","Lists, Stacks, and Queues"
"{ return prev = prevval; }","chapter-4","Lists, Stacks, and Queues"
"E element() { return element; }","chapter-4","Lists, Stacks, and Queues"
"E setElement(E it) { return element = it; }","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"Figure 4.14 Doubly linked list node implementation with a freelist.","chapter-4","Lists, Stacks, and Queues"
"of the header and tailer nodes mean that there are no special cases to worry about","chapter-4","Lists, Stacks, and Queues"
"when inserting into an empty list.","chapter-4","Lists, Stacks, and Queues"
"The append method is also simple. Again, the Link class constructor sets the","chapter-4","Lists, Stacks, and Queues"
"element, prev, and next fields of the node when the new operator is executed.","chapter-4","Lists, Stacks, and Queues"
"Method remove (illustrated by Figure 4.17) is straightforward, though the","chapter-4","Lists, Stacks, and Queues"
"code is somewhat longer. First, the variable it is assigned the value being re-","chapter-4","Lists, Stacks, and Queues"
"moved. Note that we must separate the element, which is returned to the caller,","chapter-4","Lists, Stacks, and Queues"
"from the link object. The following lines then adjust the list.","chapter-4","Lists, Stacks, and Queues"
"E it = curr.next().element(); // Remember value","chapter-4","Lists, Stacks, and Queues"
"curr.next().next().setPrev(curr);","chapter-4","Lists, Stacks, and Queues"
"curr.setNext(curr.next().next()); // Remove from list","chapter-4","Lists, Stacks, and Queues"
"The first line stores the value of the node being removed. The second line makes","chapter-4","Lists, Stacks, and Queues"
"the next node’s prev pointer point to the left of the node being removed. Finally,","chapter-4","Lists, Stacks, and Queues"
"the next field of the node preceding the one being deleted is adjusted. The final","chapter-4","Lists, Stacks, and Queues"
"steps of method remove are to update the list length and return the value of the","chapter-4","Lists, Stacks, and Queues"
"deleted element.","chapter-4","Lists, Stacks, and Queues"
"The only disadvantage of the doubly linked list as compared to the singly linked","chapter-4","Lists, Stacks, and Queues"
"list is the additional space used. The doubly linked list requires two pointers per","chapter-4","Lists, Stacks, and Queues"
"node, and so in the implementation presented it requires twice as much overhead","chapter-4","Lists, Stacks, and Queues"
"as the singly linked list.","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.1 Lists 115","chapter-4","Lists, Stacks, and Queues"
"/** Insert "it" at current position */","chapter-4","Lists, Stacks, and Queues"
"public void insert(E it) {","chapter-4","Lists, Stacks, and Queues"
"curr.setNext(new DLink<E>(it, curr, curr.next()));","chapter-4","Lists, Stacks, and Queues"
"curr.next().next().setPrev(curr.next());","chapter-4","Lists, Stacks, and Queues"
"cnt++;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Append "it" to list */","chapter-4","Lists, Stacks, and Queues"
"public void append(E it) {","chapter-4","Lists, Stacks, and Queues"
"tail.setPrev(new DLink<E>(it, tail.prev(), tail));","chapter-4","Lists, Stacks, and Queues"
"tail.prev().prev().setNext(tail.prev());","chapter-4","Lists, Stacks, and Queues"
"cnt++;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Remove and return current element */","chapter-4","Lists, Stacks, and Queues"
"public E remove() {","chapter-4","Lists, Stacks, and Queues"
"if (curr.next() == tail) return null; // Nothing to remove","chapter-4","Lists, Stacks, and Queues"
"E it = curr.next().element(); // Remember value","chapter-4","Lists, Stacks, and Queues"
"curr.next().next().setPrev(curr);","chapter-4","Lists, Stacks, and Queues"
"curr.setNext(curr.next().next()); // Remove from list","chapter-4","Lists, Stacks, and Queues"
"cnt--; // Decrement the count","chapter-4","Lists, Stacks, and Queues"
"return it; // Return value removed","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Move curr one step left; no change if at front */","chapter-4","Lists, Stacks, and Queues"
"public void prev() {","chapter-4","Lists, Stacks, and Queues"
"if (curr != head) // Can’t back up from list head","chapter-4","Lists, Stacks, and Queues"
"curr = curr.prev();","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"Figure 4.15 Implementations for doubly linked list insert, append,","chapter-4","Lists, Stacks, and Queues"
"remove, and prev methods.","chapter-4","Lists, Stacks, and Queues"
"Example 4.1 There is a space-saving technique that can be employed to","chapter-4","Lists, Stacks, and Queues"
"eliminate the additional space requirement, though it will complicate the","chapter-4","Lists, Stacks, and Queues"
"implementation and be somewhat slower. Thus, this is an example of a","chapter-4","Lists, Stacks, and Queues"
"space/time tradeoff. It is based on observing that, if we store the sum of","chapter-4","Lists, Stacks, and Queues"
"two values, then we can get either value back by subtracting the other. That","chapter-4","Lists, Stacks, and Queues"
"is, if we store a + b in variable c, then b = c − a and a = c − b. Of course,","chapter-4","Lists, Stacks, and Queues"
"to recover one of the values out of the stored summation, the other value","chapter-4","Lists, Stacks, and Queues"
"must be supplied. A pointer to the first node in the list, along with the value","chapter-4","Lists, Stacks, and Queues"
"of one of its two link fields, will allow access to all of the remaining nodes","chapter-4","Lists, Stacks, and Queues"
"of the list in order. This is because the pointer to the node must be the same","chapter-4","Lists, Stacks, and Queues"
"as the value of the following node’s prev pointer, as well as the previous","chapter-4","Lists, Stacks, and Queues"
"node’s next pointer. It is possible to move down the list breaking apart","chapter-4","Lists, Stacks, and Queues"
"the summed link fields as though you were opening a zipper. Details for","chapter-4","Lists, Stacks, and Queues"
"implementing this variation are left as an exercise.","chapter-4","Lists, Stacks, and Queues"
"116 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"23 12 ...","chapter-4","Lists, Stacks, and Queues"
"5","chapter-4","Lists, Stacks, and Queues"
"... 20","chapter-4","Lists, Stacks, and Queues"
"... 20","chapter-4","Lists, Stacks, and Queues"
"4","chapter-4","Lists, Stacks, and Queues"
"curr","chapter-4","Lists, Stacks, and Queues"
"23 12 ...","chapter-4","Lists, Stacks, and Queues"
"10","chapter-4","Lists, Stacks, and Queues"
"3 2","chapter-4","Lists, Stacks, and Queues"
"(b)","chapter-4","Lists, Stacks, and Queues"
"curr","chapter-4","Lists, Stacks, and Queues"
"Insert 10: 10","chapter-4","Lists, Stacks, and Queues"
"1","chapter-4","Lists, Stacks, and Queues"
"(a)","chapter-4","Lists, Stacks, and Queues"
"Figure 4.16 Insertion for doubly linked lists. The labels 1 , 2 , and 3 cor-","chapter-4","Lists, Stacks, and Queues"
"respond to assignments done by the linked list node constructor. 4 marks the","chapter-4","Lists, Stacks, and Queues"
"assignment to curr->next. 5 marks the assignment to the prev pointer of","chapter-4","Lists, Stacks, and Queues"
"the node following the newly inserted node.","chapter-4","Lists, Stacks, and Queues"
"... 20","chapter-4","Lists, Stacks, and Queues"
"curr","chapter-4","Lists, Stacks, and Queues"
"23 12 ...","chapter-4","Lists, Stacks, and Queues"
"... ... 20 12","chapter-4","Lists, Stacks, and Queues"
"curr","chapter-4","Lists, Stacks, and Queues"
"(b)","chapter-4","Lists, Stacks, and Queues"
"it 23","chapter-4","Lists, Stacks, and Queues"
"(a)","chapter-4","Lists, Stacks, and Queues"
"Figure 4.17 Doubly linked list removal. Element it stores the element of the","chapter-4","Lists, Stacks, and Queues"
"node being removed. Then the nodes to either side have their pointers adjusted.","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.2 Stacks 117","chapter-4","Lists, Stacks, and Queues"
"The principle behind this technique is worth remembering, as it has","chapter-4","Lists, Stacks, and Queues"
"many applications. The following code fragment will swap the contents","chapter-4","Lists, Stacks, and Queues"
"of two variables without using a temporary variable (at the cost of three","chapter-4","Lists, Stacks, and Queues"
"arithmetic operations).","chapter-4","Lists, Stacks, and Queues"
"a = a + b;","chapter-4","Lists, Stacks, and Queues"
"b = a - b; // Now b contains original value of a","chapter-4","Lists, Stacks, and Queues"
"a = a - b; // Now a contains original value of b","chapter-4","Lists, Stacks, and Queues"
"A similar effect can be had by using the exclusive-or operator. This fact","chapter-4","Lists, Stacks, and Queues"
"is widely used in computer graphics. A region of the computer screen can","chapter-4","Lists, Stacks, and Queues"
"be highlighted by XORing the outline of a box around it. XORing the box","chapter-4","Lists, Stacks, and Queues"
"outline a second time restores the original contents of the screen.","chapter-4","Lists, Stacks, and Queues"
"4.2 Stacks","chapter-4","Lists, Stacks, and Queues"
"The stack is a list-like structure in which elements may be inserted or removed","chapter-4","Lists, Stacks, and Queues"
"from only one end. While this restriction makes stacks less flexible than lists, it","chapter-4","Lists, Stacks, and Queues"
"also makes stacks both efficient (for those operations they can do) and easy to im-","chapter-4","Lists, Stacks, and Queues"
"plement. Many applications require only the limited form of insert and remove","chapter-4","Lists, Stacks, and Queues"
"operations that stacks provide. In such cases, it is more efficient to use the sim-","chapter-4","Lists, Stacks, and Queues"
"pler stack data structure rather than the generic list. For example, the freelist of","chapter-4","Lists, Stacks, and Queues"
"Section 4.1.2 is really a stack.","chapter-4","Lists, Stacks, and Queues"
"Despite their restrictions, stacks have many uses. Thus, a special vocabulary","chapter-4","Lists, Stacks, and Queues"
"for stacks has developed. Accountants used stacks long before the invention of the","chapter-4","Lists, Stacks, and Queues"
"computer. They called the stack a “LIFO” list, which stands for “Last-In, First-","chapter-4","Lists, Stacks, and Queues"
"Out.” Note that one implication of the LIFO policy is that stacks remove elements","chapter-4","Lists, Stacks, and Queues"
"in reverse order of their arrival.","chapter-4","Lists, Stacks, and Queues"
"The accessible element of the stack is called the top element. Elements are not","chapter-4","Lists, Stacks, and Queues"
"said to be inserted, they are pushed onto the stack. When removed, an element is","chapter-4","Lists, Stacks, and Queues"
"said to be popped from the stack. Figure 4.18 shows a sample stack ADT.","chapter-4","Lists, Stacks, and Queues"
"As with lists, there are many variations on stack implementation. The two ap-","chapter-4","Lists, Stacks, and Queues"
"proaches presented here are array-based and linked stacks, which are analogous","chapter-4","Lists, Stacks, and Queues"
"to array-based and linked lists, respectively.","chapter-4","Lists, Stacks, and Queues"
"4.2.1 Array-Based Stacks","chapter-4","Lists, Stacks, and Queues"
"Figure 4.19 shows a complete implementation for the array-based stack class. As","chapter-4","Lists, Stacks, and Queues"
"with the array-based list implementation, listArray must be declared of fixed","chapter-4","Lists, Stacks, and Queues"
"size when the stack is created. In the stack constructor, size serves to indicate","chapter-4","Lists, Stacks, and Queues"
"this size. Method top acts somewhat like a current position value (because the","chapter-4","Lists, Stacks, and Queues"
"“current” position is always at the top of the stack), as well as indicating the number","chapter-4","Lists, Stacks, and Queues"
"of elements currently in the stack.","chapter-4","Lists, Stacks, and Queues"
"118 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"/** Stack ADT */","chapter-4","Lists, Stacks, and Queues"
"public interface Stack<E> {","chapter-4","Lists, Stacks, and Queues"
"/** Reinitialize the stack. The user is responsible for","chapter-4","Lists, Stacks, and Queues"
"reclaiming the storage used by the stack elements. */","chapter-4","Lists, Stacks, and Queues"
"public void clear();","chapter-4","Lists, Stacks, and Queues"
"/** Push an element onto the top of the stack.","chapter-4","Lists, Stacks, and Queues"
"@param it The element being pushed onto the stack. */","chapter-4","Lists, Stacks, and Queues"
"public void push(E it);","chapter-4","Lists, Stacks, and Queues"
"/** Remove and return the element at the top of the stack.","chapter-4","Lists, Stacks, and Queues"
"@return The element at the top of the stack. */","chapter-4","Lists, Stacks, and Queues"
"public E pop();","chapter-4","Lists, Stacks, and Queues"
"/** @return A copy of the top element. */","chapter-4","Lists, Stacks, and Queues"
"public E topValue();","chapter-4","Lists, Stacks, and Queues"
"/** @return The number of elements in the stack. */","chapter-4","Lists, Stacks, and Queues"
"public int length();","chapter-4","Lists, Stacks, and Queues"
"};","chapter-4","Lists, Stacks, and Queues"
"Figure 4.18 The stack ADT.","chapter-4","Lists, Stacks, and Queues"
"The array-based stack implementation is essentially a simplified version of the","chapter-4","Lists, Stacks, and Queues"
"array-based list. The only important design decision to be made is which end of","chapter-4","Lists, Stacks, and Queues"
"the array should represent the top of the stack. One choice is to make the top be","chapter-4","Lists, Stacks, and Queues"
"at position 0 in the array. In terms of list functions, all insert and remove","chapter-4","Lists, Stacks, and Queues"
"operations would then be on the element in position 0. This implementation is","chapter-4","Lists, Stacks, and Queues"
"inefficient, because now every push or pop operation will require that all elements","chapter-4","Lists, Stacks, and Queues"
"currently in the stack be shifted one position in the array, for a cost of Θ(n) if there","chapter-4","Lists, Stacks, and Queues"
"are n elements. The other choice is have the top element be at position n − 1 when","chapter-4","Lists, Stacks, and Queues"
"there are n elements in the stack. In other words, as elements are pushed onto","chapter-4","Lists, Stacks, and Queues"
"the stack, they are appended to the tail of the list. Method pop removes the tail","chapter-4","Lists, Stacks, and Queues"
"element. In this case, the cost for each push or pop operation is only Θ(1).","chapter-4","Lists, Stacks, and Queues"
"For the implementation of Figure 4.19, top is defined to be the array index of","chapter-4","Lists, Stacks, and Queues"
"the first free position in the stack. Thus, an empty stack has top set to 0, the first","chapter-4","Lists, Stacks, and Queues"
"available free position in the array. (Alternatively, top could have been defined to","chapter-4","Lists, Stacks, and Queues"
"be the index for the top element in the stack, rather than the first free position. If","chapter-4","Lists, Stacks, and Queues"
"this had been done, the empty list would initialize top as −1.) Methods push and","chapter-4","Lists, Stacks, and Queues"
"pop simply place an element into, or remove an element from, the array position","chapter-4","Lists, Stacks, and Queues"
"indicated by top. Because top is assumed to be at the first free position, push","chapter-4","Lists, Stacks, and Queues"
"first inserts its value into the top position and then increments top, while pop first","chapter-4","Lists, Stacks, and Queues"
"decrements top and then removes the top element.","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.2 Stacks 119","chapter-4","Lists, Stacks, and Queues"
"/** Array-based stack implementation */","chapter-4","Lists, Stacks, and Queues"
"class AStack<E> implements Stack<E> {","chapter-4","Lists, Stacks, and Queues"
"private static final int defaultSize = 10;","chapter-4","Lists, Stacks, and Queues"
"private int maxSize; // Maximum size of stack","chapter-4","Lists, Stacks, and Queues"
"private int top; // Index for top Object","chapter-4","Lists, Stacks, and Queues"
"private E [] listArray; // Array holding stack","chapter-4","Lists, Stacks, and Queues"
"/** Constructors */","chapter-4","Lists, Stacks, and Queues"
"AStack() { this(defaultSize); }","chapter-4","Lists, Stacks, and Queues"
"@SuppressWarnings("unchecked") // Generic array allocation","chapter-4","Lists, Stacks, and Queues"
"AStack(int size) {","chapter-4","Lists, Stacks, and Queues"
"maxSize = size;","chapter-4","Lists, Stacks, and Queues"
"top = 0;","chapter-4","Lists, Stacks, and Queues"
"listArray = (E[])new Object[size]; // Create listArray","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Reinitialize stack */","chapter-4","Lists, Stacks, and Queues"
"public void clear() { top = 0; }","chapter-4","Lists, Stacks, and Queues"
"/** Push "it" onto stack */","chapter-4","Lists, Stacks, and Queues"
"public void push(E it) {","chapter-4","Lists, Stacks, and Queues"
"assert top != maxSize : "Stack is full";","chapter-4","Lists, Stacks, and Queues"
"listArray[top++] = it;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Remove and top element */","chapter-4","Lists, Stacks, and Queues"
"public E pop() {","chapter-4","Lists, Stacks, and Queues"
"assert top != 0 : "Stack is empty";","chapter-4","Lists, Stacks, and Queues"
"return listArray[--top];","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** @return Top element */","chapter-4","Lists, Stacks, and Queues"
"public E topValue() {","chapter-4","Lists, Stacks, and Queues"
"assert top != 0 : "Stack is empty";","chapter-4","Lists, Stacks, and Queues"
"return listArray[top-1];","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** @return Stack size */","chapter-4","Lists, Stacks, and Queues"
"public int length() { return top; }","chapter-4","Lists, Stacks, and Queues"
"Figure 4.19 Array-based stack class implementation.","chapter-4","Lists, Stacks, and Queues"
"120 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"/** Linked stack implementation */","chapter-4","Lists, Stacks, and Queues"
"class LStack<E> implements Stack<E> {","chapter-4","Lists, Stacks, and Queues"
"private Link<E> top; // Pointer to first element","chapter-4","Lists, Stacks, and Queues"
"private int size; // Number of elements","chapter-4","Lists, Stacks, and Queues"
"/** Constructors */","chapter-4","Lists, Stacks, and Queues"
"public LStack() { top = null; size = 0; }","chapter-4","Lists, Stacks, and Queues"
"public LStack(int size) { top = null; size = 0; }","chapter-4","Lists, Stacks, and Queues"
"/** Reinitialize stack */","chapter-4","Lists, Stacks, and Queues"
"public void clear() { top = null; size = 0; }","chapter-4","Lists, Stacks, and Queues"
"/** Put "it" on stack */","chapter-4","Lists, Stacks, and Queues"
"public void push(E it) {","chapter-4","Lists, Stacks, and Queues"
"top = new Link<E>(it, top);","chapter-4","Lists, Stacks, and Queues"
"size++;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Remove "it" from stack */","chapter-4","Lists, Stacks, and Queues"
"public E pop() {","chapter-4","Lists, Stacks, and Queues"
"assert top != null : "Stack is empty";","chapter-4","Lists, Stacks, and Queues"
"E it = top.element();","chapter-4","Lists, Stacks, and Queues"
"top = top.next();","chapter-4","Lists, Stacks, and Queues"
"size--;","chapter-4","Lists, Stacks, and Queues"
"return it;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** @return Top value */","chapter-4","Lists, Stacks, and Queues"
"public E topValue() {","chapter-4","Lists, Stacks, and Queues"
"assert top != null : "Stack is empty";","chapter-4","Lists, Stacks, and Queues"
"return top.element();","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** @return Stack length */","chapter-4","Lists, Stacks, and Queues"
"public int length() { return size; }","chapter-4","Lists, Stacks, and Queues"
"Figure 4.20 Linked stack class implementation.","chapter-4","Lists, Stacks, and Queues"
"4.2.2 Linked Stacks","chapter-4","Lists, Stacks, and Queues"
"The linked stack implementation is quite simple. The freelist of Section 4.1.2 is","chapter-4","Lists, Stacks, and Queues"
"an example of a linked stack. Elements are inserted and removed only from the","chapter-4","Lists, Stacks, and Queues"
"head of the list. A header node is not used because no special-case code is required","chapter-4","Lists, Stacks, and Queues"
"for lists of zero or one elements. Figure 4.20 shows the complete linked stack","chapter-4","Lists, Stacks, and Queues"
"implementation. The only data member is top, a pointer to the first (top) link node","chapter-4","Lists, Stacks, and Queues"
"of the stack. Method push first modifies the next field of the newly created link","chapter-4","Lists, Stacks, and Queues"
"node to point to the top of the stack and then sets top to point to the new link","chapter-4","Lists, Stacks, and Queues"
"node. Method pop is also quite simple. Variable temp stores the top nodes’ value,","chapter-4","Lists, Stacks, and Queues"
"while ltemp links to the top node as it is removed from the stack. The stack is","chapter-4","Lists, Stacks, and Queues"
"updated by setting top to point to the next link in the stack. The old top node is","chapter-4","Lists, Stacks, and Queues"
"then returned to free store (or the freelist), and the element value is returned.","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.2 Stacks 121","chapter-4","Lists, Stacks, and Queues"
"top1 top2","chapter-4","Lists, Stacks, and Queues"
"Figure 4.21 Two stacks implemented within in a single array, both growing","chapter-4","Lists, Stacks, and Queues"
"toward the middle.","chapter-4","Lists, Stacks, and Queues"
"4.2.3 Comparison of Array-Based and Linked Stacks","chapter-4","Lists, Stacks, and Queues"
"All operations for the array-based and linked stack implementations take constant","chapter-4","Lists, Stacks, and Queues"
"time, so from a time efficiency perspective, neither has a significant advantage.","chapter-4","Lists, Stacks, and Queues"
"Another basis for comparison is the total space required. The analysis is similar to","chapter-4","Lists, Stacks, and Queues"
"that done for list implementations. The array-based stack must declare a fixed-size","chapter-4","Lists, Stacks, and Queues"
"array initially, and some of that space is wasted whenever the stack is not full. The","chapter-4","Lists, Stacks, and Queues"
"linked stack can shrink and grow but requires the overhead of a link field for every","chapter-4","Lists, Stacks, and Queues"
"element.","chapter-4","Lists, Stacks, and Queues"
"When multiple stacks are to be implemented, it is possible to take advantage of","chapter-4","Lists, Stacks, and Queues"
"the one-way growth of the array-based stack. This can be done by using a single","chapter-4","Lists, Stacks, and Queues"
"array to store two stacks. One stack grows inward from each end as illustrated by","chapter-4","Lists, Stacks, and Queues"
"Figure 4.21, hopefully leading to less wasted space. However, this only works well","chapter-4","Lists, Stacks, and Queues"
"when the space requirements of the two stacks are inversely correlated. In other","chapter-4","Lists, Stacks, and Queues"
"words, ideally when one stack grows, the other will shrink. This is particularly","chapter-4","Lists, Stacks, and Queues"
"effective when elements are taken from one stack and given to the other. If instead","chapter-4","Lists, Stacks, and Queues"
"both stacks grow at the same time, then the free space in the middle of the array","chapter-4","Lists, Stacks, and Queues"
"will be exhausted quickly.","chapter-4","Lists, Stacks, and Queues"
"4.2.4 Implementing Recursion","chapter-4","Lists, Stacks, and Queues"
"Perhaps the most common computer application that uses stacks is not even visible","chapter-4","Lists, Stacks, and Queues"
"to its users. This is the implementation of subroutine calls in most programming","chapter-4","Lists, Stacks, and Queues"
"language runtime environments. A subroutine call is normally implemented by","chapter-4","Lists, Stacks, and Queues"
"placing necessary information about the subroutine (including the return address,","chapter-4","Lists, Stacks, and Queues"
"parameters, and local variables) onto a stack. This information is called an ac-","chapter-4","Lists, Stacks, and Queues"
"tivation record. Further subroutine calls add to the stack. Each return from a","chapter-4","Lists, Stacks, and Queues"
"subroutine pops the top activation record off the stack. Figure 4.22 illustrates the","chapter-4","Lists, Stacks, and Queues"
"implementation of the recursive factorial function of Section 2.5 from the runtime","chapter-4","Lists, Stacks, and Queues"
"environment’s point of view.","chapter-4","Lists, Stacks, and Queues"
"Consider what happens when we call fact with the value 4. We use β to","chapter-4","Lists, Stacks, and Queues"
"indicate the address of the program instruction where the call to fact is made.","chapter-4","Lists, Stacks, and Queues"
"Thus, the stack must first store the address β, and the value 4 is passed to fact.","chapter-4","Lists, Stacks, and Queues"
"Next, a recursive call to fact is made, this time with value 3. We will name the","chapter-4","Lists, Stacks, and Queues"
"program address from which the call is made β1. The address β1, along with the","chapter-4","Lists, Stacks, and Queues"
"122 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"β β β","chapter-4","Lists, Stacks, and Queues"
"β β","chapter-4","Lists, Stacks, and Queues"
"β","chapter-4","Lists, Stacks, and Queues"
"β","chapter-4","Lists, Stacks, and Queues"
"β","chapter-4","Lists, Stacks, and Queues"
"β","chapter-4","Lists, Stacks, and Queues"
"β","chapter-4","Lists, Stacks, and Queues"
"1","chapter-4","Lists, Stacks, and Queues"
"β","chapter-4","Lists, Stacks, and Queues"
"β","chapter-4","Lists, Stacks, and Queues"
"β","chapter-4","Lists, Stacks, and Queues"
"β","chapter-4","Lists, Stacks, and Queues"
"β β 1 1","chapter-4","Lists, Stacks, and Queues"
"1 1","chapter-4","Lists, Stacks, and Queues"
"2 2","chapter-4","Lists, Stacks, and Queues"
"2","chapter-4","Lists, Stacks, and Queues"
"3","chapter-4","Lists, Stacks, and Queues"
"4 4","chapter-4","Lists, Stacks, and Queues"
"3","chapter-4","Lists, Stacks, and Queues"
"2","chapter-4","Lists, Stacks, and Queues"
"3","chapter-4","Lists, Stacks, and Queues"
"4","chapter-4","Lists, Stacks, and Queues"
"Call fact(4) Call fact(3) Call fact(2) Call fact(1)","chapter-4","Lists, Stacks, and Queues"
"Return 1","chapter-4","Lists, Stacks, and Queues"
"4","chapter-4","Lists, Stacks, and Queues"
"3","chapter-4","Lists, Stacks, and Queues"
"Return 2","chapter-4","Lists, Stacks, and Queues"
"4","chapter-4","Lists, Stacks, and Queues"
"Return 6","chapter-4","Lists, Stacks, and Queues"
"Return 24","chapter-4","Lists, Stacks, and Queues"
"Currptr","chapter-4","Lists, Stacks, and Queues"
"Currptr","chapter-4","Lists, Stacks, and Queues"
"Currptr","chapter-4","Lists, Stacks, and Queues"
"Currptr","chapter-4","Lists, Stacks, and Queues"
"Currptr","chapter-4","Lists, Stacks, and Queues"
"Currptr","chapter-4","Lists, Stacks, and Queues"
"Currptr","chapter-4","Lists, Stacks, and Queues"
"Currptr","chapter-4","Lists, Stacks, and Queues"
"Currptr","chapter-4","Lists, Stacks, and Queues"
"Currptr Currptr Currptr","chapter-4","Lists, Stacks, and Queues"
"Currptr Currptr","chapter-4","Lists, Stacks, and Queues"
"n n","chapter-4","Lists, Stacks, and Queues"
"n n","chapter-4","Lists, Stacks, and Queues"
"n","chapter-4","Lists, Stacks, and Queues"
"n","chapter-4","Lists, Stacks, and Queues"
"n n","chapter-4","Lists, Stacks, and Queues"
"n","chapter-4","Lists, Stacks, and Queues"
"Currptr","chapter-4","Lists, Stacks, and Queues"
"Currptr","chapter-4","Lists, Stacks, and Queues"
"Figure 4.22 Implementing recursion with a stack. β values indicate the address","chapter-4","Lists, Stacks, and Queues"
"of the program instruction to return to after completing the current function call.","chapter-4","Lists, Stacks, and Queues"
"On each recursive function call to fact (as implemented in Section 2.5), both the","chapter-4","Lists, Stacks, and Queues"
"return address and the current value of n must be saved. Each return from fact","chapter-4","Lists, Stacks, and Queues"
"pops the top activation record off the stack.","chapter-4","Lists, Stacks, and Queues"
"current value for n (which is 4), is saved on the stack. Function fact is invoked","chapter-4","Lists, Stacks, and Queues"
"with input parameter 3.","chapter-4","Lists, Stacks, and Queues"
"In similar manner, another recursive call is made with input parameter 2, re-","chapter-4","Lists, Stacks, and Queues"
"quiring that the address from which the call is made (say β2) and the current value","chapter-4","Lists, Stacks, and Queues"
"for n (which is 3) are stored on the stack. A final recursive call with input parame-","chapter-4","Lists, Stacks, and Queues"
"ter 1 is made, requiring that the stack store the calling address (say β3) and current","chapter-4","Lists, Stacks, and Queues"
"value (which is 2).","chapter-4","Lists, Stacks, and Queues"
"At this point, we have reached the base case for fact, and so the recursion","chapter-4","Lists, Stacks, and Queues"
"begins to unwind. Each return from fact involves popping the stored value for","chapter-4","Lists, Stacks, and Queues"
"n from the stack, along with the return address from the function call. The return","chapter-4","Lists, Stacks, and Queues"
"value for fact is multiplied by the restored value for n, and the result is returned.","chapter-4","Lists, Stacks, and Queues"
"Because an activation record must be created and placed onto the stack for","chapter-4","Lists, Stacks, and Queues"
"each subroutine call, making subroutine calls is a relatively expensive operation.","chapter-4","Lists, Stacks, and Queues"
"While recursion is often used to make implementation easy and clear, sometimes","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.2 Stacks 123","chapter-4","Lists, Stacks, and Queues"
"you might want to eliminate the overhead imposed by the recursive function calls.","chapter-4","Lists, Stacks, and Queues"
"In some cases, such as the factorial function of Section 2.5, recursion can easily be","chapter-4","Lists, Stacks, and Queues"
"replaced by iteration.","chapter-4","Lists, Stacks, and Queues"
"Example 4.2 As a simple example of replacing recursion with a stack,","chapter-4","Lists, Stacks, and Queues"
"consider the following non-recursive version of the factorial function.","chapter-4","Lists, Stacks, and Queues"
"/** @return n! */","chapter-4","Lists, Stacks, and Queues"
"static long fact(int n) {","chapter-4","Lists, Stacks, and Queues"
"// To fit n! in a long variable, require n < 21","chapter-4","Lists, Stacks, and Queues"
"assert (n >= 0) && (n <= 20) : "n out of range";","chapter-4","Lists, Stacks, and Queues"
"// Make a stack just big enough","chapter-4","Lists, Stacks, and Queues"
"Stack<Integer> S = new AStack<Integer>(n);","chapter-4","Lists, Stacks, and Queues"
"while (n > 1) S.push(n--);","chapter-4","Lists, Stacks, and Queues"
"long result = 1;","chapter-4","Lists, Stacks, and Queues"
"while (S.length() > 0)","chapter-4","Lists, Stacks, and Queues"
"result = result * S.pop();","chapter-4","Lists, Stacks, and Queues"
"return result;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"Here, we simply push successively smaller values of n onto the stack un-","chapter-4","Lists, Stacks, and Queues"
"til the base case is reached, then repeatedly pop off the stored values and","chapter-4","Lists, Stacks, and Queues"
"multiply them into the result.","chapter-4","Lists, Stacks, and Queues"
"An iterative form of the factorial function is both simpler and faster than the","chapter-4","Lists, Stacks, and Queues"
"version shown in Example 4.2. But it is not always possible to replace recursion","chapter-4","Lists, Stacks, and Queues"
"with iteration. Recursion, or some imitation of it, is necessary when implementing","chapter-4","Lists, Stacks, and Queues"
"algorithms that require multiple branching such as in the Towers of Hanoi alg-","chapter-4","Lists, Stacks, and Queues"
"orithm, or when traversing a binary tree. The Mergesort and Quicksort algorithms","chapter-4","Lists, Stacks, and Queues"
"of Chapter 7 are also examples in which recursion is required. Fortunately, it is al-","chapter-4","Lists, Stacks, and Queues"
"ways possible to imitate recursion with a stack. Let us now turn to a non-recursive","chapter-4","Lists, Stacks, and Queues"
"version of the Towers of Hanoi function, which cannot be done iteratively.","chapter-4","Lists, Stacks, and Queues"
"Example 4.3 The TOH function shown in Figure 2.2 makes two recursive","chapter-4","Lists, Stacks, and Queues"
"calls: one to move n − 1 rings off the bottom ring, and another to move","chapter-4","Lists, Stacks, and Queues"
"these n − 1 rings back to the goal pole. We can eliminate the recursion by","chapter-4","Lists, Stacks, and Queues"
"using a stack to store a representation of the three operations that TOH must","chapter-4","Lists, Stacks, and Queues"
"perform: two recursive calls and a move operation. To do so, we must first","chapter-4","Lists, Stacks, and Queues"
"come up with a representation of the various operations, implemented as a","chapter-4","Lists, Stacks, and Queues"
"class whose objects will be stored on the stack.","chapter-4","Lists, Stacks, and Queues"
"Figure 4.23 shows such a class. We first define an enumerated type","chapter-4","Lists, Stacks, and Queues"
"called TOHop, with two values MOVE and TOH, to indicate calls to the","chapter-4","Lists, Stacks, and Queues"
"move function and recursive calls to TOH, respectively. Class TOHobj","chapter-4","Lists, Stacks, and Queues"
"stores five values: an operation field (indicating either a move or a new","chapter-4","Lists, Stacks, and Queues"
"TOH operation), the number of rings, and the three poles. Note that the","chapter-4","Lists, Stacks, and Queues"
"124 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"public enum operation { MOVE, TOH }","chapter-4","Lists, Stacks, and Queues"
"class TOHobj {","chapter-4","Lists, Stacks, and Queues"
"public operation op;","chapter-4","Lists, Stacks, and Queues"
"public int num;","chapter-4","Lists, Stacks, and Queues"
"public Pole start, goal, temp;","chapter-4","Lists, Stacks, and Queues"
"/** Recursive call operation */","chapter-4","Lists, Stacks, and Queues"
"TOHobj(operation o, int n, Pole s, Pole g, Pole t)","chapter-4","Lists, Stacks, and Queues"
"{ op = o; num = n; start = s; goal = g; temp = t; }","chapter-4","Lists, Stacks, and Queues"
"/** MOVE operation */","chapter-4","Lists, Stacks, and Queues"
"TOHobj(operation o, Pole s, Pole g)","chapter-4","Lists, Stacks, and Queues"
"{ op = o; start = s; goal = g; }","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"static void TOH(int n, Pole start,","chapter-4","Lists, Stacks, and Queues"
"Pole goal, Pole temp) {","chapter-4","Lists, Stacks, and Queues"
"// Make a stack just big enough","chapter-4","Lists, Stacks, and Queues"
"Stack<TOHobj> S = new AStack<TOHobj>(2*n+1);","chapter-4","Lists, Stacks, and Queues"
"S.push(new TOHobj(operation.TOH, n,","chapter-4","Lists, Stacks, and Queues"
"start, goal, temp));","chapter-4","Lists, Stacks, and Queues"
"while (S.length() > 0) {","chapter-4","Lists, Stacks, and Queues"
"TOHobj it = S.pop(); // Get next task","chapter-4","Lists, Stacks, and Queues"
"if (it.op == operation.MOVE) // Do a move","chapter-4","Lists, Stacks, and Queues"
"move(it.start, it.goal);","chapter-4","Lists, Stacks, and Queues"
"else if (it.num > 0) { // Imitate TOH recursive","chapter-4","Lists, Stacks, and Queues"
"// solution (in reverse)","chapter-4","Lists, Stacks, and Queues"
"S.push(new TOHobj(operation.TOH, it.num-1,","chapter-4","Lists, Stacks, and Queues"
"it.temp, it.goal, it.start));","chapter-4","Lists, Stacks, and Queues"
"S.push(new TOHobj(operation.MOVE, it.start,","chapter-4","Lists, Stacks, and Queues"
"it.goal)); // A move to do","chapter-4","Lists, Stacks, and Queues"
"S.push(new TOHobj(operation.TOH, it.num-1,","chapter-4","Lists, Stacks, and Queues"
"it.start, it.temp, it.goal));","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"Figure 4.23 Stack-based implementation for Towers of Hanoi.","chapter-4","Lists, Stacks, and Queues"
"move operation actually needs only to store information about two poles.","chapter-4","Lists, Stacks, and Queues"
"Thus, there are two constructors: one to store the state when imitating a","chapter-4","Lists, Stacks, and Queues"
"recursive call, and one to store the state for a move operation.","chapter-4","Lists, Stacks, and Queues"
"An array-based stack is used because we know that the stack will need","chapter-4","Lists, Stacks, and Queues"
"to store exactly 2n+1 elements. The new version of TOH begins by placing","chapter-4","Lists, Stacks, and Queues"
"on the stack a description of the initial problem for n rings. The rest of","chapter-4","Lists, Stacks, and Queues"
"the function is simply a while loop that pops the stack and executes the","chapter-4","Lists, Stacks, and Queues"
"appropriate operation. In the case of a TOH operation (for n > 0), we","chapter-4","Lists, Stacks, and Queues"
"store on the stack representations for the three operations executed by the","chapter-4","Lists, Stacks, and Queues"
"recursive version. However, these operations must be placed on the stack","chapter-4","Lists, Stacks, and Queues"
"in reverse order, so that they will be popped off in the correct order.","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.3 Queues 125","chapter-4","Lists, Stacks, and Queues"
"/** Queue ADT */","chapter-4","Lists, Stacks, and Queues"
"public interface Queue<E> {","chapter-4","Lists, Stacks, and Queues"
"/** Reinitialize the queue. The user is responsible for","chapter-4","Lists, Stacks, and Queues"
"reclaiming the storage used by the queue elements. */","chapter-4","Lists, Stacks, and Queues"
"public void clear();","chapter-4","Lists, Stacks, and Queues"
"/** Place an element at the rear of the queue.","chapter-4","Lists, Stacks, and Queues"
"@param it The element being enqueued. */","chapter-4","Lists, Stacks, and Queues"
"public void enqueue(E it);","chapter-4","Lists, Stacks, and Queues"
"/** Remove and return element at the front of the queue.","chapter-4","Lists, Stacks, and Queues"
"@return The element at the front of the queue. */","chapter-4","Lists, Stacks, and Queues"
"public E dequeue();","chapter-4","Lists, Stacks, and Queues"
"/** @return The front element. */","chapter-4","Lists, Stacks, and Queues"
"public E frontValue();","chapter-4","Lists, Stacks, and Queues"
"/** @return The number of elements in the queue. */","chapter-4","Lists, Stacks, and Queues"
"public int length();","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"Figure 4.24 The Java ADT for a queue.","chapter-4","Lists, Stacks, and Queues"
"Recursive algorithms lend themselves to efficient implementation with a stack","chapter-4","Lists, Stacks, and Queues"
"when the amount of information needed to describe a sub-problem is small. For","chapter-4","Lists, Stacks, and Queues"
"example, Section 7.5 discusses a stack-based implementation for Quicksort.","chapter-4","Lists, Stacks, and Queues"
"4.3 Queues","chapter-4","Lists, Stacks, and Queues"
"Like the stack, the queue is a list-like structure that provides restricted access to","chapter-4","Lists, Stacks, and Queues"
"its elements. Queue elements may only be inserted at the back (called an enqueue","chapter-4","Lists, Stacks, and Queues"
"operation) and removed from the front (called a dequeue operation). Queues oper-","chapter-4","Lists, Stacks, and Queues"
"ate like standing in line at a movie theater ticket counter.1","chapter-4","Lists, Stacks, and Queues"
"If nobody cheats, then","chapter-4","Lists, Stacks, and Queues"
"newcomers go to the back of the line. The person at the front of the line is the next","chapter-4","Lists, Stacks, and Queues"
"to be served. Thus, queues release their elements in order of arrival. Accountants","chapter-4","Lists, Stacks, and Queues"
"have used queues since long before the existence of computers. They call a queue","chapter-4","Lists, Stacks, and Queues"
"a “FIFO” list, which stands for “First-In, First-Out.” Figure 4.24 shows a sample","chapter-4","Lists, Stacks, and Queues"
"queue ADT. This section presents two implementations for queues: the array-based","chapter-4","Lists, Stacks, and Queues"
"queue and the linked queue.","chapter-4","Lists, Stacks, and Queues"
"4.3.1 Array-Based Queues","chapter-4","Lists, Stacks, and Queues"
"The array-based queue is somewhat tricky to implement effectively. A simple con-","chapter-4","Lists, Stacks, and Queues"
"version of the array-based list implementation is not efficient.","chapter-4","Lists, Stacks, and Queues"
"1","chapter-4","Lists, Stacks, and Queues"
"In Britain, a line of people is called a “queue,” and getting into line to wait for service is called","chapter-4","Lists, Stacks, and Queues"
"“queuing up.”","chapter-4","Lists, Stacks, and Queues"
"126 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"front rear","chapter-4","Lists, Stacks, and Queues"
"20 5 12 17","chapter-4","Lists, Stacks, and Queues"
"(a)","chapter-4","Lists, Stacks, and Queues"
"rear","chapter-4","Lists, Stacks, and Queues"
"(b)","chapter-4","Lists, Stacks, and Queues"
"12 17 3 30 4","chapter-4","Lists, Stacks, and Queues"
"front","chapter-4","Lists, Stacks, and Queues"
"Figure 4.25 After repeated use, elements in the array-based queue will drift to","chapter-4","Lists, Stacks, and Queues"
"the back of the array. (a) The queue after the initial four numbers 20, 5, 12, and 17","chapter-4","Lists, Stacks, and Queues"
"have been inserted. (b) The queue after elements 20 and 5 are deleted, following","chapter-4","Lists, Stacks, and Queues"
"which 3, 30, and 4 are inserted.","chapter-4","Lists, Stacks, and Queues"
"Assume that there are n elements in the queue. By analogy to the array-based","chapter-4","Lists, Stacks, and Queues"
"list implementation, we could require that all elements of the queue be stored in the","chapter-4","Lists, Stacks, and Queues"
"first n positions of the array. If we choose the rear element of the queue to be in","chapter-4","Lists, Stacks, and Queues"
"position 0, then dequeue operations require only Θ(1) time because the front ele-","chapter-4","Lists, Stacks, and Queues"
"ment of the queue (the one being removed) is the last element in the array. However,","chapter-4","Lists, Stacks, and Queues"
"enqueue operations will require Θ(n) time, because the n elements currently in","chapter-4","Lists, Stacks, and Queues"
"the queue must each be shifted one position in the array. If instead we chose the","chapter-4","Lists, Stacks, and Queues"
"rear element of the queue to be in position n − 1, then an enqueue operation is","chapter-4","Lists, Stacks, and Queues"
"equivalent to an append operation on a list. This requires only Θ(1) time. But","chapter-4","Lists, Stacks, and Queues"
"now, a dequeue operation requires Θ(n) time, because all of the elements must","chapter-4","Lists, Stacks, and Queues"
"be shifted down by one position to retain the property that the remaining n − 1","chapter-4","Lists, Stacks, and Queues"
"queue elements reside in the first n − 1 positions of the array.","chapter-4","Lists, Stacks, and Queues"
"A far more efficient implementation can be obtained by relaxing the require-","chapter-4","Lists, Stacks, and Queues"
"ment that all elements of the queue must be in the first n positions of the array.","chapter-4","Lists, Stacks, and Queues"
"We will still require that the queue be stored be in contiguous array positions, but","chapter-4","Lists, Stacks, and Queues"
"the contents of the queue will be permitted to drift within the array, as illustrated","chapter-4","Lists, Stacks, and Queues"
"by Figure 4.25. Now, both the enqueue and the dequeue operations can be","chapter-4","Lists, Stacks, and Queues"
"performed in Θ(1) time because no other elements in the queue need be moved.","chapter-4","Lists, Stacks, and Queues"
"This implementation raises a new problem. Assume that the front element of","chapter-4","Lists, Stacks, and Queues"
"the queue is initially at position 0, and that elements are added to successively","chapter-4","Lists, Stacks, and Queues"
"higher-numbered positions in the array. When elements are removed from the","chapter-4","Lists, Stacks, and Queues"
"queue, the front index increases. Over time, the entire queue will drift toward","chapter-4","Lists, Stacks, and Queues"
"the higher-numbered positions in the array. Once an element is inserted into the","chapter-4","Lists, Stacks, and Queues"
"highest-numbered position in the array, the queue has run out of space. This hap-","chapter-4","Lists, Stacks, and Queues"
"pens despite the fact that there might be free positions at the low end of the array","chapter-4","Lists, Stacks, and Queues"
"where elements have previously been removed from the queue.","chapter-4","Lists, Stacks, and Queues"
"The “drifting queue” problem can be solved by pretending that the array is","chapter-4","Lists, Stacks, and Queues"
"circular and so allow the queue to continue directly from the highest-numbered","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.3 Queues 127","chapter-4","Lists, Stacks, and Queues"
"rear","chapter-4","Lists, Stacks, and Queues"
"front","chapter-4","Lists, Stacks, and Queues"
"rear","chapter-4","Lists, Stacks, and Queues"
"(a) (b)","chapter-4","Lists, Stacks, and Queues"
"20 5","chapter-4","Lists, Stacks, and Queues"
"12","chapter-4","Lists, Stacks, and Queues"
"17","chapter-4","Lists, Stacks, and Queues"
"12","chapter-4","Lists, Stacks, and Queues"
"17","chapter-4","Lists, Stacks, and Queues"
"3","chapter-4","Lists, Stacks, and Queues"
"30","chapter-4","Lists, Stacks, and Queues"
"4","chapter-4","Lists, Stacks, and Queues"
"front","chapter-4","Lists, Stacks, and Queues"
"Figure 4.26 The circular queue with array positions increasing in the clockwise","chapter-4","Lists, Stacks, and Queues"
"direction. (a) The queue after the initial four numbers 20, 5, 12, and 17 have been","chapter-4","Lists, Stacks, and Queues"
"inserted. (b) The queue after elements 20 and 5 are deleted, following which 3,","chapter-4","Lists, Stacks, and Queues"
"30, and 4 are inserted.","chapter-4","Lists, Stacks, and Queues"
"position in the array to the lowest-numbered position. This is easily implemented","chapter-4","Lists, Stacks, and Queues"
"through use of the modulus operator (denoted by % in Java). In this way, positions","chapter-4","Lists, Stacks, and Queues"
"in the array are numbered from 0 through size−1, and position size−1 is de-","chapter-4","Lists, Stacks, and Queues"
"fined to immediately precede position 0 (which is equivalent to position size %","chapter-4","Lists, Stacks, and Queues"
"size). Figure 4.26 illustrates this solution.","chapter-4","Lists, Stacks, and Queues"
"There remains one more serious, though subtle, problem to the array-based","chapter-4","Lists, Stacks, and Queues"
"queue implementation. How can we recognize when the queue is empty or full?","chapter-4","Lists, Stacks, and Queues"
"Assume that front stores the array index for the front element in the queue, and","chapter-4","Lists, Stacks, and Queues"
"rear stores the array index for the rear element. If both front and rear have the","chapter-4","Lists, Stacks, and Queues"
"same position, then with this scheme there must be one element in the queue. Thus,","chapter-4","Lists, Stacks, and Queues"
"an empty queue would be recognized by having rear be one less than front (tak-","chapter-4","Lists, Stacks, and Queues"
"ing into account the fact that the queue is circular, so position size−1 is actually","chapter-4","Lists, Stacks, and Queues"
"considered to be one less than position 0). But what if the queue is completely full?","chapter-4","Lists, Stacks, and Queues"
"In other words, what is the situation when a queue with n array positions available","chapter-4","Lists, Stacks, and Queues"
"contains n elements? In this case, if the front element is in position 0, then the","chapter-4","Lists, Stacks, and Queues"
"rear element is in position size−1. But this means that the value for rear is one","chapter-4","Lists, Stacks, and Queues"
"less than the value for front when the circular nature of the queue is taken into","chapter-4","Lists, Stacks, and Queues"
"account. In other words, the full queue is indistinguishable from the empty queue!","chapter-4","Lists, Stacks, and Queues"
"You might think that the problem is in the assumption about front and rear","chapter-4","Lists, Stacks, and Queues"
"being defined to store the array indices of the front and rear elements, respectively,","chapter-4","Lists, Stacks, and Queues"
"and that some modification in this definition will allow a solution. Unfortunately,","chapter-4","Lists, Stacks, and Queues"
"the problem cannot be remedied by a simple change to the definition for front","chapter-4","Lists, Stacks, and Queues"
"and rear, because of the number of conditions or states that the queue can be in.","chapter-4","Lists, Stacks, and Queues"
"Ignoring the actual position of the first element, and ignoring the actual values of","chapter-4","Lists, Stacks, and Queues"
"the elements stored in the queue, how many different states are there? There can","chapter-4","Lists, Stacks, and Queues"
"be no elements in the queue, one element, two, and so on. At most there can be","chapter-4","Lists, Stacks, and Queues"
"128 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"n elements in the queue if there are n array positions. This means that there are","chapter-4","Lists, Stacks, and Queues"
"n + 1 different states for the queue (0 through n elements are possible).","chapter-4","Lists, Stacks, and Queues"
"If the value of front is fixed, then n+ 1 different values for rear are needed","chapter-4","Lists, Stacks, and Queues"
"to distinguish among the n+ 1 states. However, there are only n possible values for","chapter-4","Lists, Stacks, and Queues"
"rear unless we invent a special case for, say, empty queues. This is an example of","chapter-4","Lists, Stacks, and Queues"
"the Pigeonhole Principle defined in Exercise 2.30. The Pigeonhole Principle states","chapter-4","Lists, Stacks, and Queues"
"that, given n pigeonholes and n + 1 pigeons, when all of the pigeons go into the","chapter-4","Lists, Stacks, and Queues"
"holes we can be sure that at least one hole contains more than one pigeon. In similar","chapter-4","Lists, Stacks, and Queues"
"manner, we can be sure that two of the n + 1 states are indistinguishable by the n","chapter-4","Lists, Stacks, and Queues"
"relative values of front and rear. We must seek some other way to distinguish","chapter-4","Lists, Stacks, and Queues"
"full from empty queues.","chapter-4","Lists, Stacks, and Queues"
"One obvious solution is to keep an explicit count of the number of elements in","chapter-4","Lists, Stacks, and Queues"
"the queue, or at least a Boolean variable that indicates whether the queue is empty","chapter-4","Lists, Stacks, and Queues"
"or not. Another solution is to make the array be of size n + 1, and only allow","chapter-4","Lists, Stacks, and Queues"
"n elements to be stored. Which of these solutions to adopt is purely a matter of the","chapter-4","Lists, Stacks, and Queues"
"implementor’s taste in such affairs. My choice is to use an array of size n + 1.","chapter-4","Lists, Stacks, and Queues"
"Figure 4.27 shows an array-based queue implementation. listArray holds","chapter-4","Lists, Stacks, and Queues"
"the queue elements, and as usual, the queue constructor allows an optional param-","chapter-4","Lists, Stacks, and Queues"
"eter to set the maximum size of the queue. The array as created is actually large","chapter-4","Lists, Stacks, and Queues"
"enough to hold one element more than the queue will allow, so that empty queues","chapter-4","Lists, Stacks, and Queues"
"can be distinguished from full queues. Member maxSize is used to control the","chapter-4","Lists, Stacks, and Queues"
"circular motion of the queue (it is the base for the modulus operator). Member","chapter-4","Lists, Stacks, and Queues"
"rear is set to the position of the current rear element, while front is the position","chapter-4","Lists, Stacks, and Queues"
"of the current front element.","chapter-4","Lists, Stacks, and Queues"
"In this implementation, the front of the queue is defined to be toward the","chapter-4","Lists, Stacks, and Queues"
"lower numbered positions in the array (in the counter-clockwise direction in Fig-","chapter-4","Lists, Stacks, and Queues"
"ure 4.26), and the rear is defined to be toward the higher-numbered positions. Thus,","chapter-4","Lists, Stacks, and Queues"
"enqueue increments the rear pointer (modulus size), and dequeue increments","chapter-4","Lists, Stacks, and Queues"
"the front pointer. Implementation of all member functions is straightforward.","chapter-4","Lists, Stacks, and Queues"
"4.3.2 Linked Queues","chapter-4","Lists, Stacks, and Queues"
"The linked queue implementation is a straightforward adaptation of the linked list.","chapter-4","Lists, Stacks, and Queues"
"Figure 4.28 shows the linked queue class declaration. Methods front and rear","chapter-4","Lists, Stacks, and Queues"
"are pointers to the front and rear queue elements, respectively. We will use a header","chapter-4","Lists, Stacks, and Queues"
"link node, which allows for a simpler implementation of the enqueue operation by","chapter-4","Lists, Stacks, and Queues"
"avoiding any special cases when the queue is empty. On initialization, the front","chapter-4","Lists, Stacks, and Queues"
"and rear pointers will point to the header node, and front will always point to","chapter-4","Lists, Stacks, and Queues"
"the header node while rear points to the true last link node in the queue. Method","chapter-4","Lists, Stacks, and Queues"
"enqueue places the new element in a link node at the end of the linked list (i.e.,","chapter-4","Lists, Stacks, and Queues"
"the node that rear points to) and then advances rear to point to the new link","chapter-4","Lists, Stacks, and Queues"
"node. Method dequeue removes and returns the first element of the list.","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.3 Queues 129","chapter-4","Lists, Stacks, and Queues"
"/** Array-based queue implementation */","chapter-4","Lists, Stacks, and Queues"
"class AQueue<E> implements Queue<E> {","chapter-4","Lists, Stacks, and Queues"
"private static final int defaultSize = 10;","chapter-4","Lists, Stacks, and Queues"
"private int maxSize; // Maximum size of queue","chapter-4","Lists, Stacks, and Queues"
"private int front; // Index of front element","chapter-4","Lists, Stacks, and Queues"
"private int rear; // Index of rear element","chapter-4","Lists, Stacks, and Queues"
"private E[] listArray; // Array holding queue elements","chapter-4","Lists, Stacks, and Queues"
"/** Constructors */","chapter-4","Lists, Stacks, and Queues"
"AQueue() { this(defaultSize); }","chapter-4","Lists, Stacks, and Queues"
"@SuppressWarnings("unchecked") // For generic array","chapter-4","Lists, Stacks, and Queues"
"AQueue(int size) {","chapter-4","Lists, Stacks, and Queues"
"maxSize = size+1; // One extra space is allocated","chapter-4","Lists, Stacks, and Queues"
"rear = 0; front = 1;","chapter-4","Lists, Stacks, and Queues"
"listArray = (E[])new Object[maxSize]; // Create listArray","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Reinitialize */","chapter-4","Lists, Stacks, and Queues"
"public void clear()","chapter-4","Lists, Stacks, and Queues"
"{ rear = 0; front = 1; }","chapter-4","Lists, Stacks, and Queues"
"/** Put "it" in queue */","chapter-4","Lists, Stacks, and Queues"
"public void enqueue(E it) {","chapter-4","Lists, Stacks, and Queues"
"assert ((rear+2) % maxSize) != front : "Queue is full";","chapter-4","Lists, Stacks, and Queues"
"rear = (rear+1) % maxSize; // Circular increment","chapter-4","Lists, Stacks, and Queues"
"listArray[rear] = it;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Remove and return front value */","chapter-4","Lists, Stacks, and Queues"
"public E dequeue() {","chapter-4","Lists, Stacks, and Queues"
"assert length() != 0 : "Queue is empty";","chapter-4","Lists, Stacks, and Queues"
"E it = listArray[front];","chapter-4","Lists, Stacks, and Queues"
"front = (front+1) % maxSize; // Circular increment","chapter-4","Lists, Stacks, and Queues"
"return it;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** @return Front value */","chapter-4","Lists, Stacks, and Queues"
"public E frontValue() {","chapter-4","Lists, Stacks, and Queues"
"assert length() != 0 : "Queue is empty";","chapter-4","Lists, Stacks, and Queues"
"return listArray[front];","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** @return Queue size */","chapter-4","Lists, Stacks, and Queues"
"public int length()","chapter-4","Lists, Stacks, and Queues"
"{ return ((rear+maxSize) - front + 1) % maxSize; }","chapter-4","Lists, Stacks, and Queues"
"Figure 4.27 An array-based queue implementation.","chapter-4","Lists, Stacks, and Queues"
"130 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"/** Linked queue implementation */","chapter-4","Lists, Stacks, and Queues"
"class LQueue<E> implements Queue<E> {","chapter-4","Lists, Stacks, and Queues"
"private Link<E> front; // Pointer to front queue node","chapter-4","Lists, Stacks, and Queues"
"private Link<E> rear; // Pointer to rear queuenode","chapter-4","Lists, Stacks, and Queues"
"private int size; // Number of elements in queue","chapter-4","Lists, Stacks, and Queues"
"/** Constructors */","chapter-4","Lists, Stacks, and Queues"
"public LQueue() { init(); }","chapter-4","Lists, Stacks, and Queues"
"public LQueue(int size) { init(); } // Ignore size","chapter-4","Lists, Stacks, and Queues"
"/** Initialize queue */","chapter-4","Lists, Stacks, and Queues"
"private void init() {","chapter-4","Lists, Stacks, and Queues"
"front = rear = new Link<E>(null);","chapter-4","Lists, Stacks, and Queues"
"size = 0;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Reinitialize queue */","chapter-4","Lists, Stacks, and Queues"
"public void clear() { init(); }","chapter-4","Lists, Stacks, and Queues"
"/** Put element on rear */","chapter-4","Lists, Stacks, and Queues"
"public void enqueue(E it) {","chapter-4","Lists, Stacks, and Queues"
"rear.setNext(new Link<E>(it, null));","chapter-4","Lists, Stacks, and Queues"
"rear = rear.next();","chapter-4","Lists, Stacks, and Queues"
"size++;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Remove and return element from front */","chapter-4","Lists, Stacks, and Queues"
"public E dequeue() {","chapter-4","Lists, Stacks, and Queues"
"assert size != 0 : "Queue is empty";","chapter-4","Lists, Stacks, and Queues"
"E it = front.next().element(); // Store dequeued value","chapter-4","Lists, Stacks, and Queues"
"front.setNext(front.next().next()); // Advance front","chapter-4","Lists, Stacks, and Queues"
"if (front.next() == null) rear = front; // Last Object","chapter-4","Lists, Stacks, and Queues"
"size--;","chapter-4","Lists, Stacks, and Queues"
"return it; // Return Object","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** @return Front element */","chapter-4","Lists, Stacks, and Queues"
"public E frontValue() {","chapter-4","Lists, Stacks, and Queues"
"assert size != 0 : "Queue is empty";","chapter-4","Lists, Stacks, and Queues"
"return front.next().element();","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** @return Queue size */","chapter-4","Lists, Stacks, and Queues"
"public int length() { return size; }","chapter-4","Lists, Stacks, and Queues"
"Figure 4.28 Linked queue class implementation.","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.4 Dictionaries 131","chapter-4","Lists, Stacks, and Queues"
"4.3.3 Comparison of Array-Based and Linked Queues","chapter-4","Lists, Stacks, and Queues"
"All member functions for both the array-based and linked queue implementations","chapter-4","Lists, Stacks, and Queues"
"require constant time. The space comparison issues are the same as for the equiva-","chapter-4","Lists, Stacks, and Queues"
"lent stack implementations. Unlike the array-based stack implementation, there is","chapter-4","Lists, Stacks, and Queues"
"no convenient way to store two queues in the same array, unless items are always","chapter-4","Lists, Stacks, and Queues"
"transferred directly from one queue to the other.","chapter-4","Lists, Stacks, and Queues"
"4.4 Dictionaries","chapter-4","Lists, Stacks, and Queues"
"The most common objective of computer programs is to store and retrieve data.","chapter-4","Lists, Stacks, and Queues"
"Much of this book is about efficient ways to organize collections of data records","chapter-4","Lists, Stacks, and Queues"
"so that they can be stored and retrieved quickly. In this section we describe a","chapter-4","Lists, Stacks, and Queues"
"simple interface for such a collection, called a dictionary. The dictionary ADT","chapter-4","Lists, Stacks, and Queues"
"provides operations for storing records, finding records, and removing records from","chapter-4","Lists, Stacks, and Queues"
"the collection. This ADT gives us a standard basis for comparing various data","chapter-4","Lists, Stacks, and Queues"
"structures.","chapter-4","Lists, Stacks, and Queues"
"Before we can discuss the interface for a dictionary, we must first define the","chapter-4","Lists, Stacks, and Queues"
"concepts of a key and comparable objects. If we want to search for a given record","chapter-4","Lists, Stacks, and Queues"
"in a database, how should we describe what we are looking for? A database record","chapter-4","Lists, Stacks, and Queues"
"could simply be a number, or it could be quite complicated, such as a payroll record","chapter-4","Lists, Stacks, and Queues"
"with many fields of varying types. We do not want to describe what we are looking","chapter-4","Lists, Stacks, and Queues"
"for by detailing and matching the entire contents of the record. If we knew every-","chapter-4","Lists, Stacks, and Queues"
"thing about the record already, we probably would not need to look for it. Instead,","chapter-4","Lists, Stacks, and Queues"
"we typically define what record we want in terms of a key value. For example, if","chapter-4","Lists, Stacks, and Queues"
"searching for payroll records, we might wish to search for the record that matches","chapter-4","Lists, Stacks, and Queues"
"a particular ID number. In this example the ID number is the search key.","chapter-4","Lists, Stacks, and Queues"
"To implement the search function, we require that keys be comparable. At a","chapter-4","Lists, Stacks, and Queues"
"minimum, we must be able to take two keys and reliably determine whether they","chapter-4","Lists, Stacks, and Queues"
"are equal or not. That is enough to enable a sequential search through a database","chapter-4","Lists, Stacks, and Queues"
"of records and find one that matches a given key. However, we typically would","chapter-4","Lists, Stacks, and Queues"
"like for the keys to define a total order (see Section 2.1), which means that we","chapter-4","Lists, Stacks, and Queues"
"can tell which of two keys is greater than the other. Using key types with total","chapter-4","Lists, Stacks, and Queues"
"orderings gives the database implementor the opportunity to organize a collection","chapter-4","Lists, Stacks, and Queues"
"of records in a way that makes searching more efficient. An example is storing the","chapter-4","Lists, Stacks, and Queues"
"records in sorted order in an array, which permits a binary search. Fortunately, in","chapter-4","Lists, Stacks, and Queues"
"practice most fields of most records consist of simple data types with natural total","chapter-4","Lists, Stacks, and Queues"
"orders. For example, integers, floats, doubles, and character strings all are totally","chapter-4","Lists, Stacks, and Queues"
"ordered. Ordering fields that are naturally multi-dimensional, such as a point in two","chapter-4","Lists, Stacks, and Queues"
"or three dimensions, present special opportunities if we wish to take advantage of","chapter-4","Lists, Stacks, and Queues"
"their multidimensional nature. This problem is addressed in Section 13.3.","chapter-4","Lists, Stacks, and Queues"
"132 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"/** The Dictionary abstract class. */","chapter-4","Lists, Stacks, and Queues"
"public interface Dictionary<Key, E> {","chapter-4","Lists, Stacks, and Queues"
"/** Reinitialize dictionary */","chapter-4","Lists, Stacks, and Queues"
"public void clear();","chapter-4","Lists, Stacks, and Queues"
"/** Insert a record","chapter-4","Lists, Stacks, and Queues"
"@param k The key for the record being inserted.","chapter-4","Lists, Stacks, and Queues"
"@param e The record being inserted. */","chapter-4","Lists, Stacks, and Queues"
"public void insert(Key k, E e);","chapter-4","Lists, Stacks, and Queues"
"/** Remove and return a record.","chapter-4","Lists, Stacks, and Queues"
"@param k The key of the record to be removed.","chapter-4","Lists, Stacks, and Queues"
"@return A maching record. If multiple records match","chapter-4","Lists, Stacks, and Queues"
"k, remove an arbitrary one. Return null if no record","chapter-4","Lists, Stacks, and Queues"
"with key "k" exists. */","chapter-4","Lists, Stacks, and Queues"
"public E remove(Key k);","chapter-4","Lists, Stacks, and Queues"
"/** Remove and return an arbitrary record from dictionary.","chapter-4","Lists, Stacks, and Queues"
"@return the record removed, or null if none exists. */","chapter-4","Lists, Stacks, and Queues"
"public E removeAny();","chapter-4","Lists, Stacks, and Queues"
"/** @return A record matching "k" (null if none exists).","chapter-4","Lists, Stacks, and Queues"
"If multiple records match, return an arbitrary one.","chapter-4","Lists, Stacks, and Queues"
"@param k The key of the record to find */","chapter-4","Lists, Stacks, and Queues"
"public E find(Key k);","chapter-4","Lists, Stacks, and Queues"
"/** @return The number of records in the dictionary. */","chapter-4","Lists, Stacks, and Queues"
"public int size();","chapter-4","Lists, Stacks, and Queues"
"};","chapter-4","Lists, Stacks, and Queues"
"Figure 4.29 The ADT for a simple dictionary.","chapter-4","Lists, Stacks, and Queues"
"Figure 4.29 shows the definition for a simple abstract dictionary class. The","chapter-4","Lists, Stacks, and Queues"
"methods insert and find are the heart of the class. Method insert takes a","chapter-4","Lists, Stacks, and Queues"
"record and inserts it into the dictionary. Method find takes a key value and returns","chapter-4","Lists, Stacks, and Queues"
"some record from the dictionary whose key matches the one provided. If there are","chapter-4","Lists, Stacks, and Queues"
"multiple records in the dictionary with that key value, there is no requirement as to","chapter-4","Lists, Stacks, and Queues"
"which one is returned.","chapter-4","Lists, Stacks, and Queues"
"Method clear simply re-initializes the dictionary. The remove method is","chapter-4","Lists, Stacks, and Queues"
"similar to find, except that it also deletes the record returned from the dictionary.","chapter-4","Lists, Stacks, and Queues"
"Once again, if there are multiple records in the dictionary that match the desired","chapter-4","Lists, Stacks, and Queues"
"key, there is no requirement as to which one actually is removed and returned.","chapter-4","Lists, Stacks, and Queues"
"Method size returns the number of elements in the dictionary.","chapter-4","Lists, Stacks, and Queues"
"The remaining Method is removeAny. This is similar to remove, except","chapter-4","Lists, Stacks, and Queues"
"that it does not take a key value. Instead, it removes an arbitrary record from the","chapter-4","Lists, Stacks, and Queues"
"dictionary, if one exists. The purpose of this method is to allow a user the ability","chapter-4","Lists, Stacks, and Queues"
"to iterate over all elements in the dictionary (of course, the dictionary will become","chapter-4","Lists, Stacks, and Queues"
"empty in the process). Without the removeAny method, a dictionary user could","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.4 Dictionaries 133","chapter-4","Lists, Stacks, and Queues"
"not get at a record of the dictionary that he didn’t already know the key value for.","chapter-4","Lists, Stacks, and Queues"
"With the removeAny method, the user can process all records in the dictionary as","chapter-4","Lists, Stacks, and Queues"
"shown in the following code fragment.","chapter-4","Lists, Stacks, and Queues"
"while (dict.size() > 0) {","chapter-4","Lists, Stacks, and Queues"
"it = dict.removeAny();","chapter-4","Lists, Stacks, and Queues"
"doSomething(it);","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"There are other approaches that might seem more natural for iterating though a","chapter-4","Lists, Stacks, and Queues"
"dictionary, such as using a “first” and a “next” function. But not all data structures","chapter-4","Lists, Stacks, and Queues"
"that we want to use to implement a dictionary are able to do “first” efficiently. For","chapter-4","Lists, Stacks, and Queues"
"example, a hash table implementation cannot efficiently locate the record in the","chapter-4","Lists, Stacks, and Queues"
"table with the smallest key value. By using RemoveAny, we have a mechanism","chapter-4","Lists, Stacks, and Queues"
"that provides generic access.","chapter-4","Lists, Stacks, and Queues"
"Given a database storing records of a particular type, we might want to search","chapter-4","Lists, Stacks, and Queues"
"for records in multiple ways. For example, we might want to store payroll records","chapter-4","Lists, Stacks, and Queues"
"in one dictionary that allows us to search by ID, and also store those same records","chapter-4","Lists, Stacks, and Queues"
"in a second dictionary that allows us to search by name.","chapter-4","Lists, Stacks, and Queues"
"Figure 4.30 shows an implementation for a payroll record. Class Payroll has","chapter-4","Lists, Stacks, and Queues"
"multiple fields, each of which might be used as a search key. Simply by varying","chapter-4","Lists, Stacks, and Queues"
"the type for the key, and using the appropriate field in each record as the key value,","chapter-4","Lists, Stacks, and Queues"
"we can define a dictionary whose search key is the ID field, another whose search","chapter-4","Lists, Stacks, and Queues"
"key is the name field, and a third whose search key is the address field. Figure 4.31","chapter-4","Lists, Stacks, and Queues"
"shows an example where Payroll objects are stored in two separate dictionaries,","chapter-4","Lists, Stacks, and Queues"
"one using the ID field as the key and the other using the name field as the key.","chapter-4","Lists, Stacks, and Queues"
"The fundamental operation for a dictionary is finding a record that matches a","chapter-4","Lists, Stacks, and Queues"
"given key. This raises the issue of how to extract the key from a record. We would","chapter-4","Lists, Stacks, and Queues"
"like any given dictionary implementation to support arbitrary record types, so we","chapter-4","Lists, Stacks, and Queues"
"need some mechanism for extracting keys that is sufficiently general. One approach","chapter-4","Lists, Stacks, and Queues"
"is to require all record types to support some particular method that returns the key","chapter-4","Lists, Stacks, and Queues"
"value. For example, in Java the Comparable interface can be used to provide this","chapter-4","Lists, Stacks, and Queues"
"effect. Unfortunately, this approach does not work when the same record type is","chapter-4","Lists, Stacks, and Queues"
"meant to be stored in multiple dictionaries, each keyed by a different field of the","chapter-4","Lists, Stacks, and Queues"
"record. This is typical in database applications. Another, more general approach","chapter-4","Lists, Stacks, and Queues"
"is to supply a class whose job is to extract the key from the record. Unfortunately,","chapter-4","Lists, Stacks, and Queues"
"this solution also does not work in all situations, because there are record types for","chapter-4","Lists, Stacks, and Queues"
"which it is not possible to write a key extraction method.2","chapter-4","Lists, Stacks, and Queues"
"2One example of such a situation occurs when we have a collection of records that describe books","chapter-4","Lists, Stacks, and Queues"
"in a library. One of the fields for such a record might be a list of subject keywords, where the typical","chapter-4","Lists, Stacks, and Queues"
"record stores a few keywords. Our dictionary might be implemented as a list of records sorted by","chapter-4","Lists, Stacks, and Queues"
"keyword. If a book contains three keywords, it would appear three times on the list, once for each","chapter-4","Lists, Stacks, and Queues"
"associated keyword. However, given the record, there is no simple way to determine which keyword","chapter-4","Lists, Stacks, and Queues"
"134 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"/** A simple payroll entry with ID, name, address fields */","chapter-4","Lists, Stacks, and Queues"
"class Payroll {","chapter-4","Lists, Stacks, and Queues"
"private Integer ID;","chapter-4","Lists, Stacks, and Queues"
"private String name;","chapter-4","Lists, Stacks, and Queues"
"private String address;","chapter-4","Lists, Stacks, and Queues"
"/** Constructor */","chapter-4","Lists, Stacks, and Queues"
"Payroll(int inID, String inname, String inaddr) {","chapter-4","Lists, Stacks, and Queues"
"ID = inID;","chapter-4","Lists, Stacks, and Queues"
"name = inname;","chapter-4","Lists, Stacks, and Queues"
"address = inaddr;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Data member access functions */","chapter-4","Lists, Stacks, and Queues"
"public Integer getID() { return ID; }","chapter-4","Lists, Stacks, and Queues"
"public String getname() { return name; }","chapter-4","Lists, Stacks, and Queues"
"public String getaddr() { return address; }","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"Figure 4.30 A payroll record implementation.","chapter-4","Lists, Stacks, and Queues"
"// IDdict organizes Payroll records by ID","chapter-4","Lists, Stacks, and Queues"
"Dictionary<Integer, Payroll> IDdict =","chapter-4","Lists, Stacks, and Queues"
"new UALdictionary<Integer, Payroll>();","chapter-4","Lists, Stacks, and Queues"
"// namedict organizes Payroll records by name","chapter-4","Lists, Stacks, and Queues"
"Dictionary<String, Payroll> namedict =","chapter-4","Lists, Stacks, and Queues"
"new UALdictionary<String, Payroll>();","chapter-4","Lists, Stacks, and Queues"
"Payroll foo1 = new Payroll(5, "Joe" "Anytown");","chapter-4","Lists, Stacks, and Queues"
"Payroll foo2 = new Payroll(10, "John" "Mytown");","chapter-4","Lists, Stacks, and Queues"
"IDdict.insert(foo1.getID(), foo1);","chapter-4","Lists, Stacks, and Queues"
"IDdict.insert(foo2.getID(), foo2);","chapter-4","Lists, Stacks, and Queues"
"namedict.insert(foo1.getname(), foo1);","chapter-4","Lists, Stacks, and Queues"
"namedict.insert(foo2.getname(), foo2);","chapter-4","Lists, Stacks, and Queues"
"Payroll findfoo1 = IDdict.find(5);","chapter-4","Lists, Stacks, and Queues"
"Payroll findfoo2 = namedict.find("John");","chapter-4","Lists, Stacks, and Queues"
"Figure 4.31 A dictionary search example. Here, payroll records are stored in","chapter-4","Lists, Stacks, and Queues"
"two dictionaries, one organized by ID and the other organized by name. Both","chapter-4","Lists, Stacks, and Queues"
"dictionaries are implemented with an unsorted array-based list.","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.4 Dictionaries 135","chapter-4","Lists, Stacks, and Queues"
"/** Container class for a key-value pair */","chapter-4","Lists, Stacks, and Queues"
"class KVpair<Key, E> {","chapter-4","Lists, Stacks, and Queues"
"private Key k;","chapter-4","Lists, Stacks, and Queues"
"private E e;","chapter-4","Lists, Stacks, and Queues"
"/** Constructors */","chapter-4","Lists, Stacks, and Queues"
"KVpair()","chapter-4","Lists, Stacks, and Queues"
"{ k = null; e = null; }","chapter-4","Lists, Stacks, and Queues"
"KVpair(Key kval, E eval)","chapter-4","Lists, Stacks, and Queues"
"{ k = kval; e = eval; }","chapter-4","Lists, Stacks, and Queues"
"/** Data member access functions */","chapter-4","Lists, Stacks, and Queues"
"public Key key() { return k; }","chapter-4","Lists, Stacks, and Queues"
"public E value() { return e; }","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"Figure 4.32 Implementation for a class representing a key-value pair.","chapter-4","Lists, Stacks, and Queues"
"The fundamental issue is that the key value for a record is not an intrinsic prop-","chapter-4","Lists, Stacks, and Queues"
"erty of the record’s class, or of any field within the class. The key for a record is","chapter-4","Lists, Stacks, and Queues"
"actually a property of the context in which the record is used.","chapter-4","Lists, Stacks, and Queues"
"A truly general alternative is to explicitly store the key associated with a given","chapter-4","Lists, Stacks, and Queues"
"record, as a separate field in the dictionary. That is, each entry in the dictionary","chapter-4","Lists, Stacks, and Queues"
"will contain both a record and its associated key. Such entries are known as key-","chapter-4","Lists, Stacks, and Queues"
"value pairs. It is typical that storing the key explicitly duplicates some field in the","chapter-4","Lists, Stacks, and Queues"
"record. However, keys tend to be much smaller than records, so this additional","chapter-4","Lists, Stacks, and Queues"
"space overhead will not be great. A simple class for representing key-value pairs","chapter-4","Lists, Stacks, and Queues"
"is shown in Figure 4.32. The insert method of the dictionary class supports the","chapter-4","Lists, Stacks, and Queues"
"key-value pair implementation because it takes two parameters, a record and its","chapter-4","Lists, Stacks, and Queues"
"associated key for that dictionary.","chapter-4","Lists, Stacks, and Queues"
"Now that we have defined the dictionary ADT and settled on the design ap-","chapter-4","Lists, Stacks, and Queues"
"proach of storing key-value pairs for our dictionary entries, we are ready to consider","chapter-4","Lists, Stacks, and Queues"
"ways to implement it. Two possibilities would be to use an array-based or linked","chapter-4","Lists, Stacks, and Queues"
"list. Figure 4.33 shows an implementation for the dictionary using an (unsorted)","chapter-4","Lists, Stacks, and Queues"
"array-based list.","chapter-4","Lists, Stacks, and Queues"
"Examining class UALdict (UAL stands for “unsorted array-based list), we can","chapter-4","Lists, Stacks, and Queues"
"easily see that insert is a constant-time operation, because it simply inserts the","chapter-4","Lists, Stacks, and Queues"
"new record at the end of the list. However, find, and remove both require Θ(n)","chapter-4","Lists, Stacks, and Queues"
"time in the average and worst cases, because we need to do a sequential search.","chapter-4","Lists, Stacks, and Queues"
"Method remove in particular must touch every record in the list, because once the","chapter-4","Lists, Stacks, and Queues"
"desired record is found, the remaining records must be shifted down in the list to","chapter-4","Lists, Stacks, and Queues"
"fill the gap. Method removeAny removes the last record from the list, so this is a","chapter-4","Lists, Stacks, and Queues"
"constant-time operation.","chapter-4","Lists, Stacks, and Queues"
"on the keyword list triggered this appearance of the record. Thus, we cannot write a function that","chapter-4","Lists, Stacks, and Queues"
"extracts the key from such a record.","chapter-4","Lists, Stacks, and Queues"
"136 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"/** Dictionary implemented by unsorted array-based list. */","chapter-4","Lists, Stacks, and Queues"
"class UALdictionary<Key, E> implements Dictionary<Key, E> {","chapter-4","Lists, Stacks, and Queues"
"private static final int defaultSize = 10; // Default size","chapter-4","Lists, Stacks, and Queues"
"private AList<KVpair<Key,E>> list; // To store dictionary","chapter-4","Lists, Stacks, and Queues"
"/** Constructors */","chapter-4","Lists, Stacks, and Queues"
"UALdictionary() { this(defaultSize); }","chapter-4","Lists, Stacks, and Queues"
"UALdictionary(int sz)","chapter-4","Lists, Stacks, and Queues"
"{ list = new AList<KVpair<Key, E>>(sz); }","chapter-4","Lists, Stacks, and Queues"
"/** Reinitialize */","chapter-4","Lists, Stacks, and Queues"
"public void clear() { list.clear(); }","chapter-4","Lists, Stacks, and Queues"
"/** Insert an element: append to list */","chapter-4","Lists, Stacks, and Queues"
"public void insert(Key k, E e) {","chapter-4","Lists, Stacks, and Queues"
"KVpair<Key,E> temp = new KVpair<Key,E>(k, e);","chapter-4","Lists, Stacks, and Queues"
"list.append(temp);","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Use sequential search to find the element to remove */","chapter-4","Lists, Stacks, and Queues"
"public E remove(Key k) {","chapter-4","Lists, Stacks, and Queues"
"E temp = find(k);","chapter-4","Lists, Stacks, and Queues"
"if (temp != null) list.remove();","chapter-4","Lists, Stacks, and Queues"
"return temp;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Remove the last element */","chapter-4","Lists, Stacks, and Queues"
"public E removeAny() {","chapter-4","Lists, Stacks, and Queues"
"if (size() != 0) {","chapter-4","Lists, Stacks, and Queues"
"list.moveToEnd();","chapter-4","Lists, Stacks, and Queues"
"list.prev();","chapter-4","Lists, Stacks, and Queues"
"KVpair<Key,E> e = list.remove();","chapter-4","Lists, Stacks, and Queues"
"return e.value();","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"else return null;","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"/** Find k using sequential search","chapter-4","Lists, Stacks, and Queues"
"@return Record with key value k */","chapter-4","Lists, Stacks, and Queues"
"public E find(Key k) {","chapter-4","Lists, Stacks, and Queues"
"for(list.moveToStart(); list.currPos() < list.length();","chapter-4","Lists, Stacks, and Queues"
"list.next()) {","chapter-4","Lists, Stacks, and Queues"
"KVpair<Key,E> temp = list.getValue();","chapter-4","Lists, Stacks, and Queues"
"if (k == temp.key())","chapter-4","Lists, Stacks, and Queues"
"return temp.value();","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"return null; // "k" does not appear in dictionary","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"Figure 4.33 A dictionary implemented with an unsorted array-based list.","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.4 Dictionaries 137","chapter-4","Lists, Stacks, and Queues"
"/** @return List size */","chapter-4","Lists, Stacks, and Queues"
"public int size()","chapter-4","Lists, Stacks, and Queues"
"{ return list.length(); }","chapter-4","Lists, Stacks, and Queues"
"}","chapter-4","Lists, Stacks, and Queues"
"Figure 4.33 (continued)","chapter-4","Lists, Stacks, and Queues"
"As an alternative, we could implement the dictionary using a linked list. The","chapter-4","Lists, Stacks, and Queues"
"implementation would be quite similar to that shown in Figure 4.33, and the cost","chapter-4","Lists, Stacks, and Queues"
"of the functions should be the same asymptotically.","chapter-4","Lists, Stacks, and Queues"
"Another alternative would be to implement the dictionary with a sorted list. The","chapter-4","Lists, Stacks, and Queues"
"advantage of this approach would be that we might be able to speed up the find","chapter-4","Lists, Stacks, and Queues"
"operation by using a binary search. To do so, first we must define a variation on","chapter-4","Lists, Stacks, and Queues"
"the List ADT to support sorted lists. A sorted list is somewhat different from","chapter-4","Lists, Stacks, and Queues"
"an unsorted list in that it cannot permit the user to control where elements get","chapter-4","Lists, Stacks, and Queues"
"inserted. Thus, the insert method must be quite different in a sorted list than in","chapter-4","Lists, Stacks, and Queues"
"an unsorted list. Likewise, the user cannot be permitted to append elements onto","chapter-4","Lists, Stacks, and Queues"
"the list. For these reasons, a sorted list cannot be implemented with straightforward","chapter-4","Lists, Stacks, and Queues"
"inheritance from the List ADT.","chapter-4","Lists, Stacks, and Queues"
"The cost for find in a sorted list is Θ(log n) for a list of length n. This is a","chapter-4","Lists, Stacks, and Queues"
"great improvement over the cost of find in an unsorted list. Unfortunately, the","chapter-4","Lists, Stacks, and Queues"
"cost of insert changes from constant time in the unsorted list to Θ(n) time in","chapter-4","Lists, Stacks, and Queues"
"the sorted list. Whether the sorted list implementation for the dictionary ADT is","chapter-4","Lists, Stacks, and Queues"
"more or less efficient than the unsorted list implementation depends on the relative","chapter-4","Lists, Stacks, and Queues"
"number of insert and find operations to be performed. If many more find","chapter-4","Lists, Stacks, and Queues"
"operations than insert operations are used, then it might be worth using a sorted","chapter-4","Lists, Stacks, and Queues"
"list to implement the dictionary. In both cases, remove requires Θ(n) time in the","chapter-4","Lists, Stacks, and Queues"
"worst and average cases. Even if we used binary search to cut down on the time to","chapter-4","Lists, Stacks, and Queues"
"find the record prior to removal, we would still need to shift down the remaining","chapter-4","Lists, Stacks, and Queues"
"records in the list to fill the gap left by the remove operation.","chapter-4","Lists, Stacks, and Queues"
"Given two keys, we have not properly addressed the issue of how to compare","chapter-4","Lists, Stacks, and Queues"
"them. One possibility would be to simply use the basic ==, <=, and >= operators","chapter-4","Lists, Stacks, and Queues"
"built into Java. This is the approach taken by our implementations for dictionar-","chapter-4","Lists, Stacks, and Queues"
"ies shown in Figure 4.33. If the key type is int, for example, this will work","chapter-4","Lists, Stacks, and Queues"
"fine. However, if the key is a pointer to a string or any other type of object, then","chapter-4","Lists, Stacks, and Queues"
"this will not give the desired result. When we compare two strings we probably","chapter-4","Lists, Stacks, and Queues"
"want to know which comes first in alphabetical order, but what we will get from","chapter-4","Lists, Stacks, and Queues"
"the standard comparison operators is simply which object appears first in memory.","chapter-4","Lists, Stacks, and Queues"
"Unfortunately, the code will compile fine, but the answers probably will not be fine.","chapter-4","Lists, Stacks, and Queues"
"In a language like C++ that supports operator overloading, we could require","chapter-4","Lists, Stacks, and Queues"
"that the user of the dictionary overload the ==, <=, and >= operators for the given","chapter-4","Lists, Stacks, and Queues"
"key type. This requirement then becomes an obligation on the user of the dictionary","chapter-4","Lists, Stacks, and Queues"
"138 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"class. Unfortunately, this obligation is hidden within the code of the dictionary (and","chapter-4","Lists, Stacks, and Queues"
"possibly in the user’s manual) rather than exposed in the dictionary’s interface. As","chapter-4","Lists, Stacks, and Queues"
"a result, some users of the dictionary might neglect to implement the overloading,","chapter-4","Lists, Stacks, and Queues"
"with unexpected results. Again, the compiler will not catch this problem.","chapter-4","Lists, Stacks, and Queues"
"The Java Comparable interface provides an approach to solving this prob-","chapter-4","Lists, Stacks, and Queues"
"lem. In a key-value pair implementation, the keys can be required to implement","chapter-4","Lists, Stacks, and Queues"
"the Comparable interface. In other applications, the records might be required","chapter-4","Lists, Stacks, and Queues"
"to implement Comparable","chapter-4","Lists, Stacks, and Queues"
"The most general solution is to have users supply their own definition for com-","chapter-4","Lists, Stacks, and Queues"
"paring keys. The concept of a class that does comparison (called a comparator)","chapter-4","Lists, Stacks, and Queues"
"is quite important. By making these operations be generic parameters, the require-","chapter-4","Lists, Stacks, and Queues"
"ment to supply the comparator class becomes part of the interface. This design","chapter-4","Lists, Stacks, and Queues"
"is an example of the Strategy design pattern, because the “strategies” for compar-","chapter-4","Lists, Stacks, and Queues"
"ing and getting keys from records are provided by the client. Alternatively, the","chapter-4","Lists, Stacks, and Queues"
"Comparable class allows the user to define the comparator by implementing the","chapter-4","Lists, Stacks, and Queues"
"compareTo method. In some cases, it makes sense for the comparator class to","chapter-4","Lists, Stacks, and Queues"
"extract the key from the record type, as an alternative to storing key-value pairs.","chapter-4","Lists, Stacks, and Queues"
"We will use the Comparable interface in Section 5.5 to implement compari-","chapter-4","Lists, Stacks, and Queues"
"son in heaps, and in Chapter 7 to implement comparison in sorting algorithms.","chapter-4","Lists, Stacks, and Queues"
"4.5 Further Reading","chapter-4","Lists, Stacks, and Queues"
"For more discussion on choice of functions used to define the List ADT, see the","chapter-4","Lists, Stacks, and Queues"
"work of the Reusable Software Research Group from Ohio State. Their definition","chapter-4","Lists, Stacks, and Queues"
"for the List ADT can be found in [SWH93]. More information about designing","chapter-4","Lists, Stacks, and Queues"
"such classes can be found in [SW94].","chapter-4","Lists, Stacks, and Queues"
"4.6 Exercises","chapter-4","Lists, Stacks, and Queues"
"4.1 Assume a list has the following configuration:","chapter-4","Lists, Stacks, and Queues"
"h | 2, 23, 15, 5, 9 i.","chapter-4","Lists, Stacks, and Queues"
"Write a series of Java statements using the List ADT of Figure 4.1 to delete","chapter-4","Lists, Stacks, and Queues"
"the element with value 15.","chapter-4","Lists, Stacks, and Queues"
"4.2 Show the list configuration resulting from each series of list operations using","chapter-4","Lists, Stacks, and Queues"
"the List ADT of Figure 4.1. Assume that lists L1 and L2 are empty at the","chapter-4","Lists, Stacks, and Queues"
"beginning of each series. Show where the current position is in the list.","chapter-4","Lists, Stacks, and Queues"
"(a) L1.append(10);","chapter-4","Lists, Stacks, and Queues"
"L1.append(20);","chapter-4","Lists, Stacks, and Queues"
"L1.append(15);","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.6 Exercises 139","chapter-4","Lists, Stacks, and Queues"
"(b) L2.append(10);","chapter-4","Lists, Stacks, and Queues"
"L2.append(20);","chapter-4","Lists, Stacks, and Queues"
"L2.append(15);","chapter-4","Lists, Stacks, and Queues"
"L2.moveToStart();","chapter-4","Lists, Stacks, and Queues"
"L2.insert(39);","chapter-4","Lists, Stacks, and Queues"
"L2.next();","chapter-4","Lists, Stacks, and Queues"
"L2.insert(12);","chapter-4","Lists, Stacks, and Queues"
"4.3 Write a series of Java statements that uses the List ADT of Figure 4.1 to","chapter-4","Lists, Stacks, and Queues"
"create a list capable of holding twenty elements and which actually stores the","chapter-4","Lists, Stacks, and Queues"
"list with the following configuration:","chapter-4","Lists, Stacks, and Queues"
"h 2, 23 | 15, 5, 9 i.","chapter-4","Lists, Stacks, and Queues"
"4.4 Using the list ADT of Figure 4.1, write a function to interchange the current","chapter-4","Lists, Stacks, and Queues"
"element and the one following it.","chapter-4","Lists, Stacks, and Queues"
"4.5 In the linked list implementation presented in Section 4.1.2, the current po-","chapter-4","Lists, Stacks, and Queues"
"sition is implemented using a pointer to the element ahead of the logical","chapter-4","Lists, Stacks, and Queues"
"current node. The more “natural” approach might seem to be to have curr","chapter-4","Lists, Stacks, and Queues"
"point directly to the node containing the current element. However, if this","chapter-4","Lists, Stacks, and Queues"
"was done, then the pointer of the node preceding the current one cannot be","chapter-4","Lists, Stacks, and Queues"
"updated properly because there is no access to this node from curr. An","chapter-4","Lists, Stacks, and Queues"
"alternative is to add a new node after the current element, copy the value of","chapter-4","Lists, Stacks, and Queues"
"the current element to this new node, and then insert the new value into the","chapter-4","Lists, Stacks, and Queues"
"old current node.","chapter-4","Lists, Stacks, and Queues"
"(a) What happens if curr is at the end of the list already? Is there still a","chapter-4","Lists, Stacks, and Queues"
"way to make this work? Is the resulting code simpler or more complex","chapter-4","Lists, Stacks, and Queues"
"than the implementation of Section 4.1.2?","chapter-4","Lists, Stacks, and Queues"
"(b) Will deletion always work in constant time if curr points directly to","chapter-4","Lists, Stacks, and Queues"
"the current node? In particular, can you make several deletions in a","chapter-4","Lists, Stacks, and Queues"
"row?","chapter-4","Lists, Stacks, and Queues"
"4.6 Add to the LList class implementation a member function to reverse the","chapter-4","Lists, Stacks, and Queues"
"order of the elements on the list. Your algorithm should run in Θ(n) time for","chapter-4","Lists, Stacks, and Queues"
"a list of n elements.","chapter-4","Lists, Stacks, and Queues"
"4.7 Write a function to merge two linked lists. The input lists have their elements","chapter-4","Lists, Stacks, and Queues"
"in sorted order, from lowest to highest. The output list should also be sorted","chapter-4","Lists, Stacks, and Queues"
"from lowest to highest. Your algorithm should run in linear time on the length","chapter-4","Lists, Stacks, and Queues"
"of the output list.","chapter-4","Lists, Stacks, and Queues"
"4.8 A circular linked list is one in which the next field for the last link node","chapter-4","Lists, Stacks, and Queues"
"of the list points to the first link node of the list. This can be useful when","chapter-4","Lists, Stacks, and Queues"
"you wish to have a relative positioning for elements, but no concept of an","chapter-4","Lists, Stacks, and Queues"
"absolute first or last position.","chapter-4","Lists, Stacks, and Queues"
"140 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"(a) Modify the code of Figure 4.8 to implement circular singly linked lists.","chapter-4","Lists, Stacks, and Queues"
"(b) Modify the code of Figure 4.15 to implement circular doubly linked","chapter-4","Lists, Stacks, and Queues"
"lists.","chapter-4","Lists, Stacks, and Queues"
"4.9 Section 4.1.3 states “the space required by the array-based list implementa-","chapter-4","Lists, Stacks, and Queues"
"tion is Ω(n), but can be greater.” Explain why this is so.","chapter-4","Lists, Stacks, and Queues"
"4.10 Section 4.1.3 presents an equation for determining the break-even point for","chapter-4","Lists, Stacks, and Queues"
"the space requirements of two implementations of lists. The variables are D,","chapter-4","Lists, Stacks, and Queues"
"E, P, and n. What are the dimensional units for each variable? Show that","chapter-4","Lists, Stacks, and Queues"
"both sides of the equation balance in terms of their dimensional units.","chapter-4","Lists, Stacks, and Queues"
"4.11 Use the space equation of Section 4.1.3 to determine the break-even point for","chapter-4","Lists, Stacks, and Queues"
"an array-based list and linked list implementation for lists when the sizes for","chapter-4","Lists, Stacks, and Queues"
"the data field, a pointer, and the array-based list’s array are as specified. State","chapter-4","Lists, Stacks, and Queues"
"when the linked list needs less space than the array.","chapter-4","Lists, Stacks, and Queues"
"(a) The data field is eight bytes, a pointer is four bytes, and the array holds","chapter-4","Lists, Stacks, and Queues"
"twenty elements.","chapter-4","Lists, Stacks, and Queues"
"(b) The data field is two bytes, a pointer is four bytes, and the array holds","chapter-4","Lists, Stacks, and Queues"
"thirty elements.","chapter-4","Lists, Stacks, and Queues"
"(c) The data field is one byte, a pointer is four bytes, and the array holds","chapter-4","Lists, Stacks, and Queues"
"thirty elements.","chapter-4","Lists, Stacks, and Queues"
"(d) The data field is 32 bytes, a pointer is four bytes, and the array holds","chapter-4","Lists, Stacks, and Queues"
"forty elements.","chapter-4","Lists, Stacks, and Queues"
"4.12 Determine the size of an int variable, a double variable, and a pointer on","chapter-4","Lists, Stacks, and Queues"
"your computer.","chapter-4","Lists, Stacks, and Queues"
"(a) Calculate the break-even point, as a function of n, beyond which the","chapter-4","Lists, Stacks, and Queues"
"array-based list is more space efficient than the linked list for lists","chapter-4","Lists, Stacks, and Queues"
"whose elements are of type int.","chapter-4","Lists, Stacks, and Queues"
"(b) Calculate the break-even point, as a function of n, beyond which the","chapter-4","Lists, Stacks, and Queues"
"array-based list is more space efficient than the linked list for lists","chapter-4","Lists, Stacks, and Queues"
"whose elements are of type double.","chapter-4","Lists, Stacks, and Queues"
"4.13 Modify the code of Figure 4.19 to implement two stacks sharing the same","chapter-4","Lists, Stacks, and Queues"
"array, as shown in Figure 4.21.","chapter-4","Lists, Stacks, and Queues"
"4.14 Modify the array-based queue definition of Figure 4.27 to use a separate","chapter-4","Lists, Stacks, and Queues"
"Boolean member to keep track of whether the queue is empty, rather than","chapter-4","Lists, Stacks, and Queues"
"require that one array position remain empty.","chapter-4","Lists, Stacks, and Queues"
"4.15 A palindrome is a string that reads the same forwards as backwards. Using","chapter-4","Lists, Stacks, and Queues"
"only a fixed number of stacks and queues, the stack and queue ADT func-","chapter-4","Lists, Stacks, and Queues"
"tions, and a fixed number of int and char variables, write an algorithm to","chapter-4","Lists, Stacks, and Queues"
"determine if a string is a palindrome. Assume that the string is read from","chapter-4","Lists, Stacks, and Queues"
"standard input one character at a time. The algorithm should output true or","chapter-4","Lists, Stacks, and Queues"
"false as appropriate.","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.7 Projects 141","chapter-4","Lists, Stacks, and Queues"
"4.16 Re-implement function fibr from Exercise 2.11, using a stack to replace","chapter-4","Lists, Stacks, and Queues"
"the recursive call as described in Section 4.2.4.","chapter-4","Lists, Stacks, and Queues"
"4.17 Write a recursive algorithm to compute the value of the recurrence relation","chapter-4","Lists, Stacks, and Queues"
"T(n) = T(dn/2e) + T(bn/2c) + n; T(1) = 1.","chapter-4","Lists, Stacks, and Queues"
"Then, rewrite your algorithm to simulate the recursive calls with a stack.","chapter-4","Lists, Stacks, and Queues"
"4.18 Let Q be a non-empty queue, and let S be an empty stack. Using only the","chapter-4","Lists, Stacks, and Queues"
"stack and queue ADT functions and a single element variable X, write an","chapter-4","Lists, Stacks, and Queues"
"algorithm to reverse the order of the elements in Q.","chapter-4","Lists, Stacks, and Queues"
"4.19 A common problem for compilers and text editors is to determine if the","chapter-4","Lists, Stacks, and Queues"
"parentheses (or other brackets) in a string are balanced and properly nested.","chapter-4","Lists, Stacks, and Queues"
"For example, the string “((())())()” contains properly nested pairs of paren-","chapter-4","Lists, Stacks, and Queues"
"theses, but the string “)()(” does not, and the string “())” does not contain","chapter-4","Lists, Stacks, and Queues"
"properly matching parentheses.","chapter-4","Lists, Stacks, and Queues"
"(a) Give an algorithm that returns true if a string contains properly nested","chapter-4","Lists, Stacks, and Queues"
"and balanced parentheses, and false otherwise. Use a stack to keep","chapter-4","Lists, Stacks, and Queues"
"track of the number of left parentheses seen so far. Hint: At no time","chapter-4","Lists, Stacks, and Queues"
"while scanning a legal string from left to right will you have encoun-","chapter-4","Lists, Stacks, and Queues"
"tered more right parentheses than left parentheses.","chapter-4","Lists, Stacks, and Queues"
"(b) Give an algorithm that returns the position in the string of the first of-","chapter-4","Lists, Stacks, and Queues"
"fending parenthesis if the string is not properly nested and balanced.","chapter-4","Lists, Stacks, and Queues"
"That is, if an excess right parenthesis is found, return its position; if","chapter-4","Lists, Stacks, and Queues"
"there are too many left parentheses, return the position of the first ex-","chapter-4","Lists, Stacks, and Queues"
"cess left parenthesis. Return −1 if the string is properly balanced and","chapter-4","Lists, Stacks, and Queues"
"nested. Use a stack to keep track of the number and positions of left","chapter-4","Lists, Stacks, and Queues"
"parentheses seen so far.","chapter-4","Lists, Stacks, and Queues"
"4.20 Imagine that you are designing an application where you need to perform","chapter-4","Lists, Stacks, and Queues"
"the operations Insert, Delete Maximum, and Delete Minimum. For","chapter-4","Lists, Stacks, and Queues"
"this application, the cost of inserting is not important, because it can be done","chapter-4","Lists, Stacks, and Queues"
"off-line prior to startup of the time-critical section, but the performance of","chapter-4","Lists, Stacks, and Queues"
"the two deletion operations are critical. Repeated deletions of either kind","chapter-4","Lists, Stacks, and Queues"
"must work as fast as possible. Suggest a data structure that can support this","chapter-4","Lists, Stacks, and Queues"
"application, and justify your suggestion. What is the time complexity for","chapter-4","Lists, Stacks, and Queues"
"each of the three key operations?","chapter-4","Lists, Stacks, and Queues"
"4.21 Write a function that reverses the order of an array of n items.","chapter-4","Lists, Stacks, and Queues"
"4.7 Projects","chapter-4","Lists, Stacks, and Queues"
"4.1 A deque (pronounced “deck”) is like a queue, except that items may be added","chapter-4","Lists, Stacks, and Queues"
"and removed from both the front and the rear. Write either an array-based or","chapter-4","Lists, Stacks, and Queues"
"linked implementation for the deque.","chapter-4","Lists, Stacks, and Queues"
"142 Chap. 4 Lists, Stacks, and Queues","chapter-4","Lists, Stacks, and Queues"
"4.2 One solution to the problem of running out of space for an array-based list","chapter-4","Lists, Stacks, and Queues"
"implementation is to replace the array with a larger array whenever the origi-","chapter-4","Lists, Stacks, and Queues"
"nal array overflows. A good rule that leads to an implementation that is both","chapter-4","Lists, Stacks, and Queues"
"space and time efficient is to double the current size of the array when there","chapter-4","Lists, Stacks, and Queues"
"is an overflow. Re-implement the array-based List class of Figure 4.2 to","chapter-4","Lists, Stacks, and Queues"
"support this array-doubling rule.","chapter-4","Lists, Stacks, and Queues"
"4.3 Use singly linked lists to implement integers of unlimited size. Each node of","chapter-4","Lists, Stacks, and Queues"
"the list should store one digit of the integer. You should implement addition,","chapter-4","Lists, Stacks, and Queues"
"subtraction, multiplication, and exponentiation operations. Limit exponents","chapter-4","Lists, Stacks, and Queues"
"to be positive integers. What is the asymptotic running time for each of your","chapter-4","Lists, Stacks, and Queues"
"operations, expressed in terms of the number of digits for the two operands","chapter-4","Lists, Stacks, and Queues"
"of each function?","chapter-4","Lists, Stacks, and Queues"
"4.4 Implement doubly linked lists by storing the sum of the next and prev","chapter-4","Lists, Stacks, and Queues"
"pointers in a single pointer variable as described in Example 4.1.","chapter-4","Lists, Stacks, and Queues"
"4.5 Implement a city database using unordered lists. Each database record con-","chapter-4","Lists, Stacks, and Queues"
"tains the name of the city (a string of arbitrary length) and the coordinates","chapter-4","Lists, Stacks, and Queues"
"of the city expressed as integer x and y coordinates. Your database should","chapter-4","Lists, Stacks, and Queues"
"allow records to be inserted, deleted by name or coordinate, and searched","chapter-4","Lists, Stacks, and Queues"
"by name or coordinate. Another operation that should be supported is to","chapter-4","Lists, Stacks, and Queues"
"print all records within a given distance of a specified point. Implement the","chapter-4","Lists, Stacks, and Queues"
"database using an array-based list implementation, and then a linked list im-","chapter-4","Lists, Stacks, and Queues"
"plementation. Collect running time statistics for each operation in both im-","chapter-4","Lists, Stacks, and Queues"
"plementations. What are your conclusions about the relative advantages and","chapter-4","Lists, Stacks, and Queues"
"disadvantages of the two implementations? Would storing records on the","chapter-4","Lists, Stacks, and Queues"
"list in alphabetical order by city name speed any of the operations? Would","chapter-4","Lists, Stacks, and Queues"
"keeping the list in alphabetical order slow any of the operations?","chapter-4","Lists, Stacks, and Queues"
"4.6 Modify the code of Figure 4.19 to support storing variable-length strings of","chapter-4","Lists, Stacks, and Queues"
"at most 255 characters. The stack array should have type char. A string is","chapter-4","Lists, Stacks, and Queues"
"represented by a series of characters (one character per stack element), with","chapter-4","Lists, Stacks, and Queues"
"the length of the string stored in the stack element immediately above the","chapter-4","Lists, Stacks, and Queues"
"string itself, as illustrated by Figure 4.34. The push operation would store an","chapter-4","Lists, Stacks, and Queues"
"element requiring i storage units in the i positions beginning with the current","chapter-4","Lists, Stacks, and Queues"
"value of top and store the size in the position i storage units above top.","chapter-4","Lists, Stacks, and Queues"
"The value of top would then be reset above the newly inserted element. The","chapter-4","Lists, Stacks, and Queues"
"pop operation need only look at the size value stored in position top−1 and","chapter-4","Lists, Stacks, and Queues"
"then pop off the appropriate number of units. You may store the string on the","chapter-4","Lists, Stacks, and Queues"
"stack in reverse order if you prefer, provided that when it is popped from the","chapter-4","Lists, Stacks, and Queues"
"stack, it is returned in its proper order.","chapter-4","Lists, Stacks, and Queues"
"4.7 Define an ADT for a bag (see Section 2.1) and create an array-based imple-","chapter-4","Lists, Stacks, and Queues"
"mentation for bags. Be sure that your bag ADT does not rely in any way","chapter-4","Lists, Stacks, and Queues"
"on knowing or controlling the position of an element. Then, implement the","chapter-4","Lists, Stacks, and Queues"
"dictionary ADT of Figure 4.29 using your bag implementation.","chapter-4","Lists, Stacks, and Queues"
"Sec. 4.7 Projects 143","chapter-4","Lists, Stacks, and Queues"
"top = 10","chapter-4","Lists, Stacks, and Queues"
"‘a’ ‘b’ ‘c’ 3 ‘h’ ‘e’ ‘l’ ‘o’ 5","chapter-4","Lists, Stacks, and Queues"
"0 1 2 3 4 5 6 7 8 9 10","chapter-4","Lists, Stacks, and Queues"
"‘l’","chapter-4","Lists, Stacks, and Queues"
"Figure 4.34 An array-based stack storing variable-length strings. Each position","chapter-4","Lists, Stacks, and Queues"
"stores either one character or the length of the string immediately to the left of it","chapter-4","Lists, Stacks, and Queues"
"in the stack.","chapter-4","Lists, Stacks, and Queues"
"4.8 Implement the dictionary ADT of Figure 4.29 using an unsorted linked list as","chapter-4","Lists, Stacks, and Queues"
"defined by class LList in Figure 4.8. Make the implementation as efficient","chapter-4","Lists, Stacks, and Queues"
"as you can, given the restriction that your implementation must use the un-","chapter-4","Lists, Stacks, and Queues"
"sorted linked list and its access operations to implement the dictionary. State","chapter-4","Lists, Stacks, and Queues"
"the asymptotic time requirements for each function member of the dictionary","chapter-4","Lists, Stacks, and Queues"
"ADT under your implementation.","chapter-4","Lists, Stacks, and Queues"
"4.9 Implement the dictionary ADT of Figure 4.29 based on stacks. Your imple-","chapter-4","Lists, Stacks, and Queues"
"mentation should declare and use two stacks.","chapter-4","Lists, Stacks, and Queues"
"4.10 Implement the dictionary ADT of Figure 4.29 based on queues. Your imple-","chapter-4","Lists, Stacks, and Queues"
"mentation should declare and use two queues.","chapter-4","Lists, Stacks, and Queues"
"The list representations of Chapter 4 have a fundamental limitation: Either search","chapter-5","Binary Trees"
"or insert can be made efficient, but not both at the same time. Tree structures","chapter-5","Binary Trees"
"permit both efficient access and update to large collections of data. Binary trees in","chapter-5","Binary Trees"
"particular are widely used and relatively easy to implement. But binary trees are","chapter-5","Binary Trees"
"useful for many things besides searching. Just a few examples of applications that","chapter-5","Binary Trees"
"trees can speed up include prioritizing jobs, describing mathematical expressions","chapter-5","Binary Trees"
"and the syntactic elements of computer programs, or organizing the information","chapter-5","Binary Trees"
"needed to drive data compression algorithms.","chapter-5","Binary Trees"
"This chapter begins by presenting definitions and some key properties of bi-","chapter-5","Binary Trees"
"nary trees. Section 5.2 discusses how to process all nodes of the binary tree in an","chapter-5","Binary Trees"
"organized manner. Section 5.3 presents various methods for implementing binary","chapter-5","Binary Trees"
"trees and their nodes. Sections 5.4 through 5.6 present three examples of binary","chapter-5","Binary Trees"
"trees used in specific applications: the Binary Search Tree (BST) for implementing","chapter-5","Binary Trees"
"dictionaries, heaps for implementing priority queues, and Huffman coding trees for","chapter-5","Binary Trees"
"text compression. The BST, heap, and Huffman coding tree each have distinctive","chapter-5","Binary Trees"
"structural features that affect their implementation and use.","chapter-5","Binary Trees"
"5.1 Definitions and Properties","chapter-5","Binary Trees"
"A binary tree is made up of a finite set of elements called nodes. This set either","chapter-5","Binary Trees"
"is empty or consists of a node called the root together with two binary trees, called","chapter-5","Binary Trees"
"the left and right subtrees, which are disjoint from each other and from the root.","chapter-5","Binary Trees"
"(Disjoint means that they have no nodes in common.) The roots of these subtrees","chapter-5","Binary Trees"
"are children of the root. There is an edge from a node to each of its children, and","chapter-5","Binary Trees"
"a node is said to be the parent of its children.","chapter-5","Binary Trees"
"If n1, n2, ..., nk is a sequence of nodes in the tree such that ni","chapter-5","Binary Trees"
"is the parent of","chapter-5","Binary Trees"
"ni+1 for 1 ≤ i < k, then this sequence is called a path from n1 to nk. The length","chapter-5","Binary Trees"
"of the path is k − 1. If there is a path from node R to node M, then R is an ancestor","chapter-5","Binary Trees"
"of M, and M is a descendant of R. Thus, all nodes in the tree are descendants of the","chapter-5","Binary Trees"
"145","chapter-5","Binary Trees"
"146 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"G I","chapter-5","Binary Trees"
"E F","chapter-5","Binary Trees"
"A","chapter-5","Binary Trees"
"B C","chapter-5","Binary Trees"
"D","chapter-5","Binary Trees"
"H","chapter-5","Binary Trees"
"Figure 5.1 A binary tree. Node A is the root. Nodes B and C are A’s children.","chapter-5","Binary Trees"
"Nodes B and D together form a subtree. Node B has two children: Its left child","chapter-5","Binary Trees"
"is the empty tree and its right child is D. Nodes A, C, and E are ancestors of G.","chapter-5","Binary Trees"
"Nodes D, E, and F make up level 2 of the tree; node A is at level 0. The edges","chapter-5","Binary Trees"
"from A to C to E to G form a path of length 3. Nodes D, G, H, and I are leaves.","chapter-5","Binary Trees"
"Nodes A, B, C, E, and F are internal nodes. The depth of I is 3. The height of this","chapter-5","Binary Trees"
"tree is 4.","chapter-5","Binary Trees"
"root of the tree, while the root is the ancestor of all nodes. The depth of a node M","chapter-5","Binary Trees"
"in the tree is the length of the path from the root of the tree to M. The height of a","chapter-5","Binary Trees"
"tree is one more than the depth of the deepest node in the tree. All nodes of depth d","chapter-5","Binary Trees"
"are at level d in the tree. The root is the only node at level 0, and its depth is 0. A","chapter-5","Binary Trees"
"leaf node is any node that has two empty children. An internal node is any node","chapter-5","Binary Trees"
"that has at least one non-empty child.","chapter-5","Binary Trees"
"Figure 5.1 illustrates the various terms used to identify parts of a binary tree.","chapter-5","Binary Trees"
"Figure 5.2 illustrates an important point regarding the structure of binary trees.","chapter-5","Binary Trees"
"Because all binary tree nodes have two children (one or both of which might be","chapter-5","Binary Trees"
"empty), the two binary trees of Figure 5.2 are not the same.","chapter-5","Binary Trees"
"Two restricted forms of binary tree are sufficiently important to warrant special","chapter-5","Binary Trees"
"names. Each node in a full binary tree is either (1) an internal node with exactly","chapter-5","Binary Trees"
"two non-empty children or (2) a leaf. A complete binary tree has a restricted shape","chapter-5","Binary Trees"
"obtained by starting at the root and filling the tree by levels from left to right. In the","chapter-5","Binary Trees"
"complete binary tree of height d, all levels except possibly level d−1 are completely","chapter-5","Binary Trees"
"full. The bottom level has its nodes filled in from the left side.","chapter-5","Binary Trees"
"Figure 5.3 illustrates the differences between full and complete binary trees.1","chapter-5","Binary Trees"
"There is no particular relationship between these two tree shapes; that is, the tree of","chapter-5","Binary Trees"
"Figure 5.3(a) is full but not complete while the tree of Figure 5.3(b) is complete but","chapter-5","Binary Trees"
"1 While these definitions for full and complete binary tree are the ones most commonly used, they","chapter-5","Binary Trees"
"are not universal. Because the common meaning of the words “full” and “complete” are quite similar,","chapter-5","Binary Trees"
"there is little that you can do to distinguish between them other than to memorize the definitions. Here","chapter-5","Binary Trees"
"is a memory aid that you might find useful: “Complete” is a wider word than “full,” and complete","chapter-5","Binary Trees"
"binary trees tend to be wider than full binary trees because each level of a complete binary tree is as","chapter-5","Binary Trees"
"wide as possible.","chapter-5","Binary Trees"
"Sec. 5.1 Definitions and Properties 147","chapter-5","Binary Trees"
"(b)","chapter-5","Binary Trees"
"(c) (d)","chapter-5","Binary Trees"
"(a)","chapter-5","Binary Trees"
"EMPTY EMPTY B","chapter-5","Binary Trees"
"A A","chapter-5","Binary Trees"
"A","chapter-5","Binary Trees"
"B B","chapter-5","Binary Trees"
"B","chapter-5","Binary Trees"
"A","chapter-5","Binary Trees"
"Figure 5.2 Two different binary trees. (a) A binary tree whose root has a non-","chapter-5","Binary Trees"
"empty left child. (b) A binary tree whose root has a non-empty right child. (c) The","chapter-5","Binary Trees"
"binary tree of (a) with the missing right child made explicit. (d) The binary tree","chapter-5","Binary Trees"
"of (b) with the missing left child made explicit.","chapter-5","Binary Trees"
"(a) (b)","chapter-5","Binary Trees"
"Figure 5.3 Examples of full and complete binary trees. (a) This tree is full (but","chapter-5","Binary Trees"
"not complete). (b) This tree is complete (but not full).","chapter-5","Binary Trees"
"not full. The heap data structure (Section 5.5) is an example of a complete binary","chapter-5","Binary Trees"
"tree. The Huffman coding tree (Section 5.6) is an example of a full binary tree.","chapter-5","Binary Trees"
"5.1.1 The Full Binary Tree Theorem","chapter-5","Binary Trees"
"Some binary tree implementations store data only at the leaf nodes, using the inter-","chapter-5","Binary Trees"
"nal nodes to provide structure to the tree. More generally, binary tree implementa-","chapter-5","Binary Trees"
"tions might require some amount of space for internal nodes, and a different amount","chapter-5","Binary Trees"
"for leaf nodes. Thus, to analyze the space required by such implementations, it is","chapter-5","Binary Trees"
"useful to know the minimum and maximum fraction of the nodes that are leaves in","chapter-5","Binary Trees"
"a tree containing n internal nodes.","chapter-5","Binary Trees"
"Unfortunately, this fraction is not fixed. A binary tree of n internal nodes might","chapter-5","Binary Trees"
"have only one leaf. This occurs when the internal nodes are arranged in a chain","chapter-5","Binary Trees"
"ending in a single leaf as shown in Figure 5.4. In this case, the number of leaves","chapter-5","Binary Trees"
"is low because each internal node has only one non-empty child. To find an upper","chapter-5","Binary Trees"
"bound on the number of leaves for a tree of n internal nodes, first note that the upper","chapter-5","Binary Trees"
"148 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"internal nodes","chapter-5","Binary Trees"
"Any number of","chapter-5","Binary Trees"
"Figure 5.4 A tree containing many internal nodes and a single leaf.","chapter-5","Binary Trees"
"bound will occur when each internal node has two non-empty children, that is,","chapter-5","Binary Trees"
"when the tree is full. However, this observation does not tell what shape of tree will","chapter-5","Binary Trees"
"yield the highest percentage of non-empty leaves. It turns out not to matter, because","chapter-5","Binary Trees"
"all full binary trees with n internal nodes have the same number of leaves. This fact","chapter-5","Binary Trees"
"allows us to compute the space requirements for a full binary tree implementation","chapter-5","Binary Trees"
"whose leaves require a different amount of space from its internal nodes.","chapter-5","Binary Trees"
"Theorem 5.1 Full Binary Tree Theorem: The number of leaves in a non-empty","chapter-5","Binary Trees"
"full binary tree is one more than the number of internal nodes.","chapter-5","Binary Trees"
"Proof: The proof is by mathematical induction on n, the number of internal nodes.","chapter-5","Binary Trees"
"This is an example of an induction proof where we reduce from an arbitrary in-","chapter-5","Binary Trees"
"stance of size n to an instance of size n − 1 that meets the induction hypothesis.","chapter-5","Binary Trees"
"• Base Cases: The non-empty tree with zero internal nodes has one leaf node.","chapter-5","Binary Trees"
"A full binary tree with one internal node has two leaf nodes. Thus, the base","chapter-5","Binary Trees"
"cases for n = 0 and n = 1 conform to the theorem.","chapter-5","Binary Trees"
"• Induction Hypothesis: Assume that any full binary tree T containing n − 1","chapter-5","Binary Trees"
"internal nodes has n leaves.","chapter-5","Binary Trees"
"• Induction Step: Given tree T with n internal nodes, select an internal node I","chapter-5","Binary Trees"
"whose children are both leaf nodes. Remove both of I’s children, making","chapter-5","Binary Trees"
"I a leaf node. Call the new tree T","chapter-5","Binary Trees"
"0","chapter-5","Binary Trees"
". T","chapter-5","Binary Trees"
"0","chapter-5","Binary Trees"
"has n − 1 internal nodes. From","chapter-5","Binary Trees"
"the induction hypothesis, T","chapter-5","Binary Trees"
"0","chapter-5","Binary Trees"
"has n leaves. Now, restore I’s two children. We","chapter-5","Binary Trees"
"once again have tree T with n internal nodes. How many leaves does T have?","chapter-5","Binary Trees"
"Because T","chapter-5","Binary Trees"
"0","chapter-5","Binary Trees"
"has n leaves, adding the two children yields n+2. However, node","chapter-5","Binary Trees"
"I counted as one of the leaves in T","chapter-5","Binary Trees"
"0","chapter-5","Binary Trees"
"and has now become an internal node.","chapter-5","Binary Trees"
"Thus, tree T has n + 1 leaf nodes and n internal nodes.","chapter-5","Binary Trees"
"By mathematical induction the theorem holds for all values of n ≥ 0. ✷","chapter-5","Binary Trees"
"When analyzing the space requirements for a binary tree implementation, it is","chapter-5","Binary Trees"
"useful to know how many empty subtrees a tree contains. A simple extension of","chapter-5","Binary Trees"
"the Full Binary Tree Theorem tells us exactly how many empty subtrees there are","chapter-5","Binary Trees"
"in any binary tree, whether full or not. Here are two approaches to proving the","chapter-5","Binary Trees"
"following theorem, and each suggests a useful way of thinking about binary trees.","chapter-5","Binary Trees"
"Sec. 5.2 Binary Tree Traversals 149","chapter-5","Binary Trees"
"Theorem 5.2 The number of empty subtrees in a non-empty binary tree is one","chapter-5","Binary Trees"
"more than the number of nodes in the tree.","chapter-5","Binary Trees"
"Proof 1: Take an arbitrary binary tree T and replace every empty subtree with a","chapter-5","Binary Trees"
"leaf node. Call the new tree T","chapter-5","Binary Trees"
"0","chapter-5","Binary Trees"
". All nodes originally in T will be internal nodes in","chapter-5","Binary Trees"
"T","chapter-5","Binary Trees"
"0","chapter-5","Binary Trees"
"(because even the leaf nodes of T have children in T","chapter-5","Binary Trees"
"0","chapter-5","Binary Trees"
"). T","chapter-5","Binary Trees"
"0","chapter-5","Binary Trees"
"is a full binary tree,","chapter-5","Binary Trees"
"because every internal node of T now must have two children in T","chapter-5","Binary Trees"
"0","chapter-5","Binary Trees"
", and each leaf","chapter-5","Binary Trees"
"node in T must have two children in T","chapter-5","Binary Trees"
"0","chapter-5","Binary Trees"
"(the leaves just added). The Full Binary Tree","chapter-5","Binary Trees"
"Theorem tells us that the number of leaves in a full binary tree is one more than the","chapter-5","Binary Trees"
"number of internal nodes. Thus, the number of new leaves that were added to create","chapter-5","Binary Trees"
"T","chapter-5","Binary Trees"
"0","chapter-5","Binary Trees"
"is one more than the number of nodes in T. Each leaf node in T","chapter-5","Binary Trees"
"0","chapter-5","Binary Trees"
"corresponds to","chapter-5","Binary Trees"
"an empty subtree in T. Thus, the number of empty subtrees in T is one more than","chapter-5","Binary Trees"
"the number of nodes in T. ✷","chapter-5","Binary Trees"
"Proof 2: By definition, every node in binary tree T has two children, for a total of","chapter-5","Binary Trees"
"2n children in a tree of n nodes. Every node except the root node has one parent,","chapter-5","Binary Trees"
"for a total of n − 1 nodes with parents. In other words, there are n − 1 non-empty","chapter-5","Binary Trees"
"children. Because the total number of children is 2n, the remaining n + 1 children","chapter-5","Binary Trees"
"must be empty. ✷","chapter-5","Binary Trees"
"5.1.2 A Binary Tree Node ADT","chapter-5","Binary Trees"
"Just as a linked list is comprised of a collection of link objects, a tree is comprised","chapter-5","Binary Trees"
"of a collection of node objects. Figure 5.5 shows an ADT for binary tree nodes,","chapter-5","Binary Trees"
"called BinNode. This class will be used by some of the binary tree structures","chapter-5","Binary Trees"
"presented later. Class BinNode is a generic with parameter E, which is the type","chapter-5","Binary Trees"
"for the data record stored in the node. Member functions are provided that set or","chapter-5","Binary Trees"
"return the element value, set or return a reference to the left child, set or return a","chapter-5","Binary Trees"
"reference to the right child, or indicate whether the node is a leaf.","chapter-5","Binary Trees"
"5.2 Binary Tree Traversals","chapter-5","Binary Trees"
"Often we wish to process a binary tree by “visiting” each of its nodes, each time","chapter-5","Binary Trees"
"performing a specific action such as printing the contents of the node. Any process","chapter-5","Binary Trees"
"for visiting all of the nodes in some order is called a traversal. Any traversal that","chapter-5","Binary Trees"
"lists every node in the tree exactly once is called an enumeration of the tree’s","chapter-5","Binary Trees"
"nodes. Some applications do not require that the nodes be visited in any particular","chapter-5","Binary Trees"
"order as long as each node is visited precisely once. For other applications, nodes","chapter-5","Binary Trees"
"must be visited in an order that preserves some relationship. For example, we might","chapter-5","Binary Trees"
"wish to make sure that we visit any given node before we visit its children. This is","chapter-5","Binary Trees"
"called a preorder traversal.","chapter-5","Binary Trees"
"150 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"/** ADT for binary tree nodes */","chapter-5","Binary Trees"
"public interface BinNode<E> {","chapter-5","Binary Trees"
"/** Get and set the element value */","chapter-5","Binary Trees"
"public E element();","chapter-5","Binary Trees"
"public void setElement(E v);","chapter-5","Binary Trees"
"/** @return The left child */","chapter-5","Binary Trees"
"public BinNode<E> left();","chapter-5","Binary Trees"
"/** @return The right child */","chapter-5","Binary Trees"
"public BinNode<E> right();","chapter-5","Binary Trees"
"/** @return True if a leaf node, false otherwise */","chapter-5","Binary Trees"
"public boolean isLeaf();","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"Figure 5.5 A binary tree node ADT.","chapter-5","Binary Trees"
"Example 5.1 The preorder enumeration for the tree of Figure 5.1 is","chapter-5","Binary Trees"
"ABDCEGFHI.","chapter-5","Binary Trees"
"The first node printed is the root. Then all nodes of the left subtree are","chapter-5","Binary Trees"
"printed (in preorder) before any node of the right subtree.","chapter-5","Binary Trees"
"Alternatively, we might wish to visit each node only after we visit its children","chapter-5","Binary Trees"
"(and their subtrees). For example, this would be necessary if we wish to return","chapter-5","Binary Trees"
"all nodes in the tree to free store. We would like to delete the children of a node","chapter-5","Binary Trees"
"before deleting the node itself. But to do that requires that the children’s children","chapter-5","Binary Trees"
"be deleted first, and so on. This is called a postorder traversal.","chapter-5","Binary Trees"
"Example 5.2 The postorder enumeration for the tree of Figure 5.1 is","chapter-5","Binary Trees"
"DBGEHIFCA.","chapter-5","Binary Trees"
"An inorder traversal first visits the left child (including its entire subtree), then","chapter-5","Binary Trees"
"visits the node, and finally visits the right child (including its entire subtree). The","chapter-5","Binary Trees"
"binary search tree of Section 5.4 makes use of this traversal to print all nodes in","chapter-5","Binary Trees"
"ascending order of value.","chapter-5","Binary Trees"
"Example 5.3 The inorder enumeration for the tree of Figure 5.1 is","chapter-5","Binary Trees"
"BDAGECHFI.","chapter-5","Binary Trees"
"A traversal routine is naturally written as a recursive function. Its input pa-","chapter-5","Binary Trees"
"rameter is a reference to a node which we will call rt because each node can be","chapter-5","Binary Trees"
"Sec. 5.2 Binary Tree Traversals 151","chapter-5","Binary Trees"
"viewed as the root of a some subtree. The initial call to the traversal function passes","chapter-5","Binary Trees"
"in a reference to the root node of the tree. The traversal function visits rt and its","chapter-5","Binary Trees"
"children (if any) in the desired order. For example, a preorder traversal specifies","chapter-5","Binary Trees"
"that rt be visited before its children. This can easily be implemented as follows.","chapter-5","Binary Trees"
"/** @param rt is the root of the subtree */","chapter-5","Binary Trees"
"void preorder(BinNode rt)","chapter-5","Binary Trees"
"{","chapter-5","Binary Trees"
"if (rt == null) return; // Empty subtree - do nothing","chapter-5","Binary Trees"
"visit(rt); // Process root node","chapter-5","Binary Trees"
"preorder(rt.left()); // Process all nodes in left","chapter-5","Binary Trees"
"preorder(rt.right()); // Process all nodes in right","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"Function preorder first checks that the tree is not empty (if it is, then the traversal","chapter-5","Binary Trees"
"is done and preorder simply returns). Otherwise, preorder makes a call to","chapter-5","Binary Trees"
"visit, which processes the root node (i.e., prints the value or performs whatever","chapter-5","Binary Trees"
"computation as required by the application). Function preorder is then called","chapter-5","Binary Trees"
"recursively on the left subtree, which will visit all nodes in that subtree. Finally,","chapter-5","Binary Trees"
"preorder is called on the right subtree, visiting all nodes in the right subtree.","chapter-5","Binary Trees"
"Postorder and inorder traversals are similar. They simply change the order in which","chapter-5","Binary Trees"
"the node and its children are visited, as appropriate.","chapter-5","Binary Trees"
"An important decision in the implementation of any recursive function on trees","chapter-5","Binary Trees"
"is when to check for an empty subtree. Function preorder first checks to see if","chapter-5","Binary Trees"
"the value for rt is null. If not, it will recursively call itself on the left and right","chapter-5","Binary Trees"
"children of rt. In other words, preorder makes no attempt to avoid calling itself","chapter-5","Binary Trees"
"on an empty child. Some programmers use an alternate design in which the left and","chapter-5","Binary Trees"
"right pointers of the current node are checked so that the recursive call is made only","chapter-5","Binary Trees"
"on non-empty children. Such a design typically looks as follows:","chapter-5","Binary Trees"
"void preorder2(BinNode rt)","chapter-5","Binary Trees"
"{","chapter-5","Binary Trees"
"visit(rt);","chapter-5","Binary Trees"
"if (rt.left() != null) preorder2(rt.left());","chapter-5","Binary Trees"
"if (rt.right() != null) preorder2(rt.right());","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"At first it might appear that preorder2 is more efficient than preorder,","chapter-5","Binary Trees"
"because it makes only half as many recursive calls. (Why?) On the other hand,","chapter-5","Binary Trees"
"preorder2 must access the left and right child pointers twice as often. The net","chapter-5","Binary Trees"
"result is little or no performance improvement.","chapter-5","Binary Trees"
"In reality, the design of preorder2 is inferior to that of preorder for two","chapter-5","Binary Trees"
"reasons. First, while it is not apparent in this simple example, for more complex","chapter-5","Binary Trees"
"traversals it can become awkward to place the check for the null pointer in the","chapter-5","Binary Trees"
"calling code. Even here we had to write two tests for null, rather than the one","chapter-5","Binary Trees"
"needed by preorder. The more important concern with preorder2 is that it","chapter-5","Binary Trees"
"152 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"tends to be error prone. While preorder2 insures that no recursive calls will","chapter-5","Binary Trees"
"be made on empty subtrees, it will fail if the initial call passes in a null pointer.","chapter-5","Binary Trees"
"This would occur if the original tree is empty. To avoid the bug, either preorder2","chapter-5","Binary Trees"
"needs an additional test for a null pointer at the beginning (making the subsequent","chapter-5","Binary Trees"
"tests redundant after all), or the caller of preorder2 has a hidden obligation to","chapter-5","Binary Trees"
"pass in a non-empty tree, which is unreliable design. The net result is that many","chapter-5","Binary Trees"
"programmers forget to test for the possibility that the empty tree is being traversed.","chapter-5","Binary Trees"
"By using the first design, which explicitly supports processing of empty subtrees,","chapter-5","Binary Trees"
"the problem is avoided.","chapter-5","Binary Trees"
"Another issue to consider when designing a traversal is how to define the visitor","chapter-5","Binary Trees"
"function that is to be executed on every node. One approach is simply to write a","chapter-5","Binary Trees"
"new version of the traversal for each such visitor function as needed. The disad-","chapter-5","Binary Trees"
"vantage to this is that whatever function does the traversal must have access to the","chapter-5","Binary Trees"
"BinNode class. It is probably better design to permit only the tree class to have","chapter-5","Binary Trees"
"access to the BinNode class.","chapter-5","Binary Trees"
"Another approach is for the tree class to supply a generic traversal function","chapter-5","Binary Trees"
"which takes the visitor as a function parameter. This is known as the visitor design","chapter-5","Binary Trees"
"pattern. A major constraint on this approach is that the signature for all visitor","chapter-5","Binary Trees"
"functions, that is, their return type and parameters, must be fixed in advance. Thus,","chapter-5","Binary Trees"
"the designer of the generic traversal function must be able to adequately judge what","chapter-5","Binary Trees"
"parameters and return type will likely be needed by potential visitor functions.","chapter-5","Binary Trees"
"Handling information flow between parts of a program can be a significant","chapter-5","Binary Trees"
"design challenge, especially when dealing with recursive functions such as tree","chapter-5","Binary Trees"
"traversals. In general, we can run into trouble either with passing in the correct","chapter-5","Binary Trees"
"information needed by the function to do its work, or with returning information","chapter-5","Binary Trees"
"to the recursive function’s caller. We will see many examples throughout the book","chapter-5","Binary Trees"
"that illustrate methods for passing information in and out of recursive functions as","chapter-5","Binary Trees"
"they traverse a tree structure. Here are a few simple examples.","chapter-5","Binary Trees"
"First we consider the simple case where a computation requires that we com-","chapter-5","Binary Trees"
"municate information back up the tree to the end user.","chapter-5","Binary Trees"
"Example 5.4 We wish to count the number of nodes in a binary tree. The","chapter-5","Binary Trees"
"key insight is that the total count for any (non-empty) subtree is one for the","chapter-5","Binary Trees"
"root plus the counts for the left and right subtrees. Where do left and right","chapter-5","Binary Trees"
"subtree counts come from? Calls to function count on the subtrees will","chapter-5","Binary Trees"
"compute this for us. Thus, we can implement count as follows.","chapter-5","Binary Trees"
"int count(BinNode rt) {","chapter-5","Binary Trees"
"if (rt == null) return 0; // Nothing to count","chapter-5","Binary Trees"
"return 1 + count(rt.left()) + count(rt.right());","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"Sec. 5.2 Binary Tree Traversals 153","chapter-5","Binary Trees"
"20","chapter-5","Binary Trees"
"50","chapter-5","Binary Trees"
"40 75","chapter-5","Binary Trees"
"20 to 40","chapter-5","Binary Trees"
"Figure 5.6 To be a binary search tree, the left child of the node with value 40","chapter-5","Binary Trees"
"must have a value between 20 and 40.","chapter-5","Binary Trees"
"Another problem that occurs when recursively processing data collections is","chapter-5","Binary Trees"
"controlling which members of the collection will be visited. For example, some","chapter-5","Binary Trees"
"tree “traversals” might in fact visit only some tree nodes, while avoiding processing","chapter-5","Binary Trees"
"of others. Exercise 5.20 must solve exactly this problem in the context of a binary","chapter-5","Binary Trees"
"search tree. It must visit only those children of a given node that might possibly","chapter-5","Binary Trees"
"fall within a given range of values. Fortunately, it requires only a simple local","chapter-5","Binary Trees"
"calculation to determine which child(ren) to visit.","chapter-5","Binary Trees"
"A more difficult situation is illustrated by the following problem. Given an","chapter-5","Binary Trees"
"arbitrary binary tree we wish to determine if, for every node A, are all nodes in A’s","chapter-5","Binary Trees"
"left subtree less than the value of A, and are all nodes in A’s right subtree greater","chapter-5","Binary Trees"
"than the value of A? (This happens to be the definition for a binary search tree,","chapter-5","Binary Trees"
"described in Section 5.4.) Unfortunately, to make this decision we need to know","chapter-5","Binary Trees"
"some context that is not available just by looking at the node’s parent or children.","chapter-5","Binary Trees"
"As shown by Figure 5.6, it is not enough to verify that A’s left child has a value","chapter-5","Binary Trees"
"less than that of A, and that A’s right child has a greater value. Nor is it enough to","chapter-5","Binary Trees"
"verify that A has a value consistent with that of its parent. In fact, we need to know","chapter-5","Binary Trees"
"information about what range of values is legal for a given node. That information","chapter-5","Binary Trees"
"might come from any of the node’s ancestors. Thus, relevant range information","chapter-5","Binary Trees"
"must be passed down the tree. We can implement this function as follows.","chapter-5","Binary Trees"
"boolean checkBST(BinNode<Integer> rt,","chapter-5","Binary Trees"
"int low, int high) {","chapter-5","Binary Trees"
"if (rt == null) return true; // Empty subtree","chapter-5","Binary Trees"
"int rootkey = rt.element();","chapter-5","Binary Trees"
"if ((rootkey < low) || (rootkey > high))","chapter-5","Binary Trees"
"return false; // Out of range","chapter-5","Binary Trees"
"if (!checkBST(rt.left(), low, rootkey))","chapter-5","Binary Trees"
"return false; // Left side failed","chapter-5","Binary Trees"
"return checkBST(rt.right(), rootkey, high);","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"154 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"5.3 Binary Tree Node Implementations","chapter-5","Binary Trees"
"In this section we examine ways to implement binary tree nodes. We begin with","chapter-5","Binary Trees"
"some options for pointer-based binary tree node implementations. Then comes a","chapter-5","Binary Trees"
"discussion on techniques for determining the space requirements for a given imple-","chapter-5","Binary Trees"
"mentation. The section concludes with an introduction to the array-based imple-","chapter-5","Binary Trees"
"mentation for complete binary trees.","chapter-5","Binary Trees"
"5.3.1 Pointer-Based Node Implementations","chapter-5","Binary Trees"
"By definition, all binary tree nodes have two children, though one or both children","chapter-5","Binary Trees"
"can be empty. Binary tree nodes typically contain a value field, with the type of","chapter-5","Binary Trees"
"the field depending on the application. The most common node implementation","chapter-5","Binary Trees"
"includes a value field and pointers to the two children.","chapter-5","Binary Trees"
"Figure 5.7 shows a simple implementation for the BinNode abstract class,","chapter-5","Binary Trees"
"which we will name BSTNode. Class BSTNode includes a data member of type","chapter-5","Binary Trees"
"E, (which is the second generic parameter) for the element type. To support search","chapter-5","Binary Trees"
"structures such as the Binary Search Tree, an additional field is included, with","chapter-5","Binary Trees"
"corresponding access methods, to store a key value (whose purpose is explained","chapter-5","Binary Trees"
"in Section 4.4). Its type is determined by the first generic parameter, named Key.","chapter-5","Binary Trees"
"Every BSTNode object also has two pointers, one to its left child and another to its","chapter-5","Binary Trees"
"right child. Figure 5.8 illustrates the BSTNode implementation.","chapter-5","Binary Trees"
"Some programmers find it convenient to add a pointer to the node’s parent,","chapter-5","Binary Trees"
"allowing easy upward movement in the tree. Using a parent pointer is somewhat","chapter-5","Binary Trees"
"analogous to adding a link to the previous node in a doubly linked list. In practice,","chapter-5","Binary Trees"
"the parent pointer is almost always unnecessary and adds to the space overhead for","chapter-5","Binary Trees"
"the tree implementation. It is not just a problem that parent pointers take space.","chapter-5","Binary Trees"
"More importantly, many uses of the parent pointer are driven by improper under-","chapter-5","Binary Trees"
"standing of recursion and so indicate poor programming. If you are inclined toward","chapter-5","Binary Trees"
"using a parent pointer, consider if there is a more efficient implementation possible.","chapter-5","Binary Trees"
"An important decision in the design of a pointer-based node implementation","chapter-5","Binary Trees"
"is whether the same class definition will be used for leaves and internal nodes.","chapter-5","Binary Trees"
"Using the same class for both will simplify the implementation, but might be an","chapter-5","Binary Trees"
"inefficient use of space. Some applications require data values only for the leaves.","chapter-5","Binary Trees"
"Other applications require one type of value for the leaves and another for the in-","chapter-5","Binary Trees"
"ternal nodes. Examples include the binary trie of Section 13.1, the PR quadtree of","chapter-5","Binary Trees"
"Section 13.3, the Huffman coding tree of Section 5.6, and the expression tree illus-","chapter-5","Binary Trees"
"trated by Figure 5.9. By definition, only internal nodes have non-empty children.","chapter-5","Binary Trees"
"If we use the same node implementation for both internal and leaf nodes, then both","chapter-5","Binary Trees"
"must store the child pointers. But it seems wasteful to store child pointers in the","chapter-5","Binary Trees"
"leaf nodes. Thus, there are many reasons why it can save space to have separate","chapter-5","Binary Trees"
"implementations for internal and leaf nodes.","chapter-5","Binary Trees"
"Sec. 5.3 Binary Tree Node Implementations 155","chapter-5","Binary Trees"
"/** Binary tree node implementation: Pointers to children","chapter-5","Binary Trees"
"@param E The data element","chapter-5","Binary Trees"
"@param Key The associated key for the record */","chapter-5","Binary Trees"
"class BSTNode<Key, E> implements BinNode<E> {","chapter-5","Binary Trees"
"private Key key; // Key for this node","chapter-5","Binary Trees"
"private E element; // Element for this node","chapter-5","Binary Trees"
"private BSTNode<Key,E> left; // Pointer to left child","chapter-5","Binary Trees"
"private BSTNode<Key,E> right; // Pointer to right child","chapter-5","Binary Trees"
"/** Constructors */","chapter-5","Binary Trees"
"public BSTNode() {left = right = null; }","chapter-5","Binary Trees"
"public BSTNode(Key k, E val)","chapter-5","Binary Trees"
"{ left = right = null; key = k; element = val; }","chapter-5","Binary Trees"
"public BSTNode(Key k, E val,","chapter-5","Binary Trees"
"BSTNode<Key,E> l, BSTNode<Key,E> r)","chapter-5","Binary Trees"
"{ left = l; right = r; key = k; element = val; }","chapter-5","Binary Trees"
"/** Get and set the key value */","chapter-5","Binary Trees"
"public Key key() { return key; }","chapter-5","Binary Trees"
"public void setKey(Key k) { key = k; }","chapter-5","Binary Trees"
"/** Get and set the element value */","chapter-5","Binary Trees"
"public E element() { return element; }","chapter-5","Binary Trees"
"public void setElement(E v) { element = v; }","chapter-5","Binary Trees"
"/** Get and set the left child */","chapter-5","Binary Trees"
"public BSTNode<Key,E> left() { return left; }","chapter-5","Binary Trees"
"public void setLeft(BSTNode<Key,E> p) { left = p; }","chapter-5","Binary Trees"
"/** Get and set the right child */","chapter-5","Binary Trees"
"public BSTNode<Key,E> right() { return right; }","chapter-5","Binary Trees"
"public void setRight(BSTNode<Key,E> p) { right = p; }","chapter-5","Binary Trees"
"/** @return True if a leaf node, false otherwise */","chapter-5","Binary Trees"
"public boolean isLeaf()","chapter-5","Binary Trees"
"{ return (left == null) && (right == null); }","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"Figure 5.7 A binary tree node class implementation.","chapter-5","Binary Trees"
"As an example of a tree that stores different information at the leaf and inter-","chapter-5","Binary Trees"
"nal nodes, consider the expression tree illustrated by Figure 5.9. The expression","chapter-5","Binary Trees"
"tree represents an algebraic expression composed of binary operators such as ad-","chapter-5","Binary Trees"
"dition, subtraction, multiplication, and division. Internal nodes store operators,","chapter-5","Binary Trees"
"while the leaves store operands. The tree of Figure 5.9 represents the expression","chapter-5","Binary Trees"
"4x(2x + a) − c. The storage requirements for a leaf in an expression tree are quite","chapter-5","Binary Trees"
"different from those of an internal node. Internal nodes store one of a small set of","chapter-5","Binary Trees"
"operators, so internal nodes could store a small code identifying the operator such","chapter-5","Binary Trees"
"as a single byte for the operator’s character symbol. In contrast, leaves store vari-","chapter-5","Binary Trees"
"able names or numbers, which is considerably larger in order to handle the wider","chapter-5","Binary Trees"
"range of possible values. At the same time, leaf nodes need not store child pointers.","chapter-5","Binary Trees"
"156 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"A","chapter-5","Binary Trees"
"C","chapter-5","Binary Trees"
"G H","chapter-5","Binary Trees"
"D E","chapter-5","Binary Trees"
"B","chapter-5","Binary Trees"
"F","chapter-5","Binary Trees"
"I","chapter-5","Binary Trees"
"Figure 5.8 Illustration of a typical pointer-based binary tree implementation,","chapter-5","Binary Trees"
"where each node stores two child pointers and a value.","chapter-5","Binary Trees"
"4 x","chapter-5","Binary Trees"
"x","chapter-5","Binary Trees"
"c","chapter-5","Binary Trees"
"a","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
"*","chapter-5","Binary Trees"
"*","chapter-5","Binary Trees"
"*","chapter-5","Binary Trees"
"−","chapter-5","Binary Trees"
"+","chapter-5","Binary Trees"
"Figure 5.9 An expression tree for 4x(2x + a) − c.","chapter-5","Binary Trees"
"Java allows us to differentiate leaf from internal nodes through the use of class","chapter-5","Binary Trees"
"inheritance. A base class provides a general definition for an object, and a subclass","chapter-5","Binary Trees"
"modifies a base class to add more detail. A base class can be declared for binary tree","chapter-5","Binary Trees"
"nodes in general, with subclasses defined for the internal and leaf nodes. The base","chapter-5","Binary Trees"
"class of Figure 5.10 is named VarBinNode. It includes a virtual member function","chapter-5","Binary Trees"
"named isLeaf, which indicates the node type. Subclasses for the internal and leaf","chapter-5","Binary Trees"
"node types each implement isLeaf. Internal nodes store child pointers of the base","chapter-5","Binary Trees"
"class type; they do not distinguish their children’s actual subclass. Whenever a node","chapter-5","Binary Trees"
"is examined, its version of isLeaf indicates the node’s subclass.","chapter-5","Binary Trees"
"Figure 5.10 includes two subclasses derived from class VarBinNode, named","chapter-5","Binary Trees"
"LeafNode and IntlNode. Class IntlNode can access its children through","chapter-5","Binary Trees"
"pointers of type VarBinNode. Function traverse illustrates the use of these","chapter-5","Binary Trees"
"classes. When traverse calls method isLeaf, Java’s runtime environment","chapter-5","Binary Trees"
"determines which subclass this particular instance of rt happens to be and calls that","chapter-5","Binary Trees"
"subclass’s version of isLeaf. Method isLeaf then provides the actual node type","chapter-5","Binary Trees"
"Sec. 5.3 Binary Tree Node Implementations 157","chapter-5","Binary Trees"
"/** Base class for expression tree nodes */","chapter-5","Binary Trees"
"public interface VarBinNode {","chapter-5","Binary Trees"
"public boolean isLeaf(); // All subclasses must implement","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"/** Leaf node */","chapter-5","Binary Trees"
"class VarLeafNode implements VarBinNode {","chapter-5","Binary Trees"
"private String operand; // Operand value","chapter-5","Binary Trees"
"public VarLeafNode(String val) { operand = val; }","chapter-5","Binary Trees"
"public boolean isLeaf() { return true; }","chapter-5","Binary Trees"
"public String value() { return operand; }","chapter-5","Binary Trees"
"};","chapter-5","Binary Trees"
"/** Internal node */","chapter-5","Binary Trees"
"class VarIntlNode implements VarBinNode {","chapter-5","Binary Trees"
"private VarBinNode left; // Left child","chapter-5","Binary Trees"
"private VarBinNode right; // Right child","chapter-5","Binary Trees"
"private Character operator; // Operator value","chapter-5","Binary Trees"
"public VarIntlNode(Character op,","chapter-5","Binary Trees"
"VarBinNode l, VarBinNode r)","chapter-5","Binary Trees"
"{ operator = op; left = l; right = r; }","chapter-5","Binary Trees"
"public boolean isLeaf() { return false; }","chapter-5","Binary Trees"
"public VarBinNode leftchild() { return left; }","chapter-5","Binary Trees"
"public VarBinNode rightchild() { return right; }","chapter-5","Binary Trees"
"public Character value() { return operator; }","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"/** Preorder traversal */","chapter-5","Binary Trees"
"public static void traverse(VarBinNode rt) {","chapter-5","Binary Trees"
"if (rt == null) return; // Nothing to visit","chapter-5","Binary Trees"
"if (rt.isLeaf()) // Process leaf node","chapter-5","Binary Trees"
"Visit.VisitLeafNode(((VarLeafNode)rt).value());","chapter-5","Binary Trees"
"else { // Process internal node","chapter-5","Binary Trees"
"Visit.VisitInternalNode(((VarIntlNode)rt).value());","chapter-5","Binary Trees"
"traverse(((VarIntlNode)rt).leftchild());","chapter-5","Binary Trees"
"traverse(((VarIntlNode)rt).rightchild());","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"Figure 5.10 An implementation for separate internal and leaf node representa-","chapter-5","Binary Trees"
"tions using Java class inheritance and virtual functions.","chapter-5","Binary Trees"
"158 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"to its caller. The other member functions for the derived subclasses are accessed by","chapter-5","Binary Trees"
"type-casting the base class pointer as appropriate, as shown in function traverse.","chapter-5","Binary Trees"
"There is another approach that we can take to represent separate leaf and inter-","chapter-5","Binary Trees"
"nal nodes, also using a virtual base class and separate node classes for the two types.","chapter-5","Binary Trees"
"This is to implement nodes using the composite design pattern. This approach is","chapter-5","Binary Trees"
"noticeably different from the one of Figure 5.10 in that the node classes themselves","chapter-5","Binary Trees"
"implement the functionality of traverse. Figure 5.11 shows the implementa-","chapter-5","Binary Trees"
"tion. Here, base class VarBinNode declares a member function traverse that","chapter-5","Binary Trees"
"each subclass must implement. Each subclass then implements its own appropriate","chapter-5","Binary Trees"
"behavior for its role in a traversal. The whole traversal process is called by invoking","chapter-5","Binary Trees"
"traverse on the root node, which in turn invokes traverse on its children.","chapter-5","Binary Trees"
"When comparing the implementations of Figures 5.10 and 5.11, each has ad-","chapter-5","Binary Trees"
"vantages and disadvantages. The first does not require that the node classes know","chapter-5","Binary Trees"
"about the traverse function. With this approach, it is easy to add new methods","chapter-5","Binary Trees"
"to the tree class that do other traversals or other operations on nodes of the tree.","chapter-5","Binary Trees"
"However, we see that traverse in Figure 5.10 does need to be familiar with each","chapter-5","Binary Trees"
"node subclass. Adding a new node subclass would therefore require modifications","chapter-5","Binary Trees"
"to the traverse function. In contrast, the approach of Figure 5.11 requires that","chapter-5","Binary Trees"
"any new operation on the tree that requires a traversal also be implemented in the","chapter-5","Binary Trees"
"node subclasses. On the other hand, the approach of Figure 5.11 avoids the need for","chapter-5","Binary Trees"
"the traverse function to know anything about the distinct abilities of the node","chapter-5","Binary Trees"
"subclasses. Those subclasses handle the responsibility of performing a traversal on","chapter-5","Binary Trees"
"themselves. A secondary benefit is that there is no need for traverse to explic-","chapter-5","Binary Trees"
"itly enumerate all of the different node subclasses, directing appropriate action for","chapter-5","Binary Trees"
"each. With only two node classes this is a minor point. But if there were many such","chapter-5","Binary Trees"
"subclasses, this could become a bigger problem. A disadvantage is that the traversal","chapter-5","Binary Trees"
"operation must not be called on a null pointer, because there is no object to catch","chapter-5","Binary Trees"
"the call. This problem could be avoided by using a flyweight (see Section 1.3.1) to","chapter-5","Binary Trees"
"implement empty nodes.","chapter-5","Binary Trees"
"Typically, the version of Figure 5.10 would be preferred in this example if","chapter-5","Binary Trees"
"traverse is a member function of the tree class, and if the node subclasses are","chapter-5","Binary Trees"
"hidden from users of that tree class. On the other hand, if the nodes are objects","chapter-5","Binary Trees"
"that have meaning to users of the tree separate from their existence as nodes in the","chapter-5","Binary Trees"
"tree, then the version of Figure 5.11 might be preferred because hiding the internal","chapter-5","Binary Trees"
"behavior of the nodes becomes more important.","chapter-5","Binary Trees"
"Another advantage of the composite design is that implementing each node","chapter-5","Binary Trees"
"type’s functionality might be easier. This is because you can focus solely on the","chapter-5","Binary Trees"
"information passing and other behavior needed by this node type to do its job. This","chapter-5","Binary Trees"
"breaks down the complexity that many programmers feel overwhelmed by when","chapter-5","Binary Trees"
"dealing with complex information flows related to recursive processing.","chapter-5","Binary Trees"
"Sec. 5.3 Binary Tree Node Implementations 159","chapter-5","Binary Trees"
"/** Base class: Composite */","chapter-5","Binary Trees"
"public interface VarBinNode {","chapter-5","Binary Trees"
"public boolean isLeaf();","chapter-5","Binary Trees"
"public void traverse();","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"/** Leaf node: Composite */","chapter-5","Binary Trees"
"class VarLeafNode implements VarBinNode {","chapter-5","Binary Trees"
"private String operand; // Operand value","chapter-5","Binary Trees"
"public VarLeafNode(String val) { operand = val; }","chapter-5","Binary Trees"
"public boolean isLeaf() { return true; }","chapter-5","Binary Trees"
"public String value() { return operand; }","chapter-5","Binary Trees"
"public void traverse() {","chapter-5","Binary Trees"
"Visit.VisitLeafNode(operand);","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"/** Internal node: Composite */","chapter-5","Binary Trees"
"class VarIntlNode implements VarBinNode { // Internal node","chapter-5","Binary Trees"
"private VarBinNode left; // Left child","chapter-5","Binary Trees"
"private VarBinNode right; // Right child","chapter-5","Binary Trees"
"private Character operator; // Operator value","chapter-5","Binary Trees"
"public VarIntlNode(Character op,","chapter-5","Binary Trees"
"VarBinNode l, VarBinNode r)","chapter-5","Binary Trees"
"{ operator = op; left = l; right = r; }","chapter-5","Binary Trees"
"public boolean isLeaf() { return false; }","chapter-5","Binary Trees"
"public VarBinNode leftchild() { return left; }","chapter-5","Binary Trees"
"public VarBinNode rightchild() { return right; }","chapter-5","Binary Trees"
"public Character value() { return operator; }","chapter-5","Binary Trees"
"public void traverse() {","chapter-5","Binary Trees"
"Visit.VisitInternalNode(operator);","chapter-5","Binary Trees"
"if (left != null) left.traverse();","chapter-5","Binary Trees"
"if (right != null) right.traverse();","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"/** Preorder traversal */","chapter-5","Binary Trees"
"public static void traverse(VarBinNode rt) {","chapter-5","Binary Trees"
"if (rt != null) rt.traverse();","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"Figure 5.11 A second implementation for separate internal and leaf node repre-","chapter-5","Binary Trees"
"sentations using Java class inheritance and virtual functions using the composite","chapter-5","Binary Trees"
"design pattern. Here, the functionality of traverse is embedded into the node","chapter-5","Binary Trees"
"subclasses.","chapter-5","Binary Trees"
"160 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"5.3.2 Space Requirements","chapter-5","Binary Trees"
"This section presents techniques for calculating the amount of overhead required by","chapter-5","Binary Trees"
"a binary tree implementation. Recall that overhead is the amount of space necessary","chapter-5","Binary Trees"
"to maintain the data structure. In other words, it is any space not used to store","chapter-5","Binary Trees"
"data records. The amount of overhead depends on several factors including which","chapter-5","Binary Trees"
"nodes store data values (all nodes, or just the leaves), whether the leaves store child","chapter-5","Binary Trees"
"pointers, and whether the tree is a full binary tree.","chapter-5","Binary Trees"
"In a simple pointer-based implementation for the binary tree such as that of","chapter-5","Binary Trees"
"Figure 5.7, every node has two pointers to its children (even when the children are","chapter-5","Binary Trees"
"null). This implementation requires total space amounting to n(2P + D) for a","chapter-5","Binary Trees"
"tree of n nodes. Here, P stands for the amount of space required by a pointer, and","chapter-5","Binary Trees"
"D stands for the amount of space required by a data value. The total overhead space","chapter-5","Binary Trees"
"will be 2P n for the entire tree. Thus, the overhead fraction will be 2P/(2P + D).","chapter-5","Binary Trees"
"The actual value for this expression depends on the relative size of pointers versus","chapter-5","Binary Trees"
"data fields. If we arbitrarily assume that P = D, then a full tree has about two","chapter-5","Binary Trees"
"thirds of its total space taken up in overhead. Worse yet, Theorem 5.2 tells us that","chapter-5","Binary Trees"
"about half of the pointers are “wasted” null values that serve only to indicate tree","chapter-5","Binary Trees"
"structure, but which do not provide access to new data.","chapter-5","Binary Trees"
"In Java, the most typical implementation is not to store any actual data in a","chapter-5","Binary Trees"
"node, but rather a reference to the data record. In this case, each node will typically","chapter-5","Binary Trees"
"store three pointers, all of which are overhead, resulting in an overhead fraction of","chapter-5","Binary Trees"
"3P/(3P + D).","chapter-5","Binary Trees"
"If only leaves store data values, then the fraction of total space devoted to over-","chapter-5","Binary Trees"
"head depends on whether the tree is full. If the tree is not full, then conceivably","chapter-5","Binary Trees"
"there might only be one leaf node at the end of a series of internal nodes. Thus,","chapter-5","Binary Trees"
"the overhead can be an arbitrarily high percentage for non-full binary trees. The","chapter-5","Binary Trees"
"overhead fraction drops as the tree becomes closer to full, being lowest when the","chapter-5","Binary Trees"
"tree is truly full. In this case, about one half of the nodes are internal.","chapter-5","Binary Trees"
"Great savings can be had by eliminating the pointers from leaf nodes in full bi-","chapter-5","Binary Trees"
"nary trees. Again assume the tree stores a reference to the data field. Because about","chapter-5","Binary Trees"
"half of the nodes are leaves and half internal nodes, and because only internal nodes","chapter-5","Binary Trees"
"now have child pointers, the overhead fraction in this case will be approximately","chapter-5","Binary Trees"
"n","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
"(2P)","chapter-5","Binary Trees"
"n","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
"(2P) + Dn","chapter-5","Binary Trees"
"=","chapter-5","Binary Trees"
"P","chapter-5","Binary Trees"
"P + D","chapter-5","Binary Trees"
".","chapter-5","Binary Trees"
"If P = D, the overhead drops to about one half of the total space. However, if only","chapter-5","Binary Trees"
"leaf nodes store useful information, the overhead fraction for this implementation is","chapter-5","Binary Trees"
"actually three quarters of the total space, because half of the “data” space is unused.","chapter-5","Binary Trees"
"If a full binary tree needs to store data only at the leaf nodes, a better imple-","chapter-5","Binary Trees"
"mentation would have the internal nodes store two pointers and no data field while","chapter-5","Binary Trees"
"the leaf nodes store only a reference to the data field. This implementation requires","chapter-5","Binary Trees"
"Sec. 5.3 Binary Tree Node Implementations 161","chapter-5","Binary Trees"
"n","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
"2P+","chapter-5","Binary Trees"
"n","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
"(p+d) units of space. If P = D, then the overhead is 3P/(3P+D) = 3/4.","chapter-5","Binary Trees"
"It might seem counter-intuitive that the overhead ratio has gone up while the total","chapter-5","Binary Trees"
"amount of space has gone down. The reason is because we have changed our defini-","chapter-5","Binary Trees"
"tion of “data” to refer only to what is stored in the leaf nodes, so while the overhead","chapter-5","Binary Trees"
"fraction is higher, it is from a total storage requirement that is lower.","chapter-5","Binary Trees"
"There is one serious flaw with this analysis. When using separate implemen-","chapter-5","Binary Trees"
"tations for internal and leaf nodes, there must be a way to distinguish between","chapter-5","Binary Trees"
"the node types. When separate node types are implemented via Java subclasses,","chapter-5","Binary Trees"
"the runtime environment stores information with each object allowing it to deter-","chapter-5","Binary Trees"
"mine, for example, the correct subclass to use when the isLeaf virtual function is","chapter-5","Binary Trees"
"called. Thus, each node requires additional space. Only one bit is truly necessary","chapter-5","Binary Trees"
"to distinguish the two possibilities. In rare applications where space is a critical","chapter-5","Binary Trees"
"resource, implementors can often find a spare bit within the node’s value field in","chapter-5","Binary Trees"
"which to store the node type indicator. An alternative is to use a spare bit within","chapter-5","Binary Trees"
"a node pointer to indicate node type. For example, this is often possible when the","chapter-5","Binary Trees"
"compiler requires that structures and objects start on word boundaries, leaving the","chapter-5","Binary Trees"
"last bit of a pointer value always zero. Thus, this bit can be used to store the node-","chapter-5","Binary Trees"
"type flag and is reset to zero before the pointer is dereferenced. Another alternative","chapter-5","Binary Trees"
"when the leaf value field is smaller than a pointer is to replace the pointer to a leaf","chapter-5","Binary Trees"
"with that leaf’s value. When space is limited, such techniques can make the differ-","chapter-5","Binary Trees"
"ence between success and failure. In any other situation, such “bit packing” tricks","chapter-5","Binary Trees"
"should be avoided because they are difficult to debug and understand at best, and","chapter-5","Binary Trees"
"are often machine dependent at worst.2","chapter-5","Binary Trees"
"5.3.3 Array Implementation for Complete Binary Trees","chapter-5","Binary Trees"
"The previous section points out that a large fraction of the space in a typical binary","chapter-5","Binary Trees"
"tree node implementation is devoted to structural overhead, not to storing data.","chapter-5","Binary Trees"
"This section presents a simple, compact implementation for complete binary trees.","chapter-5","Binary Trees"
"Recall that complete binary trees have all levels except the bottom filled out com-","chapter-5","Binary Trees"
"pletely, and the bottom level has all of its nodes filled in from left to right. Thus,","chapter-5","Binary Trees"
"a complete binary tree of n nodes has only one possible shape. You might think","chapter-5","Binary Trees"
"that a complete binary tree is such an unusual occurrence that there is no reason","chapter-5","Binary Trees"
"to develop a special implementation for it. However, the complete binary tree has","chapter-5","Binary Trees"
"practical uses, the most important being the heap data structure discussed in Sec-","chapter-5","Binary Trees"
"tion 5.5. Heaps are often used to implement priority queues (Section 5.5) and for","chapter-5","Binary Trees"
"external sorting algorithms (Section 8.5.2).","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
"In the early to mid 1980s, I worked on a Geographic Information System that stored spatial data","chapter-5","Binary Trees"
"in quadtrees (see Section 13.3). At the time space was a critical resource, so we used a bit-packing","chapter-5","Binary Trees"
"approach where we stored the nodetype flag as the last bit in the parent node’s pointer. This worked","chapter-5","Binary Trees"
"perfectly on various 32-bit workstations. Unfortunately, in those days IBM PC-compatibles used","chapter-5","Binary Trees"
"16-bit pointers. We never did figure out how to port our code to the 16-bit machine.","chapter-5","Binary Trees"
"162 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"5 6","chapter-5","Binary Trees"
"7 8 9 10 11","chapter-5","Binary Trees"
"(a)","chapter-5","Binary Trees"
"4","chapter-5","Binary Trees"
"0","chapter-5","Binary Trees"
"1","chapter-5","Binary Trees"
"3","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
"Position 0 1 2 3 4 5 6 7 8 9 10 11","chapter-5","Binary Trees"
"Parent – 0 0 1 1 2 2 3 3 4 4 5","chapter-5","Binary Trees"
"Left Child 1 3 5 7 9 11 – – – – – –","chapter-5","Binary Trees"
"Right Child 2 4 6 8 10 – – – – – – –","chapter-5","Binary Trees"
"Left Sibling – – 1 – 3 – 5 – 7 – 9 –","chapter-5","Binary Trees"
"Right Sibling – 2 – 4 – 6 – 8 – 10 – –","chapter-5","Binary Trees"
"(b)","chapter-5","Binary Trees"
"Figure 5.12 A complete binary tree and its array implementation. (a) The com-","chapter-5","Binary Trees"
"plete binary tree with twelve nodes. Each node has been labeled with its position","chapter-5","Binary Trees"
"in the tree. (b) The positions for the relatives of each node. A dash indicates that","chapter-5","Binary Trees"
"the relative does not exist.","chapter-5","Binary Trees"
"We begin by assigning numbers to the node positions in the complete binary","chapter-5","Binary Trees"
"tree, level by level, from left to right as shown in Figure 5.12(a). An array can","chapter-5","Binary Trees"
"store the tree’s data values efficiently, placing each data value in the array position","chapter-5","Binary Trees"
"corresponding to that node’s position within the tree. Figure 5.12(b) lists the array","chapter-5","Binary Trees"
"indices for the children, parent, and siblings of each node in Figure 5.12(a). From","chapter-5","Binary Trees"
"Figure 5.12(b), you should see a pattern regarding the positions of a node’s relatives","chapter-5","Binary Trees"
"within the array. Simple formulas can be derived for calculating the array index for","chapter-5","Binary Trees"
"each relative of a node r from r’s index. No explicit pointers are necessary to","chapter-5","Binary Trees"
"reach a node’s left or right child. This means there is no overhead to the array","chapter-5","Binary Trees"
"implementation if the array is selected to be of size n for a tree of n nodes.","chapter-5","Binary Trees"
"The formulae for calculating the array indices of the various relatives of a node","chapter-5","Binary Trees"
"are as follows. The total number of nodes in the tree is n. The index of the node in","chapter-5","Binary Trees"
"question is r, which must fall in the range 0 to n − 1.","chapter-5","Binary Trees"
"• Parent(r) = b(r − 1)/2c if r 6= 0.","chapter-5","Binary Trees"
"• Left child(r) = 2r + 1 if 2r + 1 < n.","chapter-5","Binary Trees"
"• Right child(r) = 2r + 2 if 2r + 2 < n.","chapter-5","Binary Trees"
"• Left sibling(r) = r − 1 if r is even.","chapter-5","Binary Trees"
"Sec. 5.4 Binary Search Trees 163","chapter-5","Binary Trees"
"• Right sibling(r) = r + 1 if r is odd and r + 1 < n.","chapter-5","Binary Trees"
"5.4 Binary Search Trees","chapter-5","Binary Trees"
"Section 4.4 presented the dictionary ADT, along with dictionary implementations","chapter-5","Binary Trees"
"based on sorted and unsorted lists. When implementing the dictionary with an","chapter-5","Binary Trees"
"unsorted list, inserting a new record into the dictionary can be performed quickly by","chapter-5","Binary Trees"
"putting it at the end of the list. However, searching an unsorted list for a particular","chapter-5","Binary Trees"
"record requires Θ(n) time in the average case. For a large database, this is probably","chapter-5","Binary Trees"
"much too slow. Alternatively, the records can be stored in a sorted list. If the list","chapter-5","Binary Trees"
"is implemented using a linked list, then no speedup to the search operation will","chapter-5","Binary Trees"
"result from storing the records in sorted order. On the other hand, if we use a sorted","chapter-5","Binary Trees"
"array-based list to implement the dictionary, then binary search can be used to find","chapter-5","Binary Trees"
"a record in only Θ(log n) time. However, insertion will now require Θ(n) time on","chapter-5","Binary Trees"
"average because, once the proper location for the new record in the sorted list has","chapter-5","Binary Trees"
"been found, many records might be shifted to make room for the new record.","chapter-5","Binary Trees"
"Is there some way to organize a collection of records so that inserting records","chapter-5","Binary Trees"
"and searching for records can both be done quickly? This section presents the","chapter-5","Binary Trees"
"binary search tree (BST), which allows an improved solution to this problem.","chapter-5","Binary Trees"
"A BST is a binary tree that conforms to the following condition, known as","chapter-5","Binary Trees"
"the Binary Search Tree Property: All nodes stored in the left subtree of a node","chapter-5","Binary Trees"
"whose key value is K have key values less than K. All nodes stored in the right","chapter-5","Binary Trees"
"subtree of a node whose key value is K have key values greater than or equal to K.","chapter-5","Binary Trees"
"Figure 5.13 shows two BSTs for a collection of values. One consequence of the","chapter-5","Binary Trees"
"Binary Search Tree Property is that if the BST nodes are printed using an inorder","chapter-5","Binary Trees"
"traversal (see Section 5.2), the resulting enumeration will be in sorted order from","chapter-5","Binary Trees"
"lowest to highest.","chapter-5","Binary Trees"
"Figure 5.14 shows a class declaration for the BST that implements the dictio-","chapter-5","Binary Trees"
"nary ADT. The public member functions include those required by the dictionary","chapter-5","Binary Trees"
"ADT, along with a constructor and destructor. Recall from the discussion in Sec-","chapter-5","Binary Trees"
"tion 4.4 that there are various ways to deal with keys and comparing records (three","chapter-5","Binary Trees"
"approaches being key/value pairs, a special comparison method such as using the","chapter-5","Binary Trees"
"Comparator class, and passing in a comparator function). Our BST implementa-","chapter-5","Binary Trees"
"tion will handle comparison by explicitly storing a key separate from the data value","chapter-5","Binary Trees"
"at each node of the tree.","chapter-5","Binary Trees"
"To find a record with key value K in a BST, begin at the root. If the root stores","chapter-5","Binary Trees"
"a record with key value K, then the search is over. If not, then we must search","chapter-5","Binary Trees"
"deeper in the tree. What makes the BST efficient during search is that we need","chapter-5","Binary Trees"
"search only one of the node’s two subtrees. If K is less than the root node’s key","chapter-5","Binary Trees"
"value, we search only the left subtree. If K is greater than the root node’s key","chapter-5","Binary Trees"
"value, we search only the right subtree. This process continues until a record with","chapter-5","Binary Trees"
"164 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"7","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
"32","chapter-5","Binary Trees"
"42","chapter-5","Binary Trees"
"40","chapter-5","Binary Trees"
"120","chapter-5","Binary Trees"
"7 42","chapter-5","Binary Trees"
"(a)","chapter-5","Binary Trees"
"37","chapter-5","Binary Trees"
"42","chapter-5","Binary Trees"
"(b)","chapter-5","Binary Trees"
"24","chapter-5","Binary Trees"
"120","chapter-5","Binary Trees"
"42","chapter-5","Binary Trees"
"24","chapter-5","Binary Trees"
"2 32","chapter-5","Binary Trees"
"37","chapter-5","Binary Trees"
"40","chapter-5","Binary Trees"
"Figure 5.13 Two Binary Search Trees for a collection of values. Tree (a) results","chapter-5","Binary Trees"
"if values are inserted in the order 37, 24, 42, 7, 2, 40, 42, 32, 120. Tree (b) results","chapter-5","Binary Trees"
"if the same values are inserted in the order 120, 42, 42, 7, 2, 32, 37, 24, 40.","chapter-5","Binary Trees"
"key value K is found, or we reach a leaf node. If we reach a leaf node without","chapter-5","Binary Trees"
"encountering K, then no record exists in the BST whose key value is K.","chapter-5","Binary Trees"
"Example 5.5 Consider searching for the node with key value 32 in the","chapter-5","Binary Trees"
"tree of Figure 5.13(a). Because 32 is less than the root value of 37, the","chapter-5","Binary Trees"
"search proceeds to the left subtree. Because 32 is greater than 24, we search","chapter-5","Binary Trees"
"in 24’s right subtree. At this point the node containing 32 is found. If","chapter-5","Binary Trees"
"the search value were 35, the same path would be followed to the node","chapter-5","Binary Trees"
"containing 32. Because this node has no children, we know that 35 is not","chapter-5","Binary Trees"
"in the BST.","chapter-5","Binary Trees"
"Notice that in Figure 5.14, public member function find calls private member","chapter-5","Binary Trees"
"function findhelp. Method find takes the search key as an explicit parameter","chapter-5","Binary Trees"
"and its BST as an implicit parameter, and returns the record that matches the key.","chapter-5","Binary Trees"
"However, the find operation is most easily implemented as a recursive function","chapter-5","Binary Trees"
"whose parameters are the root of a subtree and the search key. Member findhelp","chapter-5","Binary Trees"
"has the desired form for this recursive subroutine and is implemented as follows.","chapter-5","Binary Trees"
"private E findhelp(BSTNode<Key,E> rt, Key k) {","chapter-5","Binary Trees"
"if (rt == null) return null;","chapter-5","Binary Trees"
"if (rt.key().compareTo(k) > 0)","chapter-5","Binary Trees"
"return findhelp(rt.left(), k);","chapter-5","Binary Trees"
"else if (rt.key().compareTo(k) == 0) return rt.element();","chapter-5","Binary Trees"
"else return findhelp(rt.right(), k);","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"Once the desired record is found, it is passed through return values up the chain of","chapter-5","Binary Trees"
"recursive calls. If a suitable record is not found, null is returned.","chapter-5","Binary Trees"
"Sec. 5.4 Binary Search Trees 165","chapter-5","Binary Trees"
"/** Binary Search Tree implementation for Dictionary ADT */","chapter-5","Binary Trees"
"class BST<Key extends Comparable<? super Key>, E>","chapter-5","Binary Trees"
"implements Dictionary<Key, E> {","chapter-5","Binary Trees"
"private BSTNode<Key,E> root; // Root of the BST","chapter-5","Binary Trees"
"private int nodecount; // Number of nodes in the BST","chapter-5","Binary Trees"
"/** Constructor */","chapter-5","Binary Trees"
"BST() { root = null; nodecount = 0; }","chapter-5","Binary Trees"
"/** Reinitialize tree */","chapter-5","Binary Trees"
"public void clear() { root = null; nodecount = 0; }","chapter-5","Binary Trees"
"/** Insert a record into the tree.","chapter-5","Binary Trees"
"@param k Key value of the record.","chapter-5","Binary Trees"
"@param e The record to insert. */","chapter-5","Binary Trees"
"public void insert(Key k, E e) {","chapter-5","Binary Trees"
"root = inserthelp(root, k, e);","chapter-5","Binary Trees"
"nodecount++;","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"/** Remove a record from the tree.","chapter-5","Binary Trees"
"@param k Key value of record to remove.","chapter-5","Binary Trees"
"@return The record removed, null if there is none. */","chapter-5","Binary Trees"
"public E remove(Key k) {","chapter-5","Binary Trees"
"E temp = findhelp(root, k); // First find it","chapter-5","Binary Trees"
"if (temp != null) {","chapter-5","Binary Trees"
"root = removehelp(root, k); // Now remove it","chapter-5","Binary Trees"
"nodecount--;","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"return temp;","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"/** Remove and return the root node from the dictionary.","chapter-5","Binary Trees"
"@return The record removed, null if tree is empty. */","chapter-5","Binary Trees"
"public E removeAny() {","chapter-5","Binary Trees"
"if (root == null) return null;","chapter-5","Binary Trees"
"E temp = root.element();","chapter-5","Binary Trees"
"root = removehelp(root, root.key());","chapter-5","Binary Trees"
"nodecount--;","chapter-5","Binary Trees"
"return temp;","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"/** @return Record with key value k, null if none exist.","chapter-5","Binary Trees"
"@param k The key value to find. */","chapter-5","Binary Trees"
"public E find(Key k) { return findhelp(root, k); }","chapter-5","Binary Trees"
"/** @return The number of records in the dictionary. */","chapter-5","Binary Trees"
"public int size() { return nodecount; }","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"Figure 5.14 The binary search tree implementation.","chapter-5","Binary Trees"
"166 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"37","chapter-5","Binary Trees"
"24","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
"32","chapter-5","Binary Trees"
"35","chapter-5","Binary Trees"
"42","chapter-5","Binary Trees"
"40 42","chapter-5","Binary Trees"
"120","chapter-5","Binary Trees"
"7","chapter-5","Binary Trees"
"Figure 5.15 An example of BST insertion. A record with value 35 is inserted","chapter-5","Binary Trees"
"into the BST of Figure 5.13(a). The node with value 32 becomes the parent of the","chapter-5","Binary Trees"
"new node containing 35.","chapter-5","Binary Trees"
"Inserting a record with key value k requires that we first find where that record","chapter-5","Binary Trees"
"would have been if it were in the tree. This takes us to either a leaf node, or to an","chapter-5","Binary Trees"
"internal node with no child in the appropriate direction.3 Call this node R","chapter-5","Binary Trees"
"0","chapter-5","Binary Trees"
". We then","chapter-5","Binary Trees"
"add a new node containing the new record as a child of R","chapter-5","Binary Trees"
"0","chapter-5","Binary Trees"
". Figure 5.15 illustrates","chapter-5","Binary Trees"
"this operation. The value 35 is added as the right child of the node with value 32.","chapter-5","Binary Trees"
"Here is the implementation for inserthelp:","chapter-5","Binary Trees"
"/** @return The current subtree, modified to contain","chapter-5","Binary Trees"
"the new item */","chapter-5","Binary Trees"
"private BSTNode<Key,E> inserthelp(BSTNode<Key,E> rt,","chapter-5","Binary Trees"
"Key k, E e) {","chapter-5","Binary Trees"
"if (rt == null) return new BSTNode<Key,E>(k, e);","chapter-5","Binary Trees"
"if (rt.key().compareTo(k) > 0)","chapter-5","Binary Trees"
"rt.setLeft(inserthelp(rt.left(), k, e));","chapter-5","Binary Trees"
"else","chapter-5","Binary Trees"
"rt.setRight(inserthelp(rt.right(), k, e));","chapter-5","Binary Trees"
"return rt;","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"You should pay careful attention to the implementation for inserthelp.","chapter-5","Binary Trees"
"Note that inserthelp returns a pointer to a BSTNode. What is being returned","chapter-5","Binary Trees"
"is a subtree identical to the old subtree, except that it has been modified to contain","chapter-5","Binary Trees"
"the new record being inserted. Each node along a path from the root to the parent","chapter-5","Binary Trees"
"of the new node added to the tree will have its appropriate child pointer assigned","chapter-5","Binary Trees"
"to it. Except for the last node in the path, none of these nodes will actually change","chapter-5","Binary Trees"
"their child’s pointer value. In that sense, many of the assignments seem redundant.","chapter-5","Binary Trees"
"However, the cost of these additional assignments is worth paying to keep the inser-","chapter-5","Binary Trees"
"tion process simple. The alternative is to check if a given assignment is necessary,","chapter-5","Binary Trees"
"which is probably more expensive than the assignment!","chapter-5","Binary Trees"
"3This assumes that no node has a key value equal to the one being inserted. If we find a node that","chapter-5","Binary Trees"
"duplicates the key value to be inserted, we have two options. If the application does not allow nodes","chapter-5","Binary Trees"
"with equal keys, then this insertion should be treated as an error (or ignored). If duplicate keys are","chapter-5","Binary Trees"
"allowed, our convention will be to insert the duplicate in the right subtree.","chapter-5","Binary Trees"
"Sec. 5.4 Binary Search Trees 167","chapter-5","Binary Trees"
"The shape of a BST depends on the order in which elements are inserted. A new","chapter-5","Binary Trees"
"element is added to the BST as a new leaf node, potentially increasing the depth of","chapter-5","Binary Trees"
"the tree. Figure 5.13 illustrates two BSTs for a collection of values. It is possible","chapter-5","Binary Trees"
"for the BST containing n nodes to be a chain of nodes with height n. This would","chapter-5","Binary Trees"
"happen if, for example, all elements were inserted in sorted order. In general, it is","chapter-5","Binary Trees"
"preferable for a BST to be as shallow as possible. This keeps the average cost of a","chapter-5","Binary Trees"
"BST operation low.","chapter-5","Binary Trees"
"Removing a node from a BST is a bit trickier than inserting a node, but it is not","chapter-5","Binary Trees"
"complicated if all of the possible cases are considered individually. Before tackling","chapter-5","Binary Trees"
"the general node removal process, let us first discuss how to remove from a given","chapter-5","Binary Trees"
"subtree the node with the smallest key value. This routine will be used later by the","chapter-5","Binary Trees"
"general node removal function. To remove the node with the minimum key value","chapter-5","Binary Trees"
"from a subtree, first find that node by continuously moving down the left link until","chapter-5","Binary Trees"
"there is no further left link to follow. Call this node S. To remove S, simply have","chapter-5","Binary Trees"
"the parent of S change its pointer to point to the right child of S. We know that S","chapter-5","Binary Trees"
"has no left child (because if S did have a left child, S would not be the node with","chapter-5","Binary Trees"
"minimum key value). Thus, changing the pointer as described will maintain a BST,","chapter-5","Binary Trees"
"with S removed. The code for this method, named deletemin, is as follows:","chapter-5","Binary Trees"
"private BSTNode<Key,E> deletemin(BSTNode<Key,E> rt) {","chapter-5","Binary Trees"
"if (rt.left() == null) return rt.right();","chapter-5","Binary Trees"
"rt.setLeft(deletemin(rt.left()));","chapter-5","Binary Trees"
"return rt;","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"Example 5.6 Figure 5.16 illustrates the deletemin process. Beginning","chapter-5","Binary Trees"
"at the root node with value 10, deletemin follows the left link until there","chapter-5","Binary Trees"
"is no further left link, in this case reaching the node with value 5. The node","chapter-5","Binary Trees"
"with value 10 is changed to point to the right child of the node containing","chapter-5","Binary Trees"
"the minimum value. This is indicated in Figure 5.16 by a dashed line.","chapter-5","Binary Trees"
"A pointer to the node containing the minimum-valued element is stored in pa-","chapter-5","Binary Trees"
"rameter S. The return value of the deletemin method is the subtree of the cur-","chapter-5","Binary Trees"
"rent node with the minimum-valued node in the subtree removed. As with method","chapter-5","Binary Trees"
"inserthelp, each node on the path back to the root has its left child pointer","chapter-5","Binary Trees"
"reassigned to the subtree resulting from its call to the deletemin method.","chapter-5","Binary Trees"
"A useful companion method is getmin which returns a reference to the node","chapter-5","Binary Trees"
"containing the minimum value in the subtree.","chapter-5","Binary Trees"
"private BSTNode<Key,E> getmin(BSTNode<Key,E> rt) {","chapter-5","Binary Trees"
"if (rt.left() == null) return rt;","chapter-5","Binary Trees"
"return getmin(rt.left());","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"168 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"9","chapter-5","Binary Trees"
"5 20","chapter-5","Binary Trees"
"5","chapter-5","Binary Trees"
"10","chapter-5","Binary Trees"
"subroot","chapter-5","Binary Trees"
"Figure 5.16 An example of deleting the node with minimum value. In this tree,","chapter-5","Binary Trees"
"the node with minimum value, 5, is the left child of the root. Thus, the root’s","chapter-5","Binary Trees"
"left pointer is changed to point to 5’s right child.","chapter-5","Binary Trees"
"Removing a node with given key value R from the BST requires that we first","chapter-5","Binary Trees"
"find R and then remove it from the tree. So, the first part of the remove operation","chapter-5","Binary Trees"
"is a search to find R. Once R is found, there are several possibilities. If R has no","chapter-5","Binary Trees"
"children, then R’s parent has its pointer set to null. If R has one child, then R’s","chapter-5","Binary Trees"
"parent has its pointer set to R’s child (similar to deletemin). The problem comes","chapter-5","Binary Trees"
"if R has two children. One simple approach, though expensive, is to set R’s parent","chapter-5","Binary Trees"
"to point to one of R’s subtrees, and then reinsert the remaining subtree’s nodes one","chapter-5","Binary Trees"
"at a time. A better alternative is to find a value in one of the subtrees that can","chapter-5","Binary Trees"
"replace the value in R.","chapter-5","Binary Trees"
"Thus, the question becomes: Which value can substitute for the one being re-","chapter-5","Binary Trees"
"moved? It cannot be any arbitrary value, because we must preserve the BST prop-","chapter-5","Binary Trees"
"erty without making major changes to the structure of the tree. Which value is","chapter-5","Binary Trees"
"most like the one being removed? The answer is the least key value greater than","chapter-5","Binary Trees"
"(or equal to) the one being removed, or else the greatest key value less than the one","chapter-5","Binary Trees"
"being removed. If either of these values replace the one being removed, then the","chapter-5","Binary Trees"
"BST property is maintained.","chapter-5","Binary Trees"
"Example 5.7 Assume that we wish to remove the value 37 from the BST","chapter-5","Binary Trees"
"of Figure 5.13(a). Instead of removing the root node, we remove the node","chapter-5","Binary Trees"
"with the least value in the right subtree (using the deletemin operation).","chapter-5","Binary Trees"
"This value can then replace the value in the root. In this example we first","chapter-5","Binary Trees"
"remove the node with value 40, because it contains the least value in the","chapter-5","Binary Trees"
"right subtree. We then substitute 40 as the new value for the root node.","chapter-5","Binary Trees"
"Figure 5.17 illustrates this process.","chapter-5","Binary Trees"
"When duplicate node values do not appear in the tree, it makes no difference","chapter-5","Binary Trees"
"whether the replacement is the greatest value from the left subtree or the least value","chapter-5","Binary Trees"
"from the right subtree. If duplicates are stored, then we must select the replacement","chapter-5","Binary Trees"
"Sec. 5.4 Binary Search Trees 169","chapter-5","Binary Trees"
"37 40","chapter-5","Binary Trees"
"24","chapter-5","Binary Trees"
"7 32","chapter-5","Binary Trees"
"42","chapter-5","Binary Trees"
"40 42","chapter-5","Binary Trees"
"2 120","chapter-5","Binary Trees"
"Figure 5.17 An example of removing the value 37 from the BST. The node","chapter-5","Binary Trees"
"containing this value has two children. We replace value 37 with the least value","chapter-5","Binary Trees"
"from the node’s right subtree, in this case 40.","chapter-5","Binary Trees"
"/** Remove a node with key value k","chapter-5","Binary Trees"
"@return The tree with the node removed */","chapter-5","Binary Trees"
"private BSTNode<Key,E> removehelp(BSTNode<Key,E> rt,Key k) {","chapter-5","Binary Trees"
"if (rt == null) return null;","chapter-5","Binary Trees"
"if (rt.key().compareTo(k) > 0)","chapter-5","Binary Trees"
"rt.setLeft(removehelp(rt.left(), k));","chapter-5","Binary Trees"
"else if (rt.key().compareTo(k) < 0)","chapter-5","Binary Trees"
"rt.setRight(removehelp(rt.right(), k));","chapter-5","Binary Trees"
"else { // Found it","chapter-5","Binary Trees"
"if (rt.left() == null) return rt.right();","chapter-5","Binary Trees"
"else if (rt.right() == null) return rt.left();","chapter-5","Binary Trees"
"else { // Two children","chapter-5","Binary Trees"
"BSTNode<Key,E> temp = getmin(rt.right());","chapter-5","Binary Trees"
"rt.setElement(temp.element());","chapter-5","Binary Trees"
"rt.setKey(temp.key());","chapter-5","Binary Trees"
"rt.setRight(deletemin(rt.right()));","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"return rt;","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"Figure 5.18 Implementation for the BST removehelp method.","chapter-5","Binary Trees"
"from the right subtree. To see why, call the greatest value in the left subtree G.","chapter-5","Binary Trees"
"If multiple nodes in the left subtree have value G, selecting G as the replacement","chapter-5","Binary Trees"
"value for the root of the subtree will result in a tree with equal values to the left of","chapter-5","Binary Trees"
"the node now containing G. Precisely this situation occurs if we replace value 120","chapter-5","Binary Trees"
"with the greatest value in the left subtree of Figure 5.13(b). Selecting the least value","chapter-5","Binary Trees"
"from the right subtree does not have a similar problem, because it does not violate","chapter-5","Binary Trees"
"the Binary Search Tree Property if equal values appear in the right subtree.","chapter-5","Binary Trees"
"From the above, we see that if we want to remove the record stored in a node","chapter-5","Binary Trees"
"with two children, then we simply call deletemin on the node’s right subtree","chapter-5","Binary Trees"
"and substitute the record returned for the record being removed. Figure 5.18 shows","chapter-5","Binary Trees"
"an implementation for removehelp.","chapter-5","Binary Trees"
"The cost for findhelp and inserthelp is the depth of the node found or","chapter-5","Binary Trees"
"inserted. The cost for removehelp is the depth of the node being removed, or","chapter-5","Binary Trees"
"170 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"in the case when this node has two children, the depth of the node with smallest","chapter-5","Binary Trees"
"value in its right subtree. Thus, in the worst case, the cost for any one of these","chapter-5","Binary Trees"
"operations is the depth of the deepest node in the tree. This is why it is desirable to","chapter-5","Binary Trees"
"keep BSTs balanced, that is, with least possible height. If a binary tree is balanced,","chapter-5","Binary Trees"
"then the height for a tree of n nodes is approximately log n. However, if the tree","chapter-5","Binary Trees"
"is completely unbalanced, for example in the shape of a linked list, then the height","chapter-5","Binary Trees"
"for a tree with n nodes can be as great as n. Thus, a balanced BST will in the","chapter-5","Binary Trees"
"average case have operations costing Θ(log n), while a badly unbalanced BST can","chapter-5","Binary Trees"
"have operations in the worst case costing Θ(n). Consider the situation where we","chapter-5","Binary Trees"
"construct a BST of n nodes by inserting records one at a time. If we are fortunate","chapter-5","Binary Trees"
"to have them arrive in an order that results in a balanced tree (a “random” order is","chapter-5","Binary Trees"
"likely to be good enough for this purpose), then each insertion will cost on average","chapter-5","Binary Trees"
"Θ(log n), for a total cost of Θ(n log n). However, if the records are inserted in","chapter-5","Binary Trees"
"order of increasing value, then the resulting tree will be a chain of height n. The","chapter-5","Binary Trees"
"cost of insertion in this case will be Pn","chapter-5","Binary Trees"
"i=1 i = Θ(n","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
").","chapter-5","Binary Trees"
"Traversing a BST costs Θ(n) regardless of the shape of the tree. Each node is","chapter-5","Binary Trees"
"visited exactly once, and each child pointer is followed exactly once.","chapter-5","Binary Trees"
"Below is an example traversal, named printhelp. It performs an inorder","chapter-5","Binary Trees"
"traversal on the BST to print the node values in ascending order.","chapter-5","Binary Trees"
"private void printhelp(BSTNode<Key,E> rt) {","chapter-5","Binary Trees"
"if (rt == null) return;","chapter-5","Binary Trees"
"printhelp(rt.left());","chapter-5","Binary Trees"
"printVisit(rt.element());","chapter-5","Binary Trees"
"printhelp(rt.right());","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"While the BST is simple to implement and efficient when the tree is balanced,","chapter-5","Binary Trees"
"the possibility of its being unbalanced is a serious liability. There are techniques","chapter-5","Binary Trees"
"for organizing a BST to guarantee good performance. Two examples are the AVL","chapter-5","Binary Trees"
"tree and the splay tree of Section 13.2. Other search trees are guaranteed to remain","chapter-5","Binary Trees"
"balanced, such as the 2-3 tree of Section 10.4.","chapter-5","Binary Trees"
"5.5 Heaps and Priority Queues","chapter-5","Binary Trees"
"There are many situations, both in real life and in computing applications, where","chapter-5","Binary Trees"
"we wish to choose the next “most important” from a collection of people, tasks,","chapter-5","Binary Trees"
"or objects. For example, doctors in a hospital emergency room often choose to","chapter-5","Binary Trees"
"see next the “most critical” patient rather than the one who arrived first. When","chapter-5","Binary Trees"
"scheduling programs for execution in a multitasking operating system, at any given","chapter-5","Binary Trees"
"moment there might be several programs (usually called jobs) ready to run. The","chapter-5","Binary Trees"
"next job selected is the one with the highest priority. Priority is indicated by a","chapter-5","Binary Trees"
"particular value associated with the job (and might change while the job remains in","chapter-5","Binary Trees"
"the wait list).","chapter-5","Binary Trees"
"Sec. 5.5 Heaps and Priority Queues 171","chapter-5","Binary Trees"
"When a collection of objects is organized by importance or priority, we call","chapter-5","Binary Trees"
"this a priority queue. A normal queue data structure will not implement a prior-","chapter-5","Binary Trees"
"ity queue efficiently because search for the element with highest priority will take","chapter-5","Binary Trees"
"Θ(n) time. A list, whether sorted or not, will also require Θ(n) time for either in-","chapter-5","Binary Trees"
"sertion or removal. A BST that organizes records by priority could be used, with the","chapter-5","Binary Trees"
"total of n inserts and n remove operations requiring Θ(n log n) time in the average","chapter-5","Binary Trees"
"case. However, there is always the possibility that the BST will become unbal-","chapter-5","Binary Trees"
"anced, leading to bad performance. Instead, we would like to find a data structure","chapter-5","Binary Trees"
"that is guaranteed to have good performance for this special application.","chapter-5","Binary Trees"
"This section presents the heap4 data structure. A heap is defined by two prop-","chapter-5","Binary Trees"
"erties. First, it is a complete binary tree, so heaps are nearly always implemented","chapter-5","Binary Trees"
"using the array representation for complete binary trees presented in Section 5.3.3.","chapter-5","Binary Trees"
"Second, the values stored in a heap are partially ordered. This means that there is","chapter-5","Binary Trees"
"a relationship between the value stored at any node and the values of its children.","chapter-5","Binary Trees"
"There are two variants of the heap, depending on the definition of this relationship.","chapter-5","Binary Trees"
"A max-heap has the property that every node stores a value that is greater than","chapter-5","Binary Trees"
"or equal to the value of either of its children. Because the root has a value greater","chapter-5","Binary Trees"
"than or equal to its children, which in turn have values greater than or equal to their","chapter-5","Binary Trees"
"children, the root stores the maximum of all values in the tree.","chapter-5","Binary Trees"
"A min-heap has the property that every node stores a value that is less than","chapter-5","Binary Trees"
"or equal to that of its children. Because the root has a value less than or equal to","chapter-5","Binary Trees"
"its children, which in turn have values less than or equal to their children, the root","chapter-5","Binary Trees"
"stores the minimum of all values in the tree.","chapter-5","Binary Trees"
"Note that there is no necessary relationship between the value of a node and that","chapter-5","Binary Trees"
"of its sibling in either the min-heap or the max-heap. For example, it is possible that","chapter-5","Binary Trees"
"the values for all nodes in the left subtree of the root are greater than the values for","chapter-5","Binary Trees"
"every node of the right subtree. We can contrast BSTs and heaps by the strength of","chapter-5","Binary Trees"
"their ordering relationships. A BST defines a total order on its nodes in that, given","chapter-5","Binary Trees"
"the positions for any two nodes in the tree, the one to the “left” (equivalently, the","chapter-5","Binary Trees"
"one appearing earlier in an inorder traversal) has a smaller key value than the one","chapter-5","Binary Trees"
"to the “right.” In contrast, a heap implements a partial order. Given their positions,","chapter-5","Binary Trees"
"we can determine the relative order for the key values of two nodes in the heap only","chapter-5","Binary Trees"
"if one is a descendant of the other.","chapter-5","Binary Trees"
"Min-heaps and max-heaps both have their uses. For example, the Heapsort","chapter-5","Binary Trees"
"of Section 7.6 uses the max-heap, while the Replacement Selection algorithm of","chapter-5","Binary Trees"
"Section 8.5.2 uses a min-heap. The examples in the rest of this section will use a","chapter-5","Binary Trees"
"max-heap.","chapter-5","Binary Trees"
"Be careful not to confuse the logical representation of a heap with its physical","chapter-5","Binary Trees"
"implementation by means of the array-based complete binary tree. The two are not","chapter-5","Binary Trees"
"4The term “heap” is also sometimes used to refer to a memory pool. See Section 12.3.","chapter-5","Binary Trees"
"172 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"synonymous because the logical view of the heap is actually a tree structure, while","chapter-5","Binary Trees"
"the typical physical implementation uses an array.","chapter-5","Binary Trees"
"Figure 5.19 shows an implementation for heaps. The class is a generic with one","chapter-5","Binary Trees"
"type parameter, E, which defines the type for the data elements stored in the heap.","chapter-5","Binary Trees"
"E must extend the Comparable interface, and so we can use the compareTo","chapter-5","Binary Trees"
"method for comparing records in the heap.","chapter-5","Binary Trees"
"This class definition makes two concessions to the fact that an array-based im-","chapter-5","Binary Trees"
"plementation is used. First, heap nodes are indicated by their logical position within","chapter-5","Binary Trees"
"the heap rather than by a pointer to the node. In practice, the logical heap position","chapter-5","Binary Trees"
"corresponds to the identically numbered physical position in the array. Second, the","chapter-5","Binary Trees"
"constructor takes as input a pointer to the array to be used. This approach provides","chapter-5","Binary Trees"
"the greatest flexibility for using the heap because all data values can be loaded into","chapter-5","Binary Trees"
"the array directly by the client. The advantage of this comes during the heap con-","chapter-5","Binary Trees"
"struction phase, as explained below. The constructor also takes an integer parame-","chapter-5","Binary Trees"
"ter indicating the initial size of the heap (based on the number of elements initially","chapter-5","Binary Trees"
"loaded into the array) and a second integer parameter indicating the maximum size","chapter-5","Binary Trees"
"allowed for the heap (the size of the array).","chapter-5","Binary Trees"
"Method heapsize returns the current size of the heap. H.isLeaf(pos)","chapter-5","Binary Trees"
"returns true if position pos is a leaf in heap H, and false otherwise. Members","chapter-5","Binary Trees"
"leftchild, rightchild, and parent return the position (actually, the array","chapter-5","Binary Trees"
"index) for the left child, right child, and parent of the position passed, respectively.","chapter-5","Binary Trees"
"One way to build a heap is to insert the elements one at a time. Method insert","chapter-5","Binary Trees"
"will insert a new element V into the heap. You might expect the heap insertion pro-","chapter-5","Binary Trees"
"cess to be similar to the insert function for a BST, starting at the root and working","chapter-5","Binary Trees"
"down through the heap. However, this approach is not likely to work because the","chapter-5","Binary Trees"
"heap must maintain the shape of a complete binary tree. Equivalently, if the heap","chapter-5","Binary Trees"
"takes up the first n positions of its array prior to the call to insert, it must take","chapter-5","Binary Trees"
"up the first n + 1 positions after. To accomplish this, insert first places V at po-","chapter-5","Binary Trees"
"sition n of the array. Of course, V is unlikely to be in the correct position. To move","chapter-5","Binary Trees"
"V to the right place, it is compared to its parent’s value. If the value of V is less","chapter-5","Binary Trees"
"than or equal to the value of its parent, then it is in the correct place and the insert","chapter-5","Binary Trees"
"routine is finished. If the value of V is greater than that of its parent, then the two","chapter-5","Binary Trees"
"elements swap positions. From here, the process of comparing V to its (current)","chapter-5","Binary Trees"
"parent continues until V reaches its correct position.","chapter-5","Binary Trees"
"Since the heap is a complete binary tree, its height is guaranteed to be the","chapter-5","Binary Trees"
"minimum possible. In particular, a heap containing n nodes will have a height of","chapter-5","Binary Trees"
"Θ(log n). Intuitively, we can see that this must be true because each level that we","chapter-5","Binary Trees"
"add will slightly more than double the number of nodes in the tree (the ith level has","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
"i nodes, and the sum of the first i levels is 2","chapter-5","Binary Trees"
"i+1 − 1). Starting at 1, we can double","chapter-5","Binary Trees"
"only log n times to reach a value of n. To be precise, the height of a heap with n","chapter-5","Binary Trees"
"nodes is dlog(n + 1)e.","chapter-5","Binary Trees"
"Sec. 5.5 Heaps and Priority Queues 173","chapter-5","Binary Trees"
"/** Max-heap implementation */","chapter-5","Binary Trees"
"public class MaxHeap<E extends Comparable<? super E>> {","chapter-5","Binary Trees"
"private E[] Heap; // Pointer to the heap array","chapter-5","Binary Trees"
"private int size; // Maximum size of the heap","chapter-5","Binary Trees"
"private int n; // Number of things in heap","chapter-5","Binary Trees"
"/** Constructor supporting preloading of heap contents */","chapter-5","Binary Trees"
"public MaxHeap(E[] h, int num, int max)","chapter-5","Binary Trees"
"{ Heap = h; n = num; size = max; buildheap(); }","chapter-5","Binary Trees"
"/** @return Current size of the heap */","chapter-5","Binary Trees"
"public int heapsize() { return n; }","chapter-5","Binary Trees"
"/** @return True if pos a leaf position, false otherwise */","chapter-5","Binary Trees"
"public boolean isLeaf(int pos)","chapter-5","Binary Trees"
"{ return (pos >= n/2) && (pos < n); }","chapter-5","Binary Trees"
"/** @return Position for left child of pos */","chapter-5","Binary Trees"
"public int leftchild(int pos) {","chapter-5","Binary Trees"
"assert pos < n/2 : "Position has no left child";","chapter-5","Binary Trees"
"return 2*pos + 1;","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"/** @return Position for right child of pos */","chapter-5","Binary Trees"
"public int rightchild(int pos) {","chapter-5","Binary Trees"
"assert pos < (n-1)/2 : "Position has no right child";","chapter-5","Binary Trees"
"return 2*pos + 2;","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"/** @return Position for parent */","chapter-5","Binary Trees"
"public int parent(int pos) {","chapter-5","Binary Trees"
"assert pos > 0 : "Position has no parent";","chapter-5","Binary Trees"
"return (pos-1)/2;","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"/** Insert val into heap */","chapter-5","Binary Trees"
"public void insert(E val) {","chapter-5","Binary Trees"
"assert n < size : "Heap is full";","chapter-5","Binary Trees"
"int curr = n++;","chapter-5","Binary Trees"
"Heap[curr] = val; // Start at end of heap","chapter-5","Binary Trees"
"// Now sift up until curr’s parent’s key > curr’s key","chapter-5","Binary Trees"
"while ((curr != 0) &&","chapter-5","Binary Trees"
"(Heap[curr].compareTo(Heap[parent(curr)]) > 0)) {","chapter-5","Binary Trees"
"DSutil.swap(Heap, curr, parent(curr));","chapter-5","Binary Trees"
"curr = parent(curr);","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"Figure 5.19 An implementation for the heap.","chapter-5","Binary Trees"
"174 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"/** Heapify contents of Heap */","chapter-5","Binary Trees"
"public void buildheap()","chapter-5","Binary Trees"
"{ for (int i=n/2-1; i>=0; i--) siftdown(i); }","chapter-5","Binary Trees"
"/** Put element in its correct place */","chapter-5","Binary Trees"
"private void siftdown(int pos) {","chapter-5","Binary Trees"
"assert (pos >= 0) && (pos < n) : "Illegal heap position";","chapter-5","Binary Trees"
"while (!isLeaf(pos)) {","chapter-5","Binary Trees"
"int j = leftchild(pos);","chapter-5","Binary Trees"
"if ((j<(n-1)) && (Heap[j].compareTo(Heap[j+1]) < 0))","chapter-5","Binary Trees"
"j++; // j is now index of child with greater value","chapter-5","Binary Trees"
"if (Heap[pos].compareTo(Heap[j]) >= 0) return;","chapter-5","Binary Trees"
"DSutil.swap(Heap, pos, j);","chapter-5","Binary Trees"
"pos = j; // Move down","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"/** Remove and return maximum value */","chapter-5","Binary Trees"
"public E removemax() {","chapter-5","Binary Trees"
"assert n > 0 : "Removing from empty heap";","chapter-5","Binary Trees"
"DSutil.swap(Heap, 0, --n); // Swap maximum with last value","chapter-5","Binary Trees"
"if (n != 0) // Not on last element","chapter-5","Binary Trees"
"siftdown(0); // Put new heap root val in correct place","chapter-5","Binary Trees"
"return Heap[n];","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"/** Remove and return element at specified position */","chapter-5","Binary Trees"
"public E remove(int pos) {","chapter-5","Binary Trees"
"assert (pos >= 0) && (pos < n) : "Illegal heap position";","chapter-5","Binary Trees"
"if (pos == (n-1)) n--; // Last element, no work to be done","chapter-5","Binary Trees"
"else","chapter-5","Binary Trees"
"{","chapter-5","Binary Trees"
"DSutil.swap(Heap, pos, --n); // Swap with last value","chapter-5","Binary Trees"
"// If we just swapped in a big value, push it up","chapter-5","Binary Trees"
"while ((pos > 0) &&","chapter-5","Binary Trees"
"(Heap[pos].compareTo(Heap[parent(pos)]) > 0)) {","chapter-5","Binary Trees"
"DSutil.swap(Heap, pos, parent(pos));","chapter-5","Binary Trees"
"pos = parent(pos);","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"if (n != 0) siftdown(pos); // If it is little, push down","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"return Heap[n];","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"Figure 5.19 (continued)","chapter-5","Binary Trees"
"Sec. 5.5 Heaps and Priority Queues 175","chapter-5","Binary Trees"
"(a)","chapter-5","Binary Trees"
"6","chapter-5","Binary Trees"
"(b)","chapter-5","Binary Trees"
"4 5 6 7","chapter-5","Binary Trees"
"4 5 7","chapter-5","Binary Trees"
"2 3","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
"6","chapter-5","Binary Trees"
"6","chapter-5","Binary Trees"
"3 5","chapter-5","Binary Trees"
"1","chapter-5","Binary Trees"
"3","chapter-5","Binary Trees"
"7","chapter-5","Binary Trees"
"5","chapter-5","Binary Trees"
"4 2 1 3","chapter-5","Binary Trees"
"7","chapter-5","Binary Trees"
"4","chapter-5","Binary Trees"
"1","chapter-5","Binary Trees"
"1","chapter-5","Binary Trees"
"Figure 5.20 Two series of exchanges to build a max-heap. (a) This heap is built","chapter-5","Binary Trees"
"by a series of nine exchanges in the order (4-2), (4-1), (2-1), (5-2), (5-4), (6-3),","chapter-5","Binary Trees"
"(6-5), (7-5), (7-6). (b) This heap is built by a series of four exchanges in the order","chapter-5","Binary Trees"
"(5-2), (7-3), (7-1), (6-1).","chapter-5","Binary Trees"
"Each call to insert takes Θ(log n) time in the worst case, because the value","chapter-5","Binary Trees"
"being inserted can move at most the distance from the bottom of the tree to the top","chapter-5","Binary Trees"
"of the tree. Thus, to insert n values into the heap, if we insert them one at a time,","chapter-5","Binary Trees"
"will take Θ(n log n) time in the worst case.","chapter-5","Binary Trees"
"If all n values are available at the beginning of the building process, we can","chapter-5","Binary Trees"
"build the heap faster than just inserting the values into the heap one by one. Con-","chapter-5","Binary Trees"
"sider Figure 5.20(a), which shows one series of exchanges that could be used to","chapter-5","Binary Trees"
"build the heap. All exchanges are between a node and one of its children. The heap","chapter-5","Binary Trees"
"is formed as a result of this exchange process. The array for the right-hand tree of","chapter-5","Binary Trees"
"Figure 5.20(a) would appear as follows:","chapter-5","Binary Trees"
"7 4 6 1 2 3 5","chapter-5","Binary Trees"
"Figure 5.20(b) shows an alternate series of exchanges that also forms a heap,","chapter-5","Binary Trees"
"but much more efficiently. The equivalent array representation would be","chapter-5","Binary Trees"
"7 5 6 4 2 1 3","chapter-5","Binary Trees"
"From this example, it is clear that the heap for any given set of numbers is not","chapter-5","Binary Trees"
"unique, and we see that some rearrangements of the input values require fewer ex-","chapter-5","Binary Trees"
"changes than others to build the heap. So, how do we pick the best rearrangement?","chapter-5","Binary Trees"
"176 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"R","chapter-5","Binary Trees"
"H1 H2","chapter-5","Binary Trees"
"Figure 5.21 Final stage in the heap-building algorithm. Both subtrees of node R","chapter-5","Binary Trees"
"are heaps. All that remains is to push R down to its proper level in the heap.","chapter-5","Binary Trees"
"(a) (b) (c)","chapter-5","Binary Trees"
"5","chapter-5","Binary Trees"
"1","chapter-5","Binary Trees"
"7","chapter-5","Binary Trees"
"7","chapter-5","Binary Trees"
"5 1","chapter-5","Binary Trees"
"7","chapter-5","Binary Trees"
"5 6","chapter-5","Binary Trees"
"4 2 4 6 2 6 3 4 2 1 3 3","chapter-5","Binary Trees"
"Figure 5.22 The siftdown operation. The subtrees of the root are assumed to","chapter-5","Binary Trees"
"be heaps. (a) The partially completed heap. (b) Values 1 and 7 are swapped.","chapter-5","Binary Trees"
"(c) Values 1 and 6 are swapped to form the final heap.","chapter-5","Binary Trees"
"One good algorithm stems from induction. Suppose that the left and right sub-","chapter-5","Binary Trees"
"trees of the root are already heaps, and R is the name of the element at the root.","chapter-5","Binary Trees"
"This situation is illustrated by Figure 5.21. In this case there are two possibilities.","chapter-5","Binary Trees"
"(1) R has a value greater than or equal to its two children. In this case, construction","chapter-5","Binary Trees"
"is complete. (2) R has a value less than one or both of its children. In this case,","chapter-5","Binary Trees"
"R should be exchanged with the child that has greater value. The result will be a","chapter-5","Binary Trees"
"heap, except that R might still be less than one or both of its (new) children. In","chapter-5","Binary Trees"
"this case, we simply continue the process of “pushing down” R until it reaches a","chapter-5","Binary Trees"
"level where it is greater than its children, or is a leaf node. This process is imple-","chapter-5","Binary Trees"
"mented by the private method siftdown. The siftdown operation is illustrated by","chapter-5","Binary Trees"
"Figure 5.22.","chapter-5","Binary Trees"
"This approach assumes that the subtrees are already heaps, suggesting that a","chapter-5","Binary Trees"
"complete algorithm can be obtained by visiting the nodes in some order such that","chapter-5","Binary Trees"
"the children of a node are visited before the node itself. One simple way to do this","chapter-5","Binary Trees"
"is simply to work from the high index of the array to the low index. Actually, the","chapter-5","Binary Trees"
"build process need not visit the leaf nodes (they can never move down because they","chapter-5","Binary Trees"
"are already at the bottom), so the building algorithm can start in the middle of the","chapter-5","Binary Trees"
"array, with the first internal node. The exchanges shown in Figure 5.20(b) result","chapter-5","Binary Trees"
"from this process. Method buildHeap implements the building algorithm.","chapter-5","Binary Trees"
"What is the cost of buildHeap? Clearly it is the sum of the costs for the calls","chapter-5","Binary Trees"
"to siftdown. Each siftdown operation can cost at most the number of levels it","chapter-5","Binary Trees"
"Sec. 5.5 Heaps and Priority Queues 177","chapter-5","Binary Trees"
"takes for the node being sifted to reach the bottom of the tree. In any complete tree,","chapter-5","Binary Trees"
"approximately half of the nodes are leaves and so cannot be moved downward at","chapter-5","Binary Trees"
"all. One quarter of the nodes are one level above the leaves, and so their elements","chapter-5","Binary Trees"
"can move down at most one level. At each step up the tree we get half the number of","chapter-5","Binary Trees"
"nodes as were at the previous level, and an additional height of one. The maximum","chapter-5","Binary Trees"
"sum of total distances that elements can go is therefore","chapter-5","Binary Trees"
"log","chapter-5","Binary Trees"
"Xn","chapter-5","Binary Trees"
"i=1","chapter-5","Binary Trees"
"(i − 1) n","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
"i","chapter-5","Binary Trees"
"=","chapter-5","Binary Trees"
"n","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
"log","chapter-5","Binary Trees"
"Xn","chapter-5","Binary Trees"
"i=1","chapter-5","Binary Trees"
"i − 1","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
"i−1","chapter-5","Binary Trees"
".","chapter-5","Binary Trees"
"From Equation 2.9 we know that this summation has a closed-form solution of","chapter-5","Binary Trees"
"approximately 2, so this algorithm takes Θ(n) time in the worst case. This is far","chapter-5","Binary Trees"
"better than building the heap one element at a time, which would cost Θ(n log n)","chapter-5","Binary Trees"
"in the worst case. It is also faster than the Θ(n log n) average-case time and Θ(n","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
")","chapter-5","Binary Trees"
"worst-case time required to build the BST.","chapter-5","Binary Trees"
"Removing the maximum (root) value from a heap containing n elements re-","chapter-5","Binary Trees"
"quires that we maintain the complete binary tree shape, and that the remaining","chapter-5","Binary Trees"
"n − 1 node values conform to the heap property. We can maintain the proper shape","chapter-5","Binary Trees"
"by moving the element in the last position in the heap (the current last element in","chapter-5","Binary Trees"
"the array) to the root position. We now consider the heap to be one element smaller.","chapter-5","Binary Trees"
"Unfortunately, the new root value is probably not the maximum value in the new","chapter-5","Binary Trees"
"heap. This problem is easily solved by using siftdown to reorder the heap. Be-","chapter-5","Binary Trees"
"cause the heap is log n levels deep, the cost of deleting the maximum element is","chapter-5","Binary Trees"
"Θ(log n) in the average and worst cases.","chapter-5","Binary Trees"
"The heap is a natural implementation for the priority queue discussed at the","chapter-5","Binary Trees"
"beginning of this section. Jobs can be added to the heap (using their priority value","chapter-5","Binary Trees"
"as the ordering key) when needed. Method removemax can be called whenever a","chapter-5","Binary Trees"
"new job is to be executed.","chapter-5","Binary Trees"
"Some applications of priority queues require the ability to change the priority of","chapter-5","Binary Trees"
"an object already stored in the queue. This might require that the object’s position","chapter-5","Binary Trees"
"in the heap representation be updated. Unfortunately, a max-heap is not efficient","chapter-5","Binary Trees"
"when searching for an arbitrary value; it is only good for finding the maximum","chapter-5","Binary Trees"
"value. However, if we already know the index for an object within the heap, it is","chapter-5","Binary Trees"
"a simple matter to update its priority (including changing its position to maintain","chapter-5","Binary Trees"
"the heap property) or remove it. The remove method takes as input the position","chapter-5","Binary Trees"
"of the node to be removed from the heap. A typical implementation for priority","chapter-5","Binary Trees"
"queues requiring updating of priorities will need to use an auxiliary data structure","chapter-5","Binary Trees"
"that supports efficient search for objects (such as a BST). Records in the auxiliary","chapter-5","Binary Trees"
"data structure will store the object’s heap index, so that the object can be deleted","chapter-5","Binary Trees"
"from the heap and reinserted with its new priority (see Project 5.5). Sections 11.4.1","chapter-5","Binary Trees"
"and 11.5.1 present applications for a priority queue with priority updating.","chapter-5","Binary Trees"
"178 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"5.6 Huffman Coding Trees","chapter-5","Binary Trees"
"The space/time tradeoff principle from Section 3.9 states that one can often gain","chapter-5","Binary Trees"
"an improvement in space requirements in exchange for a penalty in running time.","chapter-5","Binary Trees"
"There are many situations where this is a desirable tradeoff. A typical example is","chapter-5","Binary Trees"
"storing files on disk. If the files are not actively used, the owner might wish to","chapter-5","Binary Trees"
"compress them to save space. Later, they can be uncompressed for use, which costs","chapter-5","Binary Trees"
"some time, but only once.","chapter-5","Binary Trees"
"We often represent a set of items in a computer program by assigning a unique","chapter-5","Binary Trees"
"code to each item. For example, the standard ASCII coding scheme assigns a","chapter-5","Binary Trees"
"unique eight-bit value to each character. It takes a certain minimum number of","chapter-5","Binary Trees"
"bits to provide unique codes for each character. For example, it takes dlog 128e or","chapter-5","Binary Trees"
"seven bits to provide the 128 unique codes needed to represent the 128 symbols of","chapter-5","Binary Trees"
"the ASCII character set.5","chapter-5","Binary Trees"
"The requirement for dlog ne bits to represent n unique code values assumes that","chapter-5","Binary Trees"
"all codes will be the same length, as are ASCII codes. This is called a fixed-length","chapter-5","Binary Trees"
"coding scheme. If all characters were used equally often, then a fixed-length coding","chapter-5","Binary Trees"
"scheme is the most space efficient method. However, you are probably aware that","chapter-5","Binary Trees"
"not all characters are used equally often in many applications. For example, the","chapter-5","Binary Trees"
"various letters in an English language document have greatly different frequencies","chapter-5","Binary Trees"
"of use.","chapter-5","Binary Trees"
"Figure 5.23 shows the relative frequencies of the letters of the alphabet. From","chapter-5","Binary Trees"
"this table we can see that the letter ‘E’ appears about 60 times more often than the","chapter-5","Binary Trees"
"letter ‘Z.’ In normal ASCII, the words “DEED” and “MUCK” require the same","chapter-5","Binary Trees"
"amount of space (four bytes). It would seem that words such as “DEED,” which","chapter-5","Binary Trees"
"are composed of relatively common letters, should be storable in less space than","chapter-5","Binary Trees"
"words such as “MUCK,” which are composed of relatively uncommon letters.","chapter-5","Binary Trees"
"If some characters are used more frequently than others, is it possible to take","chapter-5","Binary Trees"
"advantage of this fact and somehow assign them shorter codes? The price could","chapter-5","Binary Trees"
"be that other characters require longer codes, but this might be worthwhile if such","chapter-5","Binary Trees"
"characters appear rarely enough. This concept is at the heart of file compression","chapter-5","Binary Trees"
"techniques in common use today. The next section presents one such approach to","chapter-5","Binary Trees"
"assigning variable-length codes, called Huffman coding. While it is not commonly","chapter-5","Binary Trees"
"used in its simplest form for file compression (there are better methods), Huffman","chapter-5","Binary Trees"
"coding gives the flavor of such coding schemes. One motivation for studying Huff-","chapter-5","Binary Trees"
"man coding is because it provides our first opportunity to see a type of tree structure","chapter-5","Binary Trees"
"referred to as a search trie.","chapter-5","Binary Trees"
"5The ASCII standard is eight bits, not seven, even though there are only 128 characters repre-","chapter-5","Binary Trees"
"sented. The eighth bit is used either to check for transmission errors, or to support “extended” ASCII","chapter-5","Binary Trees"
"codes with an additional 128 characters.","chapter-5","Binary Trees"
"Sec. 5.6 Huffman Coding Trees 179","chapter-5","Binary Trees"
"Letter Frequency Letter Frequency","chapter-5","Binary Trees"
"A 77 N 67","chapter-5","Binary Trees"
"B 17 O 67","chapter-5","Binary Trees"
"C 32 P 20","chapter-5","Binary Trees"
"D 42 Q 5","chapter-5","Binary Trees"
"E 120 R 59","chapter-5","Binary Trees"
"F 24 S 67","chapter-5","Binary Trees"
"G 17 T 85","chapter-5","Binary Trees"
"H 50 U 37","chapter-5","Binary Trees"
"I 76 V 12","chapter-5","Binary Trees"
"J 4 W 22","chapter-5","Binary Trees"
"K 7 X 4","chapter-5","Binary Trees"
"L 42 Y 22","chapter-5","Binary Trees"
"M 24 Z 2","chapter-5","Binary Trees"
"Figure 5.23 Relative frequencies for the 26 letters of the alphabet as they ap-","chapter-5","Binary Trees"
"pear in a selected set of English documents. “Frequency” represents the expected","chapter-5","Binary Trees"
"frequency of occurrence per 1000 letters, ignoring case.","chapter-5","Binary Trees"
"5.6.1 Building Huffman Coding Trees","chapter-5","Binary Trees"
"Huffman coding assigns codes to characters such that the length of the code de-","chapter-5","Binary Trees"
"pends on the relative frequency or weight of the corresponding character. Thus, it","chapter-5","Binary Trees"
"is a variable-length code. If the estimated frequencies for letters match the actual","chapter-5","Binary Trees"
"frequency found in an encoded message, then the length of that message will typi-","chapter-5","Binary Trees"
"cally be less than if a fixed-length code had been used. The Huffman code for each","chapter-5","Binary Trees"
"letter is derived from a full binary tree called the Huffman coding tree, or simply","chapter-5","Binary Trees"
"the Huffman tree. Each leaf of the Huffman tree corresponds to a letter, and we","chapter-5","Binary Trees"
"define the weight of the leaf node to be the weight (frequency) of its associated","chapter-5","Binary Trees"
"letter. The goal is to build a tree with the minimum external path weight. Define","chapter-5","Binary Trees"
"the weighted path length of a leaf to be its weight times its depth. The binary tree","chapter-5","Binary Trees"
"with minimum external path weight is the one with the minimum sum of weighted","chapter-5","Binary Trees"
"path lengths for the given set of leaves. A letter with high weight should have low","chapter-5","Binary Trees"
"depth, so that it will count the least against the total path length. As a result, another","chapter-5","Binary Trees"
"letter might be pushed deeper in the tree if it has less weight.","chapter-5","Binary Trees"
"The process of building the Huffman tree for n letters is quite simple. First, cre-","chapter-5","Binary Trees"
"ate a collection of n initial Huffman trees, each of which is a single leaf node con-","chapter-5","Binary Trees"
"taining one of the letters. Put the n partial trees onto a priority queue organized by","chapter-5","Binary Trees"
"weight (frequency). Next, remove the first two trees (the ones with lowest weight)","chapter-5","Binary Trees"
"from the priority queue. Join these two trees together to create a new tree whose","chapter-5","Binary Trees"
"root has the two trees as children, and whose weight is the sum of the weights of the","chapter-5","Binary Trees"
"two trees. Put this new tree back into the priority queue. This process is repeated","chapter-5","Binary Trees"
"until all of the partial Huffman trees have been combined into one.","chapter-5","Binary Trees"
"180 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"Letter C D E K L M U Z","chapter-5","Binary Trees"
"Frequency 32 42 120 7 42 24 37 2","chapter-5","Binary Trees"
"Figure 5.24 The relative frequencies for eight selected letters.","chapter-5","Binary Trees"
"Step 1:","chapter-5","Binary Trees"
"Step 2:","chapter-5","Binary Trees"
"9","chapter-5","Binary Trees"
"Step 3:","chapter-5","Binary Trees"
"Step 4:","chapter-5","Binary Trees"
"65","chapter-5","Binary Trees"
"Step 5:","chapter-5","Binary Trees"
"42","chapter-5","Binary Trees"
"32","chapter-5","Binary Trees"
"C","chapter-5","Binary Trees"
"65","chapter-5","Binary Trees"
"33","chapter-5","Binary Trees"
"9","chapter-5","Binary Trees"
"E","chapter-5","Binary Trees"
"79","chapter-5","Binary Trees"
"L","chapter-5","Binary Trees"
"24","chapter-5","Binary Trees"
"L","chapter-5","Binary Trees"
"120","chapter-5","Binary Trees"
"37 42","chapter-5","Binary Trees"
"C","chapter-5","Binary Trees"
"42","chapter-5","Binary Trees"
"32","chapter-5","Binary Trees"
"24","chapter-5","Binary Trees"
"U D E","chapter-5","Binary Trees"
"2 7","chapter-5","Binary Trees"
"Z K","chapter-5","Binary Trees"
"M","chapter-5","Binary Trees"
"9","chapter-5","Binary Trees"
"9 24","chapter-5","Binary Trees"
"37","chapter-5","Binary Trees"
"U","chapter-5","Binary Trees"
"42","chapter-5","Binary Trees"
"D","chapter-5","Binary Trees"
"42","chapter-5","Binary Trees"
"M","chapter-5","Binary Trees"
"32 120","chapter-5","Binary Trees"
"C L E","chapter-5","Binary Trees"
"M C U D","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
"Z","chapter-5","Binary Trees"
"7","chapter-5","Binary Trees"
"24 32 37 42 42","chapter-5","Binary Trees"
"L","chapter-5","Binary Trees"
"K","chapter-5","Binary Trees"
"120","chapter-5","Binary Trees"
"E","chapter-5","Binary Trees"
"2 7","chapter-5","Binary Trees"
"K M C","chapter-5","Binary Trees"
"24 32 37 42 42","chapter-5","Binary Trees"
"Z D L","chapter-5","Binary Trees"
"120","chapter-5","Binary Trees"
"U E","chapter-5","Binary Trees"
"120","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
"Z","chapter-5","Binary Trees"
"7","chapter-5","Binary Trees"
"K","chapter-5","Binary Trees"
"37 42","chapter-5","Binary Trees"
"U D","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
"Z","chapter-5","Binary Trees"
"7","chapter-5","Binary Trees"
"33","chapter-5","Binary Trees"
"33","chapter-5","Binary Trees"
"M","chapter-5","Binary Trees"
"K","chapter-5","Binary Trees"
"Figure 5.25 The first five steps of the building process for a sample Huffman","chapter-5","Binary Trees"
"tree.","chapter-5","Binary Trees"
"Sec. 5.6 Huffman Coding Trees 181","chapter-5","Binary Trees"
"306","chapter-5","Binary Trees"
"0 1","chapter-5","Binary Trees"
"E 0","chapter-5","Binary Trees"
"79","chapter-5","Binary Trees"
"0 1","chapter-5","Binary Trees"
"37","chapter-5","Binary Trees"
"U","chapter-5","Binary Trees"
"42","chapter-5","Binary Trees"
"1","chapter-5","Binary Trees"
"107","chapter-5","Binary Trees"
"0","chapter-5","Binary Trees"
"42","chapter-5","Binary Trees"
"1","chapter-5","Binary Trees"
"65 0","chapter-5","Binary Trees"
"C","chapter-5","Binary Trees"
"1","chapter-5","Binary Trees"
"0 1","chapter-5","Binary Trees"
"9","chapter-5","Binary Trees"
"0 1","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
"Z","chapter-5","Binary Trees"
"7","chapter-5","Binary Trees"
"D L","chapter-5","Binary Trees"
"M","chapter-5","Binary Trees"
"K","chapter-5","Binary Trees"
"32 33","chapter-5","Binary Trees"
"24","chapter-5","Binary Trees"
"120 186","chapter-5","Binary Trees"
"Figure 5.26 A Huffman tree for the letters of Figure 5.24.","chapter-5","Binary Trees"
"Example 5.8 Figure 5.25 illustrates part of the Huffman tree construction","chapter-5","Binary Trees"
"process for the eight letters of Figure 5.24. Ranking D and L arbitrarily by","chapter-5","Binary Trees"
"alphabetical order, the letters are ordered by frequency as","chapter-5","Binary Trees"
"Letter Z K M C U D L E","chapter-5","Binary Trees"
"Frequency 2 7 24 32 37 42 42 120","chapter-5","Binary Trees"
"Because the first two letters on the list are Z and K, they are selected to","chapter-5","Binary Trees"
"be the first trees joined together.6 They become the children of a root node","chapter-5","Binary Trees"
"with weight 9. Thus, a tree whose root has weight 9 is placed back on the","chapter-5","Binary Trees"
"list, where it takes up the first position. The next step is to take values 9","chapter-5","Binary Trees"
"and 24 off the list (corresponding to the partial tree with two leaf nodes","chapter-5","Binary Trees"
"built in the last step, and the partial tree storing the letter M, respectively)","chapter-5","Binary Trees"
"and join them together. The resulting root node has weight 33, and so this","chapter-5","Binary Trees"
"tree is placed back into the list. Its priority will be between the trees with","chapter-5","Binary Trees"
"values 32 (for letter C) and 37 (for letter U). This process continues until a","chapter-5","Binary Trees"
"tree whose root has weight 306 is built. This tree is shown in Figure 5.26.","chapter-5","Binary Trees"
"Figure 5.27 shows an implementation for Huffman tree nodes. This implemen-","chapter-5","Binary Trees"
"tation is similar to the VarBinNode implementation of Figure 5.10. There is an","chapter-5","Binary Trees"
"abstract base class, named HuffNode, and two subclasses, named LeafNode","chapter-5","Binary Trees"
"6","chapter-5","Binary Trees"
"For clarity, the examples for building Huffman trees show a sorted list to keep the letters ordered","chapter-5","Binary Trees"
"by frequency. But a real implementation would use a heap to implement the priority queue for","chapter-5","Binary Trees"
"efficiency.","chapter-5","Binary Trees"
"182 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"/** Huffman tree node implementation: Base class */","chapter-5","Binary Trees"
"public interface HuffBaseNode<E> {","chapter-5","Binary Trees"
"public boolean isLeaf();","chapter-5","Binary Trees"
"public int weight();","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"/** Huffman tree node: Leaf class */","chapter-5","Binary Trees"
"class HuffLeafNode<E> implements HuffBaseNode<E> {","chapter-5","Binary Trees"
"private E element; // Element for this node","chapter-5","Binary Trees"
"private int weight; // Weight for this node","chapter-5","Binary Trees"
"/** Constructor */","chapter-5","Binary Trees"
"public HuffLeafNode(E el, int wt)","chapter-5","Binary Trees"
"{ element = el; weight = wt; }","chapter-5","Binary Trees"
"/** @return The element value */","chapter-5","Binary Trees"
"public E element() { return element; }","chapter-5","Binary Trees"
"/** @return The weight */","chapter-5","Binary Trees"
"public int weight() { return weight; }","chapter-5","Binary Trees"
"/** Return true */","chapter-5","Binary Trees"
"public boolean isLeaf() { return true; }","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"/** Huffman tree node: Internal class */","chapter-5","Binary Trees"
"class HuffInternalNode<E> implements HuffBaseNode<E> {","chapter-5","Binary Trees"
"private int weight; // Weight (sum of children)","chapter-5","Binary Trees"
"private HuffBaseNode<E> left; // Pointer to left child","chapter-5","Binary Trees"
"private HuffBaseNode<E> right; // Pointer to right child","chapter-5","Binary Trees"
"/** Constructor */","chapter-5","Binary Trees"
"public HuffInternalNode(HuffBaseNode<E> l,","chapter-5","Binary Trees"
"HuffBaseNode<E> r, int wt)","chapter-5","Binary Trees"
"{ left = l; right = r; weight = wt; }","chapter-5","Binary Trees"
"/** @return The left child */","chapter-5","Binary Trees"
"public HuffBaseNode<E> left() { return left; }","chapter-5","Binary Trees"
"/** @return The right child */","chapter-5","Binary Trees"
"public HuffBaseNode<E> right() { return right; }","chapter-5","Binary Trees"
"/** @return The weight */","chapter-5","Binary Trees"
"public int weight() { return weight; }","chapter-5","Binary Trees"
"/** Return false */","chapter-5","Binary Trees"
"public boolean isLeaf() { return false; }","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"Figure 5.27 Implementation for Huffman tree nodes. Internal nodes and leaf","chapter-5","Binary Trees"
"nodes are represented by separate classes, each derived from an abstract base class.","chapter-5","Binary Trees"
"Sec. 5.6 Huffman Coding Trees 183","chapter-5","Binary Trees"
"/** A Huffman coding tree */","chapter-5","Binary Trees"
"class HuffTree<E> implements Comparable<HuffTree<E>>{","chapter-5","Binary Trees"
"private HuffBaseNode<E> root; // Root of the tree","chapter-5","Binary Trees"
"/** Constructors */","chapter-5","Binary Trees"
"public HuffTree(E el, int wt)","chapter-5","Binary Trees"
"{ root = new HuffLeafNode<E>(el, wt); }","chapter-5","Binary Trees"
"public HuffTree(HuffBaseNode<E> l,","chapter-5","Binary Trees"
"HuffBaseNode<E> r, int wt)","chapter-5","Binary Trees"
"{ root = new HuffInternalNode<E>(l, r, wt); }","chapter-5","Binary Trees"
"public HuffBaseNode<E> root() { return root; }","chapter-5","Binary Trees"
"public int weight() // Weight of tree is weight of root","chapter-5","Binary Trees"
"{ return root.weight(); }","chapter-5","Binary Trees"
"public int compareTo(HuffTree<E> that) {","chapter-5","Binary Trees"
"if (root.weight() < that.weight()) return -1;","chapter-5","Binary Trees"
"else if (root.weight() == that.weight()) return 0;","chapter-5","Binary Trees"
"else return 1;","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"Figure 5.28 Class declarations for the Huffman tree.","chapter-5","Binary Trees"
"and IntlNode. This implementation reflects the fact that leaf and internal nodes","chapter-5","Binary Trees"
"contain distinctly different information.","chapter-5","Binary Trees"
"Figure 5.28 shows the implementation for the Huffman tree. Figure 5.29 shows","chapter-5","Binary Trees"
"the Java code for the tree-building process.","chapter-5","Binary Trees"
"Huffman tree building is an example of a greedy algorithm. At each step, the","chapter-5","Binary Trees"
"algorithm makes a “greedy” decision to merge the two subtrees with least weight.","chapter-5","Binary Trees"
"This makes the algorithm simple, but does it give the desired result? This sec-","chapter-5","Binary Trees"
"tion concludes with a proof that the Huffman tree indeed gives the most efficient","chapter-5","Binary Trees"
"arrangement for the set of letters. The proof requires the following lemma.","chapter-5","Binary Trees"
"Lemma 5.1 For any Huffman tree built by function buildHuff containing at","chapter-5","Binary Trees"
"least two letters, the two letters with least frequency are stored in siblings nodes","chapter-5","Binary Trees"
"whose depth is at least as deep as any other leaf nodes in the tree.","chapter-5","Binary Trees"
"Proof: Call the two letters with least frequency l1 and l2. They must be siblings","chapter-5","Binary Trees"
"because buildHuff selects them in the first step of the construction process.","chapter-5","Binary Trees"
"Assume that l1 and l2 are not the deepest nodes in the tree. In this case, the Huffman","chapter-5","Binary Trees"
"tree must either look as shown in Figure 5.30, or in some sense be symmetrical","chapter-5","Binary Trees"
"to this. For this situation to occur, the parent of l1 and l2, labeled V, must have","chapter-5","Binary Trees"
"greater weight than the node labeled X. Otherwise, function buildHuff would","chapter-5","Binary Trees"
"have selected node V in place of node X as the child of node U. However, this is","chapter-5","Binary Trees"
"impossible because l1 and l2 are the letters with least frequency. ✷","chapter-5","Binary Trees"
"Theorem 5.3 Function buildHuff builds the Huffman tree with the minimum","chapter-5","Binary Trees"
"external path weight for the given set of letters.","chapter-5","Binary Trees"
"184 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"/** Build a Huffman tree from list hufflist */","chapter-5","Binary Trees"
"static HuffTree<Character> buildTree() {","chapter-5","Binary Trees"
"HuffTree tmp1, tmp2, tmp3 = null;","chapter-5","Binary Trees"
"while (Hheap.heapsize() > 1) { // While two items left","chapter-5","Binary Trees"
"tmp1 = Hheap.removemin();","chapter-5","Binary Trees"
"tmp2 = Hheap.removemin();","chapter-5","Binary Trees"
"tmp3 = new HuffTree<Character>(tmp1.root(), tmp2.root(),","chapter-5","Binary Trees"
"tmp1.weight() + tmp2.weight());","chapter-5","Binary Trees"
"Hheap.insert(tmp3); // Return new tree to heap","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"return tmp3; // Return the tree","chapter-5","Binary Trees"
"}","chapter-5","Binary Trees"
"Figure 5.29 Implementation for the Huffman tree construction function.","chapter-5","Binary Trees"
"buildHuff takes as input fl, the min-heap of partial Huffman trees, which","chapter-5","Binary Trees"
"initially are single leaf nodes as shown in Step 1 of Figure 5.25. The body of","chapter-5","Binary Trees"
"function buildTree consists mainly of a for loop. On each iteration of the","chapter-5","Binary Trees"
"for loop, the first two partial trees are taken off the heap and placed in variables","chapter-5","Binary Trees"
"temp1 and temp2. A tree is created (temp3) such that the left and right subtrees","chapter-5","Binary Trees"
"are temp1 and temp2, respectively. Finally, temp3 is returned to fl.","chapter-5","Binary Trees"
"l1 X","chapter-5","Binary Trees"
"V","chapter-5","Binary Trees"
"l2","chapter-5","Binary Trees"
"U","chapter-5","Binary Trees"
"Figure 5.30 An impossible Huffman tree, showing the situation where the two","chapter-5","Binary Trees"
"nodes with least weight, l1 and l2, are not the deepest nodes in the tree. Triangles","chapter-5","Binary Trees"
"represent subtrees.","chapter-5","Binary Trees"
"Proof: The proof is by induction on n, the number of letters.","chapter-5","Binary Trees"
"• Base Case: For n = 2, the Huffman tree must have the minimum external","chapter-5","Binary Trees"
"path weight because there are only two possible trees, each with identical","chapter-5","Binary Trees"
"weighted path lengths for the two leaves.","chapter-5","Binary Trees"
"• Induction Hypothesis: Assume that any tree created by buildHuff that","chapter-5","Binary Trees"
"contains n − 1 leaves has minimum external path length.","chapter-5","Binary Trees"
"• Induction Step: Given a Huffman tree T built by buildHuff with n","chapter-5","Binary Trees"
"leaves, n ≥ 2, suppose that w1 ≤ w2 ≤ · · · ≤ wn where w1 to wn are","chapter-5","Binary Trees"
"the weights of the letters. Call V the parent of the letters with frequencies w1","chapter-5","Binary Trees"
"and w2. From the lemma, we know that the leaf nodes containing the letters","chapter-5","Binary Trees"
"with frequencies w1 and w2 are as deep as any nodes in T. If any other leaf","chapter-5","Binary Trees"
"Sec. 5.6 Huffman Coding Trees 185","chapter-5","Binary Trees"
"Letter Freq Code Bits","chapter-5","Binary Trees"
"C 32 1110 4","chapter-5","Binary Trees"
"D 42 101 3","chapter-5","Binary Trees"
"E 120 0 1","chapter-5","Binary Trees"
"K 7 111101 6","chapter-5","Binary Trees"
"L 42 110 3","chapter-5","Binary Trees"
"M 24 11111 5","chapter-5","Binary Trees"
"U 37 100 3","chapter-5","Binary Trees"
"Z 2 111100 6","chapter-5","Binary Trees"
"Figure 5.31 The Huffman codes for the letters of Figure 5.24.","chapter-5","Binary Trees"
"nodes in the tree were deeper, we could reduce their weighted path length by","chapter-5","Binary Trees"
"swapping them with w1 or w2. But the lemma tells us that no such deeper","chapter-5","Binary Trees"
"nodes exist. Call T","chapter-5","Binary Trees"
"0","chapter-5","Binary Trees"
"the Huffman tree that is identical to T except that node","chapter-5","Binary Trees"
"V is replaced with a leaf node V","chapter-5","Binary Trees"
"0 whose weight is w1 +w2. By the induction","chapter-5","Binary Trees"
"hypothesis, T","chapter-5","Binary Trees"
"0","chapter-5","Binary Trees"
"has minimum external path length. Returning the children to","chapter-5","Binary Trees"
"V","chapter-5","Binary Trees"
"0","chapter-5","Binary Trees"
"restores tree T, which must also have minimum external path length.","chapter-5","Binary Trees"
"Thus by mathematical induction, function buildHuff creates the Huffman","chapter-5","Binary Trees"
"tree with minimum external path length. ✷","chapter-5","Binary Trees"
"5.6.2 Assigning and Using Huffman Codes","chapter-5","Binary Trees"
"Once the Huffman tree has been constructed, it is an easy matter to assign codes","chapter-5","Binary Trees"
"to individual letters. Beginning at the root, we assign either a ‘0’ or a ‘1’ to each","chapter-5","Binary Trees"
"edge in the tree. ‘0’ is assigned to edges connecting a node with its left child, and","chapter-5","Binary Trees"
"‘1’ to edges connecting a node with its right child. This process is illustrated by","chapter-5","Binary Trees"
"Figure 5.26. The Huffman code for a letter is simply a binary number determined","chapter-5","Binary Trees"
"by the path from the root to the leaf corresponding to that letter. Thus, the code","chapter-5","Binary Trees"
"for E is ‘0’ because the path from the root to the leaf node for E takes a single left","chapter-5","Binary Trees"
"branch. The code for K is ‘111101’ because the path to the node for K takes four","chapter-5","Binary Trees"
"right branches, then a left, and finally one last right. Figure 5.31 lists the codes for","chapter-5","Binary Trees"
"all eight letters.","chapter-5","Binary Trees"
"Given codes for the letters, it is a simple matter to use these codes to encode a","chapter-5","Binary Trees"
"text message. We simply replace each letter in the string with its binary code. A","chapter-5","Binary Trees"
"lookup table can be used for this purpose.","chapter-5","Binary Trees"
"Example 5.9 Using the code generated by our example Huffman tree,","chapter-5","Binary Trees"
"the word “DEED” is represented by the bit string “10100101” and the word","chapter-5","Binary Trees"
"“MUCK” is represented by the bit string “111111001110111101.”","chapter-5","Binary Trees"
"Decoding the message is done by looking at the bits in the coded string from","chapter-5","Binary Trees"
"left to right until a letter is decoded. This can be done by using the Huffman tree in","chapter-5","Binary Trees"
"186 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"a reverse process from that used to generate the codes. Decoding a bit string begins","chapter-5","Binary Trees"
"at the root of the tree. We take branches depending on the bit value — left for ‘0’","chapter-5","Binary Trees"
"and right for ‘1’ — until reaching a leaf node. This leaf contains the first character","chapter-5","Binary Trees"
"in the message. We then process the next bit in the code restarting at the root to","chapter-5","Binary Trees"
"begin the next character.","chapter-5","Binary Trees"
"Example 5.10 To decode the bit string “1011001110111101” we begin","chapter-5","Binary Trees"
"at the root of the tree and take a right branch for the first bit which is ‘1.’","chapter-5","Binary Trees"
"Because the next bit is a ‘0’ we take a left branch. We then take another","chapter-5","Binary Trees"
"right branch (for the third bit ‘1’), arriving at the leaf node corresponding","chapter-5","Binary Trees"
"to the letter D. Thus, the first letter of the coded word is D. We then begin","chapter-5","Binary Trees"
"again at the root of the tree to process the fourth bit, which is a ‘1.’ Taking","chapter-5","Binary Trees"
"a right branch, then two left branches (for the next two bits which are ‘0’),","chapter-5","Binary Trees"
"we reach the leaf node corresponding to the letter U. Thus, the second letter","chapter-5","Binary Trees"
"is U. In similar manner we complete the decoding process to find that the","chapter-5","Binary Trees"
"last two letters are C and K, spelling the word “DUCK.”","chapter-5","Binary Trees"
"A set of codes is said to meet the prefix property if no code in the set is the","chapter-5","Binary Trees"
"prefix of another. The prefix property guarantees that there will be no ambiguity in","chapter-5","Binary Trees"
"how a bit string is decoded. In other words, once we reach the last bit of a code","chapter-5","Binary Trees"
"during the decoding process, we know which letter it is the code for. Huffman codes","chapter-5","Binary Trees"
"certainly have the prefix property because any prefix for a code would correspond to","chapter-5","Binary Trees"
"an internal node, while all codes correspond to leaf nodes. For example, the code","chapter-5","Binary Trees"
"for M is ‘11111.’ Taking five right branches in the Huffman tree of Figure 5.26","chapter-5","Binary Trees"
"brings us to the leaf node containing M. We can be sure that no letter can have code","chapter-5","Binary Trees"
"‘111’ because this corresponds to an internal node of the tree, and the tree-building","chapter-5","Binary Trees"
"process places letters only at the leaf nodes.","chapter-5","Binary Trees"
"How efficient is Huffman coding? In theory, it is an optimal coding method","chapter-5","Binary Trees"
"whenever the true frequencies are known, and the frequency of a letter is indepen-","chapter-5","Binary Trees"
"dent of the context of that letter in the message. In practice, the frequencies of","chapter-5","Binary Trees"
"letters in an English text document do change depending on context. For example,","chapter-5","Binary Trees"
"while E is the most commonly used letter of the alphabet in English documents,","chapter-5","Binary Trees"
"T is more common as the first letter of a word. This is why most commercial com-","chapter-5","Binary Trees"
"pression utilities do not use Huffman coding as their primary coding method, but","chapter-5","Binary Trees"
"instead use techniques that take advantage of the context for the letters.","chapter-5","Binary Trees"
"Another factor that affects the compression efficiency of Huffman coding is the","chapter-5","Binary Trees"
"relative frequencies of the letters. Some frequency patterns will save no space as","chapter-5","Binary Trees"
"compared to fixed-length codes; others can result in great compression. In general,","chapter-5","Binary Trees"
"Huffman coding does better when there is large variation in the frequencies of","chapter-5","Binary Trees"
"letters. In the particular case of the frequencies shown in Figure 5.31, we can","chapter-5","Binary Trees"
"Sec. 5.6 Huffman Coding Trees 187","chapter-5","Binary Trees"
"determine the expected savings from Huffman coding if the actual frequencies of a","chapter-5","Binary Trees"
"coded message match the expected frequencies.","chapter-5","Binary Trees"
"Example 5.11 Because the sum of the frequencies in Figure 5.31 is 306","chapter-5","Binary Trees"
"and E has frequency 120, we expect it to appear 120 times in a message","chapter-5","Binary Trees"
"containing 306 letters. An actual message might or might not meet this","chapter-5","Binary Trees"
"expectation. Letters D, L, and U have code lengths of three, and together","chapter-5","Binary Trees"
"are expected to appear 121 times in 306 letters. Letter C has a code length of","chapter-5","Binary Trees"
"four, and is expected to appear 32 times in 306 letters. Letter M has a code","chapter-5","Binary Trees"
"length of five, and is expected to appear 24 times in 306 letters. Finally,","chapter-5","Binary Trees"
"letters K and Z have code lengths of six, and together are expected to appear","chapter-5","Binary Trees"
"only 9 times in 306 letters. The average expected cost per character is","chapter-5","Binary Trees"
"simply the sum of the cost for each character (ci) times the probability of","chapter-5","Binary Trees"
"its occurring (pi), or","chapter-5","Binary Trees"
"c1p1 + c2p2 + · · · + cnpn.","chapter-5","Binary Trees"
"This can be reorganized as","chapter-5","Binary Trees"
"c1f1 + c2f2 + · · · + cnfn","chapter-5","Binary Trees"
"fT","chapter-5","Binary Trees"
"where fi","chapter-5","Binary Trees"
"is the (relative) frequency of letter i and fT is the total for all letter","chapter-5","Binary Trees"
"frequencies. For this set of frequencies, the expected cost per letter is","chapter-5","Binary Trees"
"[(1×120)+(3×121)+(4×32)+(5×24)+(6×9)]/306 = 785/306 ≈ 2.57","chapter-5","Binary Trees"
"A fixed-length code for these eight characters would require log 8 = 3 bits","chapter-5","Binary Trees"
"per letter as opposed to about 2.57 bits per letter for Huffman coding. Thus,","chapter-5","Binary Trees"
"Huffman coding is expected to save about 14% for this set of letters.","chapter-5","Binary Trees"
"Huffman coding for all ASCII symbols should do better than this. The letters of","chapter-5","Binary Trees"
"Figure 5.31 are atypical in that there are too many common letters compared to the","chapter-5","Binary Trees"
"number of rare letters. Huffman coding for all 26 letters would yield an expected","chapter-5","Binary Trees"
"cost of 4.29 bits per letter. The equivalent fixed-length code would require about","chapter-5","Binary Trees"
"five bits. This is somewhat unfair to fixed-length coding because there is actually","chapter-5","Binary Trees"
"room for 32 codes in five bits, but only 26 letters. More generally, Huffman coding","chapter-5","Binary Trees"
"of a typical text file will save around 40% over ASCII coding if we charge ASCII","chapter-5","Binary Trees"
"coding at eight bits per character. Huffman coding for a binary file (such as a","chapter-5","Binary Trees"
"compiled executable) would have a very different set of distribution frequencies and","chapter-5","Binary Trees"
"so would have a different space savings. Most commercial compression programs","chapter-5","Binary Trees"
"use two or three coding schemes to adjust to different types of files.","chapter-5","Binary Trees"
"In the preceding example, “DEED” was coded in 8 bits, a saving of 33% over","chapter-5","Binary Trees"
"the twelve bits required from a fixed-length coding. However, “MUCK” requires","chapter-5","Binary Trees"
"188 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"18 bits, more space than required by the corresponding fixed-length coding. The","chapter-5","Binary Trees"
"problem is that “MUCK” is composed of letters that are not expected to occur","chapter-5","Binary Trees"
"often. If the message does not match the expected frequencies of the letters, than","chapter-5","Binary Trees"
"the length of the encoding will not be as expected either.","chapter-5","Binary Trees"
"5.6.3 Search in Huffman Trees","chapter-5","Binary Trees"
"When we decode a character using the Huffman coding tree, we follow a path","chapter-5","Binary Trees"
"through the tree dictated by the bits in the code string. Each ‘0’ bit indicates a left","chapter-5","Binary Trees"
"branch while each ‘1’ bit indicates a right branch. Now look at Figure 5.26 and","chapter-5","Binary Trees"
"consider this structure in terms of searching for a given letter (whose key value is","chapter-5","Binary Trees"
"its Huffman code). We see that all letters with codes beginning with ’0’ are stored","chapter-5","Binary Trees"
"in the left branch, while all letters with codes beginning with ‘1’ are stored in the","chapter-5","Binary Trees"
"right branch. Contrast this with storing records in a BST. There, all records with","chapter-5","Binary Trees"
"key value less than the root value are stored in the left branch, while all records","chapter-5","Binary Trees"
"with key values greater than the root are stored in the right branch.","chapter-5","Binary Trees"
"If we view all records stored in either of these structures as appearing at some","chapter-5","Binary Trees"
"point on a number line representing the key space, we can see that the splitting","chapter-5","Binary Trees"
"behavior of these two structures is very different. The BST splits the space based","chapter-5","Binary Trees"
"on the key values as they are encountered when going down the tree. But the splits","chapter-5","Binary Trees"
"in the key space are predetermined for the Huffman tree. Search tree structures","chapter-5","Binary Trees"
"whose splitting points in the key space are predetermined are given the special","chapter-5","Binary Trees"
"name trie to distinguish them from the type of search tree (like the BST) whose","chapter-5","Binary Trees"
"splitting points are determined by the data. Tries are discussed in more detail in","chapter-5","Binary Trees"
"Chapter 13.","chapter-5","Binary Trees"
"5.7 Further Reading","chapter-5","Binary Trees"
"See Shaffer and Brown [SB93] for an example of a tree implementation where an","chapter-5","Binary Trees"
"internal node pointer field stores the value of its child instead of a pointer to its","chapter-5","Binary Trees"
"child when the child is a leaf node.","chapter-5","Binary Trees"
"Many techniques exist for maintaining reasonably balanced BSTs in the face of","chapter-5","Binary Trees"
"an unfriendly series of insert and delete operations. One example is the AVL tree of","chapter-5","Binary Trees"
"Adelson-Velskii and Landis, which is discussed by Knuth [Knu98]. The AVL tree","chapter-5","Binary Trees"
"(see Section 13.2) is actually a BST whose insert and delete routines reorganize the","chapter-5","Binary Trees"
"tree structure so as to guarantee that the subtrees rooted by the children of any node","chapter-5","Binary Trees"
"will differ in height by at most one. Another example is the splay tree [ST85], also","chapter-5","Binary Trees"
"discussed in Section 13.2.","chapter-5","Binary Trees"
"See Bentley’s Programming Pearl “Thanks, Heaps” [Ben85, Ben88] for a good","chapter-5","Binary Trees"
"discussion on the heap data structure and its uses.","chapter-5","Binary Trees"
"The proof of Section 5.6.1 that the Huffman coding tree has minimum external","chapter-5","Binary Trees"
"path weight is from Knuth [Knu97]. For more information on data compression","chapter-5","Binary Trees"
"Sec. 5.8 Exercises 189","chapter-5","Binary Trees"
"techniques, see Managing Gigabytes by Witten, Moffat, and Bell [WMB99], and","chapter-5","Binary Trees"
"Codes and Cryptography by Dominic Welsh [Wel88]. Tables 5.23 and 5.24 are","chapter-5","Binary Trees"
"derived from Welsh [Wel88].","chapter-5","Binary Trees"
"5.8 Exercises","chapter-5","Binary Trees"
"5.1 Section 5.1.1 claims that a full binary tree has the highest number of leaf","chapter-5","Binary Trees"
"nodes among all trees with n internal nodes. Prove that this is true.","chapter-5","Binary Trees"
"5.2 Define the degree of a node as the number of its non-empty children. Prove","chapter-5","Binary Trees"
"by induction that the number of degree 2 nodes in any binary tree is one less","chapter-5","Binary Trees"
"than the number of leaves.","chapter-5","Binary Trees"
"5.3 Define the internal path length for a tree as the sum of the depths of all","chapter-5","Binary Trees"
"internal nodes, while the external path length is the sum of the depths of all","chapter-5","Binary Trees"
"leaf nodes in the tree. Prove by induction that if tree T is a full binary tree","chapter-5","Binary Trees"
"with n internal nodes, I is T’s internal path length, and E is T’s external path","chapter-5","Binary Trees"
"length, then E = I + 2n for n ≥ 0.","chapter-5","Binary Trees"
"5.4 Explain why function preorder2 from Section 5.2 makes half as many","chapter-5","Binary Trees"
"recursive calls as function preorder. Explain why it makes twice as many","chapter-5","Binary Trees"
"accesses to left and right children.","chapter-5","Binary Trees"
"5.5 (a) Modify the preorder traversal of Section 5.2 to perform an inorder","chapter-5","Binary Trees"
"traversal of a binary tree.","chapter-5","Binary Trees"
"(b) Modify the preorder traversal of Section 5.2 to perform a postorder","chapter-5","Binary Trees"
"traversal of a binary tree.","chapter-5","Binary Trees"
"5.6 Write a recursive function named search that takes as input the pointer to","chapter-5","Binary Trees"
"the root of a binary tree (not a BST!) and a value K, and returns true if","chapter-5","Binary Trees"
"value K appears in the tree and false otherwise.","chapter-5","Binary Trees"
"5.7 Write an algorithm that takes as input the pointer to the root of a binary","chapter-5","Binary Trees"
"tree and prints the node values of the tree in level order. Level order first","chapter-5","Binary Trees"
"prints the root, then all nodes of level 1, then all nodes of level 2, and so","chapter-5","Binary Trees"
"on. Hint: Preorder traversals make use of a stack through recursive calls.","chapter-5","Binary Trees"
"Consider making use of another data structure to help implement the level-","chapter-5","Binary Trees"
"order traversal.","chapter-5","Binary Trees"
"5.8 Write a recursive function that returns the height of a binary tree.","chapter-5","Binary Trees"
"5.9 Write a recursive function that returns a count of the number of leaf nodes in","chapter-5","Binary Trees"
"a binary tree.","chapter-5","Binary Trees"
"5.10 Assume that a given binary tree stores integer values in its nodes. Write a","chapter-5","Binary Trees"
"recursive function that sums the values of all nodes in the tree.","chapter-5","Binary Trees"
"5.11 Assume that a given binary tree stores integer values in its nodes. Write a","chapter-5","Binary Trees"
"recursive function that traverses a binary tree, and prints the value of every","chapter-5","Binary Trees"
"node who’s grandparent has a value that is a multiple of five.","chapter-5","Binary Trees"
"190 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"5.12 Write a recursive function that traverses a binary tree, and prints the value of","chapter-5","Binary Trees"
"every node which has at least four great-grandchildren.","chapter-5","Binary Trees"
"5.13 Compute the overhead fraction for each of the following full binary tree im-","chapter-5","Binary Trees"
"plementations.","chapter-5","Binary Trees"
"(a) All nodes store data, two child pointers, and a parent pointer. The data","chapter-5","Binary Trees"
"field requires four bytes and each pointer requires four bytes.","chapter-5","Binary Trees"
"(b) All nodes store data and two child pointers. The data field requires","chapter-5","Binary Trees"
"sixteen bytes and each pointer requires four bytes.","chapter-5","Binary Trees"
"(c) All nodes store data and a parent pointer, and internal nodes store two","chapter-5","Binary Trees"
"child pointers. The data field requires eight bytes and each pointer re-","chapter-5","Binary Trees"
"quires four bytes.","chapter-5","Binary Trees"
"(d) Only leaf nodes store data; internal nodes store two child pointers. The","chapter-5","Binary Trees"
"data field requires eight bytes and each pointer requires four bytes.","chapter-5","Binary Trees"
"5.14 Why is the BST Property defined so that nodes with values equal to the value","chapter-5","Binary Trees"
"of the root appear only in the right subtree, rather than allow equal-valued","chapter-5","Binary Trees"
"nodes to appear in either subtree?","chapter-5","Binary Trees"
"5.15 (a) Show the BST that results from inserting the values 15, 20, 25, 18, 16,","chapter-5","Binary Trees"
"5, and 7 (in that order).","chapter-5","Binary Trees"
"(b) Show the enumerations for the tree of (a) that result from doing a pre-","chapter-5","Binary Trees"
"order traversal, an inorder traversal, and a postorder traversal.","chapter-5","Binary Trees"
"5.16 Draw the BST that results from adding the value 5 to the BST shown in","chapter-5","Binary Trees"
"Figure 5.13(a).","chapter-5","Binary Trees"
"5.17 Draw the BST that results from deleting the value 7 from the BST of Fig-","chapter-5","Binary Trees"
"ure 5.13(b).","chapter-5","Binary Trees"
"5.18 Write a function that prints out the node values for a BST in sorted order","chapter-5","Binary Trees"
"from highest to lowest.","chapter-5","Binary Trees"
"5.19 Write a recursive function named smallcount that, given the pointer to","chapter-5","Binary Trees"
"the root of a BST and a key K, returns the number of nodes having key","chapter-5","Binary Trees"
"values less than or equal to K. Function smallcount should visit as few","chapter-5","Binary Trees"
"nodes in the BST as possible.","chapter-5","Binary Trees"
"5.20 Write a recursive function named printRange that, given the pointer to","chapter-5","Binary Trees"
"the root of a BST, a low key value, and a high key value, prints in sorted","chapter-5","Binary Trees"
"order all records whose key values fall between the two given keys. Function","chapter-5","Binary Trees"
"printRange should visit as few nodes in the BST as possible.","chapter-5","Binary Trees"
"5.21 Write a recursive function named checkBST that, given the pointer to the","chapter-5","Binary Trees"
"root of a binary tree, will return true if the tree is a BST, and false if it is","chapter-5","Binary Trees"
"not.","chapter-5","Binary Trees"
"5.22 Describe a simple modification to the BST that will allow it to easily support","chapter-5","Binary Trees"
"finding the Kth smallest value in Θ(log n) average case time. Then write","chapter-5","Binary Trees"
"a pseudo-code function for finding the Kth smallest value in your modified","chapter-5","Binary Trees"
"BST.","chapter-5","Binary Trees"
"Sec. 5.8 Exercises 191","chapter-5","Binary Trees"
"5.23 What are the minimum and maximum number of elements in a heap of","chapter-5","Binary Trees"
"height h?","chapter-5","Binary Trees"
"5.24 Where in a max-heap might the smallest element reside?","chapter-5","Binary Trees"
"5.25 Show the max-heap that results from running buildHeap on the following","chapter-5","Binary Trees"
"values stored in an array:","chapter-5","Binary Trees"
"10 5 12 3 2 1 8 7 9 4","chapter-5","Binary Trees"
"5.26 (a) Show the heap that results from deleting the maximum value from the","chapter-5","Binary Trees"
"max-heap of Figure 5.20b.","chapter-5","Binary Trees"
"(b) Show the heap that results from deleting the element with value 5 from","chapter-5","Binary Trees"
"the max-heap of Figure 5.20b.","chapter-5","Binary Trees"
"5.27 Revise the heap definition of Figure 5.19 to implement a min-heap. The","chapter-5","Binary Trees"
"member function removemax should be replaced by a new function called","chapter-5","Binary Trees"
"removemin.","chapter-5","Binary Trees"
"5.28 Build the Huffman coding tree and determine the codes for the following set","chapter-5","Binary Trees"
"of letters and weights:","chapter-5","Binary Trees"
"Letter A B C D E F G H I J K L","chapter-5","Binary Trees"
"Frequency 2 3 5 7 11 13 17 19 23 31 37 41","chapter-5","Binary Trees"
"What is the expected length in bits of a message containing n characters for","chapter-5","Binary Trees"
"this frequency distribution?","chapter-5","Binary Trees"
"5.29 What will the Huffman coding tree look like for a set of sixteen characters all","chapter-5","Binary Trees"
"with equal weight? What is the average code length for a letter in this case?","chapter-5","Binary Trees"
"How does this differ from the smallest possible fixed length code for sixteen","chapter-5","Binary Trees"
"characters?","chapter-5","Binary Trees"
"5.30 A set of characters with varying weights is assigned Huffman codes. If one","chapter-5","Binary Trees"
"of the characters is assigned code 001, then,","chapter-5","Binary Trees"
"(a) Describe all codes that cannot have been assigned.","chapter-5","Binary Trees"
"(b) Describe all codes that must have been assigned.","chapter-5","Binary Trees"
"5.31 Assume that a sample alphabet has the following weights:","chapter-5","Binary Trees"
"Letter Q Z F M T S O E","chapter-5","Binary Trees"
"Frequency 2 3 10 10 10 15 20 30","chapter-5","Binary Trees"
"(a) For this alphabet, what is the worst-case number of bits required by the","chapter-5","Binary Trees"
"Huffman code for a string of n letters? What string(s) have the worst-","chapter-5","Binary Trees"
"case performance?","chapter-5","Binary Trees"
"(b) For this alphabet, what is the best-case number of bits required by the","chapter-5","Binary Trees"
"Huffman code for a string of n letters? What string(s) have the best-","chapter-5","Binary Trees"
"case performance?","chapter-5","Binary Trees"
"192 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"(c) What is the average number of bits required by a character using the","chapter-5","Binary Trees"
"Huffman code for this alphabet?","chapter-5","Binary Trees"
"5.32 You must keep track of some data. Your options are:","chapter-5","Binary Trees"
"(1) A linked-list maintained in sorted order.","chapter-5","Binary Trees"
"(2) A linked-list of unsorted records.","chapter-5","Binary Trees"
"(3) A binary search tree.","chapter-5","Binary Trees"
"(4) An array-based list maintained in sorted order.","chapter-5","Binary Trees"
"(5) An array-based list of unsorted records.","chapter-5","Binary Trees"
"For each of the following scenarios, which of these choices would be best?","chapter-5","Binary Trees"
"Explain your answer.","chapter-5","Binary Trees"
"(a) The records are guaranteed to arrive already sorted from lowest to high-","chapter-5","Binary Trees"
"est (i.e., whenever a record is inserted, its key value will always be","chapter-5","Binary Trees"
"greater than that of the last record inserted). A total of 1000 inserts will","chapter-5","Binary Trees"
"be interspersed with 1000 searches.","chapter-5","Binary Trees"
"(b) The records arrive with values having a uniform random distribution","chapter-5","Binary Trees"
"(so the BST is likely to be well balanced). 1,000,000 insertions are","chapter-5","Binary Trees"
"performed, followed by 10 searches.","chapter-5","Binary Trees"
"(c) The records arrive with values having a uniform random distribution (so","chapter-5","Binary Trees"
"the BST is likely to be well balanced). 1000 insertions are interspersed","chapter-5","Binary Trees"
"with 1000 searches.","chapter-5","Binary Trees"
"(d) The records arrive with values having a uniform random distribution (so","chapter-5","Binary Trees"
"the BST is likely to be well balanced). 1000 insertions are performed,","chapter-5","Binary Trees"
"followed by 1,000,000 searches.","chapter-5","Binary Trees"
"5.9 Projects","chapter-5","Binary Trees"
"5.1 Re-implement the composite design for the binary tree node class of Fig-","chapter-5","Binary Trees"
"ure 5.11 using a flyweight in place of null pointers to empty nodes.","chapter-5","Binary Trees"
"5.2 One way to deal with the “problem” of null pointers in binary trees is to","chapter-5","Binary Trees"
"use that space for some other purpose. One example is the threaded binary","chapter-5","Binary Trees"
"tree. Extending the node implementation of Figure 5.7, the threaded binary","chapter-5","Binary Trees"
"tree stores with each node two additional bit fields that indicate if the child","chapter-5","Binary Trees"
"pointers lc and rc are regular pointers to child nodes or threads. If lc","chapter-5","Binary Trees"
"is not a pointer to a non-empty child (i.e., if it would be null in a regular","chapter-5","Binary Trees"
"binary tree), then it instead stores a pointer to the inorder predecessor of that","chapter-5","Binary Trees"
"node. The inorder predecessor is the node that would be printed immediately","chapter-5","Binary Trees"
"before the current node in an inorder traversal. If rc is not a pointer to a","chapter-5","Binary Trees"
"child, then it instead stores a pointer to the node’s inorder successor. The","chapter-5","Binary Trees"
"inorder successor is the node that would be printed immediately after the","chapter-5","Binary Trees"
"current node in an inorder traversal. The main advantage of threaded binary","chapter-5","Binary Trees"
"Sec. 5.9 Projects 193","chapter-5","Binary Trees"
"trees is that operations such as inorder traversal can be implemented without","chapter-5","Binary Trees"
"using recursion or a stack.","chapter-5","Binary Trees"
"Re-implement the BST as a threaded binary tree, and include a non-recursive","chapter-5","Binary Trees"
"version of the preorder traversal","chapter-5","Binary Trees"
"5.3 Implement a city database using a BST to store the database records. Each","chapter-5","Binary Trees"
"database record contains the name of the city (a string of arbitrary length)","chapter-5","Binary Trees"
"and the coordinates of the city expressed as integer x- and y-coordinates.","chapter-5","Binary Trees"
"The BST should be organized by city name. Your database should allow","chapter-5","Binary Trees"
"records to be inserted, deleted by name or coordinate, and searched by name","chapter-5","Binary Trees"
"or coordinate. Another operation that should be supported is to print all","chapter-5","Binary Trees"
"records within a given distance of a specified point. Collect running-time","chapter-5","Binary Trees"
"statistics for each operation. Which operations can be implemented reason-","chapter-5","Binary Trees"
"ably efficiently (i.e., in Θ(log n) time in the average case) using a BST? Can","chapter-5","Binary Trees"
"the database system be made more efficient by using one or more additional","chapter-5","Binary Trees"
"BSTs to organize the records by location?","chapter-5","Binary Trees"
"5.4 Create a binary tree ADT that includes generic traversal methods that take a","chapter-5","Binary Trees"
"visitor, as described in Section 5.2. Write functions count and BSTcheck","chapter-5","Binary Trees"
"of Section 5.2 as visitors to be used with the generic traversal method.","chapter-5","Binary Trees"
"5.5 Implement a priority queue class based on the max-heap class implementa-","chapter-5","Binary Trees"
"tion of Figure 5.19. The following methods should be supported for manip-","chapter-5","Binary Trees"
"ulating the priority queue:","chapter-5","Binary Trees"
"void enqueue(int ObjectID, int priority);","chapter-5","Binary Trees"
"int dequeue();","chapter-5","Binary Trees"
"void changeweight(int ObjectID, int newPriority);","chapter-5","Binary Trees"
"Method enqueue inserts a new object into the priority queue with ID num-","chapter-5","Binary Trees"
"ber ObjectID and priority priority. Method dequeue removes the","chapter-5","Binary Trees"
"object with highest priority from the priority queue and returns its object ID.","chapter-5","Binary Trees"
"Method changeweight changes the priority of the object with ID number","chapter-5","Binary Trees"
"ObjectID to be newPriority. The type for E should be a class that","chapter-5","Binary Trees"
"stores the object ID and the priority for that object. You will need a mech-","chapter-5","Binary Trees"
"anism for finding the position of the desired object within the heap. Use an","chapter-5","Binary Trees"
"array, storing the object with ObjectID i in position i. (Be sure in your","chapter-5","Binary Trees"
"testing to keep the ObjectIDs within the array bounds.) You must also","chapter-5","Binary Trees"
"modify the heap implementation to store the object’s position in the auxil-","chapter-5","Binary Trees"
"iary array so that updates to objects in the heap can be updated as well in the","chapter-5","Binary Trees"
"array.","chapter-5","Binary Trees"
"5.6 The Huffman coding tree function buildHuff of Figure 5.29 manipulates","chapter-5","Binary Trees"
"a sorted list. This could result in a Θ(n","chapter-5","Binary Trees"
"2","chapter-5","Binary Trees"
") algorithm, because placing an inter-","chapter-5","Binary Trees"
"mediate Huffman tree on the list could take Θ(n) time. Revise this algorithm","chapter-5","Binary Trees"
"to use a priority queue based on a min-heap instead of a list.","chapter-5","Binary Trees"
"194 Chap. 5 Binary Trees","chapter-5","Binary Trees"
"5.7 Complete the implementation of the Huffman coding tree, building on the","chapter-5","Binary Trees"
"code presented in Section 5.6. Include a function to compute and store in a","chapter-5","Binary Trees"
"table the codes for each letter, and functions to encode and decode messages.","chapter-5","Binary Trees"
"This project can be further extended to support file compression. To do so","chapter-5","Binary Trees"
"requires adding two steps: (1) Read through the input file to generate actual","chapter-5","Binary Trees"
"frequencies for all letters in the file; and (2) store a representation for the","chapter-5","Binary Trees"
"Huffman tree at the beginning of the encoded output file to be used by the","chapter-5","Binary Trees"
"decoding function. If you have trouble with devising such a representation,","chapter-5","Binary Trees"
"see Section 6.5.","chapter-5","Binary Trees"
"Many organizations are hierarchical in nature, such as the military and most busi-","chapter-6","Non-Binary Trees"
"nesses. Consider a company with a president and some number of vice presidents","chapter-6","Non-Binary Trees"
"who report to the president. Each vice president has some number of direct sub-","chapter-6","Non-Binary Trees"
"ordinates, and so on. If we wanted to model this company with a data structure, it","chapter-6","Non-Binary Trees"
"would be natural to think of the president in the root node of a tree, the vice presi-","chapter-6","Non-Binary Trees"
"dents at level 1, and their subordinates at lower levels in the tree as we go down the","chapter-6","Non-Binary Trees"
"organizational hierarchy.","chapter-6","Non-Binary Trees"
"Because the number of vice presidents is likely to be more than two, this com-","chapter-6","Non-Binary Trees"
"pany’s organization cannot easily be represented by a binary tree. We need instead","chapter-6","Non-Binary Trees"
"to use a tree whose nodes have an arbitrary number of children. Unfortunately,","chapter-6","Non-Binary Trees"
"when we permit trees to have nodes with an arbitrary number of children, they be-","chapter-6","Non-Binary Trees"
"come much harder to implement than binary trees. We consider such trees in this","chapter-6","Non-Binary Trees"
"chapter. To distinguish them from binary trees, we use the term general tree.","chapter-6","Non-Binary Trees"
"Section 6.1 presents general tree terminology. Section 6.2 presents a simple","chapter-6","Non-Binary Trees"
"representation for solving the important problem of processing equivalence classes.","chapter-6","Non-Binary Trees"
"Several pointer-based implementations for general trees are covered in Section 6.3.","chapter-6","Non-Binary Trees"
"Aside from general trees and binary trees, there are also uses for trees whose in-","chapter-6","Non-Binary Trees"
"ternal nodes have a fixed number K of children where K is something other than","chapter-6","Non-Binary Trees"
"two. Such trees are known as K-ary trees. Section 6.4 generalizes the properties","chapter-6","Non-Binary Trees"
"of binary trees to K-ary trees. Sequential representations, useful for applications","chapter-6","Non-Binary Trees"
"such as storing trees on disk, are covered in Section 6.5.","chapter-6","Non-Binary Trees"
"6.1 General Tree Definitions and Terminology","chapter-6","Non-Binary Trees"
"A tree T is a finite set of one or more nodes such that there is one designated node","chapter-6","Non-Binary Trees"
"R, called the root of T. If the set (T− {R}) is not empty, these nodes are partitioned","chapter-6","Non-Binary Trees"
"into n > 0 disjoint subsets T0, T1, ..., Tn−1, each of which is a tree, and whose","chapter-6","Non-Binary Trees"
"roots R1, R2, ..., Rn, respectively, are children of R. The subsets Ti (0 ≤ i < n) are","chapter-6","Non-Binary Trees"
"said to be subtrees of T. These subtrees are ordered in that Ti","chapter-6","Non-Binary Trees"
"is said to come before","chapter-6","Non-Binary Trees"
"195","chapter-6","Non-Binary Trees"
"196 Chap. 6 Non-Binary Trees","chapter-6","Non-Binary Trees"
"S1 S2","chapter-6","Non-Binary Trees"
"Children of V","chapter-6","Non-Binary Trees"
"Subtree rooted at V","chapter-6","Non-Binary Trees"
"Siblings of V","chapter-6","Non-Binary Trees"
"Ancestors of V","chapter-6","Non-Binary Trees"
"Root R","chapter-6","Non-Binary Trees"
"Parent of V P","chapter-6","Non-Binary Trees"
"V","chapter-6","Non-Binary Trees"
"C1 C2 C3","chapter-6","Non-Binary Trees"
"Figure 6.1 Notation for general trees. Node P is the parent of nodes V, S1,","chapter-6","Non-Binary Trees"
"and S2. Thus, V, S1, and S2 are children of P. Nodes R and P are ancestors of V.","chapter-6","Non-Binary Trees"
"Nodes V, S1, and S2 are called siblings. The oval surrounds the subtree having V","chapter-6","Non-Binary Trees"
"as its root.","chapter-6","Non-Binary Trees"
"Tj if i < j. By convention, the subtrees are arranged from left to right with subtree","chapter-6","Non-Binary Trees"
"T0 called the leftmost child of R. A node’s out degree is the number of children for","chapter-6","Non-Binary Trees"
"that node. A forest is a collection of one or more trees. Figure 6.1 presents further","chapter-6","Non-Binary Trees"
"tree notation generalized from the notation for binary trees presented in Chapter 5.","chapter-6","Non-Binary Trees"
"Each node in a tree has precisely one parent, except for the root, which has no","chapter-6","Non-Binary Trees"
"parent. From this observation, it immediately follows that a tree with n nodes must","chapter-6","Non-Binary Trees"
"have n − 1 edges because each node, aside from the root, has one edge connecting","chapter-6","Non-Binary Trees"
"that node to its parent.","chapter-6","Non-Binary Trees"
"6.1.1 An ADT for General Tree Nodes","chapter-6","Non-Binary Trees"
"Before discussing general tree implementations, we should first make precise what","chapter-6","Non-Binary Trees"
"operations such implementations must support. Any implementation must be able","chapter-6","Non-Binary Trees"
"to initialize a tree. Given a tree, we need access to the root of that tree. There","chapter-6","Non-Binary Trees"
"must be some way to access the children of a node. In the case of the ADT for","chapter-6","Non-Binary Trees"
"binary tree nodes, this was done by providing member functions that give explicit","chapter-6","Non-Binary Trees"
"access to the left and right child pointers. Unfortunately, because we do not know","chapter-6","Non-Binary Trees"
"in advance how many children a given node will have in the general tree, we cannot","chapter-6","Non-Binary Trees"
"give explicit functions to access each child. An alternative must be found that works","chapter-6","Non-Binary Trees"
"for an unknown number of children.","chapter-6","Non-Binary Trees"
"Sec. 6.1 General Tree Definitions and Terminology 197","chapter-6","Non-Binary Trees"
"/** General tree node ADT */","chapter-6","Non-Binary Trees"
"interface GTNode<E> {","chapter-6","Non-Binary Trees"
"public E value();","chapter-6","Non-Binary Trees"
"public boolean isLeaf();","chapter-6","Non-Binary Trees"
"public GTNode<E> parent();","chapter-6","Non-Binary Trees"
"public GTNode<E> leftmostChild();","chapter-6","Non-Binary Trees"
"public GTNode<E> rightSibling();","chapter-6","Non-Binary Trees"
"public void setValue(E value);","chapter-6","Non-Binary Trees"
"public void setParent(GTNode<E> par);","chapter-6","Non-Binary Trees"
"public void insertFirst(GTNode<E> n);","chapter-6","Non-Binary Trees"
"public void insertNext(GTNode<E> n);","chapter-6","Non-Binary Trees"
"public void removeFirst();","chapter-6","Non-Binary Trees"
"public void removeNext();","chapter-6","Non-Binary Trees"
"}","chapter-6","Non-Binary Trees"
"/** General tree ADT */","chapter-6","Non-Binary Trees"
"interface GenTree<E> {","chapter-6","Non-Binary Trees"
"public void clear(); // Clear the tree","chapter-6","Non-Binary Trees"
"public GTNode<E> root(); // Return the root","chapter-6","Non-Binary Trees"
"// Make the tree have a new root, give first child and sib","chapter-6","Non-Binary Trees"
"public void newroot(E value, GTNode<E> first,","chapter-6","Non-Binary Trees"
"GTNode<E> sib);","chapter-6","Non-Binary Trees"
"public void newleftchild(E value); // Add left child","chapter-6","Non-Binary Trees"
"}","chapter-6","Non-Binary Trees"
"Figure 6.2 Interfaces for the general tree and general tree node","chapter-6","Non-Binary Trees"
"One choice would be to provide a function that takes as its parameter the index","chapter-6","Non-Binary Trees"
"for the desired child. That combined with a function that returns the number of","chapter-6","Non-Binary Trees"
"children for a given node would support the ability to access any node or process","chapter-6","Non-Binary Trees"
"all children of a node. Unfortunately, this view of access tends to bias the choice for","chapter-6","Non-Binary Trees"
"node implementations in favor of an array-based approach, because these functions","chapter-6","Non-Binary Trees"
"favor random access to a list of children. In practice, an implementation based on","chapter-6","Non-Binary Trees"
"a linked list is often preferred.","chapter-6","Non-Binary Trees"
"An alternative is to provide access to the first (or leftmost) child of a node, and","chapter-6","Non-Binary Trees"
"to provide access to the next (or right) sibling of a node. Figure 6.2 shows class","chapter-6","Non-Binary Trees"
"declarations for general trees and their nodes. Based on these two access functions,","chapter-6","Non-Binary Trees"
"the children of a node can be traversed like a list. Trying to find the next sibling of","chapter-6","Non-Binary Trees"
"the rightmost sibling would return null.","chapter-6","Non-Binary Trees"
"6.1.2 General Tree Traversals","chapter-6","Non-Binary Trees"
"In Section 5.2, three tree traversals were presented for binary trees: preorder, pos-","chapter-6","Non-Binary Trees"
"torder, and inorder. For general trees, preorder and postorder traversals are defined","chapter-6","Non-Binary Trees"
"with meanings similar to their binary tree counterparts. Preorder traversal of a gen-","chapter-6","Non-Binary Trees"
"eral tree first visits the root of the tree, then performs a preorder traversal of each","chapter-6","Non-Binary Trees"
"subtree from left to right. A postorder traversal of a general tree performs a pos-","chapter-6","Non-Binary Trees"
"torder traversal of the root’s subtrees from left to right, then visits the root. Inorder","chapter-6","Non-Binary Trees"
"198 Chap. 6 Non-Binary Trees","chapter-6","Non-Binary Trees"
"B","chapter-6","Non-Binary Trees"
"C D E F","chapter-6","Non-Binary Trees"
"A","chapter-6","Non-Binary Trees"
"R","chapter-6","Non-Binary Trees"
"Figure 6.3 An example of a general tree.","chapter-6","Non-Binary Trees"
"traversal does not have a natural definition for the general tree, because there is no","chapter-6","Non-Binary Trees"
"particular number of children for an internal node. An arbitrary definition — such","chapter-6","Non-Binary Trees"
"as visit the leftmost subtree in inorder, then the root, then visit the remaining sub-","chapter-6","Non-Binary Trees"
"trees in inorder — can be invented. However, inorder traversals are generally not","chapter-6","Non-Binary Trees"
"useful with general trees.","chapter-6","Non-Binary Trees"
"Example 6.1 A preorder traversal of the tree in Figure 6.3 visits the nodes","chapter-6","Non-Binary Trees"
"in order RACDEBF.","chapter-6","Non-Binary Trees"
"A postorder traversal of this tree visits the nodes in order CDEAF BR.","chapter-6","Non-Binary Trees"
"To perform a preorder traversal, it is necessary to visit each of the children for","chapter-6","Non-Binary Trees"
"a given node (say R) from left to right. This is accomplished by starting at R’s","chapter-6","Non-Binary Trees"
"leftmost child (call it T). From T, we can move to T’s right sibling, and then to that","chapter-6","Non-Binary Trees"
"node’s right sibling, and so on.","chapter-6","Non-Binary Trees"
"Using the ADT of Figure 6.2, here is a Java implementation to print the nodes","chapter-6","Non-Binary Trees"
"of a general tree in preorder. Note the for loop at the end, which processes the","chapter-6","Non-Binary Trees"
"list of children by beginning with the leftmost child, then repeatedly moving to the","chapter-6","Non-Binary Trees"
"next child until calling next returns null.","chapter-6","Non-Binary Trees"
"/** Preorder traversal for general trees */","chapter-6","Non-Binary Trees"
"static <E> void preorder(GTNode<E> rt) {","chapter-6","Non-Binary Trees"
"PrintNode(rt);","chapter-6","Non-Binary Trees"
"if (!rt.isLeaf()) {","chapter-6","Non-Binary Trees"
"GTNode<E> temp = rt.leftmostChild();","chapter-6","Non-Binary Trees"
"while (temp != null) {","chapter-6","Non-Binary Trees"
"preorder(temp);","chapter-6","Non-Binary Trees"
"temp = temp.rightSibling();","chapter-6","Non-Binary Trees"
"}","chapter-6","Non-Binary Trees"
"}","chapter-6","Non-Binary Trees"
"}","chapter-6","Non-Binary Trees"
"Sec. 6.2 The Parent Pointer Implementation 199","chapter-6","Non-Binary Trees"
"6.2 The Parent Pointer Implementation","chapter-6","Non-Binary Trees"
"Perhaps the simplest general tree implementation is to store for each node only a","chapter-6","Non-Binary Trees"
"pointer to that node’s parent. We will call this the parent pointer implementation.","chapter-6","Non-Binary Trees"
"Clearly this implementation is not general purpose, because it is inadequate for","chapter-6","Non-Binary Trees"
"such important operations as finding the leftmost child or the right sibling for a","chapter-6","Non-Binary Trees"
"node. Thus, it may seem to be a poor idea to implement a general tree in this","chapter-6","Non-Binary Trees"
"way. However, the parent pointer implementation stores precisely the information","chapter-6","Non-Binary Trees"
"required to answer the following, useful question: “Given two nodes, are they in","chapter-6","Non-Binary Trees"
"the same tree?” To answer the question, we need only follow the series of parent","chapter-6","Non-Binary Trees"
"pointers from each node to its respective root. If both nodes reach the same root,","chapter-6","Non-Binary Trees"
"then they must be in the same tree. If the roots are different, then the two nodes are","chapter-6","Non-Binary Trees"
"not in the same tree. The process of finding the ultimate root for a given node we","chapter-6","Non-Binary Trees"
"will call FIND.","chapter-6","Non-Binary Trees"
"The parent pointer representation is most often used to maintain a collection of","chapter-6","Non-Binary Trees"
"disjoint sets. Two disjoint sets share no members in common (their intersection is","chapter-6","Non-Binary Trees"
"empty). A collection of disjoint sets partitions some objects such that every object","chapter-6","Non-Binary Trees"
"is in exactly one of the disjoint sets. There are two basic operations that we wish to","chapter-6","Non-Binary Trees"
"support:","chapter-6","Non-Binary Trees"
"(1) determine if two objects are in the same set, and","chapter-6","Non-Binary Trees"
"(2) merge two sets together.","chapter-6","Non-Binary Trees"
"Because two merged sets are united, the merging operation is called UNION and","chapter-6","Non-Binary Trees"
"the whole process of determining if two objects are in the same set and then merging","chapter-6","Non-Binary Trees"
"the sets goes by the name “UNION/FIND.”","chapter-6","Non-Binary Trees"
"To implement UNION/FIND, we represent each disjoint set with a separate","chapter-6","Non-Binary Trees"
"general tree. Two objects are in the same disjoint set if they are in the same tree.","chapter-6","Non-Binary Trees"
"Every node of the tree (except for the root) has precisely one parent. Thus, each","chapter-6","Non-Binary Trees"
"node requires the same space to represent it. The collection of objects is typically","chapter-6","Non-Binary Trees"
"stored in an array, where each element of the array corresponds to one object, and","chapter-6","Non-Binary Trees"
"each element stores the object’s value. The objects also correspond to nodes in","chapter-6","Non-Binary Trees"
"the various disjoint trees (one tree for each disjoint set), so we also store the parent","chapter-6","Non-Binary Trees"
"value with each object in the array. Those nodes that are the roots of their respective","chapter-6","Non-Binary Trees"
"trees store an appropriate indicator. Note that this representation means that a single","chapter-6","Non-Binary Trees"
"array is being used to implement a collection of trees. This makes it easy to merge","chapter-6","Non-Binary Trees"
"trees together with UNION operations.","chapter-6","Non-Binary Trees"
"Figure 6.4 shows the parent pointer implementation for the general tree, called","chapter-6","Non-Binary Trees"
"ParPtrTree. This class is greatly simplified from the declarations of Figure 6.2","chapter-6","Non-Binary Trees"
"because we need only a subset of the general tree operations. Instead of implement-","chapter-6","Non-Binary Trees"
"ing a separate node class, ParPtrTree simply stores an array where each array","chapter-6","Non-Binary Trees"
"element corresponds to a node of the tree. Each position i of the array stores the","chapter-6","Non-Binary Trees"
"value for node i and the array position for the parent of node i. Class ParPtrTree","chapter-6","Non-Binary Trees"
"200 Chap. 6 Non-Binary Trees","chapter-6","Non-Binary Trees"
"/** General Tree class implementation for UNION/FIND */","chapter-6","Non-Binary Trees"
"class ParPtrTree {","chapter-6","Non-Binary Trees"
"private Integer [] array; // Node array","chapter-6","Non-Binary Trees"
"public ParPtrTree(int size) {","chapter-6","Non-Binary Trees"
"array = new Integer[size]; // Create node array","chapter-6","Non-Binary Trees"
"for (int i=0; i<size; i++)","chapter-6","Non-Binary Trees"
"array[i] = null;","chapter-6","Non-Binary Trees"
"}","chapter-6","Non-Binary Trees"
"/** Determine if nodes are in different trees */","chapter-6","Non-Binary Trees"
"public boolean differ(int a, int b) {","chapter-6","Non-Binary Trees"
"Integer root1 = FIND(a); // Find root of node a","chapter-6","Non-Binary Trees"
"Integer root2 = FIND(b); // Find root of node b","chapter-6","Non-Binary Trees"
"return root1 != root2; // Compare roots","chapter-6","Non-Binary Trees"
"}","chapter-6","Non-Binary Trees"
"/** Merge two subtrees */","chapter-6","Non-Binary Trees"
"public void UNION(int a, int b) {","chapter-6","Non-Binary Trees"
"Integer root1 = FIND(a); // Find root of node a","chapter-6","Non-Binary Trees"
"Integer root2 = FIND(b); // Find root of node b","chapter-6","Non-Binary Trees"
"if (root1 != root2) array[root2] = root1; // Merge","chapter-6","Non-Binary Trees"
"}","chapter-6","Non-Binary Trees"
"/** @return The root of curr’s tree */","chapter-6","Non-Binary Trees"
"public Integer FIND(Integer curr) {","chapter-6","Non-Binary Trees"
"if (array[curr] == null) return curr; // At root","chapter-6","Non-Binary Trees"
"while (array[curr] != null) curr = array[curr];","chapter-6","Non-Binary Trees"
"return curr;","chapter-6","Non-Binary Trees"
"}","chapter-6","Non-Binary Trees"
"Figure 6.4 General tree implementation using parent pointers for the UNION/","chapter-6","Non-Binary Trees"
"FIND algorithm.","chapter-6","Non-Binary Trees"
"is given two new methods, differ and UNION. Method differ checks if two","chapter-6","Non-Binary Trees"
"objects are in different sets, and method UNION merges two sets together. A private","chapter-6","Non-Binary Trees"
"method FIND is used to find the ultimate root for an object.","chapter-6","Non-Binary Trees"
"An application using the UNION/FIND operations should store a set of n ob-","chapter-6","Non-Binary Trees"
"jects, where each object is assigned a unique index in the range 0 to n − 1. The","chapter-6","Non-Binary Trees"
"indices refer to the corresponding parent pointers in the array. Class ParPtrTree","chapter-6","Non-Binary Trees"
"creates and initializes the UNION/FIND array, and methods differ and UNION","chapter-6","Non-Binary Trees"
"take array indices as inputs.","chapter-6","Non-Binary Trees"
"Figure 6.5 illustrates the parent pointer implementation. Note that the nodes","chapter-6","Non-Binary Trees"
"can appear in any order within the array, and the array can store up to n separate","chapter-6","Non-Binary Trees"
"trees. For example, Figure 6.5 shows two trees stored in the same array. Thus,","chapter-6","Non-Binary Trees"
"a single array can store a collection of items distributed among an arbitrary (and","chapter-6","Non-Binary Trees"
"changing) number of disjoint subsets.","chapter-6","Non-Binary Trees"
"Consider the problem of assigning the members of a set to disjoint subsets","chapter-6","Non-Binary Trees"
"called equivalence classes. Recall from Section 2.1 that an equivalence relation is","chapter-6","Non-Binary Trees"
"Sec. 6.2 The Parent Pointer Implementation 201","chapter-6","Non-Binary Trees"
"C D F","chapter-6","Non-Binary Trees"
"W","chapter-6","Non-Binary Trees"
"X Y","chapter-6","Non-Binary Trees"
"Parent’s Index 1 1 1 2","chapter-6","Non-Binary Trees"
"Label R A B C D E","chapter-6","Non-Binary Trees"
"Z","chapter-6","Non-Binary Trees"
"F W Z X Y","chapter-6","Non-Binary Trees"
"0 7 7 7 0","chapter-6","Non-Binary Trees"
"A B","chapter-6","Non-Binary Trees"
"E","chapter-6","Non-Binary Trees"
"R","chapter-6","Non-Binary Trees"
"Node Index 0 1 2 3 4 5 6 7 8 9 10","chapter-6","Non-Binary Trees"
"Figure 6.5 The parent pointer array implementation. Each node corresponds","chapter-6","Non-Binary Trees"
"to a position in the node array, which stores its value and a pointer to its parent.","chapter-6","Non-Binary Trees"
"The parent pointers are represented by the position in the array of the parent. The","chapter-6","Non-Binary Trees"
"root of any tree stores ROOT, represented graphically by a slash in the “Parent’s","chapter-6","Non-Binary Trees"
"Index” box. This figure shows two trees stored in the same parent pointer array,","chapter-6","Non-Binary Trees"
"one rooted at R, and the other rooted at W.","chapter-6","Non-Binary Trees"
"B","chapter-6","Non-Binary Trees"
"E","chapter-6","Non-Binary Trees"
"D","chapter-6","Non-Binary Trees"
"G","chapter-6","Non-Binary Trees"
"A F J","chapter-6","Non-Binary Trees"
"C","chapter-6","Non-Binary Trees"
"I","chapter-6","Non-Binary Trees"
"H","chapter-6","Non-Binary Trees"
"Figure 6.6 A graph with two connected components.","chapter-6","Non-Binary Trees"
"reflexive, symmetric, and transitive. Thus, if objects A and B are equivalent, and","chapter-6","Non-Binary Trees"
"objects B and C are equivalent, we must be able to recognize that objects A and C","chapter-6","Non-Binary Trees"
"are also equivalent.","chapter-6","Non-Binary Trees"
"There are many practical uses for disjoint sets and representing equivalences.","chapter-6","Non-Binary Trees"
"For example, consider Figure 6.6 which shows a graph of ten nodes labeled A","chapter-6","Non-Binary Trees"
"through J. Notice that for nodes A through I, there is some series of edges that","chapter-6","Non-Binary Trees"
"connects any pair of the nodes, but node J is disconnected from the rest of the","chapter-6","Non-Binary Trees"
"nodes. Such a graph might be used to represent connections such as wires be-","chapter-6","Non-Binary Trees"
"tween components on a circuit board, or roads between cities. We can consider","chapter-6","Non-Binary Trees"
"two nodes of the graph to be equivalent if there is a path between them. Thus,","chapter-6","Non-Binary Trees"
"nodes A, H, and E would be equivalent in Figure 6.6, but J is not equivalent to any","chapter-6","Non-Binary Trees"
"other. A subset of equivalent (connected) edges in a graph is called a connected","chapter-6","Non-Binary Trees"
"component. The goal is to quickly classify the objects into disjoint sets that corre-","chapter-6","Non-Binary Trees"
"202 Chap. 6 Non-Binary Trees","chapter-6","Non-Binary Trees"
"spond to the connected components. Another application for UNION/FIND occurs","chapter-6","Non-Binary Trees"
"in Kruskal’s algorithm for computing the minimal cost spanning tree for a graph","chapter-6","Non-Binary Trees"
"(Section 11.5.2).","chapter-6","Non-Binary Trees"
"The input to the UNION/FIND algorithm is typically a series of equivalence","chapter-6","Non-Binary Trees"
"pairs. In the case of the connected components example, the equivalence pairs","chapter-6","Non-Binary Trees"
"would simply be the set of edges in the graph. An equivalence pair might say that","chapter-6","Non-Binary Trees"
"object C is equivalent to object A. If so, C and A are placed in the same subset. If","chapter-6","Non-Binary Trees"
"a later equivalence relates A and B, then by implication C is also equivalent to B.","chapter-6","Non-Binary Trees"
"Thus, an equivalence pair may cause two subsets to merge, each of which contains","chapter-6","Non-Binary Trees"
"several objects.","chapter-6","Non-Binary Trees"
"Equivalence classes can be managed efficiently with the UNION/FIND alg-","chapter-6","Non-Binary Trees"
"orithm. Initially, each object is at the root of its own tree. An equivalence pair is","chapter-6","Non-Binary Trees"
"processed by checking to see if both objects of the pair are in the same tree us-","chapter-6","Non-Binary Trees"
"ing method differ. If they are in the same tree, then no change need be made","chapter-6","Non-Binary Trees"
"because the objects are already in the same equivalence class. Otherwise, the two","chapter-6","Non-Binary Trees"
"equivalence classes should be merged by the UNION method.","chapter-6","Non-Binary Trees"
"Example 6.2 As an example of solving the equivalence class problem,","chapter-6","Non-Binary Trees"
"consider the graph of Figure 6.6. Initially, we assume that each node of the","chapter-6","Non-Binary Trees"
"graph is in a distinct equivalence class. This is represented by storing each","chapter-6","Non-Binary Trees"
"as the root of its own tree. Figure 6.7(a) shows this initial configuration","chapter-6","Non-Binary Trees"
"using the parent pointer array representation. Now, consider what happens","chapter-6","Non-Binary Trees"
"when equivalence relationship (A, B) is processed. The root of the tree","chapter-6","Non-Binary Trees"
"containing A is A, and the root of the tree containing B is B. To make them","chapter-6","Non-Binary Trees"
"equivalent, one of these two roots is set to be the parent of the other. In","chapter-6","Non-Binary Trees"
"this case it is irrelevant which points to which, so we arbitrarily select the","chapter-6","Non-Binary Trees"
"first in alphabetical order to be the root. This is represented in the parent","chapter-6","Non-Binary Trees"
"pointer array by setting the parent field of B (the node in array position 1","chapter-6","Non-Binary Trees"
"of the array) to store a pointer to A. Equivalence pairs (C, H), (G, F), and","chapter-6","Non-Binary Trees"
"(D, E) are processed in similar fashion. When processing the equivalence","chapter-6","Non-Binary Trees"
"pair (I, F), because I and F are both their own roots, I is set to point to F.","chapter-6","Non-Binary Trees"
"Note that this also makes G equivalent to I. The result of processing these","chapter-6","Non-Binary Trees"
"five equivalences is shown in Figure 6.7(b).","chapter-6","Non-Binary Trees"
"The parent pointer representation places no limit on the number of nodes that","chapter-6","Non-Binary Trees"
"can share a parent. To make equivalence processing as efficient as possible, the","chapter-6","Non-Binary Trees"
"distance from each node to the root of its respective tree should be as small as","chapter-6","Non-Binary Trees"
"possible. Thus, we would like to keep the height of the trees small when merging","chapter-6","Non-Binary Trees"
"two equivalence classes together. Ideally, each tree would have all nodes pointing","chapter-6","Non-Binary Trees"
"directly to the root. Achieving this goal all the time would require too much ad-","chapter-6","Non-Binary Trees"
"Sec. 6.2 The Parent Pointer Implementation 203","chapter-6","Non-Binary Trees"
"0 1 2 3 4 5 6 7 8 9","chapter-6","Non-Binary Trees"
"0 1 2 3 4 5 6 7 8 9","chapter-6","Non-Binary Trees"
"0 1 2 3 4 5 6 7 8 9","chapter-6","Non-Binary Trees"
"(a)","chapter-6","Non-Binary Trees"
"(b)","chapter-6","Non-Binary Trees"
"(c)","chapter-6","Non-Binary Trees"
"(d)","chapter-6","Non-Binary Trees"
"0 1 2 3 4 5 6 7 8 9","chapter-6","Non-Binary Trees"
"B H G","chapter-6","Non-Binary Trees"
"A C F","chapter-6","Non-Binary Trees"
"A F","chapter-6","Non-Binary Trees"
"C G","chapter-6","Non-Binary Trees"
"H","chapter-6","Non-Binary Trees"
"F","chapter-6","Non-Binary Trees"
"A","chapter-6","Non-Binary Trees"
"B C","chapter-6","Non-Binary Trees"
"G D","chapter-6","Non-Binary Trees"
"E","chapter-6","Non-Binary Trees"
"H","chapter-6","Non-Binary Trees"
"J","chapter-6","Non-Binary Trees"
"J D","chapter-6","Non-Binary Trees"
"E","chapter-6","Non-Binary Trees"
"B D","chapter-6","Non-Binary Trees"
"E","chapter-6","Non-Binary Trees"
"J","chapter-6","Non-Binary Trees"
"B C D E F G H","chapter-6","Non-Binary Trees"
"B C D F G H","chapter-6","Non-Binary Trees"
"G","chapter-6","Non-Binary Trees"
"B C D E G H","chapter-6","Non-Binary Trees"
"A","chapter-6","Non-Binary Trees"
"A","chapter-6","Non-Binary Trees"
"A","chapter-6","Non-Binary Trees"
"E","chapter-6","Non-Binary Trees"
"F","chapter-6","Non-Binary Trees"
"I","chapter-6","Non-Binary Trees"
"B C D","chapter-6","Non-Binary Trees"
"J","chapter-6","Non-Binary Trees"
"I","chapter-6","Non-Binary Trees"
"I","chapter-6","Non-Binary Trees"
"E","chapter-6","Non-Binary Trees"
"H","chapter-6","Non-Binary Trees"
"J","chapter-6","Non-Binary Trees"
"F","chapter-6","Non-Binary Trees"
"A","chapter-6","Non-Binary Trees"
"I","chapter-6","Non-Binary Trees"
"I","chapter-6","Non-Binary Trees"
"I","chapter-6","Non-Binary Trees"
"I","chapter-6","Non-Binary Trees"
"J","chapter-6","Non-Binary Trees"
"J","chapter-6","Non-Binary Trees"
"0 0 5 5","chapter-6","Non-Binary Trees"
"0 3 5 2","chapter-6","Non-Binary Trees"
"3 2","chapter-6","Non-Binary Trees"
"5 0 0 5 3 2 5","chapter-6","Non-Binary Trees"
"A B D E F G H I C","chapter-6","Non-Binary Trees"
"5","chapter-6","Non-Binary Trees"
"5","chapter-6","Non-Binary Trees"
"5","chapter-6","Non-Binary Trees"
"J","chapter-6","Non-Binary Trees"
"Figure 6.7 An example of equivalence processing. (a) Initial configuration for","chapter-6","Non-Binary Trees"
"the ten nodes of the graph in Figure 6.6. The nodes are placed into ten independent","chapter-6","Non-Binary Trees"
"equivalence classes. (b) The result of processing five edges: (A, B), (C, H), (G, F),","chapter-6","Non-Binary Trees"
"(D, E), and (I, F). (c) The result of processing two more edges: (H, A) and (E, G).","chapter-6","Non-Binary Trees"
"(d) The result of processing edge (H, E).","chapter-6","Non-Binary Trees"
"204 Chap. 6 Non-Binary Trees","chapter-6","Non-Binary Trees"
"ditional processing to be worth the effort, so we must settle for getting as close as","chapter-6","Non-Binary Trees"
"possible.","chapter-6","Non-Binary Trees"
"A low-cost approach to reducing the height is to be smart about how two trees","chapter-6","Non-Binary Trees"
"are joined together. One simple technique, called the weighted union rule, joins","chapter-6","Non-Binary Trees"
"the tree with fewer nodes to the tree with more nodes by making the smaller tree’s","chapter-6","Non-Binary Trees"
"root point to the root of the bigger tree. This will limit the total depth of the tree to","chapter-6","Non-Binary Trees"
"O(log n), because the depth of nodes only in the smaller tree will now increase by","chapter-6","Non-Binary Trees"
"one, and the depth of the deepest node in the combined tree can only be at most one","chapter-6","Non-Binary Trees"
"deeper than the deepest node before the trees were combined. The total number","chapter-6","Non-Binary Trees"
"of nodes in the combined tree is therefore at least twice the number in the smaller","chapter-6","Non-Binary Trees"
"subtree. Thus, the depth of any node can be increased at most log n times when n","chapter-6","Non-Binary Trees"
"equivalences are processed.","chapter-6","Non-Binary Trees"
"Example 6.3 When processing equivalence pair (I, F) in Figure 6.7(b),","chapter-6","Non-Binary Trees"
"F is the root of a tree with two nodes while I is the root of a tree with only","chapter-6","Non-Binary Trees"
"one node. Thus, I is set to point to F rather than the other way around.","chapter-6","Non-Binary Trees"
"Figure 6.7(c) shows the result of processing two more equivalence pairs:","chapter-6","Non-Binary Trees"
"(H, A) and (E, G). For the first pair, the root for H is C while the root","chapter-6","Non-Binary Trees"
"for A is itself. Both trees contain two nodes, so it is an arbitrary decision","chapter-6","Non-Binary Trees"
"as to which node is set to be the root for the combined tree. In the case","chapter-6","Non-Binary Trees"
"of equivalence pair (E, G), the root of E is D while the root of G is F.","chapter-6","Non-Binary Trees"
"Because F is the root of the larger tree, node D is set to point to F.","chapter-6","Non-Binary Trees"
"Not all equivalences will combine two trees. If equivalence (F, G) is processed","chapter-6","Non-Binary Trees"
"when the representation is in the state shown in Figure 6.7(c), no change will be","chapter-6","Non-Binary Trees"
"made because F is already the root for G.","chapter-6","Non-Binary Trees"
"The weighted union rule helps to minimize the depth of the tree, but we can do","chapter-6","Non-Binary Trees"
"better than this. Path compression is a method that tends to create extremely shal-","chapter-6","Non-Binary Trees"
"low trees. Path compression takes place while finding the root for a given node X.","chapter-6","Non-Binary Trees"
"Call this root R. Path compression resets the parent of every node on the path from","chapter-6","Non-Binary Trees"
"X to R to point directly to R. This can be implemented by first finding R. A second","chapter-6","Non-Binary Trees"
"pass is then made along the path from X to R, assigning the parent field of each","chapter-6","Non-Binary Trees"
"node encountered to R. Alternatively, a recursive algorithm can be implemented as","chapter-6","Non-Binary Trees"
"follows. This version of FIND not only returns the root of the current node, but","chapter-6","Non-Binary Trees"
"also makes all ancestors of the current node point to the root.","chapter-6","Non-Binary Trees"
"public Integer FIND(Integer curr) {","chapter-6","Non-Binary Trees"
"if (array[curr] == null) return curr; // At root","chapter-6","Non-Binary Trees"
"array[curr] = FIND(array[curr]);","chapter-6","Non-Binary Trees"
"return array[curr];","chapter-6","Non-Binary Trees"
"}","chapter-6","Non-Binary Trees"
"Sec. 6.2 The Parent Pointer Implementation 205","chapter-6","Non-Binary Trees"
"5 0 0 5 5 5 0 5","chapter-6","Non-Binary Trees"
"A B C D J","chapter-6","Non-Binary Trees"
"0 1 2 3 4 5 6 7 8 9 A","chapter-6","Non-Binary Trees"
"B","chapter-6","Non-Binary Trees"
"G E","chapter-6","Non-Binary Trees"
"E F G H I","chapter-6","Non-Binary Trees"
"J","chapter-6","Non-Binary Trees"
"C H","chapter-6","Non-Binary Trees"
"I D","chapter-6","Non-Binary Trees"
"F","chapter-6","Non-Binary Trees"
"Figure 6.8 An example of path compression, showing the result of processing","chapter-6","Non-Binary Trees"
"equivalence pair (H, E) on the representation of Figure 6.7(c).","chapter-6","Non-Binary Trees"
"Example 6.4 Figure 6.7(d) shows the result of processing equivalence","chapter-6","Non-Binary Trees"
"pair (H, E) on the the representation shown in Figure 6.7(c) using the stan-","chapter-6","Non-Binary Trees"
"dard weighted union rule without path compression. Figure 6.8 illustrates","chapter-6","Non-Binary Trees"
"the path compression process for the same equivalence pair. After locating","chapter-6","Non-Binary Trees"
"the root for node H, we can perform path compression to make H point","chapter-6","Non-Binary Trees"
"directly to root object A. Likewise, E is set to point directly to its root, F.","chapter-6","Non-Binary Trees"
"Finally, object A is set to point to root object F.","chapter-6","Non-Binary Trees"
"Note that path compression takes place during the FIND operation, not","chapter-6","Non-Binary Trees"
"during the UNION operation. In Figure 6.8, this means that nodes B, C, and","chapter-6","Non-Binary Trees"
"H have node A remain as their parent, rather than changing their parent to","chapter-6","Non-Binary Trees"
"be F. While we might prefer to have these nodes point to F, to accomplish","chapter-6","Non-Binary Trees"
"this would require that additional information from the FIND operation be","chapter-6","Non-Binary Trees"
"passed back to the UNION operation. This would not be practical.","chapter-6","Non-Binary Trees"
"Path compression keeps the cost of each FIND operation very close to constant.","chapter-6","Non-Binary Trees"
"To be more precise about what is meant by “very close to constant,” the cost of path","chapter-6","Non-Binary Trees"
"compression for n FIND operations on n nodes (when combined with the weighted","chapter-6","Non-Binary Trees"
"union rule for joining sets) is approximately1 Θ(n log∗ n). The notation “log∗ n”","chapter-6","Non-Binary Trees"
"means the number of times that the log of n must be taken before n ≤ 1. For","chapter-6","Non-Binary Trees"
"example, log∗","chapter-6","Non-Binary Trees"
"65536 is 4 because log 65536 = 16, log 16 = 4, log 4 = 2, and","chapter-6","Non-Binary Trees"
"finally log 2 = 1. Thus, log∗ n grows very slowly, so the cost for a series of n FIND","chapter-6","Non-Binary Trees"
"operations is very close to n.","chapter-6","Non-Binary Trees"
"Note that this does not mean that the tree resulting from processing n equiva-","chapter-6","Non-Binary Trees"
"lence pairs necessarily has depth Θ(log∗ n). One can devise a series of equivalence","chapter-6","Non-Binary Trees"
"operations that yields Θ(log n) depth for the resulting tree. However, many of the","chapter-6","Non-Binary Trees"
"equivalences in such a series will look only at the roots of the trees being merged,","chapter-6","Non-Binary Trees"
"requiring little processing time. The total amount of processing time required for","chapter-6","Non-Binary Trees"
"n operations will be Θ(n log∗ n), yielding nearly constant time for each equiva-","chapter-6","Non-Binary Trees"
"1To be more precise, this cost has been found to grow in time proportional to the inverse of","chapter-6","Non-Binary Trees"
"Ackermann’s function. See Section 6.6.","chapter-6","Non-Binary Trees"
"206 Chap. 6 Non-Binary Trees","chapter-6","Non-Binary Trees"
"lence operation. This is an example of amortized analysis, discussed further in","chapter-6","Non-Binary Trees"
"Section 14.3.","chapter-6","Non-Binary Trees"
"6.3 General Tree Implementations","chapter-6","Non-Binary Trees"
"We now tackle the problem of devising an implementation for general trees that","chapter-6","Non-Binary Trees"
"allows efficient processing for all member functions of the ADTs shown in Fig-","chapter-6","Non-Binary Trees"
"ure 6.2. This section presents several approaches to implementing general trees.","chapter-6","Non-Binary Trees"
"Each implementation yields advantages and disadvantages in the amount of space","chapter-6","Non-Binary Trees"
"required to store a node and the relative ease with which key operations can be","chapter-6","Non-Binary Trees"
"performed. General tree implementations should place no restriction on how many","chapter-6","Non-Binary Trees"
"children a node may have. In some applications, once a node is created the number","chapter-6","Non-Binary Trees"
"of children never changes. In such cases, a fixed amount of space can be allocated","chapter-6","Non-Binary Trees"
"for the node when it is created, based on the number of children for the node. Mat-","chapter-6","Non-Binary Trees"
"ters become more complicated if children can be added to or deleted from a node,","chapter-6","Non-Binary Trees"
"requiring that the node’s space allocation be adjusted accordingly.","chapter-6","Non-Binary Trees"
"6.3.1 List of Children","chapter-6","Non-Binary Trees"
"Our first attempt to create a general tree implementation is called the “list of chil-","chapter-6","Non-Binary Trees"
"dren” implementation for general trees. It simply stores with each internal node a","chapter-6","Non-Binary Trees"
"linked list of its children. This is illustrated by Figure 6.9.","chapter-6","Non-Binary Trees"
"The “list of children” implementation stores the tree nodes in an array. Each","chapter-6","Non-Binary Trees"
"node contains a value, a pointer (or index) to its parent, and a pointer to a linked list","chapter-6","Non-Binary Trees"
"of the node’s children, stored in order from left to right. Each linked list element","chapter-6","Non-Binary Trees"
"contains a pointer to one child. Thus, the leftmost child of a node can be found","chapter-6","Non-Binary Trees"
"directly because it is the first element in the linked list. However, to find the right","chapter-6","Non-Binary Trees"
"sibling for a node is more difficult. Consider the case of a node M and its parent P.","chapter-6","Non-Binary Trees"
"To find M’s right sibling, we must move down the child list of P until the linked list","chapter-6","Non-Binary Trees"
"element storing the pointer to M has been found. Going one step further takes us to","chapter-6","Non-Binary Trees"
"the linked list element that stores a pointer to M’s right sibling. Thus, in the worst","chapter-6","Non-Binary Trees"
"case, to find M’s right sibling requires that all children of M’s parent be searched.","chapter-6","Non-Binary Trees"
"Combining trees using this representation is difficult if each tree is stored in a","chapter-6","Non-Binary Trees"
"separate node array. If the nodes of both trees are stored in a single node array, then","chapter-6","Non-Binary Trees"
"adding tree T as a subtree of node R is done by simply adding the root of T to R’s","chapter-6","Non-Binary Trees"
"list of children.","chapter-6","Non-Binary Trees"
"6.3.2 The Left-Child/Right-Sibling Implementation","chapter-6","Non-Binary Trees"
"With the “list of children” implementation, it is difficult to access a node’s right","chapter-6","Non-Binary Trees"
"sibling. Figure 6.10 presents an improvement. Here, each node stores its value","chapter-6","Non-Binary Trees"
"and pointers to its parent, leftmost child, and right sibling. Thus, each of the basic","chapter-6","Non-Binary Trees"
"Sec. 6.3 General Tree Implementations 207","chapter-6","Non-Binary Trees"
"R","chapter-6","Non-Binary Trees"
"A B","chapter-6","Non-Binary Trees"
"C D E F","chapter-6","Non-Binary Trees"
"Index Val Par","chapter-6","Non-Binary Trees"
"0","chapter-6","Non-Binary Trees"
"1","chapter-6","Non-Binary Trees"
"2","chapter-6","Non-Binary Trees"
"3","chapter-6","Non-Binary Trees"
"4","chapter-6","Non-Binary Trees"
"5","chapter-6","Non-Binary Trees"
"6","chapter-6","Non-Binary Trees"
"7","chapter-6","Non-Binary Trees"
"R","chapter-6","Non-Binary Trees"
"A","chapter-6","Non-Binary Trees"
"C","chapter-6","Non-Binary Trees"
"B","chapter-6","Non-Binary Trees"
"D","chapter-6","Non-Binary Trees"
"F","chapter-6","Non-Binary Trees"
"E","chapter-6","Non-Binary Trees"
"0","chapter-6","Non-Binary Trees"
"1","chapter-6","Non-Binary Trees"
"0","chapter-6","Non-Binary Trees"
"1","chapter-6","Non-Binary Trees"
"3","chapter-6","Non-Binary Trees"
"1","chapter-6","Non-Binary Trees"
"3","chapter-6","Non-Binary Trees"
"2 4 6","chapter-6","Non-Binary Trees"
"5","chapter-6","Non-Binary Trees"
"1","chapter-6","Non-Binary Trees"
"Figure 6.9 The “list of children” implementation for general trees. The col-","chapter-6","Non-Binary Trees"
"umn of numbers to the left of the node array labels the array indices. The column","chapter-6","Non-Binary Trees"
"labeled “Val” stores node values. The column labeled “Par” stores indices (or","chapter-6","Non-Binary Trees"
"pointers) to the parents. The last column stores pointers to the linked list of chil-","chapter-6","Non-Binary Trees"
"dren for each internal node. Each element of the linked list stores a pointer to one","chapter-6","Non-Binary Trees"
"of the node’s children (shown as the array index of the target node).","chapter-6","Non-Binary Trees"
"ADT operations can be implemented by reading a value directly from the node.","chapter-6","Non-Binary Trees"
"If two trees are stored within the same node array, then adding one as the subtree","chapter-6","Non-Binary Trees"
"of the other simply requires setting three pointers. Combining trees in this way","chapter-6","Non-Binary Trees"
"is illustrated by Figure 6.11. This implementation is more space efficient than the","chapter-6","Non-Binary Trees"
"“list of children” implementation, and each node requires a fixed amount of space","chapter-6","Non-Binary Trees"
"in the node array.","chapter-6","Non-Binary Trees"
"6.3.3 Dynamic Node Implementations","chapter-6","Non-Binary Trees"
"The two general tree implementations just described use an array to store the col-","chapter-6","Non-Binary Trees"
"lection of nodes. In contrast, our standard implementation for binary trees stores","chapter-6","Non-Binary Trees"
"each node as a separate dynamic object containing its value and pointers to its two","chapter-6","Non-Binary Trees"
"children. Unfortunately, nodes of a general tree can have any number of children,","chapter-6","Non-Binary Trees"
"and this number may change during the life of the node. A general tree node imple-","chapter-6","Non-Binary Trees"
"mentation must support these properties. One solution is simply to limit the number","chapter-6","Non-Binary Trees"
"of children permitted for any node and allocate pointers for exactly that number of","chapter-6","Non-Binary Trees"
"children. There are two major objections to this. First, it places an undesirable","chapter-6","Non-Binary Trees"
"limit on the number of children, which makes certain trees unrepresentable by this","chapter-6","Non-Binary Trees"
"implementation. Second, this might be extremely wasteful of space because most","chapter-6","Non-Binary Trees"
"nodes will have far fewer children and thus leave some pointer positions empty.","chapter-6","Non-Binary Trees"
"208 Chap. 6 Non-Binary Trees","chapter-6","Non-Binary Trees"
"R’","chapter-6","Non-Binary Trees"
"Left Val ParRight","chapter-6","Non-Binary Trees"
"R","chapter-6","Non-Binary Trees"
"A B","chapter-6","Non-Binary Trees"
"C D E F","chapter-6","Non-Binary Trees"
"X","chapter-6","Non-Binary Trees"
"X 7","chapter-6","Non-Binary Trees"
"1","chapter-6","Non-Binary Trees"
"1","chapter-6","Non-Binary Trees"
"1","chapter-6","Non-Binary Trees"
"0","chapter-6","Non-Binary Trees"
"2","chapter-6","Non-Binary Trees"
"1","chapter-6","Non-Binary Trees"
"3","chapter-6","Non-Binary Trees"
"4","chapter-6","Non-Binary Trees"
"R","chapter-6","Non-Binary Trees"
"A","chapter-6","Non-Binary Trees"
"B","chapter-6","Non-Binary Trees"
"C","chapter-6","Non-Binary Trees"
"D","chapter-6","Non-Binary Trees"
"E","chapter-6","Non-Binary Trees"
"F","chapter-6","Non-Binary Trees"
"5","chapter-6","Non-Binary Trees"
"8 R’","chapter-6","Non-Binary Trees"
"6","chapter-6","Non-Binary Trees"
"0 2","chapter-6","Non-Binary Trees"
"Figure 6.10 The “left-child/right-sibling” implementation.","chapter-6","Non-Binary Trees"
"0","chapter-6","Non-Binary Trees"
"1","chapter-6","Non-Binary Trees"
"1","chapter-6","Non-Binary Trees"
"1","chapter-6","Non-Binary Trees"
"7","chapter-6","Non-Binary Trees"
"2","chapter-6","Non-Binary Trees"
"R’ 0","chapter-6","Non-Binary Trees"
"R X","chapter-6","Non-Binary Trees"
"A B","chapter-6","Non-Binary Trees"
"D E F","chapter-6","Non-Binary Trees"
"R","chapter-6","Non-Binary Trees"
"Left Val ParRight","chapter-6","Non-Binary Trees"
"C","chapter-6","Non-Binary Trees"
"1 8","chapter-6","Non-Binary Trees"
"3 A 2","chapter-6","Non-Binary Trees"
"6 B","chapter-6","Non-Binary Trees"
"C 4","chapter-6","Non-Binary Trees"
"D 5","chapter-6","Non-Binary Trees"
"E","chapter-6","Non-Binary Trees"
"F","chapter-6","Non-Binary Trees"
"X","chapter-6","Non-Binary Trees"
"0","chapter-6","Non-Binary Trees"
"7","chapter-6","Non-Binary Trees"
"R’","chapter-6","Non-Binary Trees"
"Figure 6.11 Combining two trees that use the “left-child/right-sibling” imple-","chapter-6","Non-Binary Trees"
"mentation. The subtree rooted at R in Figure 6.10 now becomes the first child","chapter-6","Non-Binary Trees"
"of R","chapter-6","Non-Binary Trees"
"0","chapter-6","Non-Binary Trees"
". Three pointers are adjusted in the node array: The left-child field of R","chapter-6","Non-Binary Trees"
"0 now","chapter-6","Non-Binary Trees"
"points to node R, while the right-sibling field for R points to node X. The parent","chapter-6","Non-Binary Trees"
"field of node R points to node R","chapter-6","Non-Binary Trees"
"0","chapter-6","Non-Binary Trees"
".","chapter-6","Non-Binary Trees"
"Sec. 6.3 General Tree Implementations 209","chapter-6","Non-Binary Trees"
"Val Size","chapter-6","Non-Binary Trees"
"(a) (b)","chapter-6","Non-Binary Trees"
"F","chapter-6","Non-Binary Trees"
"B","chapter-6","Non-Binary Trees"
"C D E","chapter-6","Non-Binary Trees"
"A","chapter-6","Non-Binary Trees"
"R","chapter-6","Non-Binary Trees"
"R 2","chapter-6","Non-Binary Trees"
"A 3 B 1","chapter-6","Non-Binary Trees"
"C 0 D 0 E 0 F 0","chapter-6","Non-Binary Trees"
"Figure 6.12 A dynamic general tree representation with fixed-size arrays for the","chapter-6","Non-Binary Trees"
"child pointers. (a) The general tree. (b) The tree representation. For each node,","chapter-6","Non-Binary Trees"
"the first field stores the node value while the second field stores the size of the","chapter-6","Non-Binary Trees"
"child pointer array.","chapter-6","Non-Binary Trees"
"The alternative is to allocate variable space for each node. There are two basic","chapter-6","Non-Binary Trees"
"approaches. One is to allocate an array of child pointers as part of the node. In","chapter-6","Non-Binary Trees"
"essence, each node stores an array-based list of child pointers. Figure 6.12 illus-","chapter-6","Non-Binary Trees"
"trates the concept. This approach assumes that the number of children is known","chapter-6","Non-Binary Trees"
"when the node is created, which is true for some applications but not for others.","chapter-6","Non-Binary Trees"
"It also works best if the number of children does not change. If the number of","chapter-6","Non-Binary Trees"
"children does change (especially if it increases), then some special recovery mech-","chapter-6","Non-Binary Trees"
"anism must be provided to support a change in the size of the child pointer array.","chapter-6","Non-Binary Trees"
"One possibility is to allocate a new node of the correct size from free store and re-","chapter-6","Non-Binary Trees"
"turn the old copy of the node to free store for later reuse. This works especially well","chapter-6","Non-Binary Trees"
"in a language with built-in garbage collection such as Java. For example, assume","chapter-6","Non-Binary Trees"
"that a node M initially has two children, and that space for two child pointers is al-","chapter-6","Non-Binary Trees"
"located when M is created. If a third child is added to M, space for a new node with","chapter-6","Non-Binary Trees"
"three child pointers can be allocated, the contents of M is copied over to the new","chapter-6","Non-Binary Trees"
"space, and the old space is then returned to free store. As an alternative to relying","chapter-6","Non-Binary Trees"
"on the system’s garbage collector, a memory manager for variable size storage units","chapter-6","Non-Binary Trees"
"can be implemented, as described in Section 12.3. Another possibility is to use a","chapter-6","Non-Binary Trees"
"collection of free lists, one for each array size, as described in Section 4.1.2. Note","chapter-6","Non-Binary Trees"
"in Figure 6.12 that the current number of children for each node is stored explicitly","chapter-6","Non-Binary Trees"
"in a size field. The child pointers are stored in an array with size elements.","chapter-6","Non-Binary Trees"
"Another approach that is more flexible, but which requires more space, is to","chapter-6","Non-Binary Trees"
"store a linked list of child pointers with each node as illustrated by Figure 6.13.","chapter-6","Non-Binary Trees"
"This implementation is essentially the same as the “list of children” implementation","chapter-6","Non-Binary Trees"
"of Section 6.3.1, but with dynamically allocated nodes rather than storing the nodes","chapter-6","Non-Binary Trees"
"in an array.","chapter-6","Non-Binary Trees"
"210 Chap. 6 Non-Binary Trees","chapter-6","Non-Binary Trees"
"(a) (b)","chapter-6","Non-Binary Trees"
"B","chapter-6","Non-Binary Trees"
"D E F","chapter-6","Non-Binary Trees"
"R","chapter-6","Non-Binary Trees"
"C","chapter-6","Non-Binary Trees"
"A","chapter-6","Non-Binary Trees"
"R","chapter-6","Non-Binary Trees"
"A B","chapter-6","Non-Binary Trees"
"C D E F","chapter-6","Non-Binary Trees"
"Figure 6.13 A dynamic general tree representation with linked lists of child","chapter-6","Non-Binary Trees"
"pointers. (a) The general tree. (b) The tree representation.","chapter-6","Non-Binary Trees"
"6.3.4 Dynamic “Left-Child/Right-Sibling” Implementation","chapter-6","Non-Binary Trees"
"The “left-child/right-sibling” implementation of Section 6.3.2 stores a fixed number","chapter-6","Non-Binary Trees"
"of pointers with each node. This can be readily adapted to a dynamic implemen-","chapter-6","Non-Binary Trees"
"tation. In essence, we substitute a binary tree for a general tree. Each node of the","chapter-6","Non-Binary Trees"
"“left-child/right-sibling” implementation points to two “children” in a new binary","chapter-6","Non-Binary Trees"
"tree structure. The left child of this new structure is the node’s first child in the","chapter-6","Non-Binary Trees"
"general tree. The right child is the node’s right sibling. We can easily extend this","chapter-6","Non-Binary Trees"
"conversion to a forest of general trees, because the roots of the trees can be con-","chapter-6","Non-Binary Trees"
"sidered siblings. Converting from a forest of general trees to a single binary tree","chapter-6","Non-Binary Trees"
"is illustrated by Figure 6.14. Here we simply include links from each node to its","chapter-6","Non-Binary Trees"
"right sibling and remove links to all children except the leftmost child. Figure 6.15","chapter-6","Non-Binary Trees"
"shows how this might look in an implementation with two pointers at each node.","chapter-6","Non-Binary Trees"
"Compared with the implementation illustrated by Figure 6.13 which requires over-","chapter-6","Non-Binary Trees"
"head of three pointers/node, the implementation of Figure 6.15 only requires two","chapter-6","Non-Binary Trees"
"pointers per node. The representation of Figure 6.15 is likely to be easier to imple-","chapter-6","Non-Binary Trees"
"ment, space efficient, and more flexible than the other implementations presented","chapter-6","Non-Binary Trees"
"in this section.","chapter-6","Non-Binary Trees"
"6.4 K-ary Trees","chapter-6","Non-Binary Trees"
"K-ary trees are trees whose internal nodes all have exactly K children. Thus,","chapter-6","Non-Binary Trees"
"a full binary tree is a 2-ary tree. The PR quadtree discussed in Section 13.3 is an","chapter-6","Non-Binary Trees"
"example of a 4-ary tree. Because K-ary tree nodes have a fixed number of children,","chapter-6","Non-Binary Trees"
"unlike general trees, they are relatively easy to implement. In general, K-ary trees","chapter-6","Non-Binary Trees"
"Sec. 6.4 K-ary Trees 211","chapter-6","Non-Binary Trees"
"(a)","chapter-6","Non-Binary Trees"
"root","chapter-6","Non-Binary Trees"
"(b)","chapter-6","Non-Binary Trees"
"Figure 6.14 Converting from a forest of general trees to a single binary tree.","chapter-6","Non-Binary Trees"
"Each node stores pointers to its left child and right sibling. The tree roots are","chapter-6","Non-Binary Trees"
"assumed to be siblings for the purpose of converting.","chapter-6","Non-Binary Trees"
"(a)","chapter-6","Non-Binary Trees"
"B","chapter-6","Non-Binary Trees"
"D E F","chapter-6","Non-Binary Trees"
"R","chapter-6","Non-Binary Trees"
"C","chapter-6","Non-Binary Trees"
"A","chapter-6","Non-Binary Trees"
"A","chapter-6","Non-Binary Trees"
"R","chapter-6","Non-Binary Trees"
"C B","chapter-6","Non-Binary Trees"
"D F","chapter-6","Non-Binary Trees"
"E","chapter-6","Non-Binary Trees"
"(b)","chapter-6","Non-Binary Trees"
"Figure 6.15 A general tree converted to the dynamic “left-child/right-sibling”","chapter-6","Non-Binary Trees"
"representation. Compared to the representation of Figure 6.13, this representation","chapter-6","Non-Binary Trees"
"requires less space.","chapter-6","Non-Binary Trees"
"bear many similarities to binary trees, and similar implementations can be used for","chapter-6","Non-Binary Trees"
"K-ary tree nodes. Note that as K becomes large, the potential number of null","chapter-6","Non-Binary Trees"
"pointers grows, and the difference between the required sizes for internal nodes","chapter-6","Non-Binary Trees"
"and leaf nodes increases. Thus, as K becomes larger, the need to choose separate","chapter-6","Non-Binary Trees"
"implementations for the internal and leaf nodes becomes more pressing.","chapter-6","Non-Binary Trees"
"Full and complete K-ary trees are analogous to full and complete binary trees,","chapter-6","Non-Binary Trees"
"respectively. Figure 6.16 shows full and complete K-ary trees for K = 3. In","chapter-6","Non-Binary Trees"
"practice, most applications of K-ary trees limit them to be either full or complete.","chapter-6","Non-Binary Trees"
"Many of the properties of binary trees extend to K-ary trees. Equivalent theo-","chapter-6","Non-Binary Trees"
"rems to those in Section 5.1.1 regarding the number of NULL pointers in a K-ary","chapter-6","Non-Binary Trees"
"tree and the relationship between the number of leaves and the number of internal","chapter-6","Non-Binary Trees"
"nodes in a K-ary tree can be derived. We can also store a complete K-ary tree in","chapter-6","Non-Binary Trees"
"an array, using simple formulas to compute a node’s relations in a manner similar","chapter-6","Non-Binary Trees"
"to that used in Section 5.3.3.","chapter-6","Non-Binary Trees"
"212 Chap. 6 Non-Binary Trees","chapter-6","Non-Binary Trees"
"(a) (b)","chapter-6","Non-Binary Trees"
"Figure 6.16 Full and complete 3-ary trees. (a) This tree is full (but not complete).","chapter-6","Non-Binary Trees"
"(b) This tree is complete (but not full).","chapter-6","Non-Binary Trees"
"6.5 Sequential Tree Implementations","chapter-6","Non-Binary Trees"
"Next we consider a fundamentally different approach to implementing trees. The","chapter-6","Non-Binary Trees"
"goal is to store a series of node values with the minimum information needed to","chapter-6","Non-Binary Trees"
"reconstruct the tree structure. This approach, known as a sequential tree imple-","chapter-6","Non-Binary Trees"
"mentation, has the advantage of saving space because no pointers are stored. It has","chapter-6","Non-Binary Trees"
"the disadvantage that accessing any node in the tree requires sequentially process-","chapter-6","Non-Binary Trees"
"ing all nodes that appear before it in the node list. In other words, node access must","chapter-6","Non-Binary Trees"
"start at the beginning of the node list, processing nodes sequentially in whatever","chapter-6","Non-Binary Trees"
"order they are stored until the desired node is reached. Thus, one primary virtue","chapter-6","Non-Binary Trees"
"of the other implementations discussed in this section is lost: efficient access (typi-","chapter-6","Non-Binary Trees"
"cally Θ(log n) time) to arbitrary nodes in the tree. Sequential tree implementations","chapter-6","Non-Binary Trees"
"are ideal for archiving trees on disk for later use because they save space, and the","chapter-6","Non-Binary Trees"
"tree structure can be reconstructed as needed for later processing.","chapter-6","Non-Binary Trees"
"Sequential tree implementations can be used to serialize a tree structure. Seri-","chapter-6","Non-Binary Trees"
"alization is the process of storing an object as a series of bytes, typically so that the","chapter-6","Non-Binary Trees"
"data structure can be transmitted between computers. This capability is important","chapter-6","Non-Binary Trees"
"when using data structures in a distributed processing environment.","chapter-6","Non-Binary Trees"
"A sequential tree implementation typically stores the node values as they would","chapter-6","Non-Binary Trees"
"be enumerated by a preorder traversal, along with sufficient information to describe","chapter-6","Non-Binary Trees"
"the tree’s shape. If the tree has restricted form, for example if it is a full binary tree,","chapter-6","Non-Binary Trees"
"then less information about structure typically needs to be stored. A general tree,","chapter-6","Non-Binary Trees"
"because it has the most flexible shape, tends to require the most additional shape","chapter-6","Non-Binary Trees"
"information. There are many possible sequential tree implementation schemes. We","chapter-6","Non-Binary Trees"
"will begin by describing methods appropriate to binary trees, then generalize to an","chapter-6","Non-Binary Trees"
"implementation appropriate to a general tree structure.","chapter-6","Non-Binary Trees"
"Because every node of a binary tree is either a leaf or has two (possibly empty)","chapter-6","Non-Binary Trees"
"children, we can take advantage of this fact to implicitly represent the tree’s struc-","chapter-6","Non-Binary Trees"
"ture. The most straightforward sequential tree implementation lists every node","chapter-6","Non-Binary Trees"
"value as it would be enumerated by a preorder traversal. Unfortunately, the node","chapter-6","Non-Binary Trees"
"values alone do not provide enough information to recover the shape of the tree. In","chapter-6","Non-Binary Trees"
"particular, as we read the series of node values, we do not know when a leaf node","chapter-6","Non-Binary Trees"
"has been reached. However, we can treat all non-empty nodes as internal nodes","chapter-6","Non-Binary Trees"
"Sec. 6.5 Sequential Tree Implementations 213","chapter-6","Non-Binary Trees"
"G I","chapter-6","Non-Binary Trees"
"E F","chapter-6","Non-Binary Trees"
"A","chapter-6","Non-Binary Trees"
"B C","chapter-6","Non-Binary Trees"
"D","chapter-6","Non-Binary Trees"
"H","chapter-6","Non-Binary Trees"
"Figure 6.17 Sample binary tree for sequential tree implementation examples.","chapter-6","Non-Binary Trees"
"with two (possibly empty) children. Only null values will be interpreted as leaf","chapter-6","Non-Binary Trees"
"nodes, and these can be listed explicitly. Such an augmented node list provides","chapter-6","Non-Binary Trees"
"enough information to recover the tree structure.","chapter-6","Non-Binary Trees"
"Example 6.5 For the binary tree of Figure 6.17, the corresponding se-","chapter-6","Non-Binary Trees"
"quential representation would be as follows (assuming that ‘/’ stands for","chapter-6","Non-Binary Trees"
"null):","chapter-6","Non-Binary Trees"
"AB/D//CEG///FH//I// (6.1)","chapter-6","Non-Binary Trees"
"To reconstruct the tree structure from this node list, we begin by setting","chapter-6","Non-Binary Trees"
"node A to be the root. A’s left child will be node B. Node B’s left child is","chapter-6","Non-Binary Trees"
"a null pointer, so node D must be B’s right child. Node D has two null","chapter-6","Non-Binary Trees"
"children, so node C must be the right child of node A.","chapter-6","Non-Binary Trees"
"To illustrate the difficulty involved in using the sequential tree representation","chapter-6","Non-Binary Trees"
"for processing, consider searching for the right child of the root node. We must first","chapter-6","Non-Binary Trees"
"move sequentially through the node list of the left subtree. Only at this point do","chapter-6","Non-Binary Trees"
"we reach the value of the root’s right child. Clearly the sequential representation","chapter-6","Non-Binary Trees"
"is space efficient, but not time efficient for descending through the tree along some","chapter-6","Non-Binary Trees"
"arbitrary path.","chapter-6","Non-Binary Trees"
"Assume that each node value takes a constant amount of space. An example","chapter-6","Non-Binary Trees"
"would be if the node value is a positive integer and null is indicated by the value","chapter-6","Non-Binary Trees"
"zero. From the Full Binary Tree Theorem of Section 5.1.1, we know that the size","chapter-6","Non-Binary Trees"
"of the node list will be about twice the number of nodes (i.e., the overhead fraction","chapter-6","Non-Binary Trees"
"is 1/2). The extra space is required by the null pointers. We should be able to","chapter-6","Non-Binary Trees"
"store the node list more compactly. However, any sequential implementation must","chapter-6","Non-Binary Trees"
"recognize when a leaf node has been reached, that is, a leaf node indicates the end","chapter-6","Non-Binary Trees"
"of a subtree. One way to do this is to explicitly list with each node whether it is","chapter-6","Non-Binary Trees"
"an internal node or a leaf. If a node X is an internal node, then we know that its","chapter-6","Non-Binary Trees"
"214 Chap. 6 Non-Binary Trees","chapter-6","Non-Binary Trees"
"two children (which may be subtrees) immediately follow X in the node list. If X","chapter-6","Non-Binary Trees"
"is a leaf node, then the next node in the list is the right child of some ancestor","chapter-6","Non-Binary Trees"
"of X, not the right child of X. In particular, the next node will be the child of X’s","chapter-6","Non-Binary Trees"
"most recent ancestor that has not yet seen its right child. However, this assumes","chapter-6","Non-Binary Trees"
"that each internal node does in fact have two children, in other words, that the","chapter-6","Non-Binary Trees"
"tree is full. Empty children must be indicated in the node list explicitly. Assume","chapter-6","Non-Binary Trees"
"that internal nodes are marked with a prime (0","chapter-6","Non-Binary Trees"
") and that leaf nodes show no mark.","chapter-6","Non-Binary Trees"
"Empty children of internal nodes are indicated by ‘/’, but the (empty) children of","chapter-6","Non-Binary Trees"
"leaf nodes are not represented at all. Note that a full binary tree stores no null","chapter-6","Non-Binary Trees"
"values with this implementation, and so requires less overhead.","chapter-6","Non-Binary Trees"
"Example 6.6 We can represent the tree of Figure 6.17 as follows:","chapter-6","Non-Binary Trees"
"A","chapter-6","Non-Binary Trees"
"0B","chapter-6","Non-Binary Trees"
"0","chapter-6","Non-Binary Trees"
"/DC0E","chapter-6","Non-Binary Trees"
"0G/F","chapter-6","Non-Binary Trees"
"0HI (6.2)","chapter-6","Non-Binary Trees"
"Note that slashes are needed for the empty children because this is not a full","chapter-6","Non-Binary Trees"
"binary tree.","chapter-6","Non-Binary Trees"
"Storing n extra bits can be a considerable savings over storing n null values.","chapter-6","Non-Binary Trees"
"In Example 6.6, each node is shown with a mark if it is internal, or no mark if it","chapter-6","Non-Binary Trees"
"is a leaf. This requires that each node value has space to store the mark bit. This","chapter-6","Non-Binary Trees"
"might be true if, for example, the node value were stored as a 4-byte integer but","chapter-6","Non-Binary Trees"
"the range of the values sored was small enough so that not all bits are used. An","chapter-6","Non-Binary Trees"
"example would be if all node values must be positive. Then the high-order (sign)","chapter-6","Non-Binary Trees"
"bit of the integer value could be used as the mark bit.","chapter-6","Non-Binary Trees"
"Another approach is to store a separate bit vector to represent the status of each","chapter-6","Non-Binary Trees"
"node. In this case, each node of the tree corresponds to one bit in the bit vector. A","chapter-6","Non-Binary Trees"
"value of ‘1’ could indicate an internal node, and ‘0’ could indicate a leaf node.","chapter-6","Non-Binary Trees"
"Example 6.7 The bit vector for the tree if Figure 6.17 (including positions","chapter-6","Non-Binary Trees"
"for the null children of nodes B and E) would be","chapter-6","Non-Binary Trees"
"11001100100 (6.3)","chapter-6","Non-Binary Trees"
"Storing general trees by means of a sequential implementation requires that","chapter-6","Non-Binary Trees"
"more explicit structural information be included with the node list. Not only must","chapter-6","Non-Binary Trees"
"the general tree implementation indicate whether a node is leaf or internal, it must","chapter-6","Non-Binary Trees"
"also indicate how many children the node has. Alternatively, the implementation","chapter-6","Non-Binary Trees"
"can indicate when a node’s child list has come to an end. The next example dis-","chapter-6","Non-Binary Trees"
"penses with marks for internal or leaf nodes. Instead it includes a special mark (we","chapter-6","Non-Binary Trees"
"Sec. 6.6 Further Reading 215","chapter-6","Non-Binary Trees"
"will use the “)” symbol) to indicate the end of a child list. All leaf nodes are fol-","chapter-6","Non-Binary Trees"
"lowed by a “)” symbol because they have no children. A leaf node that is also the","chapter-6","Non-Binary Trees"
"last child for its parent would indicate this by two or more successive “)” symbols.","chapter-6","Non-Binary Trees"
"Example 6.8 For the general tree of Figure 6.3, we get the sequential","chapter-6","Non-Binary Trees"
"representation","chapter-6","Non-Binary Trees"
"RAC)D)E))BF))) (6.4)","chapter-6","Non-Binary Trees"
"Note that F is followed by three “)” marks, because it is a leaf, the last node","chapter-6","Non-Binary Trees"
"of B’s rightmost subtree, and the last node of R’s rightmost subtree.","chapter-6","Non-Binary Trees"
"Note that this representation for serializing general trees cannot be used for bi-","chapter-6","Non-Binary Trees"
"nary trees. This is because a binary tree is not merely a restricted form of general","chapter-6","Non-Binary Trees"
"tree with at most two children. Every binary tree node has a left and a right child,","chapter-6","Non-Binary Trees"
"though either or both might be empty. For example, the representation of Exam-","chapter-6","Non-Binary Trees"
"ple 6.8 cannot let us distinguish whether node D in Figure 6.17 is the left or right","chapter-6","Non-Binary Trees"
"child of node B.","chapter-6","Non-Binary Trees"
"6.6 Further Reading","chapter-6","Non-Binary Trees"
"The expression log∗ n cited in Section 6.2 is closely related to the inverse of Ack-","chapter-6","Non-Binary Trees"
"ermann’s function. For more information about Ackermann’s function and the cost","chapter-6","Non-Binary Trees"
"of path compression for UNION/FIND, see Robert E. Tarjan’s paper “On the effi-","chapter-6","Non-Binary Trees"
"ciency of a good but not linear set merging algorithm” [Tar75]. The article “Data","chapter-6","Non-Binary Trees"
"Structures and Algorithms for Disjoint Set Union Problems” by Galil and Italiano","chapter-6","Non-Binary Trees"
"[GI91] covers many aspects of the equivalence class problem.","chapter-6","Non-Binary Trees"
"Foundations of Multidimensional and Metric Data Structures by Hanan Samet","chapter-6","Non-Binary Trees"
"[Sam06] treats various implementations of tree structures in detail within the con-","chapter-6","Non-Binary Trees"
"text of K-ary trees. Samet covers sequential implementations as well as the linked","chapter-6","Non-Binary Trees"
"and array implementations such as those described in this chapter and Chapter 5.","chapter-6","Non-Binary Trees"
"While these books are ostensibly concerned with spatial data structures, many of","chapter-6","Non-Binary Trees"
"the concepts treated are relevant to anyone who must implement tree structures.","chapter-6","Non-Binary Trees"
"6.7 Exercises","chapter-6","Non-Binary Trees"
"6.1 Write an algorithm to determine if two general trees are identical. Make the","chapter-6","Non-Binary Trees"
"algorithm as efficient as you can. Analyze your algorithm’s running time.","chapter-6","Non-Binary Trees"
"6.2 Write an algorithm to determine if two binary trees are identical when the","chapter-6","Non-Binary Trees"
"ordering of the subtrees for a node is ignored. For example, if a tree has root","chapter-6","Non-Binary Trees"
"node with value R, left child with value A and right child with value B, this","chapter-6","Non-Binary Trees"
"would be considered identical to another tree with root node value R, left","chapter-6","Non-Binary Trees"
"216 Chap. 6 Non-Binary Trees","chapter-6","Non-Binary Trees"
"child value B, and right child value A. Make the algorithm as efficient as you","chapter-6","Non-Binary Trees"
"can. Analyze your algorithm’s running time. How much harder would it be","chapter-6","Non-Binary Trees"
"to make this algorithm work on a general tree?","chapter-6","Non-Binary Trees"
"6.3 Write a postorder traversal function for general trees, similar to the preorder","chapter-6","Non-Binary Trees"
"traversal function named preorder given in Section 6.1.2.","chapter-6","Non-Binary Trees"
"6.4 Write a function that takes as input a general tree and returns the number of","chapter-6","Non-Binary Trees"
"nodes in that tree. Write your function to use the GenTree and GTNode","chapter-6","Non-Binary Trees"
"ADTs of Figure 6.2.","chapter-6","Non-Binary Trees"
"6.5 Describe how to implement the weighted union rule efficiently. In particular,","chapter-6","Non-Binary Trees"
"describe what information must be stored with each node and how this infor-","chapter-6","Non-Binary Trees"
"mation is updated when two trees are merged. Modify the implementation of","chapter-6","Non-Binary Trees"
"Figure 6.4 to support the weighted union rule.","chapter-6","Non-Binary Trees"
"6.6 A potential alternative to the weighted union rule for combining two trees is","chapter-6","Non-Binary Trees"
"the height union rule. The height union rule requires that the root of the tree","chapter-6","Non-Binary Trees"
"with greater height become the root of the union. Explain why the height","chapter-6","Non-Binary Trees"
"union rule can lead to worse average time behavior than the weighted union","chapter-6","Non-Binary Trees"
"rule.","chapter-6","Non-Binary Trees"
"6.7 Using the weighted union rule and path compression, show the array for","chapter-6","Non-Binary Trees"
"the parent pointer implementation that results from the following series of","chapter-6","Non-Binary Trees"
"equivalences on a set of objects indexed by the values 0 through 15. Initially,","chapter-6","Non-Binary Trees"
"each element in the set should be in a separate equivalence class. When two","chapter-6","Non-Binary Trees"
"trees to be merged are the same size, make the root with greater index value","chapter-6","Non-Binary Trees"
"be the child of the root with lesser index value.","chapter-6","Non-Binary Trees"
"(0, 2) (1, 2) (3, 4) (3, 1) (3, 5) (9, 11) (12, 14) (3, 9) (4, 14) (6, 7) (8, 10) (8, 7)","chapter-6","Non-Binary Trees"
"(7, 0) (10, 15) (10, 13)","chapter-6","Non-Binary Trees"
"6.8 Using the weighted union rule and path compression, show the array for","chapter-6","Non-Binary Trees"
"the parent pointer implementation that results from the following series of","chapter-6","Non-Binary Trees"
"equivalences on a set of objects indexed by the values 0 through 15. Initially,","chapter-6","Non-Binary Trees"
"each element in the set should be in a separate equivalence class. When two","chapter-6","Non-Binary Trees"
"trees to be merged are the same size, make the root with greater index value","chapter-6","Non-Binary Trees"
"be the child of the root with lesser index value.","chapter-6","Non-Binary Trees"
"(2, 3) (4, 5) (6, 5) (3, 5) (1, 0) (7, 8) (1, 8) (3, 8) (9, 10) (11, 14) (11, 10)","chapter-6","Non-Binary Trees"
"(12, 13) (11, 13) (14, 1)","chapter-6","Non-Binary Trees"
"6.9 Devise a series of equivalence statements for a collection of sixteen items","chapter-6","Non-Binary Trees"
"that yields a tree of height 5 when both the weighted union rule and path","chapter-6","Non-Binary Trees"
"compression are used. What is the total number of parent pointers followed","chapter-6","Non-Binary Trees"
"to perform this series?","chapter-6","Non-Binary Trees"
"6.10 One alternative to path compression that gives similar performance gains","chapter-6","Non-Binary Trees"
"is called path halving. In path halving, when the path is traversed from","chapter-6","Non-Binary Trees"
"the node to the root, we make the grandparent of every other node i on the","chapter-6","Non-Binary Trees"
"Sec. 6.7 Exercises 217","chapter-6","Non-Binary Trees"
"path the new parent of i. Write a version of FIND that implements path","chapter-6","Non-Binary Trees"
"halving. Your FIND operation should work as you move up the tree, rather","chapter-6","Non-Binary Trees"
"than require the two passes needed by path compression.","chapter-6","Non-Binary Trees"
"6.11 Analyze the fraction of overhead required by the “list of children” imple-","chapter-6","Non-Binary Trees"
"mentation, the “left-child/right-sibling” implementation, and the two linked","chapter-6","Non-Binary Trees"
"implementations of Section 6.3.3. How do these implementations compare","chapter-6","Non-Binary Trees"
"in space efficiency?","chapter-6","Non-Binary Trees"
"6.12 Using the general tree ADT of Figure 6.2, write a function that takes as input","chapter-6","Non-Binary Trees"
"the root of a general tree and returns a binary tree generated by the conversion","chapter-6","Non-Binary Trees"
"process illustrated by Figure 6.14.","chapter-6","Non-Binary Trees"
"6.13 Use mathematical induction to prove that the number of leaves in a non-","chapter-6","Non-Binary Trees"
"empty full K-ary tree is (K − 1)n + 1, where n is the number of internal","chapter-6","Non-Binary Trees"
"nodes.","chapter-6","Non-Binary Trees"
"6.14 Derive the formulas for computing the relatives of a non-empty complete","chapter-6","Non-Binary Trees"
"K-ary tree node stored in the complete tree representation of Section 5.3.3.","chapter-6","Non-Binary Trees"
"6.15 Find the overhead fraction for a full K-ary tree implementation with space","chapter-6","Non-Binary Trees"
"requirements as follows:","chapter-6","Non-Binary Trees"
"(a) All nodes store data, K child pointers, and a parent pointer. The data","chapter-6","Non-Binary Trees"
"field requires four bytes and each pointer requires four bytes.","chapter-6","Non-Binary Trees"
"(b) All nodes store data and K child pointers. The data field requires six-","chapter-6","Non-Binary Trees"
"teen bytes and each pointer requires four bytes.","chapter-6","Non-Binary Trees"
"(c) All nodes store data and a parent pointer, and internal nodes store K","chapter-6","Non-Binary Trees"
"child pointers. The data field requires eight bytes and each pointer re-","chapter-6","Non-Binary Trees"
"quires four bytes.","chapter-6","Non-Binary Trees"
"(d) Only leaf nodes store data; only internal nodes store K child pointers.","chapter-6","Non-Binary Trees"
"The data field requires four bytes and each pointer requires two bytes.","chapter-6","Non-Binary Trees"
"6.16 (a) Write out the sequential representation for Figure 6.18 using the coding","chapter-6","Non-Binary Trees"
"illustrated by Example 6.5.","chapter-6","Non-Binary Trees"
"(b) Write out the sequential representation for Figure 6.18 using the coding","chapter-6","Non-Binary Trees"
"illustrated by Example 6.6.","chapter-6","Non-Binary Trees"
"6.17 Draw the binary tree representing the following sequential representation for","chapter-6","Non-Binary Trees"
"binary trees illustrated by Example 6.5:","chapter-6","Non-Binary Trees"
"ABD//E//C/F//","chapter-6","Non-Binary Trees"
"6.18 Draw the binary tree representing the following sequential representation for","chapter-6","Non-Binary Trees"
"binary trees illustrated by Example 6.6:","chapter-6","Non-Binary Trees"
"A","chapter-6","Non-Binary Trees"
"0","chapter-6","Non-Binary Trees"
"/B","chapter-6","Non-Binary Trees"
"0","chapter-6","Non-Binary Trees"
"/C","chapter-6","Non-Binary Trees"
"0D","chapter-6","Non-Binary Trees"
"0G/E","chapter-6","Non-Binary Trees"
"Show the bit vector for leaf and internal nodes (as illustrated by Example 6.7)","chapter-6","Non-Binary Trees"
"for this tree.","chapter-6","Non-Binary Trees"
"218 Chap. 6 Non-Binary Trees","chapter-6","Non-Binary Trees"
"C","chapter-6","Non-Binary Trees"
"A","chapter-6","Non-Binary Trees"
"B","chapter-6","Non-Binary Trees"
"F","chapter-6","Non-Binary Trees"
"E H","chapter-6","Non-Binary Trees"
"G D I","chapter-6","Non-Binary Trees"
"Figure 6.18 A sample tree for Exercise 6.16.","chapter-6","Non-Binary Trees"
"6.19 Draw the general tree represented by the following sequential representation","chapter-6","Non-Binary Trees"
"for general trees illustrated by Example 6.8:","chapter-6","Non-Binary Trees"
"XPC)Q)RV)M))))","chapter-6","Non-Binary Trees"
"6.20 (a) Write a function to decode the sequential representation for binary trees","chapter-6","Non-Binary Trees"
"illustrated by Example 6.5. The input should be the sequential repre-","chapter-6","Non-Binary Trees"
"sentation and the output should be a pointer to the root of the resulting","chapter-6","Non-Binary Trees"
"binary tree.","chapter-6","Non-Binary Trees"
"(b) Write a function to decode the sequential representation for full binary","chapter-6","Non-Binary Trees"
"trees illustrated by Example 6.6. The input should be the sequential","chapter-6","Non-Binary Trees"
"representation and the output should be a pointer to the root of the re-","chapter-6","Non-Binary Trees"
"sulting binary tree.","chapter-6","Non-Binary Trees"
"(c) Write a function to decode the sequential representation for general","chapter-6","Non-Binary Trees"
"trees illustrated by Example 6.8. The input should be the sequential","chapter-6","Non-Binary Trees"
"representation and the output should be a pointer to the root of the re-","chapter-6","Non-Binary Trees"
"sulting general tree.","chapter-6","Non-Binary Trees"
"6.21 Devise a sequential representation for Huffman coding trees suitable for use","chapter-6","Non-Binary Trees"
"as part of a file compression utility (see Project 5.7).","chapter-6","Non-Binary Trees"
"6.8 Projects","chapter-6","Non-Binary Trees"
"6.1 Write classes that implement the general tree class declarations of Figure 6.2","chapter-6","Non-Binary Trees"
"using the dynamic “left-child/right-sibling” representation described in Sec-","chapter-6","Non-Binary Trees"
"tion 6.3.4.","chapter-6","Non-Binary Trees"
"6.2 Write classes that implement the general tree class declarations of Figure 6.2","chapter-6","Non-Binary Trees"
"using the linked general tree implementation with child pointer arrays of Fig-","chapter-6","Non-Binary Trees"
"ure 6.12. Your implementation should support only fixed-size nodes that","chapter-6","Non-Binary Trees"
"do not change their number of children once they are created. Then, re-","chapter-6","Non-Binary Trees"
"implement these classes with the linked list of children representation of","chapter-6","Non-Binary Trees"
"Sec. 6.8 Projects 219","chapter-6","Non-Binary Trees"
"Figure 6.13. How do the two implementations compare in space and time","chapter-6","Non-Binary Trees"
"efficiency and ease of implementation?","chapter-6","Non-Binary Trees"
"6.3 Write classes that implement the general tree class declarations of Figure 6.2","chapter-6","Non-Binary Trees"
"using the linked general tree implementation with child pointer arrays of Fig-","chapter-6","Non-Binary Trees"
"ure 6.12. Your implementation must be able to support changes in the num-","chapter-6","Non-Binary Trees"
"ber of children for a node. When created, a node should be allocated with","chapter-6","Non-Binary Trees"
"only enough space to store its initial set of children. Whenever a new child is","chapter-6","Non-Binary Trees"
"added to a node such that the array overflows, allocate a new array from free","chapter-6","Non-Binary Trees"
"store that can store twice as many children.","chapter-6","Non-Binary Trees"
"6.4 Implement a BST file archiver. Your program should take a BST created in","chapter-6","Non-Binary Trees"
"main memory using the implementation of Figure 5.14 and write it out to","chapter-6","Non-Binary Trees"
"disk using one of the sequential representations of Section 6.5. It should also","chapter-6","Non-Binary Trees"
"be able to read in disk files using your sequential representation and create","chapter-6","Non-Binary Trees"
"the equivalent main memory representation.","chapter-6","Non-Binary Trees"
"6.5 Use the UNION/FIND algorithm to implement a solution to the following","chapter-6","Non-Binary Trees"
"problem. Given a set of points represented by their xy-coordinates, assign","chapter-6","Non-Binary Trees"
"the points to clusters. Any two points are defined to be in the same cluster if","chapter-6","Non-Binary Trees"
"they are within a specified distance d of each other. For the purpose of this","chapter-6","Non-Binary Trees"
"problem, clustering is an equivalence relationship. In other words, points A,","chapter-6","Non-Binary Trees"
"B, and C are defined to be in the same cluster if the distance between A and B","chapter-6","Non-Binary Trees"
"is less than d and the distance between A and C is also less than d, even if the","chapter-6","Non-Binary Trees"
"distance between B and C is greater than d. To solve the problem, compute","chapter-6","Non-Binary Trees"
"the distance between each pair of points, using the equivalence processing","chapter-6","Non-Binary Trees"
"algorithm to merge clusters whenever two points are within the specified","chapter-6","Non-Binary Trees"
"distance. What is the asymptotic complexity of this algorithm? Where is the","chapter-6","Non-Binary Trees"
"bottleneck in processing?","chapter-6","Non-Binary Trees"
"6.6 In this project, you will run some empirical tests to determine if some vari-","chapter-6","Non-Binary Trees"
"ations on path compression in the UNION/FIND algorithm will lead to im-","chapter-6","Non-Binary Trees"
"proved performance. You should compare the following five implementa-","chapter-6","Non-Binary Trees"
"tions:","chapter-6","Non-Binary Trees"
"(a) Standard UNION/FIND with path compression and weighted union.","chapter-6","Non-Binary Trees"
"(b) Path compression and weighted union, except that path compression is","chapter-6","Non-Binary Trees"
"done after the UNION, instead of during the FIND operation. That is,","chapter-6","Non-Binary Trees"
"make all nodes along the paths traversed in both trees point directly to","chapter-6","Non-Binary Trees"
"the root of the larger tree.","chapter-6","Non-Binary Trees"
"(c) Weighted union and path halving as described in Exercise 6.10.","chapter-6","Non-Binary Trees"
"(d) Weighted union and a simplified form of path compression. At the end","chapter-6","Non-Binary Trees"
"of every FIND operation, make the node point to its tree’s root (but","chapter-6","Non-Binary Trees"
"don’t change the pointers for other nodes along the path).","chapter-6","Non-Binary Trees"
"(e) Weighted union and a simplified form of path compression. Both nodes","chapter-6","Non-Binary Trees"
"in the equivalence will be set to point directly to the root of the larger","chapter-6","Non-Binary Trees"
"220 Chap. 6 Non-Binary Trees","chapter-6","Non-Binary Trees"
"tree after the UNION operation. For example, consider processing the","chapter-6","Non-Binary Trees"
"equivalence (A, B) where A","chapter-6","Non-Binary Trees"
"0","chapter-6","Non-Binary Trees"
"is the root of A and B","chapter-6","Non-Binary Trees"
"0","chapter-6","Non-Binary Trees"
"is the root of B.","chapter-6","Non-Binary Trees"
"Assume the tree with root A","chapter-6","Non-Binary Trees"
"0","chapter-6","Non-Binary Trees"
"is bigger than the tree with root B","chapter-6","Non-Binary Trees"
"0","chapter-6","Non-Binary Trees"
". At the","chapter-6","Non-Binary Trees"
"end of the UNION/FIND operation, nodes A, B, and B","chapter-6","Non-Binary Trees"
"0 will all point","chapter-6","Non-Binary Trees"
"directly to A","chapter-6","Non-Binary Trees"
"0","chapter-6","Non-Binary Trees"
".","chapter-6","Non-Binary Trees"
"We sort many things in our everyday lives: A handful of cards when playing Bridge;","chapter-7","Internal Sorting"
"bills and other piles of paper; jars of spices; and so on. And we have many intuitive","chapter-7","Internal Sorting"
"strategies that we can use to do the sorting, depending on how many objects we","chapter-7","Internal Sorting"
"have to sort and how hard they are to move around. Sorting is also one of the most","chapter-7","Internal Sorting"
"frequently performed computing tasks. We might sort the records in a database","chapter-7","Internal Sorting"
"so that we can search the collection efficiently. We might sort the records by zip","chapter-7","Internal Sorting"
"code so that we can print and mail them more cheaply. We might use sorting as an","chapter-7","Internal Sorting"
"intrinsic part of an algorithm to solve some other problem, such as when computing","chapter-7","Internal Sorting"
"the minimum-cost spanning tree (see Section 11.5).","chapter-7","Internal Sorting"
"Because sorting is so important, naturally it has been studied intensively and","chapter-7","Internal Sorting"
"many algorithms have been devised. Some of these algorithms are straightforward","chapter-7","Internal Sorting"
"adaptations of schemes we use in everyday life. Others are totally alien to how hu-","chapter-7","Internal Sorting"
"mans do things, having been invented to sort thousands or even millions of records","chapter-7","Internal Sorting"
"stored on the computer. After years of study, there are still unsolved problems","chapter-7","Internal Sorting"
"related to sorting. New algorithms are still being developed and refined for special-","chapter-7","Internal Sorting"
"purpose applications.","chapter-7","Internal Sorting"
"While introducing this central problem in computer science, this chapter has","chapter-7","Internal Sorting"
"a secondary purpose of illustrating issues in algorithm design and analysis. For","chapter-7","Internal Sorting"
"example, this collection of sorting algorithms shows multiple approaches to us-","chapter-7","Internal Sorting"
"ing divide-and-conquer. In particular, there are multiple ways to do the dividing:","chapter-7","Internal Sorting"
"Mergesort divides a list in half; Quicksort divides a list into big values and small","chapter-7","Internal Sorting"
"values; and Radix Sort divides the problem by working on one digit of the key at","chapter-7","Internal Sorting"
"a time. Sorting algorithms can also illustrate a wide variety of analysis techniques.","chapter-7","Internal Sorting"
"We’ll find that it is possible for an algorithm to have an average case whose growth","chapter-7","Internal Sorting"
"rate is significantly smaller than its worse case (Quicksort). We’ll see how it is","chapter-7","Internal Sorting"
"possible to speed up sorting algorithms (both Shellsort and Quicksort) by taking","chapter-7","Internal Sorting"
"advantage of the best case behavior of another algorithm (Insertion sort). We’ll see","chapter-7","Internal Sorting"
"several examples of how we can tune an algorithm for better performance. We’ll","chapter-7","Internal Sorting"
"see that special case behavior by some algorithms makes them a good solution for","chapter-7","Internal Sorting"
"223","chapter-7","Internal Sorting"
"224 Chap. 7 Internal Sorting","chapter-7","Internal Sorting"
"special niche applications (Heapsort). Sorting provides an example of a significant","chapter-7","Internal Sorting"
"technique for analyzing the lower bound for a problem. Sorting will also be used","chapter-7","Internal Sorting"
"to motivate the introduction to file processing presented in Chapter 8.","chapter-7","Internal Sorting"
"The present chapter covers several standard algorithms appropriate for sorting","chapter-7","Internal Sorting"
"a collection of records that fit in the computer’s main memory. It begins with a dis-","chapter-7","Internal Sorting"
"cussion of three simple, but relatively slow, algorithms requiring Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") time in the","chapter-7","Internal Sorting"
"average and worst cases. Several algorithms with considerably better performance","chapter-7","Internal Sorting"
"are then presented, some with Θ(n log n) worst-case running time. The final sort-","chapter-7","Internal Sorting"
"ing method presented requires only Θ(n) worst-case time under special conditions.","chapter-7","Internal Sorting"
"The chapter concludes with a proof that sorting in general requires Ω(n log n) time","chapter-7","Internal Sorting"
"in the worst case.","chapter-7","Internal Sorting"
"7.1 Sorting Terminology and Notation","chapter-7","Internal Sorting"
"Except where noted otherwise, input to the sorting algorithms presented in this","chapter-7","Internal Sorting"
"chapter is a collection of records stored in an array. Records are compared to","chapter-7","Internal Sorting"
"one another by requiring that their type extend the Comparable class. This will","chapter-7","Internal Sorting"
"ensure that the class implements the compareTo method, which returns a value","chapter-7","Internal Sorting"
"less than zero, equal to zero, or greater than zero depending on its relationship to","chapter-7","Internal Sorting"
"the record being compared to. The compareTo method is defined to extract the","chapter-7","Internal Sorting"
"appropriate key field from the record. We also assume that for every record type","chapter-7","Internal Sorting"
"there is a swap function that can interchange the contents of two records in the","chapter-7","Internal Sorting"
"array.","chapter-7","Internal Sorting"
"Given a set of records r1, r2, ..., rn with key values k1, k2, ..., kn, the Sorting","chapter-7","Internal Sorting"
"Problem is to arrange the records into any order s such that records rs1","chapter-7","Internal Sorting"
", rs2","chapter-7","Internal Sorting"
", ..., rsn","chapter-7","Internal Sorting"
"have keys obeying the property ks1 ≤ ks2 ≤ ... ≤ ksn","chapter-7","Internal Sorting"
". In other words, the sorting","chapter-7","Internal Sorting"
"problem is to arrange a set of records so that the values of their key fields are in","chapter-7","Internal Sorting"
"non-decreasing order.","chapter-7","Internal Sorting"
"As defined, the Sorting Problem allows input with two or more records that have","chapter-7","Internal Sorting"
"the same key value. Certain applications require that input not contain duplicate","chapter-7","Internal Sorting"
"key values. The sorting algorithms presented in this chapter and in Chapter 8 can","chapter-7","Internal Sorting"
"handle duplicate key values unless noted otherwise.","chapter-7","Internal Sorting"
"When duplicate key values are allowed, there might be an implicit ordering","chapter-7","Internal Sorting"
"to the duplicates, typically based on their order of occurrence within the input. It","chapter-7","Internal Sorting"
"might be desirable to maintain this initial ordering among duplicates. A sorting","chapter-7","Internal Sorting"
"algorithm is said to be stable if it does not change the relative ordering of records","chapter-7","Internal Sorting"
"with identical key values. Many, but not all, of the sorting algorithms presented in","chapter-7","Internal Sorting"
"this chapter are stable, or can be made stable with minor changes.","chapter-7","Internal Sorting"
"When comparing two sorting algorithms, the most straightforward approach","chapter-7","Internal Sorting"
"would seem to be simply program both and measure their running times. An ex-","chapter-7","Internal Sorting"
"ample of such timings is presented in Figure 7.20. However, such a comparison","chapter-7","Internal Sorting"
"Sec. 7.2 Three Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") Sorting Algorithms 225","chapter-7","Internal Sorting"
"can be misleading because the running time for many sorting algorithms depends","chapter-7","Internal Sorting"
"on specifics of the input values. In particular, the number of records, the size of","chapter-7","Internal Sorting"
"the keys and the records, the allowable range of the key values, and the amount by","chapter-7","Internal Sorting"
"which the input records are “out of order” can all greatly affect the relative running","chapter-7","Internal Sorting"
"times for sorting algorithms.","chapter-7","Internal Sorting"
"When analyzing sorting algorithms, it is traditional to measure the number of","chapter-7","Internal Sorting"
"comparisons made between keys. This measure is usually closely related to the","chapter-7","Internal Sorting"
"running time for the algorithm and has the advantage of being machine and data-","chapter-7","Internal Sorting"
"type independent. However, in some cases records might be so large that their","chapter-7","Internal Sorting"
"physical movement might take a significant fraction of the total running time. If so,","chapter-7","Internal Sorting"
"it might be appropriate to measure the number of swap operations performed by the","chapter-7","Internal Sorting"
"algorithm. In most applications we can assume that all records and keys are of fixed","chapter-7","Internal Sorting"
"length, and that a single comparison or a single swap operation requires a constant","chapter-7","Internal Sorting"
"amount of time regardless of which keys are involved. Some special situations","chapter-7","Internal Sorting"
"“change the rules” for comparing sorting algorithms. For example, an application","chapter-7","Internal Sorting"
"with records or keys having widely varying length (such as sorting a sequence of","chapter-7","Internal Sorting"
"variable length strings) will benefit from a special-purpose sorting technique. Some","chapter-7","Internal Sorting"
"applications require that a small number of records be sorted, but that the sort be","chapter-7","Internal Sorting"
"performed frequently. An example would be an application that repeatedly sorts","chapter-7","Internal Sorting"
"groups of five numbers. In such cases, the constants in the runtime equations that","chapter-7","Internal Sorting"
"are usually ignored in an asymptotic analysis now become crucial. Finally, some","chapter-7","Internal Sorting"
"situations require that a sorting algorithm use as little memory as possible. We will","chapter-7","Internal Sorting"
"note which sorting algorithms require significant extra memory beyond the input","chapter-7","Internal Sorting"
"array.","chapter-7","Internal Sorting"
"7.2 Three Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") Sorting Algorithms","chapter-7","Internal Sorting"
"This section presents three simple sorting algorithms. While easy to understand","chapter-7","Internal Sorting"
"and implement, we will soon see that they are unacceptably slow when there are","chapter-7","Internal Sorting"
"many records to sort. Nonetheless, there are situations where one of these simple","chapter-7","Internal Sorting"
"algorithms is the best tool for the job.","chapter-7","Internal Sorting"
"7.2.1 Insertion Sort","chapter-7","Internal Sorting"
"Imagine that you have a stack of phone bills from the past two years and that you","chapter-7","Internal Sorting"
"wish to organize them by date. A fairly natural way to do this might be to look at","chapter-7","Internal Sorting"
"the first two bills and put them in order. Then take the third bill and put it into the","chapter-7","Internal Sorting"
"right order with respect to the first two, and so on. As you take each bill, you would","chapter-7","Internal Sorting"
"add it to the sorted pile that you have already made. This naturally intuitive process","chapter-7","Internal Sorting"
"is the inspiration for our first sorting algorithm, called Insertion Sort. Insertion","chapter-7","Internal Sorting"
"Sort iterates through a list of records. Each record is inserted in turn at the correct","chapter-7","Internal Sorting"
"position within a sorted list composed of those records already processed. The","chapter-7","Internal Sorting"
"226 Chap. 7 Internal Sorting","chapter-7","Internal Sorting"
"i=1 3 4 5 6","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"15","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"15","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"15","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"15","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"7","chapter-7","Internal Sorting"
"15 15 15 15","chapter-7","Internal Sorting"
"Figure 7.1 An illustration of Insertion Sort. Each column shows the array after","chapter-7","Internal Sorting"
"the iteration with the indicated value of i in the outer for loop. Values above","chapter-7","Internal Sorting"
"the line in each column have been sorted. Arrows indicate the upward motions of","chapter-7","Internal Sorting"
"records through the array.","chapter-7","Internal Sorting"
"following is a Java implementation. The input is an array of n records stored in","chapter-7","Internal Sorting"
"array A.","chapter-7","Internal Sorting"
"static <E extends Comparable<? super E>>","chapter-7","Internal Sorting"
"void inssort(E[] A) {","chapter-7","Internal Sorting"
"for (int i=1; i<A.length; i++) // Insert i’th record","chapter-7","Internal Sorting"
"for (int j=i; (j>0) && (A[j].compareTo(A[j-1])<0); j--)","chapter-7","Internal Sorting"
"DSutil.swap(A, j, j-1);","chapter-7","Internal Sorting"
"}","chapter-7","Internal Sorting"
"Consider the case where inssort is processing the ith record, which has key","chapter-7","Internal Sorting"
"value X. The record is moved upward in the array as long as X is less than the","chapter-7","Internal Sorting"
"key value immediately above it. As soon as a key value less than or equal to X is","chapter-7","Internal Sorting"
"encountered, inssort is done with that record because all records above it in the","chapter-7","Internal Sorting"
"array must have smaller keys. Figure 7.1 illustrates how Insertion Sort works.","chapter-7","Internal Sorting"
"The body of inssort is made up of two nested for loops. The outer for","chapter-7","Internal Sorting"
"loop is executed n − 1 times. The inner for loop is harder to analyze because","chapter-7","Internal Sorting"
"the number of times it executes depends on how many keys in positions 1 to i − 1","chapter-7","Internal Sorting"
"have a value less than that of the key in position i. In the worst case, each record","chapter-7","Internal Sorting"
"must make its way to the top of the array. This would occur if the keys are initially","chapter-7","Internal Sorting"
"arranged from highest to lowest, in the reverse of sorted order. In this case, the","chapter-7","Internal Sorting"
"number of comparisons will be one the first time through the for loop, two the","chapter-7","Internal Sorting"
"second time, and so on. Thus, the total number of comparisons will be","chapter-7","Internal Sorting"
"Xn","chapter-7","Internal Sorting"
"i=2","chapter-7","Internal Sorting"
"i ≈ n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
"/2 = Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
").","chapter-7","Internal Sorting"
"In contrast, consider the best-case cost. This occurs when the keys begin in","chapter-7","Internal Sorting"
"sorted order from lowest to highest. In this case, every pass through the inner","chapter-7","Internal Sorting"
"for loop will fail immediately, and no values will be moved. The total number","chapter-7","Internal Sorting"
"Sec. 7.2 Three Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") Sorting Algorithms 227","chapter-7","Internal Sorting"
"of comparisons will be n − 1, which is the number of times the outer for loop","chapter-7","Internal Sorting"
"executes. Thus, the cost for Insertion Sort in the best case is Θ(n).","chapter-7","Internal Sorting"
"While the best case is significantly faster than the worst case, the worst case","chapter-7","Internal Sorting"
"is usually a more reliable indication of the “typical” running time. However, there","chapter-7","Internal Sorting"
"are situations where we can expect the input to be in sorted or nearly sorted order.","chapter-7","Internal Sorting"
"One example is when an already sorted list is slightly disordered by a small number","chapter-7","Internal Sorting"
"of additions to the list; restoring sorted order using Insertion Sort might be a good","chapter-7","Internal Sorting"
"idea if we know that the disordering is slight. Examples of algorithms that take ad-","chapter-7","Internal Sorting"
"vantage of Insertion Sort’s near-best-case running time are the Shellsort algorithm","chapter-7","Internal Sorting"
"of Section 7.3 and the Quicksort algorithm of Section 7.5.","chapter-7","Internal Sorting"
"What is the average-case cost of Insertion Sort? When record i is processed,","chapter-7","Internal Sorting"
"the number of times through the inner for loop depends on how far “out of order”","chapter-7","Internal Sorting"
"the record is. In particular, the inner for loop is executed once for each key greater","chapter-7","Internal Sorting"
"than the key of record i that appears in array positions 0 through i−1. For example,","chapter-7","Internal Sorting"
"in the leftmost column of Figure 7.1 the value 15 is preceded by five values greater","chapter-7","Internal Sorting"
"than 15. Each such occurrence is called an inversion. The number of inversions","chapter-7","Internal Sorting"
"(i.e., the number of values greater than a given value that occur prior to it in the","chapter-7","Internal Sorting"
"array) will determine the number of comparisons and swaps that must take place.","chapter-7","Internal Sorting"
"We need to determine what the average number of inversions will be for the record","chapter-7","Internal Sorting"
"in position i. We expect on average that half of the keys in the first i − 1 array","chapter-7","Internal Sorting"
"positions will have a value greater than that of the key at position i. Thus, the","chapter-7","Internal Sorting"
"average case should be about half the cost of the worst case, or around n","chapter-7","Internal Sorting"
"2/4, which","chapter-7","Internal Sorting"
"is still Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
"). So, the average case is no better than the worst case in asymptotic","chapter-7","Internal Sorting"
"complexity.","chapter-7","Internal Sorting"
"Counting comparisons or swaps yields similar results. Each time through the","chapter-7","Internal Sorting"
"inner for loop yields both a comparison and a swap, except the last (i.e., the","chapter-7","Internal Sorting"
"comparison that fails the inner for loop’s test), which has no swap. Thus, the","chapter-7","Internal Sorting"
"number of swaps for the entire sort operation is n − 1 less than the number of","chapter-7","Internal Sorting"
"comparisons. This is 0 in the best case, and Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") in the average and worst cases.","chapter-7","Internal Sorting"
"7.2.2 Bubble Sort","chapter-7","Internal Sorting"
"Our next sorting algorithm is called Bubble Sort. Bubble Sort is often taught to","chapter-7","Internal Sorting"
"novice programmers in introductory computer science courses. This is unfortunate,","chapter-7","Internal Sorting"
"because Bubble Sort has no redeeming features whatsoever. It is a relatively slow","chapter-7","Internal Sorting"
"sort, it is no easier to understand than Insertion Sort, it does not correspond to any","chapter-7","Internal Sorting"
"intuitive counterpart in “everyday” use, and it has a poor best-case running time.","chapter-7","Internal Sorting"
"However, Bubble Sort can serve as the inspiration for a better sorting algorithm that","chapter-7","Internal Sorting"
"will be presented in Section 7.2.3.","chapter-7","Internal Sorting"
"Bubble Sort consists of a simple double for loop. The first iteration of the","chapter-7","Internal Sorting"
"inner for loop moves through the record array from bottom to top, comparing","chapter-7","Internal Sorting"
"adjacent keys. If the lower-indexed key’s value is greater than its higher-indexed","chapter-7","Internal Sorting"
"228 Chap. 7 Internal Sorting","chapter-7","Internal Sorting"
"i=0 1 2 3 4 5 6","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"15","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"15","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"15","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"15","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"15","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"15","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"15","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"15 23 28 42","chapter-7","Internal Sorting"
"Figure 7.2 An illustration of Bubble Sort. Each column shows the array after","chapter-7","Internal Sorting"
"the iteration with the indicated value of i in the outer for loop. Values above the","chapter-7","Internal Sorting"
"line in each column have been sorted. Arrows indicate the swaps that take place","chapter-7","Internal Sorting"
"during a given iteration.","chapter-7","Internal Sorting"
"neighbor, then the two values are swapped. Once the smallest value is encountered,","chapter-7","Internal Sorting"
"this process will cause it to “bubble” up to the top of the array. The second pass","chapter-7","Internal Sorting"
"through the array repeats this process. However, because we know that the smallest","chapter-7","Internal Sorting"
"value reached the top of the array on the first pass, there is no need to compare","chapter-7","Internal Sorting"
"the top two elements on the second pass. Likewise, each succeeding pass through","chapter-7","Internal Sorting"
"the array compares adjacent elements, looking at one less value than the preceding","chapter-7","Internal Sorting"
"pass. Figure 7.2 illustrates Bubble Sort. A Java implementation is as follows:","chapter-7","Internal Sorting"
"static <E extends Comparable<? super E>>","chapter-7","Internal Sorting"
"void bubblesort(E[] A) {","chapter-7","Internal Sorting"
"for (int i=0; i<A.length-1; i++) // Bubble up i’th record","chapter-7","Internal Sorting"
"for (int j=A.length-1; j>i; j--)","chapter-7","Internal Sorting"
"if ((A[j].compareTo(A[j-1]) < 0))","chapter-7","Internal Sorting"
"DSutil.swap(A, j, j-1);","chapter-7","Internal Sorting"
"}","chapter-7","Internal Sorting"
"Determining Bubble Sort’s number of comparisons is easy. Regardless of the","chapter-7","Internal Sorting"
"arrangement of the values in the array, the number of comparisons made by the","chapter-7","Internal Sorting"
"inner for loop is always i, leading to a total cost of","chapter-7","Internal Sorting"
"Xn","chapter-7","Internal Sorting"
"i=1","chapter-7","Internal Sorting"
"i ≈ n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
"/2 = Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
").","chapter-7","Internal Sorting"
"Bubble Sort’s running time is roughly the same in the best, average, and worst","chapter-7","Internal Sorting"
"cases.","chapter-7","Internal Sorting"
"The number of swaps required depends on how often a value is less than the","chapter-7","Internal Sorting"
"one immediately preceding it in the array. We can expect this to occur for about","chapter-7","Internal Sorting"
"half the comparisons in the average case, leading to Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") for the expected number","chapter-7","Internal Sorting"
"of swaps. The actual number of swaps performed by Bubble Sort will be identical","chapter-7","Internal Sorting"
"to that performed by Insertion Sort.","chapter-7","Internal Sorting"
"Sec. 7.2 Three Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") Sorting Algorithms 229","chapter-7","Internal Sorting"
"i=0 1 2 3 4 5 6","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"15","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"15","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"15","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"15","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"15","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"15","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"15","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"13","chapter-7","Internal Sorting"
"14","chapter-7","Internal Sorting"
"15","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"20","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"42","chapter-7","Internal Sorting"
"Figure 7.3 An example of Selection Sort. Each column shows the array after the","chapter-7","Internal Sorting"
"iteration with the indicated value of i in the outer for loop. Numbers above the","chapter-7","Internal Sorting"
"line in each column have been sorted and are in their final positions.","chapter-7","Internal Sorting"
"7.2.3 Selection Sort","chapter-7","Internal Sorting"
"Consider again the problem of sorting a pile of phone bills for the past year. An-","chapter-7","Internal Sorting"
"other intuitive approach might be to look through the pile until you find the bill for","chapter-7","Internal Sorting"
"January, and pull that out. Then look through the remaining pile until you find the","chapter-7","Internal Sorting"
"bill for February, and add that behind January. Proceed through the ever-shrinking","chapter-7","Internal Sorting"
"pile of bills to select the next one in order until you are done. This is the inspiration","chapter-7","Internal Sorting"
"for our last Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") sort, called Selection Sort. The ith pass of Selection Sort “se-","chapter-7","Internal Sorting"
"lects” the ith smallest key in the array, placing that record into position i. In other","chapter-7","Internal Sorting"
"words, Selection Sort first finds the smallest key in an unsorted list, then the second","chapter-7","Internal Sorting"
"smallest, and so on. Its unique feature is that there are few record swaps. To find","chapter-7","Internal Sorting"
"the next smallest key value requires searching through the entire unsorted portion","chapter-7","Internal Sorting"
"of the array, but only one swap is required to put the record in place. Thus, the total","chapter-7","Internal Sorting"
"number of swaps required will be n − 1 (we get the last record in place “for free”).","chapter-7","Internal Sorting"
"Figure 7.3 illustrates Selection Sort. Below is a Java implementation.","chapter-7","Internal Sorting"
"static <E extends Comparable<? super E>>","chapter-7","Internal Sorting"
"void selectsort(E[] A) {","chapter-7","Internal Sorting"
"for (int i=0; i<A.length-1; i++) { // Select i’th record","chapter-7","Internal Sorting"
"int lowindex = i; // Remember its index","chapter-7","Internal Sorting"
"for (int j=A.length-1; j>i; j--) // Find the least value","chapter-7","Internal Sorting"
"if (A[j].compareTo(A[lowindex]) < 0)","chapter-7","Internal Sorting"
"lowindex = j; // Put it in place","chapter-7","Internal Sorting"
"DSutil.swap(A, i, lowindex);","chapter-7","Internal Sorting"
"}","chapter-7","Internal Sorting"
"}","chapter-7","Internal Sorting"
"Selection Sort (as written here) is essentially a Bubble Sort, except that rather","chapter-7","Internal Sorting"
"than repeatedly swapping adjacent values to get the next smallest record into place,","chapter-7","Internal Sorting"
"we instead remember the position of the element to be selected and do one swap","chapter-7","Internal Sorting"
"at the end. Thus, the number of comparisons is still Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
"), but the number of","chapter-7","Internal Sorting"
"swaps is much less than that required by bubble sort. Selection Sort is particularly","chapter-7","Internal Sorting"
"230 Chap. 7 Internal Sorting","chapter-7","Internal Sorting"
"Key = 42","chapter-7","Internal Sorting"
"Key = 5","chapter-7","Internal Sorting"
"Key = 42","chapter-7","Internal Sorting"
"Key = 5","chapter-7","Internal Sorting"
"(a) (b)","chapter-7","Internal Sorting"
"Key = 23","chapter-7","Internal Sorting"
"Key = 10","chapter-7","Internal Sorting"
"Key = 23","chapter-7","Internal Sorting"
"Key = 10","chapter-7","Internal Sorting"
"Figure 7.4 An example of swapping pointers to records. (a) A series of four","chapter-7","Internal Sorting"
"records. The record with key value 42 comes before the record with key value 5.","chapter-7","Internal Sorting"
"(b) The four records after the top two pointers have been swapped. Now the record","chapter-7","Internal Sorting"
"with key value 5 comes before the record with key value 42.","chapter-7","Internal Sorting"
"advantageous when the cost to do a swap is high, for example, when the elements","chapter-7","Internal Sorting"
"are long strings or other large records. Selection Sort is more efficient than Bubble","chapter-7","Internal Sorting"
"Sort (by a constant factor) in most other situations as well.","chapter-7","Internal Sorting"
"There is another approach to keeping the cost of swapping records low that","chapter-7","Internal Sorting"
"can be used by any sorting algorithm even when the records are large. This is","chapter-7","Internal Sorting"
"to have each element of the array store a pointer to a record rather than store the","chapter-7","Internal Sorting"
"record itself. In this implementation, a swap operation need only exchange the","chapter-7","Internal Sorting"
"pointer values; the records themselves do not move. This technique is illustrated","chapter-7","Internal Sorting"
"by Figure 7.4. Additional space is needed to store the pointers, but the return is a","chapter-7","Internal Sorting"
"faster swap operation.","chapter-7","Internal Sorting"
"7.2.4 The Cost of Exchange Sorting","chapter-7","Internal Sorting"
"Figure 7.5 summarizes the cost of Insertion, Bubble, and Selection Sort in terms of","chapter-7","Internal Sorting"
"their required number of comparisons and swaps1","chapter-7","Internal Sorting"
"in the best, average, and worst","chapter-7","Internal Sorting"
"cases. The running time for each of these sorts is Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") in the average and worst","chapter-7","Internal Sorting"
"cases.","chapter-7","Internal Sorting"
"The remaining sorting algorithms presented in this chapter are significantly bet-","chapter-7","Internal Sorting"
"ter than these three under typical conditions. But before continuing on, it is instruc-","chapter-7","Internal Sorting"
"tive to investigate what makes these three sorts so slow. The crucial bottleneck","chapter-7","Internal Sorting"
"is that only adjacent records are compared. Thus, comparisons and moves (in all","chapter-7","Internal Sorting"
"but Selection Sort) are by single steps. Swapping adjacent records is called an ex-","chapter-7","Internal Sorting"
"change. Thus, these sorts are sometimes referred to as exchange sorts. The cost","chapter-7","Internal Sorting"
"of any exchange sort can be at best the total number of steps that the records in the","chapter-7","Internal Sorting"
"1There is a slight anomaly with Selection Sort. The supposed advantage for Selection Sort is its","chapter-7","Internal Sorting"
"low number of swaps required, yet Selection Sort’s best-case number of swaps is worse than that for","chapter-7","Internal Sorting"
"Insertion Sort or Bubble Sort. This is because the implementation given for Selection Sort does not","chapter-7","Internal Sorting"
"avoid a swap in the case where record i is already in position i. One could put in a test to avoid","chapter-7","Internal Sorting"
"swapping in this situation. But it usually takes more time to do the tests than would be saved by","chapter-7","Internal Sorting"
"avoiding such swaps.","chapter-7","Internal Sorting"
"Sec. 7.3 Shellsort 231","chapter-7","Internal Sorting"
"Insertion Bubble Selection","chapter-7","Internal Sorting"
"Comparisons:","chapter-7","Internal Sorting"
"Best Case Θ(n) Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
")","chapter-7","Internal Sorting"
"Average Case Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
")","chapter-7","Internal Sorting"
"Worst Case Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
")","chapter-7","Internal Sorting"
"Swaps:","chapter-7","Internal Sorting"
"Best Case 0 0 Θ(n)","chapter-7","Internal Sorting"
"Average Case Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") Θ(n)","chapter-7","Internal Sorting"
"Worst Case Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") Θ(n)","chapter-7","Internal Sorting"
"Figure 7.5 A comparison of the asymptotic complexities for three simple sorting","chapter-7","Internal Sorting"
"algorithms.","chapter-7","Internal Sorting"
"array must move to reach their “correct” location (i.e., the number of inversions for","chapter-7","Internal Sorting"
"each record).","chapter-7","Internal Sorting"
"What is the average number of inversions? Consider a list L containing n val-","chapter-7","Internal Sorting"
"ues. Define LR to be L in reverse. L has n(n−1)/2 distinct pairs of values, each of","chapter-7","Internal Sorting"
"which could potentially be an inversion. Each such pair must either be an inversion","chapter-7","Internal Sorting"
"in L or in LR. Thus, the total number of inversions in L and LR together is exactly","chapter-7","Internal Sorting"
"n(n−1)/2 for an average of n(n−1)/4 per list. We therefore know with certainty","chapter-7","Internal Sorting"
"that any sorting algorithm which limits comparisons to adjacent items will cost at","chapter-7","Internal Sorting"
"least n(n − 1)/4 = Ω(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") in the average case.","chapter-7","Internal Sorting"
"7.3 Shellsort","chapter-7","Internal Sorting"
"The next sorting algorithm that we consider is called Shellsort, named after its","chapter-7","Internal Sorting"
"inventor, D.L. Shell. It is also sometimes called the diminishing increment sort.","chapter-7","Internal Sorting"
"Unlike Insertion and Selection Sort, there is no real life intuitive equivalent to Shell-","chapter-7","Internal Sorting"
"sort. Unlike the exchange sorts, Shellsort makes comparisons and swaps between","chapter-7","Internal Sorting"
"non-adjacent elements. Shellsort also exploits the best-case performance of Inser-","chapter-7","Internal Sorting"
"tion Sort. Shellsort’s strategy is to make the list “mostly sorted” so that a final","chapter-7","Internal Sorting"
"Insertion Sort can finish the job. When properly implemented, Shellsort will give","chapter-7","Internal Sorting"
"substantially better performance than Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") in the worst case.","chapter-7","Internal Sorting"
"Shellsort uses a process that forms the basis for many of the sorts presented","chapter-7","Internal Sorting"
"in the following sections: Break the list into sublists, sort them, then recombine","chapter-7","Internal Sorting"
"the sublists. Shellsort breaks the array of elements into “virtual” sublists. Each","chapter-7","Internal Sorting"
"sublist is sorted using an Insertion Sort. Another group of sublists is then chosen","chapter-7","Internal Sorting"
"and sorted, and so on.","chapter-7","Internal Sorting"
"During each iteration, Shellsort breaks the list into disjoint sublists so that each","chapter-7","Internal Sorting"
"element in a sublist is a fixed number of positions apart. For example, let us as-","chapter-7","Internal Sorting"
"sume for convenience that n, the number of values to be sorted, is a power of two.","chapter-7","Internal Sorting"
"One possible implementation of Shellsort will begin by breaking the list into n/2","chapter-7","Internal Sorting"
"232 Chap. 7 Internal Sorting","chapter-7","Internal Sorting"
"59 20 17 13 28 14 23 83 36 98","chapter-7","Internal Sorting"
"36 20 11 13 28 14 23 15 59","chapter-7","Internal Sorting"
"28 14 11 13 36 20 17 15","chapter-7","Internal Sorting"
"11 13 17 14 23 15 28 20 36 98","chapter-7","Internal Sorting"
"11 13 14 15 17 20 23 28 36 41 42 59 65 70 83 98","chapter-7","Internal Sorting"
"11 70 65 41 42 15","chapter-7","Internal Sorting"
"98 17 70 65 41 42 83","chapter-7","Internal Sorting"
"59 41 23 70 65 98 42 83","chapter-7","Internal Sorting"
"41 42 70 59 83 65","chapter-7","Internal Sorting"
"Figure 7.6 An example of Shellsort. Sixteen items are sorted in four passes.","chapter-7","Internal Sorting"
"The first pass sorts 8 sublists of size 2 and increment 8. The second pass sorts","chapter-7","Internal Sorting"
"4 sublists of size 4 and increment 4. The third pass sorts 2 sublists of size 8 and","chapter-7","Internal Sorting"
"increment 2. The fourth pass sorts 1 list of size 16 and increment 1 (a regular","chapter-7","Internal Sorting"
"Insertion Sort).","chapter-7","Internal Sorting"
"sublists of 2 elements each, where the array index of the 2 elements in each sublist","chapter-7","Internal Sorting"
"differs by n/2. If there are 16 elements in the array indexed from 0 to 15, there","chapter-7","Internal Sorting"
"would initially be 8 sublists of 2 elements each. The first sublist would be the ele-","chapter-7","Internal Sorting"
"ments in positions 0 and 8, the second in positions 1 and 9, and so on. Each list of","chapter-7","Internal Sorting"
"two elements is sorted using Insertion Sort.","chapter-7","Internal Sorting"
"The second pass of Shellsort looks at fewer, bigger lists. For our example the","chapter-7","Internal Sorting"
"second pass would have n/4 lists of size 4, with the elements in the list being n/4","chapter-7","Internal Sorting"
"positions apart. Thus, the second pass would have as its first sublist the 4 elements","chapter-7","Internal Sorting"
"in positions 0, 4, 8, and 12; the second sublist would have elements in positions 1,","chapter-7","Internal Sorting"
"5, 9, and 13; and so on. Each sublist of four elements would also be sorted using","chapter-7","Internal Sorting"
"an Insertion Sort.","chapter-7","Internal Sorting"
"The third pass would be made on two lists, one consisting of the odd positions","chapter-7","Internal Sorting"
"and the other consisting of the even positions.","chapter-7","Internal Sorting"
"The culminating pass in this example would be a “normal” Insertion Sort of all","chapter-7","Internal Sorting"
"elements. Figure 7.6 illustrates the process for an array of 16 values where the sizes","chapter-7","Internal Sorting"
"of the increments (the distances between elements on the successive passes) are 8,","chapter-7","Internal Sorting"
"4, 2, and 1. Figure 7.7 presents a Java implementation for Shellsort.","chapter-7","Internal Sorting"
"Shellsort will work correctly regardless of the size of the increments, provided","chapter-7","Internal Sorting"
"that the final pass has increment 1 (i.e., provided the final pass is a regular Insertion","chapter-7","Internal Sorting"
"Sort). If Shellsort will always conclude with a regular Insertion Sort, then how","chapter-7","Internal Sorting"
"can it be any improvement on Insertion Sort? The expectation is that each of the","chapter-7","Internal Sorting"
"(relatively cheap) sublist sorts will make the list “more sorted” than it was before.","chapter-7","Internal Sorting"
"Sec. 7.4 Mergesort 233","chapter-7","Internal Sorting"
"static <E extends Comparable<? super E>>","chapter-7","Internal Sorting"
"void shellsort(E[] A) {","chapter-7","Internal Sorting"
"for (int i=A.length/2; i>2; i/=2) // For each increment","chapter-7","Internal Sorting"
"for (int j=0; j<i; j++) // Sort each sublist","chapter-7","Internal Sorting"
"inssort2(A, j, i);","chapter-7","Internal Sorting"
"inssort2(A, 0, 1); // Could call regular inssort here","chapter-7","Internal Sorting"
"}","chapter-7","Internal Sorting"
"/** Modified Insertion Sort for varying increments */","chapter-7","Internal Sorting"
"static <E extends Comparable<? super E>>","chapter-7","Internal Sorting"
"void inssort2(E[] A, int start, int incr) {","chapter-7","Internal Sorting"
"for (int i=start+incr; i<A.length; i+=incr)","chapter-7","Internal Sorting"
"for (int j=i; (j>=incr)&&","chapter-7","Internal Sorting"
"(A[j].compareTo(A[j-incr])<0); j-=incr)","chapter-7","Internal Sorting"
"DSutil.swap(A, j, j-incr);","chapter-7","Internal Sorting"
"}","chapter-7","Internal Sorting"
"Figure 7.7 An implementation for Shell Sort.","chapter-7","Internal Sorting"
"It is not necessarily the case that this will be true, but it is almost always true in","chapter-7","Internal Sorting"
"practice. When the final Insertion Sort is conducted, the list should be “almost","chapter-7","Internal Sorting"
"sorted,” yielding a relatively cheap final Insertion Sort pass.","chapter-7","Internal Sorting"
"Some choices for increments will make Shellsort run more efficiently than oth-","chapter-7","Internal Sorting"
"ers. In particular, the choice of increments described above (2","chapter-7","Internal Sorting"
"k","chapter-7","Internal Sorting"
", 2","chapter-7","Internal Sorting"
"k−1","chapter-7","Internal Sorting"
", ..., 2, 1)","chapter-7","Internal Sorting"
"turns out to be relatively inefficient. A better choice is the following series based","chapter-7","Internal Sorting"
"on division by three: (..., 121, 40, 13, 4, 1).","chapter-7","Internal Sorting"
"The analysis of Shellsort is difficult, so we must accept without proof that","chapter-7","Internal Sorting"
"the average-case performance of Shellsort (for “divisions by three” increments)","chapter-7","Internal Sorting"
"is O(n","chapter-7","Internal Sorting"
"1.5","chapter-7","Internal Sorting"
"). Other choices for the increment series can reduce this upper bound","chapter-7","Internal Sorting"
"somewhat. Thus, Shellsort is substantially better than Insertion Sort, or any of the","chapter-7","Internal Sorting"
"Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") sorts presented in Section 7.2. In fact, Shellsort is not terrible when com-","chapter-7","Internal Sorting"
"pared with the asymptotically better sorts to be presented whenever n is of medium","chapter-7","Internal Sorting"
"size (thought is tends to be a little slower than these other algorithms when they","chapter-7","Internal Sorting"
"are well implemented). Shellsort illustrates how we can sometimes exploit the spe-","chapter-7","Internal Sorting"
"cial properties of an algorithm (in this case Insertion Sort) even if in general that","chapter-7","Internal Sorting"
"algorithm is unacceptably slow.","chapter-7","Internal Sorting"
"7.4 Mergesort","chapter-7","Internal Sorting"
"A natural approach to problem solving is divide and conquer. In terms of sorting,","chapter-7","Internal Sorting"
"we might consider breaking the list to be sorted into pieces, process the pieces, and","chapter-7","Internal Sorting"
"then put them back together somehow. A simple way to do this would be to split","chapter-7","Internal Sorting"
"the list in half, sort the halves, and then merge the sorted halves together. This is","chapter-7","Internal Sorting"
"the idea behind Mergesort.","chapter-7","Internal Sorting"
"234 Chap. 7 Internal Sorting","chapter-7","Internal Sorting"
"36 20 17 13 28 14 23 15","chapter-7","Internal Sorting"
"13 17 20 36 14 15 23 28","chapter-7","Internal Sorting"
"20 36 13 17 14 28 15 23","chapter-7","Internal Sorting"
"13 14 15 17 20 23 28 36","chapter-7","Internal Sorting"
"Figure 7.8 An illustration of Mergesort. The first row shows eight numbers that","chapter-7","Internal Sorting"
"are to be sorted. Mergesort will recursively subdivide the list into sublists of one","chapter-7","Internal Sorting"
"element each, then recombine the sublists. The second row shows the four sublists","chapter-7","Internal Sorting"
"of size 2 created by the first merging pass. The third row shows the two sublists","chapter-7","Internal Sorting"
"of size 4 created by the next merging pass on the sublists of row 2. The last row","chapter-7","Internal Sorting"
"shows the final sorted list created by merging the two sublists of row 3.","chapter-7","Internal Sorting"
"Mergesort is one of the simplest sorting algorithms conceptually, and has good","chapter-7","Internal Sorting"
"performance both in the asymptotic sense and in empirical running time. Surpris-","chapter-7","Internal Sorting"
"ingly, even though it is based on a simple concept, it is relatively difficult to im-","chapter-7","Internal Sorting"
"plement in practice. Figure 7.8 illustrates Mergesort. A pseudocode sketch of","chapter-7","Internal Sorting"
"Mergesort is as follows:","chapter-7","Internal Sorting"
"List mergesort(List inlist) {","chapter-7","Internal Sorting"
"if (inlist.length() <= 1) return inlist;;","chapter-7","Internal Sorting"
"List L1 = half of the items from inlist;","chapter-7","Internal Sorting"
"List L2 = other half of the items from inlist;","chapter-7","Internal Sorting"
"return merge(mergesort(L1), mergesort(L2));","chapter-7","Internal Sorting"
"}","chapter-7","Internal Sorting"
"Before discussing how to implement Mergesort, we will first examine the merge","chapter-7","Internal Sorting"
"function. Merging two sorted sublists is quite simple. Function merge examines","chapter-7","Internal Sorting"
"the first element of each sublist and picks the smaller value as the smallest element","chapter-7","Internal Sorting"
"overall. This smaller value is removed from its sublist and placed into the output","chapter-7","Internal Sorting"
"list. Merging continues in this way, comparing the front elements of the sublists and","chapter-7","Internal Sorting"
"continually appending the smaller to the output list until no more input elements","chapter-7","Internal Sorting"
"remain.","chapter-7","Internal Sorting"
"Implementing Mergesort presents a number of technical difficulties. The first","chapter-7","Internal Sorting"
"decision is how to represent the lists. Mergesort lends itself well to sorting a singly","chapter-7","Internal Sorting"
"linked list because merging does not require random access to the list elements.","chapter-7","Internal Sorting"
"Thus, Mergesort is the method of choice when the input is in the form of a linked","chapter-7","Internal Sorting"
"list. Implementing merge for linked lists is straightforward, because we need only","chapter-7","Internal Sorting"
"remove items from the front of the input lists and append items to the output list.","chapter-7","Internal Sorting"
"Breaking the input list into two equal halves presents some difficulty. Ideally we","chapter-7","Internal Sorting"
"would just break the lists into front and back halves. However, even if we know the","chapter-7","Internal Sorting"
"length of the list in advance, it would still be necessary to traverse halfway down","chapter-7","Internal Sorting"
"the linked list to reach the beginning of the second half. A simpler method, which","chapter-7","Internal Sorting"
"does not rely on knowing the length of the list in advance, assigns elements of the","chapter-7","Internal Sorting"
"Sec. 7.4 Mergesort 235","chapter-7","Internal Sorting"
"static <E extends Comparable<? super E>>","chapter-7","Internal Sorting"
"void mergesort(E[] A, E[] temp, int l, int r) {","chapter-7","Internal Sorting"
"int mid = (l+r)/2; // Select midpoint","chapter-7","Internal Sorting"
"if (l == r) return; // List has one element","chapter-7","Internal Sorting"
"mergesort(A, temp, l, mid); // Mergesort first half","chapter-7","Internal Sorting"
"mergesort(A, temp, mid+1, r); // Mergesort second half","chapter-7","Internal Sorting"
"for (int i=l; i<=r; i++) // Copy subarray to temp","chapter-7","Internal Sorting"
"temp[i] = A[i];","chapter-7","Internal Sorting"
"// Do the merge operation back to A","chapter-7","Internal Sorting"
"int i1 = l; int i2 = mid + 1;","chapter-7","Internal Sorting"
"for (int curr=l; curr<=r; curr++) {","chapter-7","Internal Sorting"
"if (i1 == mid+1) // Left sublist exhausted","chapter-7","Internal Sorting"
"A[curr] = temp[i2++];","chapter-7","Internal Sorting"
"else if (i2 > r) // Right sublist exhausted","chapter-7","Internal Sorting"
"A[curr] = temp[i1++];","chapter-7","Internal Sorting"
"else if (temp[i1].compareTo(temp[i2])<0) // Get smaller","chapter-7","Internal Sorting"
"A[curr] = temp[i1++];","chapter-7","Internal Sorting"
"else A[curr] = temp[i2++];","chapter-7","Internal Sorting"
"}","chapter-7","Internal Sorting"
"}","chapter-7","Internal Sorting"
"Figure 7.9 Standard implementation for Mergesort.","chapter-7","Internal Sorting"
"input list alternating between the two sublists. The first element is assigned to the","chapter-7","Internal Sorting"
"first sublist, the second element to the second sublist, the third to first sublist, the","chapter-7","Internal Sorting"
"fourth to the second sublist, and so on. This requires one complete pass through","chapter-7","Internal Sorting"
"the input list to build the sublists.","chapter-7","Internal Sorting"
"When the input to Mergesort is an array, splitting input into two subarrays is","chapter-7","Internal Sorting"
"easy if we know the array bounds. Merging is also easy if we merge the subarrays","chapter-7","Internal Sorting"
"into a second array. Note that this approach requires twice the amount of space","chapter-7","Internal Sorting"
"as any of the sorting methods presented so far, which is a serious disadvantage for","chapter-7","Internal Sorting"
"Mergesort. It is possible to merge the subarrays without using a second array, but","chapter-7","Internal Sorting"
"this is extremely difficult to do efficiently and is not really practical. Merging the","chapter-7","Internal Sorting"
"two subarrays into a second array, while simple to implement, presents another dif-","chapter-7","Internal Sorting"
"ficulty. The merge process ends with the sorted list in the auxiliary array. Consider","chapter-7","Internal Sorting"
"how the recursive nature of Mergesort breaks the original array into subarrays, as","chapter-7","Internal Sorting"
"shown in Figure 7.8. Mergesort is recursively called until subarrays of size 1 have","chapter-7","Internal Sorting"
"been created, requiring log n levels of recursion. These subarrays are merged into","chapter-7","Internal Sorting"
"subarrays of size 2, which are in turn merged into subarrays of size 4, and so on.","chapter-7","Internal Sorting"
"We need to avoid having each merge operation require a new array. With some","chapter-7","Internal Sorting"
"difficulty, an algorithm can be devised that alternates between two arrays. A much","chapter-7","Internal Sorting"
"simpler approach is to copy the sorted sublists to the auxiliary array first, and then","chapter-7","Internal Sorting"
"merge them back to the original array. Figure 7.9 shows a complete implementation","chapter-7","Internal Sorting"
"for mergesort following this approach.","chapter-7","Internal Sorting"
"An optimized Mergesort implementation is shown in Figure 7.10. It reverses","chapter-7","Internal Sorting"
"the order of the second subarray during the initial copy. Now the current positions","chapter-7","Internal Sorting"
"of the two subarrays work inwards from the ends, allowing the end of each subarray","chapter-7","Internal Sorting"
"236 Chap. 7 Internal Sorting","chapter-7","Internal Sorting"
"static <E extends Comparable<? super E>>","chapter-7","Internal Sorting"
"void mergesort(E[] A, E[] temp, int l, int r) {","chapter-7","Internal Sorting"
"int i, j, k, mid = (l+r)/2; // Select the midpoint","chapter-7","Internal Sorting"
"if (l == r) return; // List has one element","chapter-7","Internal Sorting"
"if ((mid-l) >= THRESHOLD) mergesort(A, temp, l, mid);","chapter-7","Internal Sorting"
"else inssort(A, l, mid-l+1);","chapter-7","Internal Sorting"
"if ((r-mid) > THRESHOLD) mergesort(A, temp, mid+1, r);","chapter-7","Internal Sorting"
"else inssort(A, mid+1, r-mid);","chapter-7","Internal Sorting"
"// Do the merge operation. First, copy 2 halves to temp.","chapter-7","Internal Sorting"
"for (i=l; i<=mid; i++) temp[i] = A[i];","chapter-7","Internal Sorting"
"for (j=1; j<=r-mid; j++) temp[r-j+1] = A[j+mid];","chapter-7","Internal Sorting"
"// Merge sublists back to array","chapter-7","Internal Sorting"
"for (i=l,j=r,k=l; k<=r; k++)","chapter-7","Internal Sorting"
"if (temp[i].compareTo(temp[j])<0) A[k] = temp[i++];","chapter-7","Internal Sorting"
"else A[k] = temp[j--];","chapter-7","Internal Sorting"
"}","chapter-7","Internal Sorting"
"Figure 7.10 Optimized implementation for Mergesort.","chapter-7","Internal Sorting"
"to act as a sentinel for the other. Unlike the previous implementation, no test is","chapter-7","Internal Sorting"
"needed to check for when one of the two subarrays becomes empty. This version","chapter-7","Internal Sorting"
"also uses Insertion Sort to sort small subarrays.","chapter-7","Internal Sorting"
"Analysis of Mergesort is straightforward, despite the fact that it is a recursive","chapter-7","Internal Sorting"
"algorithm. The merging part takes time Θ(i) where i is the total length of the two","chapter-7","Internal Sorting"
"subarrays being merged. The array to be sorted is repeatedly split in half until","chapter-7","Internal Sorting"
"subarrays of size 1 are reached, at which time they are merged to be of size 2, these","chapter-7","Internal Sorting"
"merged to subarrays of size 4, and so on as shown in Figure 7.8. Thus, the depth","chapter-7","Internal Sorting"
"of the recursion is log n for n elements (assume for simplicity that n is a power","chapter-7","Internal Sorting"
"of two). The first level of recursion can be thought of as working on one array of","chapter-7","Internal Sorting"
"size n, the next level working on two arrays of size n/2, the next on four arrays","chapter-7","Internal Sorting"
"of size n/4, and so on. The bottom of the recursion has n arrays of size 1. Thus,","chapter-7","Internal Sorting"
"n arrays of size 1 are merged (requiring Θ(n) total steps), n/2 arrays of size 2","chapter-7","Internal Sorting"
"(again requiring Θ(n) total steps), n/4 arrays of size 4, and so on. At each of the","chapter-7","Internal Sorting"
"log n levels of recursion, Θ(n) work is done, for a total cost of Θ(n log n). This","chapter-7","Internal Sorting"
"cost is unaffected by the relative order of the values being sorted, thus this analysis","chapter-7","Internal Sorting"
"holds for the best, average, and worst cases.","chapter-7","Internal Sorting"
"7.5 Quicksort","chapter-7","Internal Sorting"
"While Mergesort uses the most obvious form of divide and conquer (split the list in","chapter-7","Internal Sorting"
"half then sort the halves), it is not the only way that we can break down the sorting","chapter-7","Internal Sorting"
"problem. And we saw that doing the merge step for Mergesort when using an array","chapter-7","Internal Sorting"
"implementation is not so easy. So perhaps a different divide and conquer strategy","chapter-7","Internal Sorting"
"might turn out to be more efficient?","chapter-7","Internal Sorting"
"Sec. 7.5 Quicksort 237","chapter-7","Internal Sorting"
"Quicksort is aptly named because, when properly implemented, it is the fastest","chapter-7","Internal Sorting"
"known general-purpose in-memory sorting algorithm in the average case. It does","chapter-7","Internal Sorting"
"not require the extra array needed by Mergesort, so it is space efficient as well.","chapter-7","Internal Sorting"
"Quicksort is widely used, and is typically the algorithm implemented in a library","chapter-7","Internal Sorting"
"sort routine such as the UNIX qsort function. Interestingly, Quicksort is ham-","chapter-7","Internal Sorting"
"pered by exceedingly poor worst-case performance, thus making it inappropriate","chapter-7","Internal Sorting"
"for certain applications.","chapter-7","Internal Sorting"
"Before we get to Quicksort, consider for a moment the practicality of using a","chapter-7","Internal Sorting"
"Binary Search Tree for sorting. You could insert all of the values to be sorted into","chapter-7","Internal Sorting"
"the BST one by one, then traverse the completed tree using an inorder traversal.","chapter-7","Internal Sorting"
"The output would form a sorted list. This approach has a number of drawbacks,","chapter-7","Internal Sorting"
"including the extra space required by BST pointers and the amount of time required","chapter-7","Internal Sorting"
"to insert nodes into the tree. However, this method introduces some interesting","chapter-7","Internal Sorting"
"ideas. First, the root of the BST (i.e., the first node inserted) splits the list into two","chapter-7","Internal Sorting"
"sublists: The left subtree contains those values in the list less than the root value","chapter-7","Internal Sorting"
"while the right subtree contains those values in the list greater than or equal to the","chapter-7","Internal Sorting"
"root value. Thus, the BST implicitly implements a “divide and conquer” approach","chapter-7","Internal Sorting"
"to sorting the left and right subtrees. Quicksort implements this concept in a much","chapter-7","Internal Sorting"
"more efficient way.","chapter-7","Internal Sorting"
"Quicksort first selects a value called the pivot. (This is conceptually like the","chapter-7","Internal Sorting"
"root node’s value in the BST.) Assume that the input array contains k values less","chapter-7","Internal Sorting"
"than the pivot. The records are then rearranged in such a way that the k values","chapter-7","Internal Sorting"
"less than the pivot are placed in the first, or leftmost, k positions in the array, and","chapter-7","Internal Sorting"
"the values greater than or equal to the pivot are placed in the last, or rightmost,","chapter-7","Internal Sorting"
"n−k positions. This is called a partition of the array. The values placed in a given","chapter-7","Internal Sorting"
"partition need not (and typically will not) be sorted with respect to each other. All","chapter-7","Internal Sorting"
"that is required is that all values end up in the correct partition. The pivot value itself","chapter-7","Internal Sorting"
"is placed in position k. Quicksort then proceeds to sort the resulting subarrays now","chapter-7","Internal Sorting"
"on either side of the pivot, one of size k and the other of size n − k − 1. How are","chapter-7","Internal Sorting"
"these values sorted? Because Quicksort is such a good algorithm, using Quicksort","chapter-7","Internal Sorting"
"on the subarrays would be appropriate.","chapter-7","Internal Sorting"
"Unlike some of the sorts that we have seen earlier in this chapter, Quicksort","chapter-7","Internal Sorting"
"might not seem very “natural” in that it is not an approach that a person is likely to","chapter-7","Internal Sorting"
"use to sort real objects. But it should not be too surprising that a really efficient sort","chapter-7","Internal Sorting"
"for huge numbers of abstract objects on a computer would be rather different from","chapter-7","Internal Sorting"
"our experiences with sorting a relatively few physical objects.","chapter-7","Internal Sorting"
"The Java code for Quicksort is shown in Figure 7.11. Parameters i and j define","chapter-7","Internal Sorting"
"the left and right indices, respectively, for the subarray being sorted. The initial call","chapter-7","Internal Sorting"
"to Quicksort would be qsort(array, 0, n-1).","chapter-7","Internal Sorting"
"Function partition will move records to the appropriate partition and then","chapter-7","Internal Sorting"
"return k, the first position in the right partition. Note that the pivot value is initially","chapter-7","Internal Sorting"
"238 Chap. 7 Internal Sorting","chapter-7","Internal Sorting"
"static <E extends Comparable<? super E>>","chapter-7","Internal Sorting"
"void qsort(E[] A, int i, int j) { // Quicksort","chapter-7","Internal Sorting"
"int pivotindex = findpivot(A, i, j); // Pick a pivot","chapter-7","Internal Sorting"
"DSutil.swap(A, pivotindex, j); // Stick pivot at end","chapter-7","Internal Sorting"
"// k will be the first position in the right subarray","chapter-7","Internal Sorting"
"int k = partition(A, i-1, j, A[j]);","chapter-7","Internal Sorting"
"DSutil.swap(A, k, j); // Put pivot in place","chapter-7","Internal Sorting"
"if ((k-i) > 1) qsort(A, i, k-1); // Sort left partition","chapter-7","Internal Sorting"
"if ((j-k) > 1) qsort(A, k+1, j); // Sort right partition","chapter-7","Internal Sorting"
"}","chapter-7","Internal Sorting"
"Figure 7.11 Implementation for Quicksort.","chapter-7","Internal Sorting"
"placed at the end of the array (position j). Thus, partition must not affect the","chapter-7","Internal Sorting"
"value of array position j. After partitioning, the pivot value is placed in position k,","chapter-7","Internal Sorting"
"which is its correct position in the final, sorted array. By doing so, we guarantee","chapter-7","Internal Sorting"
"that at least one value (the pivot) will not be processed in the recursive calls to","chapter-7","Internal Sorting"
"qsort. Even if a bad pivot is selected, yielding a completely empty partition to","chapter-7","Internal Sorting"
"one side of the pivot, the larger partition will contain at most n − 1 elements.","chapter-7","Internal Sorting"
"Selecting a pivot can be done in many ways. The simplest is to use the first","chapter-7","Internal Sorting"
"key. However, if the input is sorted or reverse sorted, this will produce a poor","chapter-7","Internal Sorting"
"partitioning with all values to one side of the pivot. It is better to pick a value","chapter-7","Internal Sorting"
"at random, thereby reducing the chance of a bad input order affecting the sort.","chapter-7","Internal Sorting"
"Unfortunately, using a random number generator is relatively expensive, and we","chapter-7","Internal Sorting"
"can do nearly as well by selecting the middle position in the array. Here is a simple","chapter-7","Internal Sorting"
"findpivot function:","chapter-7","Internal Sorting"
"static <E extends Comparable<? super E>>","chapter-7","Internal Sorting"
"int findpivot(E[] A, int i, int j)","chapter-7","Internal Sorting"
"{ return (i+j)/2; }","chapter-7","Internal Sorting"
"We now turn to function partition. If we knew in advance how many keys","chapter-7","Internal Sorting"
"are less than the pivot, partition could simply copy elements with key values","chapter-7","Internal Sorting"
"less than the pivot to the low end of the array, and elements with larger keys to","chapter-7","Internal Sorting"
"the high end. Because we do not know in advance how many keys are less than","chapter-7","Internal Sorting"
"the pivot, we use a clever algorithm that moves indices inwards from the ends of","chapter-7","Internal Sorting"
"the subarray, swapping values as necessary until the two indices meet. Figure 7.12","chapter-7","Internal Sorting"
"shows a Java implementation for the partition step.","chapter-7","Internal Sorting"
"Figure 7.13 illustrates partition. Initially, variables l and r are immedi-","chapter-7","Internal Sorting"
"ately outside the actual bounds of the subarray being partitioned. Each pass through","chapter-7","Internal Sorting"
"the outer do loop moves the counters l and r inwards, until eventually they meet.","chapter-7","Internal Sorting"
"Note that at each iteration of the inner while loops, the bounds are moved prior","chapter-7","Internal Sorting"
"to checking against the pivot value. This ensures that progress is made by each","chapter-7","Internal Sorting"
"while loop, even when the two values swapped on the last iteration of the do","chapter-7","Internal Sorting"
"loop were equal to the pivot. Also note the check that r > l in the second while","chapter-7","Internal Sorting"
"loop. This ensures that r does not run off the low end of the partition in the case","chapter-7","Internal Sorting"
"Sec. 7.5 Quicksort 239","chapter-7","Internal Sorting"
"static <E extends Comparable<? super E>>","chapter-7","Internal Sorting"
"int partition(E[] A, int l, int r, E pivot) {","chapter-7","Internal Sorting"
"do { // Move bounds inward until they meet","chapter-7","Internal Sorting"
"while (A[++l].compareTo(pivot)<0);","chapter-7","Internal Sorting"
"while ((r!=0) && (A[--r].compareTo(pivot)>0));","chapter-7","Internal Sorting"
"DSutil.swap(A, l, r); // Swap out-of-place values","chapter-7","Internal Sorting"
"} while (l < r); // Stop when they cross","chapter-7","Internal Sorting"
"DSutil.swap(A, l, r); // Reverse last, wasted swap","chapter-7","Internal Sorting"
"return l; // Return first position in right partition","chapter-7","Internal Sorting"
"}","chapter-7","Internal Sorting"
"Figure 7.12 The Quicksort partition implementation.","chapter-7","Internal Sorting"
"Pass 1","chapter-7","Internal Sorting"
"Swap 1","chapter-7","Internal Sorting"
"Pass 2","chapter-7","Internal Sorting"
"Swap 2","chapter-7","Internal Sorting"
"Pass 3","chapter-7","Internal Sorting"
"72 6 57 88 85 42 83 73 48 60","chapter-7","Internal Sorting"
"l r","chapter-7","Internal Sorting"
"72 6 57 88 85 42 83 73 48 60","chapter-7","Internal Sorting"
"48 6 57 88 85 42 83 73 72 60","chapter-7","Internal Sorting"
"r","chapter-7","Internal Sorting"
"48 6 57 88 85 42 83 73 72 60","chapter-7","Internal Sorting"
"l","chapter-7","Internal Sorting"
"48 6 57 42 85 88 83 73 72 60","chapter-7","Internal Sorting"
"l r","chapter-7","Internal Sorting"
"48 6 57 42 88 83 73 72 60","chapter-7","Internal Sorting"
"Initial","chapter-7","Internal Sorting"
"l","chapter-7","Internal Sorting"
"l","chapter-7","Internal Sorting"
"r","chapter-7","Internal Sorting"
"r","chapter-7","Internal Sorting"
"85","chapter-7","Internal Sorting"
"l,r","chapter-7","Internal Sorting"
"Figure 7.13 The Quicksort partition step. The first row shows the initial po-","chapter-7","Internal Sorting"
"sitions for a collection of ten key values. The pivot value is 60, which has been","chapter-7","Internal Sorting"
"swapped to the end of the array. The do loop makes three iterations, each time","chapter-7","Internal Sorting"
"moving counters l and r inwards until they meet in the third pass. In the end,","chapter-7","Internal Sorting"
"the left partition contains four values and the right partition contains six values.","chapter-7","Internal Sorting"
"Function qsort will place the pivot value into position 4.","chapter-7","Internal Sorting"
"where the pivot is the least value in that partition. Function partition returns","chapter-7","Internal Sorting"
"the first index of the right partition so that the subarray bound for the recursive","chapter-7","Internal Sorting"
"calls to qsort can be determined. Figure 7.14 illustrates the complete Quicksort","chapter-7","Internal Sorting"
"algorithm.","chapter-7","Internal Sorting"
"To analyze Quicksort, we first analyze the findpivot and partition","chapter-7","Internal Sorting"
"functions operating on a subarray of length k. Clearly, findpivot takes con-","chapter-7","Internal Sorting"
"stant time. Function partition contains a do loop with two nested while","chapter-7","Internal Sorting"
"loops. The total cost of the partition operation is constrained by how far l and r","chapter-7","Internal Sorting"
"can move inwards. In particular, these two bounds variables together can move a","chapter-7","Internal Sorting"
"total of s steps for a subarray of length s. However, this does not directly tell us","chapter-7","Internal Sorting"
"240 Chap. 7 Internal Sorting","chapter-7","Internal Sorting"
"Pivot = 6 Pivot = 73","chapter-7","Internal Sorting"
"Pivot = 57","chapter-7","Internal Sorting"
"Final Sorted Array","chapter-7","Internal Sorting"
"Pivot = 60","chapter-7","Internal Sorting"
"Pivot = 88","chapter-7","Internal Sorting"
"42 57 48","chapter-7","Internal Sorting"
"57","chapter-7","Internal Sorting"
"6 42 48 57 60 72 73 83 85 88","chapter-7","Internal Sorting"
"Pivot = 42 Pivot = 85","chapter-7","Internal Sorting"
"6 57 88 60 42 83 73 48 85","chapter-7","Internal Sorting"
"48 6 57 42 60 88 83 73 72 85","chapter-7","Internal Sorting"
"6","chapter-7","Internal Sorting"
"42 48","chapter-7","Internal Sorting"
"42 48","chapter-7","Internal Sorting"
"85 83 88","chapter-7","Internal Sorting"
"83 85","chapter-7","Internal Sorting"
"72 73 85 88 83","chapter-7","Internal Sorting"
"72","chapter-7","Internal Sorting"
"Figure 7.14 An illustration of Quicksort.","chapter-7","Internal Sorting"
"how much work is done by the nested while loops. The do loop as a whole is","chapter-7","Internal Sorting"
"guaranteed to move both l and r inward at least one position on each first pass.","chapter-7","Internal Sorting"
"Each while loop moves its variable at least once (except in the special case where","chapter-7","Internal Sorting"
"r is at the left edge of the array, but this can happen only once). Thus, we see that","chapter-7","Internal Sorting"
"the do loop can be executed at most s times, the total amount of work done moving","chapter-7","Internal Sorting"
"l and r is s, and each while loop can fail its test at most s times. The total work","chapter-7","Internal Sorting"
"for the entire partition function is therefore Θ(s).","chapter-7","Internal Sorting"
"Knowing the cost of findpivot and partition, we can determine the","chapter-7","Internal Sorting"
"cost of Quicksort. We begin with a worst-case analysis. The worst case will occur","chapter-7","Internal Sorting"
"when the pivot does a poor job of breaking the array, that is, when there are no","chapter-7","Internal Sorting"
"elements in one partition, and n − 1 elements in the other. In this case, the divide","chapter-7","Internal Sorting"
"and conquer strategy has done a poor job of dividing, so the conquer phase will","chapter-7","Internal Sorting"
"work on a subproblem only one less than the size of the original problem. If this","chapter-7","Internal Sorting"
"happens at each partition step, then the total cost of the algorithm will be","chapter-7","Internal Sorting"
"Xn","chapter-7","Internal Sorting"
"k=1","chapter-7","Internal Sorting"
"k = Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
").","chapter-7","Internal Sorting"
"In the worst case, Quicksort is Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
"). This is terrible, no better than Bubble","chapter-7","Internal Sorting"
"Sort.2 When will this worst case occur? Only when each pivot yields a bad parti-","chapter-7","Internal Sorting"
"tioning of the array. If the pivot values are selected at random, then this is extremely","chapter-7","Internal Sorting"
"unlikely to happen. When selecting the middle position of the current subarray, it","chapter-7","Internal Sorting"
"2The worst insult that I can think of for a sorting algorithm.","chapter-7","Internal Sorting"
"Sec. 7.5 Quicksort 241","chapter-7","Internal Sorting"
"is still unlikely to happen. It does not take many good partitionings for Quicksort","chapter-7","Internal Sorting"
"to work fairly well.","chapter-7","Internal Sorting"
"Quicksort’s best case occurs when findpivot always breaks the array into","chapter-7","Internal Sorting"
"two equal halves. Quicksort repeatedly splits the array into smaller partitions, as","chapter-7","Internal Sorting"
"shown in Figure 7.14. In the best case, the result will be log n levels of partitions,","chapter-7","Internal Sorting"
"with the top level having one array of size n, the second level two arrays of size n/2,","chapter-7","Internal Sorting"
"the next with four arrays of size n/4, and so on. Thus, at each level, all partition","chapter-7","Internal Sorting"
"steps for that level do a total of n work, for an overall cost of n log n work when","chapter-7","Internal Sorting"
"Quicksort finds perfect pivots.","chapter-7","Internal Sorting"
"Quicksort’s average-case behavior falls somewhere between the extremes of","chapter-7","Internal Sorting"
"worst and best case. Average-case analysis considers the cost for all possible ar-","chapter-7","Internal Sorting"
"rangements of input, summing the costs and dividing by the number of cases. We","chapter-7","Internal Sorting"
"make one reasonable simplifying assumption: At each partition step, the pivot is","chapter-7","Internal Sorting"
"equally likely to end in any position in the (sorted) array. In other words, the pivot","chapter-7","Internal Sorting"
"is equally likely to break an array into partitions of sizes 0 and n−1, or 1 and n−2,","chapter-7","Internal Sorting"
"and so on.","chapter-7","Internal Sorting"
"Given this assumption, the average-case cost is computed from the following","chapter-7","Internal Sorting"
"equation:","chapter-7","Internal Sorting"
"T(n) = cn +","chapter-7","Internal Sorting"
"1","chapter-7","Internal Sorting"
"n","chapter-7","Internal Sorting"
"nX−1","chapter-7","Internal Sorting"
"k=0","chapter-7","Internal Sorting"
"[T(k) + T(n − 1 − k)], T(0) = T(1) = c.","chapter-7","Internal Sorting"
"This equation is in the form of a recurrence relation. Recurrence relations are","chapter-7","Internal Sorting"
"discussed in Chapters 2 and 14, and this one is solved in Section 14.2.4. This","chapter-7","Internal Sorting"
"equation says that there is one chance in n that the pivot breaks the array into","chapter-7","Internal Sorting"
"subarrays of size 0 and n − 1, one chance in n that the pivot breaks the array into","chapter-7","Internal Sorting"
"subarrays of size 1 and n−2, and so on. The expression “T(k) + T(n−1−k)” is","chapter-7","Internal Sorting"
"the cost for the two recursive calls to Quicksort on two arrays of size k and n−1−k.","chapter-7","Internal Sorting"
"The initial cn term is the cost of doing the findpivot and partition steps, for","chapter-7","Internal Sorting"
"some constant c. The closed-form solution to this recurrence relation is Θ(n log n).","chapter-7","Internal Sorting"
"Thus, Quicksort has average-case cost Θ(n log n).","chapter-7","Internal Sorting"
"This is an unusual situation that the average case cost and the worst case cost","chapter-7","Internal Sorting"
"have asymptotically different growth rates. Consider what “average case” actually","chapter-7","Internal Sorting"
"means. We compute an average cost for inputs of size n by summing up for every","chapter-7","Internal Sorting"
"possible input of size n the product of the running time cost of that input times the","chapter-7","Internal Sorting"
"probability that that input will occur. To simplify things, we assumed that every","chapter-7","Internal Sorting"
"permutation is equally likely to occur. Thus, finding the average means summing","chapter-7","Internal Sorting"
"up the cost for every permutation and dividing by the number of inputs (n!). We","chapter-7","Internal Sorting"
"know that some of these n! inputs cost O(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
"). But the sum of all the permutation","chapter-7","Internal Sorting"
"costs has to be (n!)(O(n log n)). Given the extremely high cost of the worst inputs,","chapter-7","Internal Sorting"
"there must be very few of them. In fact, there cannot be a constant fraction of the","chapter-7","Internal Sorting"
"inputs with cost O(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
"). Even, say, 1% of the inputs with cost O(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") would lead to","chapter-7","Internal Sorting"
"242 Chap. 7 Internal Sorting","chapter-7","Internal Sorting"
"an average cost of O(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
"). Thus, as n grows, the fraction of inputs with high cost","chapter-7","Internal Sorting"
"must be going toward a limit of zero. We can conclude that Quicksort will have","chapter-7","Internal Sorting"
"good behavior if we can avoid those very few bad input permutations.","chapter-7","Internal Sorting"
"The running time for Quicksort can be improved (by a constant factor), and","chapter-7","Internal Sorting"
"much study has gone into optimizing this algorithm. The most obvious place for","chapter-7","Internal Sorting"
"improvement is the findpivot function. Quicksort’s worst case arises when the","chapter-7","Internal Sorting"
"pivot does a poor job of splitting the array into equal size subarrays. If we are","chapter-7","Internal Sorting"
"willing to do more work searching for a better pivot, the effects of a bad pivot can","chapter-7","Internal Sorting"
"be decreased or even eliminated. One good choice is to use the “median of three”","chapter-7","Internal Sorting"
"algorithm, which uses as a pivot the middle of three randomly selected values.","chapter-7","Internal Sorting"
"Using a random number generator to choose the positions is relatively expensive,","chapter-7","Internal Sorting"
"so a common compromise is to look at the first, middle, and last positions of the","chapter-7","Internal Sorting"
"current subarray. However, our simple findpivot function that takes the middle","chapter-7","Internal Sorting"
"value as its pivot has the virtue of making it highly unlikely to get a bad input by","chapter-7","Internal Sorting"
"chance, and it is quite cheap to implement. This is in sharp contrast to selecting","chapter-7","Internal Sorting"
"the first or last element as the pivot, which would yield bad performance for many","chapter-7","Internal Sorting"
"permutations that are nearly sorted or nearly reverse sorted.","chapter-7","Internal Sorting"
"A significant improvement can be gained by recognizing that Quicksort is rel-","chapter-7","Internal Sorting"
"atively slow when n is small. This might not seem to be relevant if most of the","chapter-7","Internal Sorting"
"time we sort large arrays, nor should it matter how long Quicksort takes in the","chapter-7","Internal Sorting"
"rare instance when a small array is sorted because it will be fast anyway. But you","chapter-7","Internal Sorting"
"should notice that Quicksort itself sorts many, many small arrays! This happens as","chapter-7","Internal Sorting"
"a natural by-product of the divide and conquer approach.","chapter-7","Internal Sorting"
"A simple improvement might then be to replace Quicksort with a faster sort","chapter-7","Internal Sorting"
"for small numbers, say Insertion Sort or Selection Sort. However, there is an even","chapter-7","Internal Sorting"
"better — and still simpler — optimization. When Quicksort partitions are below","chapter-7","Internal Sorting"
"a certain size, do nothing! The values within that partition will be out of order.","chapter-7","Internal Sorting"
"However, we do know that all values in the array to the left of the partition are","chapter-7","Internal Sorting"
"smaller than all values in the partition. All values in the array to the right of the","chapter-7","Internal Sorting"
"partition are greater than all values in the partition. Thus, even if Quicksort only","chapter-7","Internal Sorting"
"gets the values to “nearly” the right locations, the array will be close to sorted. This","chapter-7","Internal Sorting"
"is an ideal situation in which to take advantage of the best-case performance of","chapter-7","Internal Sorting"
"Insertion Sort. The final step is a single call to Insertion Sort to process the entire","chapter-7","Internal Sorting"
"array, putting the elements into final sorted order. Empirical testing shows that","chapter-7","Internal Sorting"
"the subarrays should be left unordered whenever they get down to nine or fewer","chapter-7","Internal Sorting"
"elements.","chapter-7","Internal Sorting"
"The last speedup to be considered reduces the cost of making recursive calls.","chapter-7","Internal Sorting"
"Quicksort is inherently recursive, because each Quicksort operation must sort two","chapter-7","Internal Sorting"
"sublists. Thus, there is no simple way to turn Quicksort into an iterative algorithm.","chapter-7","Internal Sorting"
"However, Quicksort can be implemented using a stack to imitate recursion, as the","chapter-7","Internal Sorting"
"amount of information that must be stored is small. We need not store copies of a","chapter-7","Internal Sorting"
"Sec. 7.6 Heapsort 243","chapter-7","Internal Sorting"
"subarray, only the subarray bounds. Furthermore, the stack depth can be kept small","chapter-7","Internal Sorting"
"if care is taken on the order in which Quicksort’s recursive calls are executed. We","chapter-7","Internal Sorting"
"can also place the code for findpivot and partition inline to eliminate the","chapter-7","Internal Sorting"
"remaining function calls. Note however that by not processing sublists of size nine","chapter-7","Internal Sorting"
"or less as suggested above, about three quarters of the function calls will already","chapter-7","Internal Sorting"
"have been eliminated. Thus, eliminating the remaining function calls will yield","chapter-7","Internal Sorting"
"only a modest speedup.","chapter-7","Internal Sorting"
"7.6 Heapsort","chapter-7","Internal Sorting"
"Our discussion of Quicksort began by considering the practicality of using a binary","chapter-7","Internal Sorting"
"search tree for sorting. The BST requires more space than the other sorting meth-","chapter-7","Internal Sorting"
"ods and will be slower than Quicksort or Mergesort due to the relative expense of","chapter-7","Internal Sorting"
"inserting values into the tree. There is also the possibility that the BST might be un-","chapter-7","Internal Sorting"
"balanced, leading to a Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") worst-case running time. Subtree balance in the BST","chapter-7","Internal Sorting"
"is closely related to Quicksort’s partition step. Quicksort’s pivot serves roughly the","chapter-7","Internal Sorting"
"same purpose as the BST root value in that the left partition (subtree) stores val-","chapter-7","Internal Sorting"
"ues less than the pivot (root) value, while the right partition (subtree) stores values","chapter-7","Internal Sorting"
"greater than or equal to the pivot (root).","chapter-7","Internal Sorting"
"A good sorting algorithm can be devised based on a tree structure more suited","chapter-7","Internal Sorting"
"to the purpose. In particular, we would like the tree to be balanced, space efficient,","chapter-7","Internal Sorting"
"and fast. The algorithm should take advantage of the fact that sorting is a special-","chapter-7","Internal Sorting"
"purpose application in that all of the values to be stored are available at the start.","chapter-7","Internal Sorting"
"This means that we do not necessarily need to insert one value at a time into the","chapter-7","Internal Sorting"
"tree structure.","chapter-7","Internal Sorting"
"Heapsort is based on the heap data structure presented in Section 5.5. Heapsort","chapter-7","Internal Sorting"
"has all of the advantages just listed. The complete binary tree is balanced, its array","chapter-7","Internal Sorting"
"representation is space efficient, and we can load all values into the tree at once,","chapter-7","Internal Sorting"
"taking advantage of the efficient buildheap function. The asymptotic perfor-","chapter-7","Internal Sorting"
"mance of Heapsort is Θ(n log n) in the best, average, and worst cases. It is not as","chapter-7","Internal Sorting"
"fast as Quicksort in the average case (by a constant factor), but Heapsort has special","chapter-7","Internal Sorting"
"properties that will make it particularly useful when sorting data sets too large to fit","chapter-7","Internal Sorting"
"in main memory, as discussed in Chapter 8.","chapter-7","Internal Sorting"
"A sorting algorithm based on max-heaps is quite straightforward. First we use","chapter-7","Internal Sorting"
"the heap building algorithm of Section 5.5 to convert the array into max-heap order.","chapter-7","Internal Sorting"
"Then we repeatedly remove the maximum value from the heap, restoring the heap","chapter-7","Internal Sorting"
"property each time that we do so, until the heap is empty. Note that each time","chapter-7","Internal Sorting"
"we remove the maximum element from the heap, it is placed at the end of the","chapter-7","Internal Sorting"
"array. Assume the n elements are stored in array positions 0 through n − 1. After","chapter-7","Internal Sorting"
"removing the maximum value from the heap and readjusting, the maximum value","chapter-7","Internal Sorting"
"will now be placed in position n − 1 of the array. The heap is now considered to be","chapter-7","Internal Sorting"
"244 Chap. 7 Internal Sorting","chapter-7","Internal Sorting"
"of size n − 1. Removing the new maximum (root) value places the second largest","chapter-7","Internal Sorting"
"value in position n−2 of the array. After removing each of the remaining values in","chapter-7","Internal Sorting"
"turn, the array will be properly sorted from least to greatest. This is why Heapsort","chapter-7","Internal Sorting"
"uses a max-heap rather than a min-heap as might have been expected. Figure 7.15","chapter-7","Internal Sorting"
"illustrates Heapsort. The complete Java implementation is as follows:","chapter-7","Internal Sorting"
"static <E extends Comparable<? super E>>","chapter-7","Internal Sorting"
"void heapsort(E[] A) {","chapter-7","Internal Sorting"
"// The heap constructor invokes the buildheap method","chapter-7","Internal Sorting"
"MaxHeap<E> H = new MaxHeap<E>(A, A.length, A.length);","chapter-7","Internal Sorting"
"for (int i=0; i<A.length; i++) // Now sort","chapter-7","Internal Sorting"
"H.removemax(); // Removemax places max at end of heap","chapter-7","Internal Sorting"
"}","chapter-7","Internal Sorting"
"Because building the heap takes Θ(n) time (see Section 5.5), and because","chapter-7","Internal Sorting"
"n deletions of the maximum element each take Θ(log n) time, we see that the en-","chapter-7","Internal Sorting"
"tire Heapsort operation takes Θ(n log n) time in the worst, average, and best cases.","chapter-7","Internal Sorting"
"While typically slower than Quicksort by a constant factor, Heapsort has one spe-","chapter-7","Internal Sorting"
"cial advantage over the other sorts studied so far. Building the heap is relatively","chapter-7","Internal Sorting"
"cheap, requiring Θ(n) time. Removing the maximum element from the heap re-","chapter-7","Internal Sorting"
"quires Θ(log n) time. Thus, if we wish to find the k largest elements in an array,","chapter-7","Internal Sorting"
"we can do so in time Θ(n + k log n). If k is small, this is a substantial improve-","chapter-7","Internal Sorting"
"ment over the time required to find the k largest elements using one of the other","chapter-7","Internal Sorting"
"sorting methods described earlier (many of which would require sorting all of the","chapter-7","Internal Sorting"
"array first). One situation where we are able to take advantage of this concept is","chapter-7","Internal Sorting"
"in the implementation of Kruskal’s minimum-cost spanning tree (MST) algorithm","chapter-7","Internal Sorting"
"of Section 11.5.2. That algorithm requires that edges be visited in ascending order","chapter-7","Internal Sorting"
"(so, use a min-heap), but this process stops as soon as the MST is complete. Thus,","chapter-7","Internal Sorting"
"only a relatively small fraction of the edges need be sorted.","chapter-7","Internal Sorting"
"7.7 Binsort and Radix Sort","chapter-7","Internal Sorting"
"Imagine that for the past year, as you paid your various bills, you then simply piled","chapter-7","Internal Sorting"
"all the paperwork onto the top of a table somewhere. Now the year has ended and","chapter-7","Internal Sorting"
"its time to sort all of these papers by what the bill was for (phone, electricity, rent,","chapter-7","Internal Sorting"
"etc.) and date. A pretty natural approach is to make some space on the floor, and as","chapter-7","Internal Sorting"
"you go through the pile of papers, put the phone bills into one pile, the electric bills","chapter-7","Internal Sorting"
"into another pile, and so on. Once this initial assignment of bills to piles is done (in","chapter-7","Internal Sorting"
"one pass), you can sort each pile by date relatively quickly because they are each","chapter-7","Internal Sorting"
"fairly small. This is the basic idea behind a Binsort.","chapter-7","Internal Sorting"
"Section 3.9 presented the following code fragment to sort a permutation of the","chapter-7","Internal Sorting"
"numbers 0 through n − 1:","chapter-7","Internal Sorting"
"for (i=0; i<n; i++)","chapter-7","Internal Sorting"
"B[A[i]] = A[i];","chapter-7","Internal Sorting"
"Sec. 7.7 Binsort and Radix Sort 245","chapter-7","Internal Sorting"
"Original Numbers","chapter-7","Internal Sorting"
"Build Heap","chapter-7","Internal Sorting"
"Remove 88","chapter-7","Internal Sorting"
"Remove 85","chapter-7","Internal Sorting"
"Remove 83","chapter-7","Internal Sorting"
"73","chapter-7","Internal Sorting"
"88 60","chapter-7","Internal Sorting"
"48","chapter-7","Internal Sorting"
"48 60","chapter-7","Internal Sorting"
"72","chapter-7","Internal Sorting"
"6 48","chapter-7","Internal Sorting"
"60 42 57","chapter-7","Internal Sorting"
"72 60","chapter-7","Internal Sorting"
"6","chapter-7","Internal Sorting"
"42 48","chapter-7","Internal Sorting"
"6 60 42 48","chapter-7","Internal Sorting"
"6 57","chapter-7","Internal Sorting"
"85","chapter-7","Internal Sorting"
"72 85","chapter-7","Internal Sorting"
"42 83","chapter-7","Internal Sorting"
"72 73 42 57","chapter-7","Internal Sorting"
"6","chapter-7","Internal Sorting"
"72 57","chapter-7","Internal Sorting"
"73 83","chapter-7","Internal Sorting"
"73 57","chapter-7","Internal Sorting"
"83","chapter-7","Internal Sorting"
"73 72 6 60 42 48 83 85 88 57","chapter-7","Internal Sorting"
"83 48 73 57 72 60 42 6 88","chapter-7","Internal Sorting"
"85 73 72 60 42 57 88 83 6","chapter-7","Internal Sorting"
"85","chapter-7","Internal Sorting"
"48","chapter-7","Internal Sorting"
"88 85 83 72 73 57 6 42 60 48","chapter-7","Internal Sorting"
"73 72 6 57 88 60 42 83 48 85","chapter-7","Internal Sorting"
"88","chapter-7","Internal Sorting"
"85","chapter-7","Internal Sorting"
"83","chapter-7","Internal Sorting"
"73","chapter-7","Internal Sorting"
"Figure 7.15 An illustration of Heapsort. The top row shows the values in their","chapter-7","Internal Sorting"
"original order. The second row shows the values after building the heap. The","chapter-7","Internal Sorting"
"third row shows the result of the first removefirst operation on key value 88.","chapter-7","Internal Sorting"
"Note that 88 is now at the end of the array. The fourth row shows the result of the","chapter-7","Internal Sorting"
"second removefirst operation on key value 85. The fifth row shows the result","chapter-7","Internal Sorting"
"of the third removefirst operation on key value 83. At this point, the last","chapter-7","Internal Sorting"
"three positions of the array hold the three greatest values in sorted order. Heapsort","chapter-7","Internal Sorting"
"continues in this manner until the entire array is sorted.","chapter-7","Internal Sorting"
"246 Chap. 7 Internal Sorting","chapter-7","Internal Sorting"
"static void binsort(Integer A[]) {","chapter-7","Internal Sorting"
"List<Integer>[] B = (LList<Integer>[])new LList[MaxKey];","chapter-7","Internal Sorting"
"Integer item;","chapter-7","Internal Sorting"
"for (int i=0; i<MaxKey; i++)","chapter-7","Internal Sorting"
"B[i] = new LList<Integer>();","chapter-7","Internal Sorting"
"for (int i=0; i<A.length; i++) B[A[i]].append(A[i]);","chapter-7","Internal Sorting"
"for (int i=0; i<MaxKey; i++)","chapter-7","Internal Sorting"
"for (B[i].moveToStart();","chapter-7","Internal Sorting"
"(item = B[i].getValue()) != null; B[i].next())","chapter-7","Internal Sorting"
"output(item);","chapter-7","Internal Sorting"
"}","chapter-7","Internal Sorting"
"Figure 7.16 The extended Binsort algorithm.","chapter-7","Internal Sorting"
"Here the key value is used to determine the position for a record in the final sorted","chapter-7","Internal Sorting"
"array. This is the most basic example of a Binsort, where key values are used","chapter-7","Internal Sorting"
"to assign records to bins. This algorithm is extremely efficient, taking Θ(n) time","chapter-7","Internal Sorting"
"regardless of the initial ordering of the keys. This is far better than the performance","chapter-7","Internal Sorting"
"of any sorting algorithm that we have seen so far. The only problem is that this","chapter-7","Internal Sorting"
"algorithm has limited use because it works only for a permutation of the numbers","chapter-7","Internal Sorting"
"from 0 to n − 1.","chapter-7","Internal Sorting"
"We can extend this simple Binsort algorithm to be more useful. Because Binsort","chapter-7","Internal Sorting"
"must perform direct computation on the key value (as opposed to just asking which","chapter-7","Internal Sorting"
"of two records comes first as our previous sorting algorithms did), we will assume","chapter-7","Internal Sorting"
"that the records use an integer key type.","chapter-7","Internal Sorting"
"The simplest extension is to allow for duplicate values among the keys. This","chapter-7","Internal Sorting"
"can be done by turning array slots into arbitrary-length bins by turning B into an","chapter-7","Internal Sorting"
"array of linked lists. In this way, all records with key value i can be placed in bin","chapter-7","Internal Sorting"
"B[i]. A second extension allows for a key range greater than n. For example,","chapter-7","Internal Sorting"
"a set of n records might have keys in the range 1 to 2n. The only requirement is","chapter-7","Internal Sorting"
"that each possible key value have a corresponding bin in B. The extended Binsort","chapter-7","Internal Sorting"
"algorithm is shown in Figure 7.16.","chapter-7","Internal Sorting"
"This version of Binsort can sort any collection of records whose key values fall","chapter-7","Internal Sorting"
"in the range from 0 to MaxKeyValue−1. The total work required is simply that","chapter-7","Internal Sorting"
"needed to place each record into the appropriate bin and then take all of the records","chapter-7","Internal Sorting"
"out of the bins. Thus, we need to process each record twice, for Θ(n) work.","chapter-7","Internal Sorting"
"Unfortunately, there is a crucial oversight in this analysis. Binsort must also","chapter-7","Internal Sorting"
"look at each of the bins to see if it contains a record. The algorithm must process","chapter-7","Internal Sorting"
"MaxKeyValue bins, regardless of how many actually hold records. If MaxKey-","chapter-7","Internal Sorting"
"Value is small compared to n, then this is not a great expense. Suppose that","chapter-7","Internal Sorting"
"MaxKeyValue = n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
". In this case, the total amount of work done will be Θ(n +","chapter-7","Internal Sorting"
"n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") = Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
"). This results in a poor sorting algorithm, and the algorithm becomes","chapter-7","Internal Sorting"
"even worse as the disparity between n and MaxKeyValue increases. In addition,","chapter-7","Internal Sorting"
"Sec. 7.7 Binsort and Radix Sort 247","chapter-7","Internal Sorting"
"0","chapter-7","Internal Sorting"
"1","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
"3","chapter-7","Internal Sorting"
"4","chapter-7","Internal Sorting"
"5","chapter-7","Internal Sorting"
"6","chapter-7","Internal Sorting"
"7","chapter-7","Internal Sorting"
"8","chapter-7","Internal Sorting"
"9","chapter-7","Internal Sorting"
"0","chapter-7","Internal Sorting"
"1","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
"3","chapter-7","Internal Sorting"
"4","chapter-7","Internal Sorting"
"5","chapter-7","Internal Sorting"
"6","chapter-7","Internal Sorting"
"7","chapter-7","Internal Sorting"
"8","chapter-7","Internal Sorting"
"9","chapter-7","Internal Sorting"
"Result of first pass: 91 1 72 23 84 5 25 27 97 17 67 28","chapter-7","Internal Sorting"
"Result of second pass: 1 17 5 23 25 27 28 67 72 84 91 97","chapter-7","Internal Sorting"
"First pass","chapter-7","Internal Sorting"
"(on right digit)","chapter-7","Internal Sorting"
"Second pass","chapter-7","Internal Sorting"
"(on left digit)","chapter-7","Internal Sorting"
"Initial List: 27 91 1 97 17 23 84 28 72 5 67 25","chapter-7","Internal Sorting"
"23","chapter-7","Internal Sorting"
"84","chapter-7","Internal Sorting"
"5 25","chapter-7","Internal Sorting"
"27","chapter-7","Internal Sorting"
"28","chapter-7","Internal Sorting"
"91 1","chapter-7","Internal Sorting"
"72","chapter-7","Internal Sorting"
"97 17 67","chapter-7","Internal Sorting"
"17","chapter-7","Internal Sorting"
"91 97","chapter-7","Internal Sorting"
"72","chapter-7","Internal Sorting"
"84","chapter-7","Internal Sorting"
"1 5","chapter-7","Internal Sorting"
"23 25","chapter-7","Internal Sorting"
"67","chapter-7","Internal Sorting"
"27 28","chapter-7","Internal Sorting"
"Figure 7.17 An example of Radix Sort for twelve two-digit numbers in base ten.","chapter-7","Internal Sorting"
"Two passes are required to sort the list.","chapter-7","Internal Sorting"
"a large key range requires an unacceptably large array B. Thus, even the extended","chapter-7","Internal Sorting"
"Binsort is useful only for a limited key range.","chapter-7","Internal Sorting"
"A further generalization to Binsort yields a bucket sort. Each bin is associated","chapter-7","Internal Sorting"
"with not just one key, but rather a range of key values. A bucket sort assigns records","chapter-7","Internal Sorting"
"to bins and then relies on some other sorting technique to sort the records within","chapter-7","Internal Sorting"
"each bin. The hope is that the relatively inexpensive bucketing process will put","chapter-7","Internal Sorting"
"only a small number of records in each bin, and that a “cleanup sort” within the","chapter-7","Internal Sorting"
"bins will then be relatively cheap.","chapter-7","Internal Sorting"
"There is a way to keep the number of bins and the related processing small","chapter-7","Internal Sorting"
"while allowing the cleanup sort to be based on Binsort. Consider a sequence of","chapter-7","Internal Sorting"
"records with keys in the range 0 to 99. If we have ten bins available, we can first","chapter-7","Internal Sorting"
"assign records to bins by taking their key value modulo 10. Thus, every key will","chapter-7","Internal Sorting"
"be assigned to the bin matching its rightmost decimal digit. We can then take these","chapter-7","Internal Sorting"
"records from the bins in order and reassign them to the bins on the basis of their","chapter-7","Internal Sorting"
"leftmost (10’s place) digit (define values in the range 0 to 9 to have a leftmost digit","chapter-7","Internal Sorting"
"of 0). In other words, assign the ith record from array A to a bin using the formula","chapter-7","Internal Sorting"
"A[i]/10. If we now gather the values from the bins in order, the result is a sorted","chapter-7","Internal Sorting"
"list. Figure 7.17 illustrates this process.","chapter-7","Internal Sorting"
"248 Chap. 7 Internal Sorting","chapter-7","Internal Sorting"
"static void radix(Integer[] A, Integer[] B,","chapter-7","Internal Sorting"
"int k, int r, int[] count) {","chapter-7","Internal Sorting"
"// Count[i] stores number of records in bin[i]","chapter-7","Internal Sorting"
"int i, j, rtok;","chapter-7","Internal Sorting"
"for (i=0, rtok=1; i<k; i++, rtok*=r) { // For k digits","chapter-7","Internal Sorting"
"for (j=0; j<r; j++) count[j] = 0; // Initialize count","chapter-7","Internal Sorting"
"// Count the number of records for each bin on this pass","chapter-7","Internal Sorting"
"for (j=0; j<A.length; j++) count[(A[j]/rtok)%r]++;","chapter-7","Internal Sorting"
"// count[j] will be index in B for last slot of bin j.","chapter-7","Internal Sorting"
"for (j=1; j<r; j++) count[j] = count[j-1] + count[j];","chapter-7","Internal Sorting"
"// Put records into bins, working from bottom of bin","chapter-7","Internal Sorting"
"// Since bins fill from bottom, j counts downwards","chapter-7","Internal Sorting"
"for (j=A.length-1; j>=0; j--)","chapter-7","Internal Sorting"
"B[--count[(A[j]/rtok)%r]] = A[j];","chapter-7","Internal Sorting"
"for (j=0; j<A.length; j++) A[j] = B[j]; // Copy B back","chapter-7","Internal Sorting"
"}","chapter-7","Internal Sorting"
"}","chapter-7","Internal Sorting"
"Figure 7.18 The Radix Sort algorithm.","chapter-7","Internal Sorting"
"In this example, we have r = 10 bins and n = 12 keys in the range 0 to r","chapter-7","Internal Sorting"
"2 − 1.","chapter-7","Internal Sorting"
"The total computation is Θ(n), because we look at each record and each bin a","chapter-7","Internal Sorting"
"constant number of times. This is a great improvement over the simple Binsort","chapter-7","Internal Sorting"
"where the number of bins must be as large as the key range. Note that the example","chapter-7","Internal Sorting"
"uses r = 10 so as to make the bin computations easy to visualize: Records were","chapter-7","Internal Sorting"
"placed into bins based on the value of first the rightmost and then the leftmost","chapter-7","Internal Sorting"
"decimal digits. Any number of bins would have worked. This is an example of a","chapter-7","Internal Sorting"
"Radix Sort, so called because the bin computations are based on the radix or the","chapter-7","Internal Sorting"
"base of the key values. This sorting algorithm can be extended to any number of","chapter-7","Internal Sorting"
"keys in any key range. We simply assign records to bins based on the keys’ digit","chapter-7","Internal Sorting"
"values working from the rightmost digit to the leftmost. If there are k digits, then","chapter-7","Internal Sorting"
"this requires that we assign keys to bins k times.","chapter-7","Internal Sorting"
"As with Mergesort, an efficient implementation of Radix Sort is somewhat dif-","chapter-7","Internal Sorting"
"ficult to achieve. In particular, we would prefer to sort an array of values and avoid","chapter-7","Internal Sorting"
"processing linked lists. If we know how many values will be in each bin, then an","chapter-7","Internal Sorting"
"auxiliary array of size r can be used to hold the bins. For example, if during the","chapter-7","Internal Sorting"
"first pass the 0 bin will receive three records and the 1 bin will receive five records,","chapter-7","Internal Sorting"
"then we could simply reserve the first three array positions for the 0 bin and the","chapter-7","Internal Sorting"
"next five array positions for the 1 bin. Exactly this approach is taken by the Java","chapter-7","Internal Sorting"
"implementation of Figure 7.18. At the end of each pass, the records are copied back","chapter-7","Internal Sorting"
"to the original array.","chapter-7","Internal Sorting"
"Sec. 7.7 Binsort and Radix Sort 249","chapter-7","Internal Sorting"
"The first inner for loop initializes array cnt. The second loop counts the","chapter-7","Internal Sorting"
"number of records to be assigned to each bin. The third loop sets the values in cnt","chapter-7","Internal Sorting"
"to their proper indices within array B. Note that the index stored in cnt[j] is the","chapter-7","Internal Sorting"
"last index for bin j; bins are filled from high index to low index. The fourth loop","chapter-7","Internal Sorting"
"assigns the records to the bins (within array B). The final loop simply copies the","chapter-7","Internal Sorting"
"records back to array A to be ready for the next pass. Variable rtoi stores r","chapter-7","Internal Sorting"
"i","chapter-7","Internal Sorting"
"for","chapter-7","Internal Sorting"
"use in bin computation on the i’th iteration. Figure 7.19 shows how this algorithm","chapter-7","Internal Sorting"
"processes the input shown in Figure 7.17.","chapter-7","Internal Sorting"
"This algorithm requires k passes over the list of n numbers in base r, with","chapter-7","Internal Sorting"
"Θ(n + r) work done at each pass. Thus the total work is Θ(nk + rk). What is","chapter-7","Internal Sorting"
"this in terms of n? Because r is the size of the base, it might be rather small.","chapter-7","Internal Sorting"
"One could use base 2 or 10. Base 26 would be appropriate for sorting character","chapter-7","Internal Sorting"
"strings. For now, we will treat r as a constant value and ignore it for the purpose of","chapter-7","Internal Sorting"
"determining asymptotic complexity. Variable k is related to the key range: It is the","chapter-7","Internal Sorting"
"maximum number of digits that a key may have in base r. In some applications we","chapter-7","Internal Sorting"
"can determine k to be of limited size and so might wish to consider it a constant.","chapter-7","Internal Sorting"
"In this case, Radix Sort is Θ(n) in the best, average, and worst cases, making it the","chapter-7","Internal Sorting"
"sort with best asymptotic complexity that we have studied.","chapter-7","Internal Sorting"
"Is it a reasonable assumption to treat k as a constant? Or is there some rela-","chapter-7","Internal Sorting"
"tionship between k and n? If the key range is limited and duplicate key values are","chapter-7","Internal Sorting"
"common, there might be no relationship between k and n. To make this distinction","chapter-7","Internal Sorting"
"clear, use N to denote the number of distinct key values used by the n records.","chapter-7","Internal Sorting"
"Thus, N ≤ n. Because it takes a minimum of logr N base r digits to represent N","chapter-7","Internal Sorting"
"distinct key values, we know that k ≥ logr N.","chapter-7","Internal Sorting"
"Now, consider the situation in which no keys are duplicated. If there are n","chapter-7","Internal Sorting"
"unique keys (n = N), then it requires n distinct code values to represent them.","chapter-7","Internal Sorting"
"Thus, k ≥ logr n. Because it requires at least Ω(log n) digits (within a constant","chapter-7","Internal Sorting"
"factor) to distinguish between the n distinct keys, k is in Ω(log n). This yields","chapter-7","Internal Sorting"
"an asymptotic complexity of Ω(n log n) for Radix Sort to process n distinct key","chapter-7","Internal Sorting"
"values.","chapter-7","Internal Sorting"
"It is possible that the key range is much larger; logr n bits is merely the best","chapter-7","Internal Sorting"
"case possible for n distinct values. Thus, the logr n estimate for k could be overly","chapter-7","Internal Sorting"
"optimistic. The moral of this analysis is that, for the general case of n distinct key","chapter-7","Internal Sorting"
"values, Radix Sort is at best a Ω(n log n) sorting algorithm.","chapter-7","Internal Sorting"
"Radix Sort can be much improved by making base r be as large as possible.","chapter-7","Internal Sorting"
"Consider the case of an integer key value. Set r = 2i","chapter-7","Internal Sorting"
"for some i. In other words,","chapter-7","Internal Sorting"
"the value of r is related to the number of bits of the key processed on each pass.","chapter-7","Internal Sorting"
"Each time the number of bits is doubled, the number of passes is cut in half. When","chapter-7","Internal Sorting"
"processing an integer key value, setting r = 256 allows the key to be processed one","chapter-7","Internal Sorting"
"byte at a time. Processing a 32-bit key requires only four passes. It is not unrea-","chapter-7","Internal Sorting"
"sonable on most computers to use r = 216 = 64K, resulting in only two passes for","chapter-7","Internal Sorting"
"250 Chap. 7 Internal Sorting","chapter-7","Internal Sorting"
"0 5 6 9 1 2 3 4 10 11 7 8","chapter-7","Internal Sorting"
"0 5 6 9 1 2 3 4 10 11 7 8","chapter-7","Internal Sorting"
"First pass values for Count.","chapter-7","Internal Sorting"
"Count array:","chapter-7","Internal Sorting"
"Index positions for Array B.","chapter-7","Internal Sorting"
"rtoi = 1.","chapter-7","Internal Sorting"
"Second pass values for Count.","chapter-7","Internal Sorting"
"rtoi = 10.","chapter-7","Internal Sorting"
"Count array:","chapter-7","Internal Sorting"
"Index positions for Array B.","chapter-7","Internal Sorting"
"End of Pass 2: Array A.","chapter-7","Internal Sorting"
"0 1 2 3 4 5 6 7 8 9","chapter-7","Internal Sorting"
"0 1 2 3 4 5 6 7 8 9","chapter-7","Internal Sorting"
"Initial Input: Array A","chapter-7","Internal Sorting"
"91 23 84 25 27 97 17 67 28 72","chapter-7","Internal Sorting"
"27 91 1 97 17 23 84 28 72 5 67 25","chapter-7","Internal Sorting"
"0 2 3 4 5 7 7 11 12 12","chapter-7","Internal Sorting"
"1 5","chapter-7","Internal Sorting"
"0 1 2 3 4 5 6 7 8 9","chapter-7","Internal Sorting"
"10 12","chapter-7","Internal Sorting"
"0 1 2 3 4 5 6 7 8 9","chapter-7","Internal Sorting"
"1 5 17 23 25 27 28 67 72 84 91 97","chapter-7","Internal Sorting"
"0 2 1 1 1 2 0 4 1 0","chapter-7","Internal Sorting"
"2 1 0 4 0 0 1 1 1 2","chapter-7","Internal Sorting"
"3 7 7 7 7 8 9","chapter-7","Internal Sorting"
"End of Pass 1: Array A.","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
"Figure 7.19 An example showing function radix applied to the input of Fig-","chapter-7","Internal Sorting"
"ure 7.17. Row 1 shows the initial values within the input array. Row 2 shows the","chapter-7","Internal Sorting"
"values for array cnt after counting the number of records for each bin. Row 3","chapter-7","Internal Sorting"
"shows the index values stored in array cnt. For example, cnt[0] is 0, indicat-","chapter-7","Internal Sorting"
"ing no input values are in bin 0. Cnt[1] is 2, indicating that array B positions 0","chapter-7","Internal Sorting"
"and 1 will hold the values for bin 1. Cnt[2] is 3, indicating that array B position","chapter-7","Internal Sorting"
"2 will hold the (single) value for bin 2. Cnt[7] is 11, indicating that array B","chapter-7","Internal Sorting"
"positions 7 through 10 will hold the four values for bin 7. Row 4 shows the results","chapter-7","Internal Sorting"
"of the first pass of the Radix Sort. Rows 5 through 7 show the equivalent steps for","chapter-7","Internal Sorting"
"the second pass.","chapter-7","Internal Sorting"
"Sec. 7.8 An Empirical Comparison of Sorting Algorithms 251","chapter-7","Internal Sorting"
"a 32-bit key. Of course, this requires a cnt array of size 64K. Performance will","chapter-7","Internal Sorting"
"be good only if the number of records is close to 64K or greater. In other words,","chapter-7","Internal Sorting"
"the number of records must be large compared to the key size for Radix Sort to be","chapter-7","Internal Sorting"
"efficient. In many sorting applications, Radix Sort can be tuned in this way to give","chapter-7","Internal Sorting"
"good performance.","chapter-7","Internal Sorting"
"Radix Sort depends on the ability to make a fixed number of multiway choices","chapter-7","Internal Sorting"
"based on a digit value, as well as random access to the bins. Thus, Radix Sort","chapter-7","Internal Sorting"
"might be difficult to implement for certain key types. For example, if the keys","chapter-7","Internal Sorting"
"are real numbers or arbitrary length strings, then some care will be necessary in","chapter-7","Internal Sorting"
"implementation. In particular, Radix Sort will need to be careful about deciding","chapter-7","Internal Sorting"
"when the “last digit” has been found to distinguish among real numbers, or the last","chapter-7","Internal Sorting"
"character in variable length strings. Implementing the concept of Radix Sort with","chapter-7","Internal Sorting"
"the trie data structure (Section 13.1) is most appropriate for these situations.","chapter-7","Internal Sorting"
"At this point, the perceptive reader might begin to question our earlier assump-","chapter-7","Internal Sorting"
"tion that key comparison takes constant time. If the keys are “normal integer”","chapter-7","Internal Sorting"
"values stored in, say, an integer variable, what is the size of this variable compared","chapter-7","Internal Sorting"
"to n? In fact, it is almost certain that 32 (the number of bits in a standard int vari-","chapter-7","Internal Sorting"
"able) is greater than log n for any practical computation. In this sense, comparison","chapter-7","Internal Sorting"
"of two long integers requires Ω(log n) work.","chapter-7","Internal Sorting"
"Computers normally do arithmetic in units of a particular size, such as a 32-bit","chapter-7","Internal Sorting"
"word. Regardless of the size of the variables, comparisons use this native word","chapter-7","Internal Sorting"
"size and require a constant amount of time since the comparison is implemented in","chapter-7","Internal Sorting"
"hardware. In practice, comparisons of two 32-bit values take constant time, even","chapter-7","Internal Sorting"
"though 32 is much greater than log n. To some extent the truth of the proposition","chapter-7","Internal Sorting"
"that there are constant time operations (such as integer comparison) is in the eye","chapter-7","Internal Sorting"
"of the beholder. At the gate level of computer architecture, individual bits are","chapter-7","Internal Sorting"
"compared. However, constant time comparison for integers is true in practice on","chapter-7","Internal Sorting"
"most computers (they require a fixed number of machine instructions), and we rely","chapter-7","Internal Sorting"
"on such assumptions as the basis for our analyses. In contrast, Radix Sort must do","chapter-7","Internal Sorting"
"several arithmetic calculations on key values (each requiring constant time), where","chapter-7","Internal Sorting"
"the number of such calculations is proportional to the key length. Thus, Radix Sort","chapter-7","Internal Sorting"
"truly does Ω(n log n) work to process n distinct key values.","chapter-7","Internal Sorting"
"7.8 An Empirical Comparison of Sorting Algorithms","chapter-7","Internal Sorting"
"Which sorting algorithm is fastest? Asymptotic complexity analysis lets us distin-","chapter-7","Internal Sorting"
"guish between Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") and Θ(n log n) algorithms, but it does not help distinguish","chapter-7","Internal Sorting"
"between algorithms with the same asymptotic complexity. Nor does asymptotic","chapter-7","Internal Sorting"
"analysis say anything about which algorithm is best for sorting small lists. For","chapter-7","Internal Sorting"
"answers to these questions, we can turn to empirical testing.","chapter-7","Internal Sorting"
"252 Chap. 7 Internal Sorting","chapter-7","Internal Sorting"
"Sort 10 100 1K 10K 100K 1M Up Down","chapter-7","Internal Sorting"
"Insertion .00023 .007 0.66 64.98 7381.0 674420 0.04 129.05","chapter-7","Internal Sorting"
"Bubble .00035 .020 2.25 277.94 27691.0 2820680 70.64 108.69","chapter-7","Internal Sorting"
"Selection .00039 .012 0.69 72.47 7356.0 780000 69.76 69.58","chapter-7","Internal Sorting"
"Shell .00034 .008 0.14 1.99 30.2 554 0.44 0.79","chapter-7","Internal Sorting"
"Shell/O .00034 .008 0.12 1.91 29.0 530 0.36 0.64","chapter-7","Internal Sorting"
"Merge .00050 .010 0.12 1.61 19.3 219 0.83 0.79","chapter-7","Internal Sorting"
"Merge/O .00024 .007 0.10 1.31 17.2 197 0.47 0.66","chapter-7","Internal Sorting"
"Quick .00048 .008 0.11 1.37 15.7 162 0.37 0.40","chapter-7","Internal Sorting"
"Quick/O .00031 .006 0.09 1.14 13.6 143 0.32 0.36","chapter-7","Internal Sorting"
"Heap .00050 .011 0.16 2.08 26.7 391 1.57 1.56","chapter-7","Internal Sorting"
"Heap/O .00033 .007 0.11 1.61 20.8 334 1.01 1.04","chapter-7","Internal Sorting"
"Radix/4 .00838 .081 0.79 7.99 79.9 808 7.97 7.97","chapter-7","Internal Sorting"
"Radix/8 .00799 .044 0.40 3.99 40.0 404 4.00 3.99","chapter-7","Internal Sorting"
"Figure 7.20 Empirical comparison of sorting algorithms run on a 3.4-GHz Intel","chapter-7","Internal Sorting"
"Pentium 4 CPU running Linux. Shellsort, Quicksort, Mergesort, and Heapsort","chapter-7","Internal Sorting"
"each are shown with regular and optimized versions. Radix Sort is shown for 4-","chapter-7","Internal Sorting"
"and 8-bit-per-pass versions. All times shown are milliseconds.","chapter-7","Internal Sorting"
"Figure 7.20 shows timing results for actual implementations of the sorting algo-","chapter-7","Internal Sorting"
"rithms presented in this chapter. The algorithms compared include Insertion Sort,","chapter-7","Internal Sorting"
"Bubble Sort, Selection Sort, Shellsort, Quicksort, Mergesort, Heapsort and Radix","chapter-7","Internal Sorting"
"Sort. Shellsort shows both the basic version from Section 7.3 and another with","chapter-7","Internal Sorting"
"increments based on division by three. Mergesort shows both the basic implemen-","chapter-7","Internal Sorting"
"tation from Section 7.4 and the optimized version (including calls to Insertion Sort","chapter-7","Internal Sorting"
"for lists of length below nine). For Quicksort, two versions are compared: the basic","chapter-7","Internal Sorting"
"implementation from Section 7.5 and an optimized version that does not partition","chapter-7","Internal Sorting"
"sublists below length nine (with Insertion Sort performed at the end). The first","chapter-7","Internal Sorting"
"Heapsort version uses the class definitions from Section 5.5. The second version","chapter-7","Internal Sorting"
"removes all the class definitions and operates directly on the array using inlined","chapter-7","Internal Sorting"
"code for all access functions.","chapter-7","Internal Sorting"
"Except for the rightmost columns, the input to each algorithm is a random array","chapter-7","Internal Sorting"
"of integers. This affects the timing for some of the sorting algorithms. For exam-","chapter-7","Internal Sorting"
"ple, Selection Sort is not being used to best advantage because the record size is","chapter-7","Internal Sorting"
"small, so it does not get the best possible showing. The Radix Sort implementation","chapter-7","Internal Sorting"
"certainly takes advantage of this key range in that it does not look at more digits","chapter-7","Internal Sorting"
"than necessary. On the other hand, it was not optimized to use bit shifting instead","chapter-7","Internal Sorting"
"of division, even though the bases used would permit this.","chapter-7","Internal Sorting"
"The various sorting algorithms are shown for lists of sizes 10, 100, 1000,","chapter-7","Internal Sorting"
"10,000, 100,000, and 1,000,000. The final two columns of each table show the","chapter-7","Internal Sorting"
"performance for the algorithms on inputs of size 10,000 where the numbers are","chapter-7","Internal Sorting"
"in ascending (sorted) and descending (reverse sorted) order, respectively. These","chapter-7","Internal Sorting"
"columns demonstrate best-case performance for some algorithms and worst-case","chapter-7","Internal Sorting"
"Sec. 7.9 Lower Bounds for Sorting 253","chapter-7","Internal Sorting"
"performance for others. They also show that for some algorithms, the order of","chapter-7","Internal Sorting"
"input has little effect.","chapter-7","Internal Sorting"
"These figures show a number of interesting results. As expected, the O(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
")","chapter-7","Internal Sorting"
"sorts are quite poor performers for large arrays. Insertion Sort is by far the best of","chapter-7","Internal Sorting"
"this group, unless the array is already reverse sorted. Shellsort is clearly superior","chapter-7","Internal Sorting"
"to any of these O(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") sorts for lists of even 100 elements. Optimized Quicksort is","chapter-7","Internal Sorting"
"clearly the best overall algorithm for all but lists of 10 elements. Even for small","chapter-7","Internal Sorting"
"arrays, optimized Quicksort performs well because it does one partition step be-","chapter-7","Internal Sorting"
"fore calling Insertion Sort. Compared to the other O(n log n) sorts, unoptimized","chapter-7","Internal Sorting"
"Heapsort is quite slow due to the overhead of the class structure. When all of this","chapter-7","Internal Sorting"
"is stripped away and the algorithm is implemented to manipulate an array directly,","chapter-7","Internal Sorting"
"it is still somewhat slower than mergesort. In general, optimizing the various algo-","chapter-7","Internal Sorting"
"rithms makes a noticeable improvement for larger array sizes.","chapter-7","Internal Sorting"
"Overall, Radix Sort is a surprisingly poor performer. If the code had been tuned","chapter-7","Internal Sorting"
"to use bit shifting of the key value, it would likely improve substantially; but this","chapter-7","Internal Sorting"
"would seriously limit the range of element types that the sort could support.","chapter-7","Internal Sorting"
"7.9 Lower Bounds for Sorting","chapter-7","Internal Sorting"
"This book contains many analyses for algorithms. These analyses generally define","chapter-7","Internal Sorting"
"the upper and lower bounds for algorithms in their worst and average cases. For","chapter-7","Internal Sorting"
"many of the algorithms presented so far, analysis has been easy. This section con-","chapter-7","Internal Sorting"
"siders a more difficult task — an analysis for the cost of a problem as opposed to an","chapter-7","Internal Sorting"
"algorithm. The upper bound for a problem can be defined as the asymptotic cost of","chapter-7","Internal Sorting"
"the fastest known algorithm. The lower bound defines the best possible efficiency","chapter-7","Internal Sorting"
"for any algorithm that solves the problem, including algorithms not yet invented.","chapter-7","Internal Sorting"
"Once the upper and lower bounds for the problem meet, we know that no future","chapter-7","Internal Sorting"
"algorithm can possibly be (asymptotically) more efficient.","chapter-7","Internal Sorting"
"A simple estimate for a problem’s lower bound can be obtained by measuring","chapter-7","Internal Sorting"
"the size of the input that must be read and the output that must be written. Certainly","chapter-7","Internal Sorting"
"no algorithm can be more efficient than the problem’s I/O time. From this we see","chapter-7","Internal Sorting"
"that the sorting problem cannot be solved by any algorithm in less than Ω(n) time","chapter-7","Internal Sorting"
"because it takes at least n steps to read and write the n values to be sorted. Alter-","chapter-7","Internal Sorting"
"natively, any sorting algorithm must at least look at every input vale to recognize","chapter-7","Internal Sorting"
"whether the input values are in sort order. So, based on our current knowledge of","chapter-7","Internal Sorting"
"sorting algorithms and the size of the input, we know that the problem of sorting is","chapter-7","Internal Sorting"
"bounded by Ω(n) and O(n log n).","chapter-7","Internal Sorting"
"Computer scientists have spent much time devising efficient general-purpose","chapter-7","Internal Sorting"
"sorting algorithms, but no one has ever found one that is faster than O(n log n) in","chapter-7","Internal Sorting"
"the worst or average cases. Should we keep searching for a faster sorting algorithm?","chapter-7","Internal Sorting"
"254 Chap. 7 Internal Sorting","chapter-7","Internal Sorting"
"Or can we prove that there is no faster sorting algorithm by finding a tighter lower","chapter-7","Internal Sorting"
"bound?","chapter-7","Internal Sorting"
"This section presents one of the most important and most useful proofs in com-","chapter-7","Internal Sorting"
"puter science: No sorting algorithm based on key comparisons can possibly be","chapter-7","Internal Sorting"
"faster than Ω(n log n) in the worst case. This proof is important for three reasons.","chapter-7","Internal Sorting"
"First, knowing that widely used sorting algorithms are asymptotically optimal is re-","chapter-7","Internal Sorting"
"assuring. In particular, it means that you need not bang your head against the wall","chapter-7","Internal Sorting"
"searching for an O(n) sorting algorithm (or at least not one in any way based on key","chapter-7","Internal Sorting"
"comparisons). Second, this proof is one of the few non-trivial lower-bounds proofs","chapter-7","Internal Sorting"
"that we have for any problem; that is, this proof provides one of the relatively few","chapter-7","Internal Sorting"
"instances where our lower bound is tighter than simply measuring the size of the","chapter-7","Internal Sorting"
"input and output. As such, it provides a useful model for proving lower bounds on","chapter-7","Internal Sorting"
"other problems. Finally, knowing a lower bound for sorting gives us a lower bound","chapter-7","Internal Sorting"
"in turn for other problems whose solution could be used as the basis for a sorting","chapter-7","Internal Sorting"
"algorithm. The process of deriving asymptotic bounds for one problem from the","chapter-7","Internal Sorting"
"asymptotic bounds of another is called a reduction, a concept further explored in","chapter-7","Internal Sorting"
"Chapter 17.","chapter-7","Internal Sorting"
"Except for the Radix Sort and Binsort, all of the sorting algorithms presented","chapter-7","Internal Sorting"
"in this chapter make decisions based on the direct comparison of two key values.","chapter-7","Internal Sorting"
"For example, Insertion Sort sequentially compares the value to be inserted into the","chapter-7","Internal Sorting"
"sorted list until a comparison against the next value in the list fails. In contrast,","chapter-7","Internal Sorting"
"Radix Sort has no direct comparison of key values. All decisions are based on the","chapter-7","Internal Sorting"
"value of specific digits in the key value, so it is possible to take approaches to sorting","chapter-7","Internal Sorting"
"that do not involve key comparisons. Of course, Radix Sort in the end does not","chapter-7","Internal Sorting"
"provide a more efficient sorting algorithm than comparison-based sorting. Thus,","chapter-7","Internal Sorting"
"empirical evidence suggests that comparison-based sorting is a good approach.3","chapter-7","Internal Sorting"
"The proof that any comparison sort requires Ω(n log n) comparisons in the","chapter-7","Internal Sorting"
"worst case is structured as follows. First, comparison-based decisions can be mod-","chapter-7","Internal Sorting"
"eled as the branches in a tree. This means that any sorting algorithm based on com-","chapter-7","Internal Sorting"
"parisons between records can be viewed as a binary tree whose nodes correspond to","chapter-7","Internal Sorting"
"the comparisons, and whose branches correspond to the possible outcomes. Next,","chapter-7","Internal Sorting"
"the minimum number of leaves in the resulting tree is shown to be the factorial of","chapter-7","Internal Sorting"
"n. Finally, the minimum depth of a tree with n! leaves is shown to be in Ω(n log n).","chapter-7","Internal Sorting"
"Before presenting the proof of an Ω(n log n) lower bound for sorting, we first","chapter-7","Internal Sorting"
"must define the concept of a decision tree. A decision tree is a binary tree that can","chapter-7","Internal Sorting"
"model the processing for any algorithm that makes binary decisions. Each (binary)","chapter-7","Internal Sorting"
"decision is represented by a branch in the tree. For the purpose of modeling sorting","chapter-7","Internal Sorting"
"algorithms, we count all comparisons of key values as decisions. If two keys are","chapter-7","Internal Sorting"
"3The truth is stronger than this statement implies. In reality, Radix Sort relies on comparisons as","chapter-7","Internal Sorting"
"well and so can be modeled by the technique used in this section. The result is an Ω(n log n) bound","chapter-7","Internal Sorting"
"in the general case even for algorithms that look like Radix Sort.","chapter-7","Internal Sorting"
"Sec. 7.9 Lower Bounds for Sorting 255","chapter-7","Internal Sorting"
"Yes No","chapter-7","Internal Sorting"
"Yes No Yes No","chapter-7","Internal Sorting"
"Yes No Yes No","chapter-7","Internal Sorting"
"A[1]<A[0]?","chapter-7","Internal Sorting"
"A[2]<A[1]? A[2]<A[1]?","chapter-7","Internal Sorting"
"A[1]<A[0]? A[1]<A[0]?","chapter-7","Internal Sorting"
"(Y<X?)","chapter-7","Internal Sorting"
"(Z<Y?)","chapter-7","Internal Sorting"
"(Z<Y?) (Z<X?)","chapter-7","Internal Sorting"
"XYZ","chapter-7","Internal Sorting"
"ZYX YZX","chapter-7","Internal Sorting"
"XYZ","chapter-7","Internal Sorting"
"XZY","chapter-7","Internal Sorting"
"YXZ","chapter-7","Internal Sorting"
"YZX","chapter-7","Internal Sorting"
"ZXY","chapter-7","Internal Sorting"
"ZYX","chapter-7","Internal Sorting"
"YXZ","chapter-7","Internal Sorting"
"YXZ","chapter-7","Internal Sorting"
"YZX","chapter-7","Internal Sorting"
"ZYX","chapter-7","Internal Sorting"
"XZY","chapter-7","Internal Sorting"
"YZX YXZ","chapter-7","Internal Sorting"
"YZX","chapter-7","Internal Sorting"
"ZYX","chapter-7","Internal Sorting"
"XYZ","chapter-7","Internal Sorting"
"XYZ","chapter-7","Internal Sorting"
"XZY","chapter-7","Internal Sorting"
"ZXY","chapter-7","Internal Sorting"
"XZY","chapter-7","Internal Sorting"
"ZXY","chapter-7","Internal Sorting"
"XYZ","chapter-7","Internal Sorting"
"ZXY XZY","chapter-7","Internal Sorting"
"(Z<X?)","chapter-7","Internal Sorting"
"Figure 7.21 Decision tree for Insertion Sort when processing three values la-","chapter-7","Internal Sorting"
"beled X, Y, and Z, initially stored at positions 0, 1, and 2, respectively, in input","chapter-7","Internal Sorting"
"array A.","chapter-7","Internal Sorting"
"compared and the first is less than the second, then this is modeled as a left branch","chapter-7","Internal Sorting"
"in the decision tree. In the case where the first value is greater than the second, the","chapter-7","Internal Sorting"
"algorithm takes the right branch.","chapter-7","Internal Sorting"
"Figure 7.21 shows the decision tree that models Insertion Sort on three input","chapter-7","Internal Sorting"
"values. The first input value is labeled X, the second Y, and the third Z. They are","chapter-7","Internal Sorting"
"initially stored in positions 0, 1, and 2, respectively, of input array A. Consider the","chapter-7","Internal Sorting"
"possible outputs. Initially, we know nothing about the final positions of the three","chapter-7","Internal Sorting"
"values in the sorted output array. The correct output could be any permutation of","chapter-7","Internal Sorting"
"the input values. For three values, there are n! = 6 permutations. Thus, the root","chapter-7","Internal Sorting"
"node of the decision tree lists all six permutations that might be the eventual result","chapter-7","Internal Sorting"
"of the algorithm.","chapter-7","Internal Sorting"
"When n = 3, the first comparison made by Insertion Sort is between the sec-","chapter-7","Internal Sorting"
"ond item in the input array (Y) and the first item in the array (X). There are two","chapter-7","Internal Sorting"
"possibilities: Either the value of Y is less than that of X, or the value of Y is not","chapter-7","Internal Sorting"
"less than that of X. This decision is modeled by the first branch in the tree. If Y is","chapter-7","Internal Sorting"
"less than X, then the left branch should be taken and Y must appear before X in the","chapter-7","Internal Sorting"
"final output. Only three of the original six permutations have this property, so the","chapter-7","Internal Sorting"
"left child of the root lists the three permutations where Y appears before X: YXZ,","chapter-7","Internal Sorting"
"YZX, and ZYX. Likewise, if Y were not less than X, then the right branch would","chapter-7","Internal Sorting"
"be taken, and only the three permutations in which Y appears after X are possible","chapter-7","Internal Sorting"
"outcomes: XYZ, XZY, and ZXY. These are listed in the right child of the root.","chapter-7","Internal Sorting"
"Let us assume for the moment that Y is less than X and so the left branch is","chapter-7","Internal Sorting"
"taken. In this case, Insertion Sort swaps the two values. At this point the array","chapter-7","Internal Sorting"
"256 Chap. 7 Internal Sorting","chapter-7","Internal Sorting"
"stores YXZ. Thus, in Figure 7.21 the left child of the root shows YXZ above the","chapter-7","Internal Sorting"
"line. Next, the third value in the array is compared against the second (i.e., Z is","chapter-7","Internal Sorting"
"compared with X). Again, there are two possibilities. If Z is less than X, then these","chapter-7","Internal Sorting"
"items should be swapped (the left branch). If Z is not less than X, then Insertion","chapter-7","Internal Sorting"
"Sort is complete (the right branch).","chapter-7","Internal Sorting"
"Note that the right branch reaches a leaf node, and that this leaf node contains","chapter-7","Internal Sorting"
"only one permutation: YXZ. This means that only permutation YXZ can be the","chapter-7","Internal Sorting"
"outcome based on the results of the decisions taken to reach this node. In other","chapter-7","Internal Sorting"
"words, Insertion Sort has “found” the single permutation of the original input that","chapter-7","Internal Sorting"
"yields a sorted list. Likewise, if the second decision resulted in taking the left","chapter-7","Internal Sorting"
"branch, a third comparison, regardless of the outcome, yields nodes in the decision","chapter-7","Internal Sorting"
"tree with only single permutations. Again, Insertion Sort has “found” the correct","chapter-7","Internal Sorting"
"permutation that yields a sorted list.","chapter-7","Internal Sorting"
"Any sorting algorithm based on comparisons can be modeled by a decision tree","chapter-7","Internal Sorting"
"in this way, regardless of the size of the input. Thus, all sorting algorithms can","chapter-7","Internal Sorting"
"be viewed as algorithms to “find” the correct permutation of the input that yields","chapter-7","Internal Sorting"
"a sorted list. Each algorithm based on comparisons can be viewed as proceeding","chapter-7","Internal Sorting"
"by making branches in the tree based on the results of key comparisons, and each","chapter-7","Internal Sorting"
"algorithm can terminate once a node with a single permutation has been reached.","chapter-7","Internal Sorting"
"How is the worst-case cost of an algorithm expressed by the decision tree? The","chapter-7","Internal Sorting"
"decision tree shows the decisions made by an algorithm for all possible inputs of a","chapter-7","Internal Sorting"
"given size. Each path through the tree from the root to a leaf is one possible series","chapter-7","Internal Sorting"
"of decisions taken by the algorithm. The depth of the deepest node represents the","chapter-7","Internal Sorting"
"longest series of decisions required by the algorithm to reach an answer.","chapter-7","Internal Sorting"
"There are many comparison-based sorting algorithms, and each will be mod-","chapter-7","Internal Sorting"
"eled by a different decision tree. Some decision trees might be well-balanced, oth-","chapter-7","Internal Sorting"
"ers might be unbalanced. Some trees will have more nodes than others (those with","chapter-7","Internal Sorting"
"more nodes might be making “unnecessary” comparisons). In fact, a poor sorting","chapter-7","Internal Sorting"
"algorithm might have an arbitrarily large number of nodes in its decision tree, with","chapter-7","Internal Sorting"
"leaves of arbitrary depth. There is no limit to how slow the “worst” possible sort-","chapter-7","Internal Sorting"
"ing algorithm could be. However, we are interested here in knowing what the best","chapter-7","Internal Sorting"
"sorting algorithm could have as its minimum cost in the worst case. In other words,","chapter-7","Internal Sorting"
"we would like to know what is the smallest depth possible for the deepest node in","chapter-7","Internal Sorting"
"the tree for any sorting algorithm.","chapter-7","Internal Sorting"
"The smallest depth of the deepest node will depend on the number of nodes","chapter-7","Internal Sorting"
"in the tree. Clearly we would like to “push up” the nodes in the tree, but there is","chapter-7","Internal Sorting"
"limited room at the top. A tree of height 1 can only store one node (the root); the","chapter-7","Internal Sorting"
"tree of height 2 can store three nodes; the tree of height 3 can store seven nodes,","chapter-7","Internal Sorting"
"and so on.","chapter-7","Internal Sorting"
"Here are some important facts worth remembering:","chapter-7","Internal Sorting"
"• A binary tree of height n can store at most 2","chapter-7","Internal Sorting"
"n − 1 nodes.","chapter-7","Internal Sorting"
"Sec. 7.10 Further Reading 257","chapter-7","Internal Sorting"
"• Equivalently, a tree with n nodes requires at least dlog(n + 1)e levels.","chapter-7","Internal Sorting"
"What is the minimum number of nodes that must be in the decision tree for any","chapter-7","Internal Sorting"
"comparison-based sorting algorithm for n values? Because sorting algorithms are","chapter-7","Internal Sorting"
"in the business of determining which unique permutation of the input corresponds","chapter-7","Internal Sorting"
"to the sorted list, the decision tree for any sorting algorithm must contain at least","chapter-7","Internal Sorting"
"one leaf node for each possible permutation. There are n! permutations for a set of","chapter-7","Internal Sorting"
"n numbers (see Section 2.2).","chapter-7","Internal Sorting"
"Because there are at least n! nodes in the tree, we know that the tree must","chapter-7","Internal Sorting"
"have Ω(log n!) levels. From Stirling’s approximation (Section 2.2), we know log n!","chapter-7","Internal Sorting"
"is in Ω(n log n). The decision tree for any comparison-based sorting algorithm","chapter-7","Internal Sorting"
"must have nodes Ω(n log n) levels deep. Thus, in the worst case, any such sorting","chapter-7","Internal Sorting"
"algorithm must require Ω(n log n) comparisons.","chapter-7","Internal Sorting"
"Any sorting algorithm requiring Ω(n log n) comparisons in the worst case re-","chapter-7","Internal Sorting"
"quires Ω(n log n) running time in the worst case. Because any sorting algorithm","chapter-7","Internal Sorting"
"requires Ω(n log n) running time, the problem of sorting also requires Ω(n log n)","chapter-7","Internal Sorting"
"time. We already know of sorting algorithms with O(n log n) running time, so we","chapter-7","Internal Sorting"
"can conclude that the problem of sorting requires Θ(n log n) time. As a corol-","chapter-7","Internal Sorting"
"lary, we know that no comparison-based sorting algorithm can improve on existing","chapter-7","Internal Sorting"
"Θ(n log n) time sorting algorithms by more than a constant factor.","chapter-7","Internal Sorting"
"7.10 Further Reading","chapter-7","Internal Sorting"
"The definitive reference on sorting is Donald E. Knuth’s Sorting and Searching","chapter-7","Internal Sorting"
"[Knu98]. A wealth of details is covered there, including optimal sorts for small","chapter-7","Internal Sorting"
"size n and special purpose sorting networks. It is a thorough (although somewhat","chapter-7","Internal Sorting"
"dated) treatment on sorting. For an analysis of Quicksort and a thorough survey","chapter-7","Internal Sorting"
"on its optimizations, see Robert Sedgewick’s Quicksort [Sed80]. Sedgewick’s Al-","chapter-7","Internal Sorting"
"gorithms [Sed11] discusses most of the sorting algorithms described here and pays","chapter-7","Internal Sorting"
"special attention to efficient implementation. The optimized Mergesort version of","chapter-7","Internal Sorting"
"Section 7.4 comes from Sedgewick.","chapter-7","Internal Sorting"
"While Ω(n log n) is the theoretical lower bound in the worst case for sorting,","chapter-7","Internal Sorting"
"many times the input is sufficiently well ordered that certain algorithms can take","chapter-7","Internal Sorting"
"advantage of this fact to speed the sorting process. A simple example is Insertion","chapter-7","Internal Sorting"
"Sort’s best-case running time. Sorting algorithms whose running time is based on","chapter-7","Internal Sorting"
"the amount of disorder in the input are called adaptive. For more information on","chapter-7","Internal Sorting"
"adaptive sorting algorithms, see “A Survey of Adaptive Sorting Algorithms” by","chapter-7","Internal Sorting"
"Estivill-Castro and Wood [ECW92].","chapter-7","Internal Sorting"
"7.11 Exercises","chapter-7","Internal Sorting"
"7.1 Using induction, prove that Insertion Sort will always produce a sorted array.","chapter-7","Internal Sorting"
"258 Chap. 7 Internal Sorting","chapter-7","Internal Sorting"
"7.2 Write an Insertion Sort algorithm for integer key values. However, here’s","chapter-7","Internal Sorting"
"the catch: The input is a stack (not an array), and the only variables that","chapter-7","Internal Sorting"
"your algorithm may use are a fixed number of integers and a fixed number of","chapter-7","Internal Sorting"
"stacks. The algorithm should return a stack containing the records in sorted","chapter-7","Internal Sorting"
"order (with the least value being at the top of the stack). Your algorithm","chapter-7","Internal Sorting"
"should be Θ(n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
") in the worst case.","chapter-7","Internal Sorting"
"7.3 The Bubble Sort implementation has the following inner for loop:","chapter-7","Internal Sorting"
"for (int j=n-1; j>i; j--)","chapter-7","Internal Sorting"
"Consider the effect of replacing this with the following statement:","chapter-7","Internal Sorting"
"for (int j=n-1; j>0; j--)","chapter-7","Internal Sorting"
"Would the new implementation work correctly? Would the change affect the","chapter-7","Internal Sorting"
"asymptotic complexity of the algorithm? How would the change affect the","chapter-7","Internal Sorting"
"running time of the algorithm?","chapter-7","Internal Sorting"
"7.4 When implementing Insertion Sort, a binary search could be used to locate","chapter-7","Internal Sorting"
"the position within the first i − 1 elements of the array into which element","chapter-7","Internal Sorting"
"i should be inserted. How would this affect the number of comparisons re-","chapter-7","Internal Sorting"
"quired? How would using such a binary search affect the asymptotic running","chapter-7","Internal Sorting"
"time for Insertion Sort?","chapter-7","Internal Sorting"
"7.5 Figure 7.5 shows the best-case number of swaps for Selection Sort as Θ(n).","chapter-7","Internal Sorting"
"This is because the algorithm does not check to see if the ith record is already","chapter-7","Internal Sorting"
"in the ith position; that is, it might perform unnecessary swaps.","chapter-7","Internal Sorting"
"(a) Modify the algorithm so that it does not make unnecessary swaps.","chapter-7","Internal Sorting"
"(b) What is your prediction regarding whether this modification actually","chapter-7","Internal Sorting"
"improves the running time?","chapter-7","Internal Sorting"
"(c) Write two programs to compare the actual running times of the origi-","chapter-7","Internal Sorting"
"nal Selection Sort and the modified algorithm. Which one is actually","chapter-7","Internal Sorting"
"faster?","chapter-7","Internal Sorting"
"7.6 Recall that a sorting algorithm is said to be stable if the original ordering for","chapter-7","Internal Sorting"
"duplicate keys is preserved. Of the sorting algorithms Insertion Sort, Bub-","chapter-7","Internal Sorting"
"ble Sort, Selection Sort, Shellsort, Mergesort, Quicksort, Heapsort, Binsort,","chapter-7","Internal Sorting"
"and Radix Sort, which of these are stable, and which are not? For each one,","chapter-7","Internal Sorting"
"describe either why it is or is not stable. If a minor change to the implemen-","chapter-7","Internal Sorting"
"tation would make it stable, describe the change.","chapter-7","Internal Sorting"
"7.7 Recall that a sorting algorithm is said to be stable if the original ordering for","chapter-7","Internal Sorting"
"duplicate keys is preserved. We can make any algorithm stable if we alter","chapter-7","Internal Sorting"
"the input keys so that (potentially) duplicate key values are made unique in","chapter-7","Internal Sorting"
"a way that the first occurrence of the original duplicate value is less than the","chapter-7","Internal Sorting"
"second occurrence, which in turn is less than the third, and so on. In the worst","chapter-7","Internal Sorting"
"case, it is possible that all n input records have the same key value. Give an","chapter-7","Internal Sorting"
"Sec. 7.11 Exercises 259","chapter-7","Internal Sorting"
"algorithm to modify the key values such that every modified key value is","chapter-7","Internal Sorting"
"unique, the resulting key values give the same sort order as the original keys,","chapter-7","Internal Sorting"
"the result is stable (in that the duplicate original key values remain in their","chapter-7","Internal Sorting"
"original order), and the process of altering the keys is done in linear time","chapter-7","Internal Sorting"
"using only a constant amount of additional space.","chapter-7","Internal Sorting"
"7.8 The discussion of Quicksort in Section 7.5 described using a stack instead of","chapter-7","Internal Sorting"
"recursion to reduce the number of function calls made.","chapter-7","Internal Sorting"
"(a) How deep can the stack get in the worst case?","chapter-7","Internal Sorting"
"(b) Quicksort makes two recursive calls. The algorithm could be changed","chapter-7","Internal Sorting"
"to make these two calls in a specific order. In what order should the","chapter-7","Internal Sorting"
"two calls be made, and how does this affect how deep the stack can","chapter-7","Internal Sorting"
"become?","chapter-7","Internal Sorting"
"7.9 Give a permutation for the values 0 through 7 that will cause Quicksort (as","chapter-7","Internal Sorting"
"implemented in Section 7.5) to have its worst case behavior.","chapter-7","Internal Sorting"
"7.10 Assume L is an array, L.length() returns the number of records in the","chapter-7","Internal Sorting"
"array, and qsort(L, i, j) sorts the records of L from i to j (leaving","chapter-7","Internal Sorting"
"the records sorted in L) using the Quicksort algorithm. What is the average-","chapter-7","Internal Sorting"
"case time complexity for each of the following code fragments?","chapter-7","Internal Sorting"
"(a) for (i=0; i<L.length; i++)","chapter-7","Internal Sorting"
"qsort(L, 0, i);","chapter-7","Internal Sorting"
"(b) for (i=0; i<L.length; i++)","chapter-7","Internal Sorting"
"qsort(L, 0, L.length-1);","chapter-7","Internal Sorting"
"7.11 Modify Quicksort to find the smallest k values in an array of records. Your","chapter-7","Internal Sorting"
"output should be the array modified so that the k smallest values are sorted","chapter-7","Internal Sorting"
"in the first k positions of the array. Your algorithm should do the minimum","chapter-7","Internal Sorting"
"amount of work necessary, that is, no more of the array than necessary should","chapter-7","Internal Sorting"
"be sorted.","chapter-7","Internal Sorting"
"7.12 Modify Quicksort to sort a sequence of variable-length strings stored one","chapter-7","Internal Sorting"
"after the other in a character array, with a second array (storing pointers to","chapter-7","Internal Sorting"
"strings) used to index the strings. Your function should modify the index","chapter-7","Internal Sorting"
"array so that the first pointer points to the beginning of the lowest valued","chapter-7","Internal Sorting"
"string, and so on.","chapter-7","Internal Sorting"
"7.13 Graph f1(n) = n log n, f2(n) = n","chapter-7","Internal Sorting"
"1.5","chapter-7","Internal Sorting"
", and f3(n) = n","chapter-7","Internal Sorting"
"2","chapter-7","Internal Sorting"
"in the range 1 ≤ n ≤","chapter-7","Internal Sorting"
"1000 to visually compare their growth rates. Typically, the constant factor","chapter-7","Internal Sorting"
"in the running-time expression for an implementation of Insertion Sort will","chapter-7","Internal Sorting"
"be less than the constant factors for Shellsort or Quicksort. How many times","chapter-7","Internal Sorting"
"greater can the constant factor be for Shellsort to be faster than Insertion Sort","chapter-7","Internal Sorting"
"when n = 1000? How many times greater can the constant factor be for","chapter-7","Internal Sorting"
"Quicksort to be faster than Insertion Sort when n = 1000?","chapter-7","Internal Sorting"
"260 Chap. 7 Internal Sorting","chapter-7","Internal Sorting"
"7.14 Imagine that there exists an algorithm SPLITk that can split a list L of n","chapter-7","Internal Sorting"
"elements into k sublists, each containing one or more elements, such that","chapter-7","Internal Sorting"
"sublist i contains only elements whose values are less than all elements in","chapter-7","Internal Sorting"
"sublist j for i < j <= k. If n < k, then k−n sublists are empty, and the rest","chapter-7","Internal Sorting"
"are of length 1. Assume that SPLITk has time complexity O(length of L).","chapter-7","Internal Sorting"
"Furthermore, assume that the k lists can be concatenated again in constant","chapter-7","Internal Sorting"
"time. Consider the following algorithm:","chapter-7","Internal Sorting"
"List SORTk(List L) {","chapter-7","Internal Sorting"
"List sub[k]; // To hold the sublists","chapter-7","Internal Sorting"
"if (L.length() > 1) {","chapter-7","Internal Sorting"
"SPLITk(L, sub); // SPLITk places sublists into sub","chapter-7","Internal Sorting"
"for (i=0; i<k; i++)","chapter-7","Internal Sorting"
"sub[i] = SORTk(sub[i]); // Sort each sublist","chapter-7","Internal Sorting"
"L = concatenation of k sublists in sub;","chapter-7","Internal Sorting"
"return L;","chapter-7","Internal Sorting"
"}","chapter-7","Internal Sorting"
"}","chapter-7","Internal Sorting"
"(a) What is the worst-case asymptotic running time for SORTk? Why?","chapter-7","Internal Sorting"
"(b) What is the average-case asymptotic running time of SORTk? Why?","chapter-7","Internal Sorting"
"7.15 Here is a variation on sorting. The problem is to sort a collection of n nuts","chapter-7","Internal Sorting"
"and n bolts by size. It is assumed that for each bolt in the collection, there","chapter-7","Internal Sorting"
"is a corresponding nut of the same size, but initially we do not know which","chapter-7","Internal Sorting"
"nut goes with which bolt. The differences in size between two nuts or two","chapter-7","Internal Sorting"
"bolts can be too small to see by eye, so you cannot rely on comparing the","chapter-7","Internal Sorting"
"sizes of two nuts or two bolts directly. Instead, you can only compare the","chapter-7","Internal Sorting"
"sizes of a nut and a bolt by attempting to screw one into the other (assume","chapter-7","Internal Sorting"
"this comparison to be a constant time operation). This operation tells you","chapter-7","Internal Sorting"
"that either the nut is bigger than the bolt, the bolt is bigger than the nut, or","chapter-7","Internal Sorting"
"they are the same size. What is the minimum number of comparisons needed","chapter-7","Internal Sorting"
"to sort the nuts and bolts in the worst case?","chapter-7","Internal Sorting"
"7.16 (a) Devise an algorithm to sort three numbers. It should make as few com-","chapter-7","Internal Sorting"
"parisons as possible. How many comparisons and swaps are required","chapter-7","Internal Sorting"
"in the best, worst, and average cases?","chapter-7","Internal Sorting"
"(b) Devise an algorithm to sort five numbers. It should make as few com-","chapter-7","Internal Sorting"
"parisons as possible. How many comparisons and swaps are required","chapter-7","Internal Sorting"
"in the best, worst, and average cases?","chapter-7","Internal Sorting"
"(c) Devise an algorithm to sort eight numbers. It should make as few com-","chapter-7","Internal Sorting"
"parisons as possible. How many comparisons and swaps are required","chapter-7","Internal Sorting"
"in the best, worst, and average cases?","chapter-7","Internal Sorting"
"7.17 Devise an efficient algorithm to sort a set of numbers with values in the range","chapter-7","Internal Sorting"
"0 to 30,000. There are no duplicates. Keep memory requirements to a mini-","chapter-7","Internal Sorting"
"mum.","chapter-7","Internal Sorting"
"Sec. 7.12 Projects 261","chapter-7","Internal Sorting"
"7.18 Which of the following operations are best implemented by first sorting the","chapter-7","Internal Sorting"
"list of numbers? For each operation, briefly describe an algorithm to imple-","chapter-7","Internal Sorting"
"ment it, and state the algorithm’s asymptotic complexity.","chapter-7","Internal Sorting"
"(a) Find the minimum value.","chapter-7","Internal Sorting"
"(b) Find the maximum value.","chapter-7","Internal Sorting"
"(c) Compute the arithmetic mean.","chapter-7","Internal Sorting"
"(d) Find the median (i.e., the middle value).","chapter-7","Internal Sorting"
"(e) Find the mode (i.e., the value that appears the most times).","chapter-7","Internal Sorting"
"7.19 Consider a recursive Mergesort implementation that calls Insertion Sort on","chapter-7","Internal Sorting"
"sublists smaller than some threshold. If there are n calls to Mergesort, how","chapter-7","Internal Sorting"
"many calls will there be to Insertion Sort? Why?","chapter-7","Internal Sorting"
"7.20 Implement Mergesort for the case where the input is a linked list.","chapter-7","Internal Sorting"
"7.21 Counting sort (assuming the input key values are integers in the range 0 to","chapter-7","Internal Sorting"
"m − 1) works by counting the number of records with each key value in the","chapter-7","Internal Sorting"
"first pass, and then uses this information to place the records in order in a","chapter-7","Internal Sorting"
"second pass. Write an implementation of counting sort (see the implementa-","chapter-7","Internal Sorting"
"tion of radix sort for some ideas). What can we say about the relative values","chapter-7","Internal Sorting"
"of m and n for this to be effective? If m < n, what is the running time of","chapter-7","Internal Sorting"
"this algorithm?","chapter-7","Internal Sorting"
"7.22 Use an argument similar to that given in Section 7.9 to prove that log n is a","chapter-7","Internal Sorting"
"worst-case lower bound for the problem of searching for a given value in a","chapter-7","Internal Sorting"
"sorted array containing n elements.","chapter-7","Internal Sorting"
"7.23 A simpler way to do the Quicksort partition step is to set index split","chapter-7","Internal Sorting"
"to the position of the first value greater than the pivot. Then from posi-","chapter-7","Internal Sorting"
"tion split+1 have another index curr move to the right until it finds a","chapter-7","Internal Sorting"
"value less than a pivot. Swap the values at split and next, and incre-","chapter-7","Internal Sorting"
"ment split. Continue in this way, swapping the smaller values to the left","chapter-7","Internal Sorting"
"side. When curr reaches the end of the subarray, split will be at the split","chapter-7","Internal Sorting"
"point between the two partitions. Unfortunately, this approach requires more","chapter-7","Internal Sorting"
"swaps than does the version presented in Section 7.5, resulting in a slower","chapter-7","Internal Sorting"
"implementation. Give an example and explain why.","chapter-7","Internal Sorting"
"7.12 Projects","chapter-7","Internal Sorting"
"7.1 One possible improvement for Bubble Sort would be to add a flag variable","chapter-7","Internal Sorting"
"and a test that determines if an exchange was made during the current iter-","chapter-7","Internal Sorting"
"ation. If no exchange was made, then the list is sorted and so the algorithm","chapter-7","Internal Sorting"
"can stop early. This makes the best case performance become O(n) (because","chapter-7","Internal Sorting"
"if the list is already sorted, then no iterations will take place on the first pass,","chapter-7","Internal Sorting"
"and the sort will stop right there).","chapter-7","Internal Sorting"
"262 Chap. 7 Internal Sorting","chapter-7","Internal Sorting"
"Modify the Bubble Sort implementation to add this flag and test. Compare","chapter-7","Internal Sorting"
"the modified implementation on a range of inputs to determine if it does or","chapter-7","Internal Sorting"
"does not improve performance in practice.","chapter-7","Internal Sorting"
"7.2 Double Insertion Sort is a variation on Insertion Sort that works from the","chapter-7","Internal Sorting"
"middle of the array out. At each iteration, some middle portion of the array","chapter-7","Internal Sorting"
"is sorted. On the next iteration, take the two adjacent elements to the sorted","chapter-7","Internal Sorting"
"portion of the array. If they are out of order with respect to each other, than","chapter-7","Internal Sorting"
"swap them. Now, push the left element toward the right in the array so long","chapter-7","Internal Sorting"
"as it is greater than the element to its right. And push the right element","chapter-7","Internal Sorting"
"toward the left in the array so long as it is less than the element to its left.","chapter-7","Internal Sorting"
"The algorithm begins by processing the middle two elements of the array if","chapter-7","Internal Sorting"
"the array is even. If the array is odd, then skip processing the middle item","chapter-7","Internal Sorting"
"and begin with processing the elements to its immediate left and right.","chapter-7","Internal Sorting"
"First, explain what the cost of Double Insertion Sort will be in comparison to","chapter-7","Internal Sorting"
"standard Insertion sort, and why. (Note that the two elements being processed","chapter-7","Internal Sorting"
"in the current iteration, once initially swapped to be sorted with with respect","chapter-7","Internal Sorting"
"to each other, cannot cross as they are pushed into sorted position.) Then, im-","chapter-7","Internal Sorting"
"plement Double Insertion Sort, being careful to properly handle both when","chapter-7","Internal Sorting"
"the array is odd and when it is even. Compare its running time in practice","chapter-7","Internal Sorting"
"against standard Insertion Sort. Finally, explain how this speedup might af-","chapter-7","Internal Sorting"
"fect the threshold level and running time for a Quicksort implementation.","chapter-7","Internal Sorting"
"7.3 Perform a study of Shellsort, using different increments. Compare the ver-","chapter-7","Internal Sorting"
"sion shown in Section 7.3, where each increment is half the previous one,","chapter-7","Internal Sorting"
"with others. In particular, try implementing “division by 3” where the incre-","chapter-7","Internal Sorting"
"ments on a list of length n will be n/3, n/9, etc. Do other increment schemes","chapter-7","Internal Sorting"
"work as well?","chapter-7","Internal Sorting"
"7.4 The implementation for Mergesort given in Section 7.4 takes an array as in-","chapter-7","Internal Sorting"
"put and sorts that array. At the beginning of Section 7.4 there is a simple","chapter-7","Internal Sorting"
"pseudocode implementation for sorting a linked list using Mergesort. Im-","chapter-7","Internal Sorting"
"plement both a linked list-based version of Mergesort and the array-based","chapter-7","Internal Sorting"
"version of Mergesort, and compare their running times.","chapter-7","Internal Sorting"
"7.5 Starting with the Java code for Quicksort given in this chapter, write a series","chapter-7","Internal Sorting"
"of Quicksort implementations to test the following optimizations on a wide","chapter-7","Internal Sorting"
"range of input data sizes. Try these optimizations in various combinations to","chapter-7","Internal Sorting"
"try and develop the fastest possible Quicksort implementation that you can.","chapter-7","Internal Sorting"
"(a) Look at more values when selecting a pivot.","chapter-7","Internal Sorting"
"(b) Do not make a recursive call to qsort when the list size falls below a","chapter-7","Internal Sorting"
"given threshold, and use Insertion Sort to complete the sorting process.","chapter-7","Internal Sorting"
"Test various values for the threshold size.","chapter-7","Internal Sorting"
"(c) Eliminate recursion by using a stack and inline functions.","chapter-7","Internal Sorting"
"Sec. 7.12 Projects 263","chapter-7","Internal Sorting"
"7.6 It has been proposed that Heapsort can be optimized by altering the heap’s","chapter-7","Internal Sorting"
"siftdown function. Call the value being sifted down X. Siftdown does two","chapter-7","Internal Sorting"
"comparisons per level: First the children of X are compared, then the winner","chapter-7","Internal Sorting"
"is compared to X. If X is too small, it is swapped with its larger child and the","chapter-7","Internal Sorting"
"process repeated. The proposed optimization dispenses with the test against","chapter-7","Internal Sorting"
"X. Instead, the larger child automatically replaces X, until X reaches the","chapter-7","Internal Sorting"
"bottom level of the heap. At this point, X might be too large to remain in","chapter-7","Internal Sorting"
"that position. This is corrected by repeatedly comparing X with its parent","chapter-7","Internal Sorting"
"and swapping as necessary to “bubble” it up to its proper level. The claim","chapter-7","Internal Sorting"
"is that this process will save a number of comparisons because most nodes","chapter-7","Internal Sorting"
"when sifted down end up near the bottom of the tree anyway. Implement both","chapter-7","Internal Sorting"
"versions of siftdown, and do an empirical study to compare their running","chapter-7","Internal Sorting"
"times.","chapter-7","Internal Sorting"
"7.7 Radix Sort is typically implemented to support only a radix that is a power","chapter-7","Internal Sorting"
"of two. This allows for a direct conversion from the radix to some number","chapter-7","Internal Sorting"
"of bits in an integer key value. For example, if the radix is 16, then a 32-bit","chapter-7","Internal Sorting"
"key will be processed in 8 steps of 4 bits each. This can lead to a more effi-","chapter-7","Internal Sorting"
"cient implementation because bit shifting can replace the division operations","chapter-7","Internal Sorting"
"shown in the implementation of Section 7.7. Re-implement the Radix Sort","chapter-7","Internal Sorting"
"code given in Section 7.7 to use bit shifting in place of division. Compare","chapter-7","Internal Sorting"
"the running time of the old and new Radix Sort implementations.","chapter-7","Internal Sorting"
"7.8 Write your own collection of sorting programs to implement the algorithms","chapter-7","Internal Sorting"
"described in this chapter, and compare their running times. Be sure to im-","chapter-7","Internal Sorting"
"plement optimized versions, trying to make each program as fast as possible.","chapter-7","Internal Sorting"
"Do you get the same relative timings as shown in Figure 7.20? If not, why do","chapter-7","Internal Sorting"
"you think this happened? How do your results compare with those of your","chapter-7","Internal Sorting"
"classmates? What does this say about the difficulty of doing empirical timing","chapter-7","Internal Sorting"
"studies?","chapter-7","Internal Sorting"
"Earlier chapters presented basic data structures and algorithms that operate on data","chapter-8","File Processing and External Sorting"
"stored in main memory. Some applications require that large amounts of informa-","chapter-8","File Processing and External Sorting"
"tion be stored and processed — so much information that it cannot all fit into main","chapter-8","File Processing and External Sorting"
"memory. In that case, the information must reside on disk and be brought into main","chapter-8","File Processing and External Sorting"
"memory selectively for processing.","chapter-8","File Processing and External Sorting"
"You probably already realize that main memory access is much faster than ac-","chapter-8","File Processing and External Sorting"
"cess to data stored on disk or other storage devices. The relative difference in access","chapter-8","File Processing and External Sorting"
"times is so great that efficient disk-based programs require a different approach to","chapter-8","File Processing and External Sorting"
"algorithm design than most programmers are used to. As a result, many program-","chapter-8","File Processing and External Sorting"
"mers do a poor job when it comes to file processing applications.","chapter-8","File Processing and External Sorting"
"This chapter presents the fundamental issues relating to the design of algo-","chapter-8","File Processing and External Sorting"
"rithms and data structures for disk-based applications.1 We begin with a descrip-","chapter-8","File Processing and External Sorting"
"tion of the significant differences between primary memory and secondary storage.","chapter-8","File Processing and External Sorting"
"Section 8.2 discusses the physical aspects of disk drives. Section 8.3 presents ba-","chapter-8","File Processing and External Sorting"
"sic methods for managing buffer pools. Section 8.4 discusses the Java model for","chapter-8","File Processing and External Sorting"
"random access to data stored on disk. Section 8.5 discusses the basic principles for","chapter-8","File Processing and External Sorting"
"sorting collections of records too large to fit in main memory.","chapter-8","File Processing and External Sorting"
"8.1 Primary versus Secondary Storage","chapter-8","File Processing and External Sorting"
"Computer storage devices are typically classified into primary or main memory","chapter-8","File Processing and External Sorting"
"and secondary or peripheral storage. Primary memory usually refers to Random","chapter-8","File Processing and External Sorting"
"1 Computer technology changes rapidly. I provide examples of disk drive specifications and other","chapter-8","File Processing and External Sorting"
"hardware performance numbers that are reasonably up to date as of the time when the book was","chapter-8","File Processing and External Sorting"
"written. When you read it, the numbers might seem out of date. However, the basic principles do not","chapter-8","File Processing and External Sorting"
"change. The approximate ratios for time, space, and cost between memory and disk have remained","chapter-8","File Processing and External Sorting"
"surprisingly steady for over 20 years.","chapter-8","File Processing and External Sorting"
"265","chapter-8","File Processing and External Sorting"
"266 Chap. 8 File Processing and External Sorting","chapter-8","File Processing and External Sorting"
"Medium 1996 1997 2000 2004 2006 2008 2011","chapter-8","File Processing and External Sorting"
"RAM $45.00 7.00 1.500 0.3500 0.1500 0.0339 0.0138","chapter-8","File Processing and External Sorting"
"Disk 0.25 0.10 0.010 0.0010 0.0005 0.0001 0.0001","chapter-8","File Processing and External Sorting"
"USB drive – – – 0.1000 0.0900 0.0029 0.0018","chapter-8","File Processing and External Sorting"
"Floppy 0.50 0.36 0.250 0.2500 – – –","chapter-8","File Processing and External Sorting"
"Tape 0.03 0.01 0.001 0.0003 – – –","chapter-8","File Processing and External Sorting"
"Solid State – – – – – – 0.0021","chapter-8","File Processing and External Sorting"
"Figure 8.1 Price comparison table for some writable electronic data storage","chapter-8","File Processing and External Sorting"
"media in common use. Prices are in US Dollars/MB.","chapter-8","File Processing and External Sorting"
"Access Memory (RAM), while secondary storage refers to devices such as hard","chapter-8","File Processing and External Sorting"
"disk drives, solid state drives, removable “USB” drives, CDs, and DVDs. Primary","chapter-8","File Processing and External Sorting"
"memory also includes registers, cache, and video memories, but we will ignore","chapter-8","File Processing and External Sorting"
"them for this discussion because their existence does not affect the principal differ-","chapter-8","File Processing and External Sorting"
"ences between primary and secondary memory.","chapter-8","File Processing and External Sorting"
"Along with a faster CPU, every new model of computer seems to come with","chapter-8","File Processing and External Sorting"
"more main memory. As memory size continues to increase, is it possible that rel-","chapter-8","File Processing and External Sorting"
"atively slow disk storage will be unnecessary? Probably not, because the desire to","chapter-8","File Processing and External Sorting"
"store and process larger files grows at least as fast as main memory size. Prices","chapter-8","File Processing and External Sorting"
"for both main memory and peripheral storage devices have dropped dramatically","chapter-8","File Processing and External Sorting"
"in recent years, as demonstrated by Figure 8.1. However, the cost per unit of disk","chapter-8","File Processing and External Sorting"
"drive storage is about two orders of magnitude less than RAM and has been for","chapter-8","File Processing and External Sorting"
"many years.","chapter-8","File Processing and External Sorting"
"There is now a wide range of removable media available for transferring data","chapter-8","File Processing and External Sorting"
"or storing data offline in relative safety. These include floppy disks (now largely","chapter-8","File Processing and External Sorting"
"obsolete), writable CDs and DVDs, “flash” drives, and magnetic tape. Optical stor-","chapter-8","File Processing and External Sorting"
"age such as CDs and DVDs costs roughly half the price of hard disk drive space","chapter-8","File Processing and External Sorting"
"per megabyte, and have become practical for use as backup storage within the past","chapter-8","File Processing and External Sorting"
"few years. Tape used to be much cheaper than other media, and was the preferred","chapter-8","File Processing and External Sorting"
"means of backup, but are not so popular now as other media have decreased in","chapter-8","File Processing and External Sorting"
"price. “Flash” drives cost the most per megabyte, but due to their storage capac-","chapter-8","File Processing and External Sorting"
"ity and flexibility, quickly replaced floppy disks as the primary storage device for","chapter-8","File Processing and External Sorting"
"transferring data between computer when direct network transfer is not available.","chapter-8","File Processing and External Sorting"
"Secondary storage devices have at least two other advantages over RAM mem-","chapter-8","File Processing and External Sorting"
"ory. Perhaps most importantly, disk, “flash,” and optical media are persistent,","chapter-8","File Processing and External Sorting"
"meaning that they are not erased from the media when the power is turned off. In","chapter-8","File Processing and External Sorting"
"contrast, RAM used for main memory is usually volatile — all information is lost","chapter-8","File Processing and External Sorting"
"with the power. A second advantage is that CDs and “USB” drives can easily be","chapter-8","File Processing and External Sorting"
"transferred between computers. This provides a convenient way to take information","chapter-8","File Processing and External Sorting"
"from one computer to another.","chapter-8","File Processing and External Sorting"
"Sec. 8.1 Primary versus Secondary Storage 267","chapter-8","File Processing and External Sorting"
"In exchange for reduced storage costs, persistence, and portability, secondary","chapter-8","File Processing and External Sorting"
"storage devices pay a penalty in terms of increased access time. While not all ac-","chapter-8","File Processing and External Sorting"
"cesses to disk take the same amount of time (more on this later), the typical time","chapter-8","File Processing and External Sorting"
"required to access a byte of storage from a disk drive in 2011 is around 9 ms (i.e.,","chapter-8","File Processing and External Sorting"
"9 thousandths of a second). This might not seem slow, but compared to the time","chapter-8","File Processing and External Sorting"
"required to access a byte from main memory, this is fantastically slow. Typical","chapter-8","File Processing and External Sorting"
"access time from standard personal computer RAM in 2011 is about 5-10 nanosec-","chapter-8","File Processing and External Sorting"
"onds (i.e., 5-10 billionths of a second). Thus, the time to access a byte of data from","chapter-8","File Processing and External Sorting"
"a disk drive is about six orders of magnitude greater than that required to access a","chapter-8","File Processing and External Sorting"
"byte from main memory. While disk drive and RAM access times are both decreas-","chapter-8","File Processing and External Sorting"
"ing, they have done so at roughly the same rate. The relative speeds have remained","chapter-8","File Processing and External Sorting"
"the same for over several decades, in that the difference in access time between","chapter-8","File Processing and External Sorting"
"RAM and a disk drive has remained in the range between a factor of 100,000 and","chapter-8","File Processing and External Sorting"
"1,000,000.","chapter-8","File Processing and External Sorting"
"To gain some intuition for the significance of this speed difference, consider the","chapter-8","File Processing and External Sorting"
"time that it might take for you to look up the entry for disk drives in the index of","chapter-8","File Processing and External Sorting"
"this book, and then turn to the appropriate page. Call this your “primary memory”","chapter-8","File Processing and External Sorting"
"access time. If it takes you about 20 seconds to perform this access, then an access","chapter-8","File Processing and External Sorting"
"taking 500,000 times longer would require months.","chapter-8","File Processing and External Sorting"
"It is interesting to note that while processing speeds have increased dramat-","chapter-8","File Processing and External Sorting"
"ically, and hardware prices have dropped dramatically, disk and memory access","chapter-8","File Processing and External Sorting"
"times have improved by less than an order of magnitude over the past 15 years.","chapter-8","File Processing and External Sorting"
"However, the situation is really much better than that modest speedup would sug-","chapter-8","File Processing and External Sorting"
"gest. During the same time period, the size of both disk and main memory has","chapter-8","File Processing and External Sorting"
"increased by over three orders of magnitude. Thus, the access times have actually","chapter-8","File Processing and External Sorting"
"decreased in the face of a massive increase in the density of these storage devices.","chapter-8","File Processing and External Sorting"
"Due to the relatively slow access time for data on disk as compared to main","chapter-8","File Processing and External Sorting"
"memory, great care is required to create efficient applications that process disk-","chapter-8","File Processing and External Sorting"
"based information. The million-to-one ratio of disk access time versus main mem-","chapter-8","File Processing and External Sorting"
"ory access time makes the following rule of paramount importance when designing","chapter-8","File Processing and External Sorting"
"disk-based applications:","chapter-8","File Processing and External Sorting"
"Minimize the number of disk accesses!","chapter-8","File Processing and External Sorting"
"There are generally two approaches to minimizing disk accesses. The first is","chapter-8","File Processing and External Sorting"
"to arrange information so that if you do access data from secondary memory, you","chapter-8","File Processing and External Sorting"
"will get what you need in as few accesses as possible, and preferably on the first","chapter-8","File Processing and External Sorting"
"access. File structure is the term used for a data structure that organizes data","chapter-8","File Processing and External Sorting"
"stored in secondary memory. File structures should be organized so as to minimize","chapter-8","File Processing and External Sorting"
"the required number of disk accesses. The other way to minimize disk accesses is to","chapter-8","File Processing and External Sorting"
"save information previously retrieved (or retrieve additional data with each access","chapter-8","File Processing and External Sorting"
"at little additional cost) that can be used to minimize the need for future accesses.","chapter-8","File Processing and External Sorting"
"268 Chap. 8 File Processing and External Sorting","chapter-8","File Processing and External Sorting"
"This requires the ability to guess accurately what information will be needed later","chapter-8","File Processing and External Sorting"
"and store it in primary memory now. This is referred to as caching.","chapter-8","File Processing and External Sorting"
"8.2 Disk Drives","chapter-8","File Processing and External Sorting"
"A Java programmer views a random access file stored on disk as a contiguous","chapter-8","File Processing and External Sorting"
"series of bytes, with those bytes possibly combining to form data records. This","chapter-8","File Processing and External Sorting"
"is called the logical file. The physical file actually stored on disk is usually not","chapter-8","File Processing and External Sorting"
"a contiguous series of bytes. It could well be in pieces spread all over the disk.","chapter-8","File Processing and External Sorting"
"The file manager, a part of the operating system, is responsible for taking requests","chapter-8","File Processing and External Sorting"
"for data from a logical file and mapping those requests to the physical location","chapter-8","File Processing and External Sorting"
"of the data on disk. Likewise, when writing to a particular logical byte position","chapter-8","File Processing and External Sorting"
"with respect to the beginning of the file, this position must be converted by the","chapter-8","File Processing and External Sorting"
"file manager into the corresponding physical location on the disk. To gain some","chapter-8","File Processing and External Sorting"
"appreciation for the the approximate time costs for these operations, you need to","chapter-8","File Processing and External Sorting"
"understand the physical structure and basic workings of a disk drive.","chapter-8","File Processing and External Sorting"
"Disk drives are often referred to as direct access storage devices. This means","chapter-8","File Processing and External Sorting"
"that it takes roughly equal time to access any record in the file. This is in contrast","chapter-8","File Processing and External Sorting"
"to sequential access storage devices such as tape drives, which require the tape","chapter-8","File Processing and External Sorting"
"reader to process data from the beginning of the tape until the desired position has","chapter-8","File Processing and External Sorting"
"been reached. As you will see, the disk drive is only approximately direct access:","chapter-8","File Processing and External Sorting"
"At any given time, some records are more quickly accessible than others.","chapter-8","File Processing and External Sorting"
"8.2.1 Disk Drive Architecture","chapter-8","File Processing and External Sorting"
"A hard disk drive is composed of one or more round platters, stacked one on top of","chapter-8","File Processing and External Sorting"
"another and attached to a central spindle. Platters spin continuously at a constant","chapter-8","File Processing and External Sorting"
"rate. Each usable surface of each platter is assigned a read/write head or I/O","chapter-8","File Processing and External Sorting"
"head through which data are read or written, somewhat like the arrangement of","chapter-8","File Processing and External Sorting"
"a phonograph player’s arm “reading” sound from a phonograph record. Unlike a","chapter-8","File Processing and External Sorting"
"phonograph needle, the disk read/write head does not actually touch the surface of","chapter-8","File Processing and External Sorting"
"a hard disk. Instead, it remains slightly above the surface, and any contact during","chapter-8","File Processing and External Sorting"
"normal operation would damage the disk. This distance is very small, much smaller","chapter-8","File Processing and External Sorting"
"than the height of a dust particle. It can be likened to a 5000-kilometer airplane trip","chapter-8","File Processing and External Sorting"
"across the United States, with the plane flying at a height of one meter!","chapter-8","File Processing and External Sorting"
"A hard disk drive typically has several platters and several read/write heads, as","chapter-8","File Processing and External Sorting"
"shown in Figure 8.2(a). Each head is attached to an arm, which connects to the","chapter-8","File Processing and External Sorting"
"boom.","chapter-8","File Processing and External Sorting"
"2 The boom moves all of the heads in or out together. When the heads are","chapter-8","File Processing and External Sorting"
"in some position over the platters, there are data on each platter directly accessible","chapter-8","File Processing and External Sorting"
"2 This arrangement, while typical, is not necessarily true for all disk drives. Nearly everything","chapter-8","File Processing and External Sorting"
"said here about the physical arrangement of disk drives represents a typical engineering compromise,","chapter-8","File Processing and External Sorting"
"not a fundamental design principle. There are many ways to design disk drives, and the engineering","chapter-8","File Processing and External Sorting"
"Sec. 8.2 Disk Drives 269","chapter-8","File Processing and External Sorting"
"(b)","chapter-8","File Processing and External Sorting"
"Heads","chapter-8","File Processing and External Sorting"
"Platters","chapter-8","File Processing and External Sorting"
"(arm)","chapter-8","File Processing and External Sorting"
"Boom","chapter-8","File Processing and External Sorting"
"(a)","chapter-8","File Processing and External Sorting"
"Track Read/Write","chapter-8","File Processing and External Sorting"
"Spindle","chapter-8","File Processing and External Sorting"
"Figure 8.2 (a) A typical disk drive arranged as a stack of platters. (b) One track","chapter-8","File Processing and External Sorting"
"on a disk drive platter.","chapter-8","File Processing and External Sorting"
"to each head. The data on a single platter that are accessible to any one position of","chapter-8","File Processing and External Sorting"
"the head for that platter are collectively called a track, that is, all data on a platter","chapter-8","File Processing and External Sorting"
"that are a fixed distance from the spindle, as shown in Figure 8.2(b). The collection","chapter-8","File Processing and External Sorting"
"of all tracks that are a fixed distance from the spindle is called a cylinder. Thus, a","chapter-8","File Processing and External Sorting"
"cylinder is all of the data that can be read when the arms are in a particular position.","chapter-8","File Processing and External Sorting"
"Each track is subdivided into sectors. Between each sector there are inter-","chapter-8","File Processing and External Sorting"
"sector gaps in which no data are stored. These gaps allow the read head to recog-","chapter-8","File Processing and External Sorting"
"nize the end of a sector. Note that each sector contains the same amount of data.","chapter-8","File Processing and External Sorting"
"Because the outer tracks have greater length, they contain fewer bits per inch than","chapter-8","File Processing and External Sorting"
"do the inner tracks. Thus, about half of the potential storage space is wasted, be-","chapter-8","File Processing and External Sorting"
"cause only the innermost tracks are stored at the highest possible data density. This","chapter-8","File Processing and External Sorting"
"arrangement is illustrated by Figure 8.3a. Disk drives today actually group tracks","chapter-8","File Processing and External Sorting"
"into “zones” such that the tracks in the innermost zone adjust their data density","chapter-8","File Processing and External Sorting"
"going out to maintain the same radial data density, then the tracks of the next zone","chapter-8","File Processing and External Sorting"
"reset the data density to make better use of their storage ability, and so on. This","chapter-8","File Processing and External Sorting"
"arrangement is shown in Figure 8.3b.","chapter-8","File Processing and External Sorting"
"In contrast to the physical layout of a hard disk, a CD-ROM consists of a single","chapter-8","File Processing and External Sorting"
"spiral track. Bits of information along the track are equally spaced, so the informa-","chapter-8","File Processing and External Sorting"
"tion density is the same at both the outer and inner portions of the track. To keep","chapter-8","File Processing and External Sorting"
"the information flow at a constant rate along the spiral, the drive must speed up the","chapter-8","File Processing and External Sorting"
"rate of disk spin as the I/O head moves toward the center of the disk. This makes","chapter-8","File Processing and External Sorting"
"for a more complicated and slower mechanism.","chapter-8","File Processing and External Sorting"
"Three separate steps take place when reading a particular byte or series of bytes","chapter-8","File Processing and External Sorting"
"of data from a hard disk. First, the I/O head moves so that it is positioned over the","chapter-8","File Processing and External Sorting"
"track containing the data. This movement is called a seek. Second, the sector","chapter-8","File Processing and External Sorting"
"containing the data rotates to come under the head. When in use the disk is always","chapter-8","File Processing and External Sorting"
"compromises change over time. In addition, most of the description given here for disk drives is a","chapter-8","File Processing and External Sorting"
"simplified version of the reality. But this is a useful working model to understand what is going on.","chapter-8","File Processing and External Sorting"
"270 Chap. 8 File Processing and External Sorting","chapter-8","File Processing and External Sorting"
"(a) (b)","chapter-8","File Processing and External Sorting"
"Intersector","chapter-8","File Processing and External Sorting"
"Gaps","chapter-8","File Processing and External Sorting"
"Sectors Bits of Data","chapter-8","File Processing and External Sorting"
"Figure 8.3 The organization of a disk platter. Dots indicate density of informa-","chapter-8","File Processing and External Sorting"
"tion. (a) Nominal arrangement of tracks showing decreasing data density when","chapter-8","File Processing and External Sorting"
"moving outward from the center of the disk. (b) A “zoned” arrangement with the","chapter-8","File Processing and External Sorting"
"sector size and density periodically reset in tracks further away from the center.","chapter-8","File Processing and External Sorting"
"spinning. At the time of this writing, typical disk spin rates are 7200 rotations per","chapter-8","File Processing and External Sorting"
"minute (rpm). The time spent waiting for the desired sector to come under the","chapter-8","File Processing and External Sorting"
"I/O head is called rotational delay or rotational latency. The third step is the","chapter-8","File Processing and External Sorting"
"actual transfer (i.e., reading or writing) of data. It takes relatively little time to","chapter-8","File Processing and External Sorting"
"read information once the first byte is positioned under the I/O head, simply the","chapter-8","File Processing and External Sorting"
"amount of time required for it all to move under the head. In fact, disk drives are","chapter-8","File Processing and External Sorting"
"designed not to read one byte of data, but rather to read an entire sector of data at","chapter-8","File Processing and External Sorting"
"each request. Thus, a sector is the minimum amount of data that can be read or","chapter-8","File Processing and External Sorting"
"written at one time.","chapter-8","File Processing and External Sorting"
"In general, it is desirable to keep all sectors for a file together on as few tracks","chapter-8","File Processing and External Sorting"
"as possible. This desire stems from two assumptions:","chapter-8","File Processing and External Sorting"
"1. Seek time is slow (it is typically the most expensive part of an I/O operation),","chapter-8","File Processing and External Sorting"
"and","chapter-8","File Processing and External Sorting"
"2. If one sector of the file is read, the next sector will probably soon be read.","chapter-8","File Processing and External Sorting"
"Assumption (2) is called locality of reference, a concept that comes up frequently","chapter-8","File Processing and External Sorting"
"in computer applications.","chapter-8","File Processing and External Sorting"
"Contiguous sectors are often grouped to form a cluster. A cluster is the smallest","chapter-8","File Processing and External Sorting"
"unit of allocation for a file, so all files are a multiple of the cluster size. The cluster","chapter-8","File Processing and External Sorting"
"size is determined by the operating system. The file manager keeps track of which","chapter-8","File Processing and External Sorting"
"clusters make up each file.","chapter-8","File Processing and External Sorting"
"In Microsoft Windows systems, there is a designated portion of the disk called","chapter-8","File Processing and External Sorting"
"the File Allocation Table, which stores information about which sectors belong","chapter-8","File Processing and External Sorting"
"to which file. In contrast, UNIX does not use clusters. The smallest unit of file","chapter-8","File Processing and External Sorting"
"Sec. 8.2 Disk Drives 271","chapter-8","File Processing and External Sorting"
"allocation and the smallest unit that can be read/written is a sector, which in UNIX","chapter-8","File Processing and External Sorting"
"terminology is called a block. UNIX maintains information about file organization","chapter-8","File Processing and External Sorting"
"in certain disk blocks called i-nodes.","chapter-8","File Processing and External Sorting"
"A group of physically contiguous clusters from the same file is called an extent.","chapter-8","File Processing and External Sorting"
"Ideally, all clusters making up a file will be contiguous on the disk (i.e., the file will","chapter-8","File Processing and External Sorting"
"consist of one extent), so as to minimize seek time required to access different","chapter-8","File Processing and External Sorting"
"portions of the file. If the disk is nearly full when a file is created, there might not","chapter-8","File Processing and External Sorting"
"be an extent available that is large enough to hold the new file. Furthermore, if a file","chapter-8","File Processing and External Sorting"
"grows, there might not be free space physically adjacent. Thus, a file might consist","chapter-8","File Processing and External Sorting"
"of several extents widely spaced on the disk. The fuller the disk, and the more that","chapter-8","File Processing and External Sorting"
"files on the disk change, the worse this file fragmentation (and the resulting seek","chapter-8","File Processing and External Sorting"
"time) becomes. File fragmentation leads to a noticeable degradation in performance","chapter-8","File Processing and External Sorting"
"as additional seeks are required to access data.","chapter-8","File Processing and External Sorting"
"Another type of problem arises when the file’s logical record size does not","chapter-8","File Processing and External Sorting"
"match the sector size. If the sector size is not a multiple of the record size (or","chapter-8","File Processing and External Sorting"
"vice versa), records will not fit evenly within a sector. For example, a sector might","chapter-8","File Processing and External Sorting"
"be 2048 bytes long, and a logical record 100 bytes. This leaves room to store","chapter-8","File Processing and External Sorting"
"20 records with 48 bytes left over. Either the extra space is wasted, or else records","chapter-8","File Processing and External Sorting"
"are allowed to cross sector boundaries. If a record crosses a sector boundary, two","chapter-8","File Processing and External Sorting"
"disk accesses might be required to read it. If the space is left empty instead, such","chapter-8","File Processing and External Sorting"
"wasted space is called internal fragmentation.","chapter-8","File Processing and External Sorting"
"A second example of internal fragmentation occurs at cluster boundaries. Files","chapter-8","File Processing and External Sorting"
"whose size is not an even multiple of the cluster size must waste some space at","chapter-8","File Processing and External Sorting"
"the end of the last cluster. The worst case will occur when file size modulo cluster","chapter-8","File Processing and External Sorting"
"size is one (for example, a file of 4097 bytes and a cluster of 4096 bytes). Thus,","chapter-8","File Processing and External Sorting"
"cluster size is a tradeoff between large files processed sequentially (where a large","chapter-8","File Processing and External Sorting"
"cluster size is desirable to minimize seeks) and small files (where small clusters are","chapter-8","File Processing and External Sorting"
"desirable to minimize wasted storage).","chapter-8","File Processing and External Sorting"
"Every disk drive organization requires that some disk space be used to organize","chapter-8","File Processing and External Sorting"
"the sectors, clusters, and so forth. The layout of sectors within a track is illustrated","chapter-8","File Processing and External Sorting"
"by Figure 8.4. Typical information that must be stored on the disk itself includes","chapter-8","File Processing and External Sorting"
"the File Allocation Table, sector headers that contain address marks and informa-","chapter-8","File Processing and External Sorting"
"tion about the condition (whether usable or not) for each sector, and gaps between","chapter-8","File Processing and External Sorting"
"sectors. The sector header also contains error detection codes to help verify that the","chapter-8","File Processing and External Sorting"
"data have not been corrupted. This is why most disk drives have a “nominal” size","chapter-8","File Processing and External Sorting"
"that is greater than the actual amount of user data that can be stored on the drive.","chapter-8","File Processing and External Sorting"
"The difference is the amount of space required to organize the information on the","chapter-8","File Processing and External Sorting"
"disk. Even more space will be lost due to fragmentation.","chapter-8","File Processing and External Sorting"
"272 Chap. 8 File Processing and External Sorting","chapter-8","File Processing and External Sorting"
"Data","chapter-8","File Processing and External Sorting"
"Sector Sector","chapter-8","File Processing and External Sorting"
"Header","chapter-8","File Processing and External Sorting"
"Intersector Gap","chapter-8","File Processing and External Sorting"
"Data","chapter-8","File Processing and External Sorting"
"Sector","chapter-8","File Processing and External Sorting"
"Header","chapter-8","File Processing and External Sorting"
"Sector","chapter-8","File Processing and External Sorting"
"Intrasector Gap","chapter-8","File Processing and External Sorting"
"Figure 8.4 An illustration of sector gaps within a track. Each sector begins with","chapter-8","File Processing and External Sorting"
"a sector header containing the sector address and an error detection code for the","chapter-8","File Processing and External Sorting"
"contents of that sector. The sector header is followed by a small intra-sector gap,","chapter-8","File Processing and External Sorting"
"followed in turn by the sector data. Each sector is separated from the next sector","chapter-8","File Processing and External Sorting"
"by a larger inter-sector gap.","chapter-8","File Processing and External Sorting"
"8.2.2 Disk Access Costs","chapter-8","File Processing and External Sorting"
"When a seek is required, it is usually the primary cost when accessing information","chapter-8","File Processing and External Sorting"
"on disk. This assumes of course that a seek is necessary. When reading a file in","chapter-8","File Processing and External Sorting"
"sequential order (if the sectors comprising the file are contiguous on disk), little","chapter-8","File Processing and External Sorting"
"seeking is necessary. However, when accessing a random disk sector, seek time","chapter-8","File Processing and External Sorting"
"becomes the dominant cost for the data access. While the actual seek time is highly","chapter-8","File Processing and External Sorting"
"variable, depending on the distance between the track where the I/O head currently","chapter-8","File Processing and External Sorting"
"is and the track where the head is moving to, we will consider only two numbers.","chapter-8","File Processing and External Sorting"
"One is the track-to-track cost, or the minimum time necessary to move from a","chapter-8","File Processing and External Sorting"
"track to an adjacent track. This is appropriate when you want to analyze access","chapter-8","File Processing and External Sorting"
"times for files that are well placed on the disk. The second number is the average","chapter-8","File Processing and External Sorting"
"seek time for a random access. These two numbers are often provided by disk","chapter-8","File Processing and External Sorting"
"manufacturers. A typical example is the Western Digital Caviar serial ATA drive.","chapter-8","File Processing and External Sorting"
"The manufacturer’s specifications indicate that the track-to-track time is 2.0 ms and","chapter-8","File Processing and External Sorting"
"the average seek time is 9.0 ms. In 2008 a typical drive in this line might be 120GB","chapter-8","File Processing and External Sorting"
"in size. In 2011, that same line of drives had sizes of up to 2 or 3TB. In both years,","chapter-8","File Processing and External Sorting"
"the advertised track-to-track and average seek times were identical.","chapter-8","File Processing and External Sorting"
"For many years, typical rotation speed for disk drives was 3600 rpm, or one","chapter-8","File Processing and External Sorting"
"rotation every 16.7 ms. Most disk drives in 2011 had a rotation speed of 7200 rpm,","chapter-8","File Processing and External Sorting"
"or 8.3 ms per rotation. When reading a sector at random, you can expect that the","chapter-8","File Processing and External Sorting"
"disk will need to rotate halfway around to bring the desired sector under the I/O","chapter-8","File Processing and External Sorting"
"head, or 4.2 ms for a 7200-rpm disk drive.","chapter-8","File Processing and External Sorting"
"Once under the I/O head, a sector of data can be transferred as fast as that","chapter-8","File Processing and External Sorting"
"sector rotates under the head. If an entire track is to be read, then it will require one","chapter-8","File Processing and External Sorting"
"rotation (8.3 ms at 7200 rpm) to move the full track under the head. If only part of","chapter-8","File Processing and External Sorting"
"the track is to be read, then proportionately less time will be required. For example,","chapter-8","File Processing and External Sorting"
"if there are 16,000 sectors on the track and one sector is to be read, this will require","chapter-8","File Processing and External Sorting"
"a trivial amount of time (1/16,000 of a rotation).","chapter-8","File Processing and External Sorting"
"Example 8.1 Assume that an older disk drive has a total (nominal) ca-","chapter-8","File Processing and External Sorting"
"pacity of 16.8GB spread among 10 platters, yielding 1.68GB/platter. Each","chapter-8","File Processing and External Sorting"
"Sec. 8.2 Disk Drives 273","chapter-8","File Processing and External Sorting"
"platter contains 13,085 tracks and each track contains (after formatting)","chapter-8","File Processing and External Sorting"
"256 sectors of 512 bytes/sector. Track-to-track seek time is 2.2 ms and av-","chapter-8","File Processing and External Sorting"
"erage seek time for random access is 9.5 ms. Assume the operating system","chapter-8","File Processing and External Sorting"
"maintains a cluster size of 8 sectors per cluster (4KB), yielding 32 clusters","chapter-8","File Processing and External Sorting"
"per track. The disk rotation rate is 5400 rpm (11.1 ms per rotation). Based","chapter-8","File Processing and External Sorting"
"on this information we can estimate the cost for various file processing op-","chapter-8","File Processing and External Sorting"
"erations.","chapter-8","File Processing and External Sorting"
"How much time is required to read the track? On average, it will require","chapter-8","File Processing and External Sorting"
"half a rotation to bring the first sector of the track under the I/O head, and","chapter-8","File Processing and External Sorting"
"then one complete rotation to read the track.","chapter-8","File Processing and External Sorting"
"How long will it take to read a file of 1MB divided into 2048 sector-","chapter-8","File Processing and External Sorting"
"sized (512 byte) records? This file will be stored in 256 clusters, because","chapter-8","File Processing and External Sorting"
"each cluster holds 8 sectors. The answer to the question depends largely","chapter-8","File Processing and External Sorting"
"on how the file is stored on the disk, that is, whether it is all together or","chapter-8","File Processing and External Sorting"
"broken into multiple extents. We will calculate both cases to see how much","chapter-8","File Processing and External Sorting"
"difference this makes.","chapter-8","File Processing and External Sorting"
"If the file is stored so as to fill all of the sectors of eight adjacent tracks,","chapter-8","File Processing and External Sorting"
"then the cost to read the first sector will be the time to seek to the first track","chapter-8","File Processing and External Sorting"
"(assuming this requires a random seek), then a wait for the initial rotational","chapter-8","File Processing and External Sorting"
"delay, and then the time to read (which is the same as the time to rotate the","chapter-8","File Processing and External Sorting"
"disk again). This requires","chapter-8","File Processing and External Sorting"
"9.5 + 11.1 × 1.5 = 26.2 ms.","chapter-8","File Processing and External Sorting"
"At this point, because we assume that the next seven tracks require only a","chapter-8","File Processing and External Sorting"
"track-to-track seek because they are adjacent. Each requires","chapter-8","File Processing and External Sorting"
"2.2 + 11.1 × 1.5 = 18.9 ms.","chapter-8","File Processing and External Sorting"
"The total time required is therefore","chapter-8","File Processing and External Sorting"
"26.2ms + 7 × 18.9ms = 158.5ms.","chapter-8","File Processing and External Sorting"
"If the file’s clusters are spread randomly across the disk, then we must","chapter-8","File Processing and External Sorting"
"perform a seek for each cluster, followed by the time for rotational delay.","chapter-8","File Processing and External Sorting"
"Once the first sector of the cluster comes under the I/O head, very little time","chapter-8","File Processing and External Sorting"
"is needed to read the cluster because only 8/256 of the track needs to rotate","chapter-8","File Processing and External Sorting"
"under the head, for a total time of about 5.9 ms for latency and read time.","chapter-8","File Processing and External Sorting"
"Thus, the total time required is about","chapter-8","File Processing and External Sorting"
"256(9.5 + 5.9) ≈ 3942ms","chapter-8","File Processing and External Sorting"
"or close to 4 seconds. This is much longer than the time required when the","chapter-8","File Processing and External Sorting"
"file is all together on disk!","chapter-8","File Processing and External Sorting"
"274 Chap. 8 File Processing and External Sorting","chapter-8","File Processing and External Sorting"
"This example illustrates why it is important to keep disk files from be-","chapter-8","File Processing and External Sorting"
"coming fragmented, and why so-called “disk defragmenters” can speed up","chapter-8","File Processing and External Sorting"
"file processing time. File fragmentation happens most commonly when the","chapter-8","File Processing and External Sorting"
"disk is nearly full and the file manager must search for free space whenever","chapter-8","File Processing and External Sorting"
"a file is created or changed.","chapter-8","File Processing and External Sorting"
"8.3 Buffers and Buffer Pools","chapter-8","File Processing and External Sorting"
"Given the specifications of the disk drive from Example 8.1, we find that it takes","chapter-8","File Processing and External Sorting"
"about 9.5+11.1×1.5 = 26.2 ms to read one track of data on average. It takes about","chapter-8","File Processing and External Sorting"
"9.5+11.1/2+(1/256)×11.1 = 15.1 ms on average to read a single sector of data.","chapter-8","File Processing and External Sorting"
"This is a good savings (slightly over half the time), but less than 1% of the data on","chapter-8","File Processing and External Sorting"
"the track are read. If we want to read only a single byte, it would save us effectively","chapter-8","File Processing and External Sorting"
"no time over that required to read an entire sector. For this reason, nearly all disk","chapter-8","File Processing and External Sorting"
"drives automatically read or write an entire sector’s worth of information whenever","chapter-8","File Processing and External Sorting"
"the disk is accessed, even when only one byte of information is requested.","chapter-8","File Processing and External Sorting"
"Once a sector is read, its information is stored in main memory. This is known","chapter-8","File Processing and External Sorting"
"as buffering or caching the information. If the next disk request is to that same","chapter-8","File Processing and External Sorting"
"sector, then it is not necessary to read from disk again because the information","chapter-8","File Processing and External Sorting"
"is already stored in main memory. Buffering is an example of one method for","chapter-8","File Processing and External Sorting"
"minimizing disk accesses mentioned at the beginning of the chapter: Bring off","chapter-8","File Processing and External Sorting"
"additional information from disk to satisfy future requests. If information from files","chapter-8","File Processing and External Sorting"
"were accessed at random, then the chance that two consecutive disk requests are to","chapter-8","File Processing and External Sorting"
"the same sector would be low. However, in practice most disk requests are close","chapter-8","File Processing and External Sorting"
"to the location (in the logical file at least) of the previous request. This means that","chapter-8","File Processing and External Sorting"
"the probability of the next request “hitting the cache” is much higher than chance","chapter-8","File Processing and External Sorting"
"would indicate.","chapter-8","File Processing and External Sorting"
"This principle explains one reason why average access times for new disk drives","chapter-8","File Processing and External Sorting"
"are lower than in the past. Not only is the hardware faster, but information is also","chapter-8","File Processing and External Sorting"
"now stored using better algorithms and larger caches that minimize the number of","chapter-8","File Processing and External Sorting"
"times information needs to be fetched from disk. This same concept is also used","chapter-8","File Processing and External Sorting"
"to store parts of programs in faster memory within the CPU, using the CPU cache","chapter-8","File Processing and External Sorting"
"that is prevalent in modern microprocessors.","chapter-8","File Processing and External Sorting"
"Sector-level buffering is normally provided by the operating system and is of-","chapter-8","File Processing and External Sorting"
"ten built directly into the disk drive controller hardware. Most operating systems","chapter-8","File Processing and External Sorting"
"maintain at least two buffers, one for input and one for output. Consider what","chapter-8","File Processing and External Sorting"
"would happen if there were only one buffer during a byte-by-byte copy operation.","chapter-8","File Processing and External Sorting"
"The sector containing the first byte would be read into the I/O buffer. The output","chapter-8","File Processing and External Sorting"
"operation would need to destroy the contents of the single I/O buffer to write this","chapter-8","File Processing and External Sorting"
"byte. Then the buffer would need to be filled again from disk for the second byte,","chapter-8","File Processing and External Sorting"
"Sec. 8.3 Buffers and Buffer Pools 275","chapter-8","File Processing and External Sorting"
"only to be destroyed during output. The simple solution to this problem is to keep","chapter-8","File Processing and External Sorting"
"one buffer for input, and a second for output.","chapter-8","File Processing and External Sorting"
"Most disk drive controllers operate independently from the CPU once an I/O","chapter-8","File Processing and External Sorting"
"request is received. This is useful because the CPU can typically execute millions","chapter-8","File Processing and External Sorting"
"of instructions during the time required for a single I/O operation. A technique that","chapter-8","File Processing and External Sorting"
"takes maximum advantage of this micro-parallelism is double buffering. Imagine","chapter-8","File Processing and External Sorting"
"that a file is being processed sequentially. While the first sector is being read, the","chapter-8","File Processing and External Sorting"
"CPU cannot process that information and so must wait or find something else to do","chapter-8","File Processing and External Sorting"
"in the meantime. Once the first sector is read, the CPU can start processing while","chapter-8","File Processing and External Sorting"
"the disk drive (in parallel) begins reading the second sector. If the time required for","chapter-8","File Processing and External Sorting"
"the CPU to process a sector is approximately the same as the time required by the","chapter-8","File Processing and External Sorting"
"disk controller to read a sector, it might be possible to keep the CPU continuously","chapter-8","File Processing and External Sorting"
"fed with data from the file. The same concept can also be applied to output, writing","chapter-8","File Processing and External Sorting"
"one sector to disk while the CPU is writing to a second output buffer in memory.","chapter-8","File Processing and External Sorting"
"Thus, in computers that support double buffering, it pays to have at least two input","chapter-8","File Processing and External Sorting"
"buffers and two output buffers available.","chapter-8","File Processing and External Sorting"
"Caching information in memory is such a good idea that it is usually extended","chapter-8","File Processing and External Sorting"
"to multiple buffers. The operating system or an application program might store","chapter-8","File Processing and External Sorting"
"many buffers of information taken from some backing storage such as a disk file.","chapter-8","File Processing and External Sorting"
"This process of using buffers as an intermediary between a user and a disk file is","chapter-8","File Processing and External Sorting"
"called buffering the file. The information stored in a buffer is often called a page,","chapter-8","File Processing and External Sorting"
"and the collection of buffers is called a buffer pool. The goal of the buffer pool","chapter-8","File Processing and External Sorting"
"is to increase the amount of information stored in memory in hopes of increasing","chapter-8","File Processing and External Sorting"
"the likelihood that new information requests can be satisfied from the buffer pool","chapter-8","File Processing and External Sorting"
"rather than requiring new information to be read from disk.","chapter-8","File Processing and External Sorting"
"As long as there is an unused buffer available in the buffer pool, new informa-","chapter-8","File Processing and External Sorting"
"tion can be read in from disk on demand. When an application continues to read","chapter-8","File Processing and External Sorting"
"new information from disk, eventually all of the buffers in the buffer pool will be-","chapter-8","File Processing and External Sorting"
"come full. Once this happens, some decision must be made about what information","chapter-8","File Processing and External Sorting"
"in the buffer pool will be sacrificed to make room for newly requested information.","chapter-8","File Processing and External Sorting"
"When replacing information contained in the buffer pool, the goal is to select a","chapter-8","File Processing and External Sorting"
"buffer that has “unnecessary” information, that is, the information least likely to be","chapter-8","File Processing and External Sorting"
"requested again. Because the buffer pool cannot know for certain what the pattern","chapter-8","File Processing and External Sorting"
"of future requests will look like, a decision based on some heuristic, or best guess,","chapter-8","File Processing and External Sorting"
"must be used. There are several approaches to making this decision.","chapter-8","File Processing and External Sorting"
"One heuristic is “first-in, first-out” (FIFO). This scheme simply orders the","chapter-8","File Processing and External Sorting"
"buffers in a queue. The buffer at the front of the queue is used next to store new","chapter-8","File Processing and External Sorting"
"information and then placed at the end of the queue. In this way, the buffer to be","chapter-8","File Processing and External Sorting"
"replaced is the one that has held its information the longest, in hopes that this in-","chapter-8","File Processing and External Sorting"
"formation is no longer needed. This is a reasonable assumption when processing","chapter-8","File Processing and External Sorting"
"moves along the file at some steady pace in roughly sequential order. However,","chapter-8","File Processing and External Sorting"
"276 Chap. 8 File Processing and External Sorting","chapter-8","File Processing and External Sorting"
"many programs work with certain key pieces of information over and over again,","chapter-8","File Processing and External Sorting"
"and the importance of information has little to do with how long ago the informa-","chapter-8","File Processing and External Sorting"
"tion was first accessed. Typically it is more important to know how many times the","chapter-8","File Processing and External Sorting"
"information has been accessed, or how recently the information was last accessed.","chapter-8","File Processing and External Sorting"
"Another approach is called “least frequently used” (LFU). LFU tracks the num-","chapter-8","File Processing and External Sorting"
"ber of accesses to each buffer in the buffer pool. When a buffer must be reused, the","chapter-8","File Processing and External Sorting"
"buffer that has been accessed the fewest number of times is considered to contain","chapter-8","File Processing and External Sorting"
"the “least important” information, and so it is used next. LFU, while it seems in-","chapter-8","File Processing and External Sorting"
"tuitively reasonable, has many drawbacks. First, it is necessary to store and update","chapter-8","File Processing and External Sorting"
"access counts for each buffer. Second, what was referenced many times in the past","chapter-8","File Processing and External Sorting"
"might now be irrelevant. Thus, some time mechanism where counts “expire” is","chapter-8","File Processing and External Sorting"
"often desirable. This also avoids the problem of buffers that slowly build up big","chapter-8","File Processing and External Sorting"
"counts because they get used just often enough to avoid being replaced. An alter-","chapter-8","File Processing and External Sorting"
"native is to maintain counts for all sectors ever read, not just the sectors currently","chapter-8","File Processing and External Sorting"
"in the buffer pool. This avoids immediately replacing the buffer just read, which","chapter-8","File Processing and External Sorting"
"has not yet had time to build a high access count.","chapter-8","File Processing and External Sorting"
"The third approach is called “least recently used” (LRU). LRU simply keeps the","chapter-8","File Processing and External Sorting"
"buffers in a list. Whenever information in a buffer is accessed, this buffer is brought","chapter-8","File Processing and External Sorting"
"to the front of the list. When new information must be read, the buffer at the back","chapter-8","File Processing and External Sorting"
"of the list (the one least recently used) is taken and its “old” information is either","chapter-8","File Processing and External Sorting"
"discarded or written to disk, as appropriate. This is an easily implemented approx-","chapter-8","File Processing and External Sorting"
"imation to LFU and is often the method of choice for managing buffer pools unless","chapter-8","File Processing and External Sorting"
"special knowledge about information access patterns for an application suggests a","chapter-8","File Processing and External Sorting"
"special-purpose buffer management scheme.","chapter-8","File Processing and External Sorting"
"The main purpose of a buffer pool is to minimize disk I/O. When the contents of","chapter-8","File Processing and External Sorting"
"a block are modified, we could write the updated information to disk immediately.","chapter-8","File Processing and External Sorting"
"But what if the block is changed again? If we write the block’s contents after every","chapter-8","File Processing and External Sorting"
"change, that might be a lot of disk write operations that can be avoided. It is more","chapter-8","File Processing and External Sorting"
"efficient to wait until either the file is to be closed, or the contents of the buffer","chapter-8","File Processing and External Sorting"
"containing that block is to be flushed from the buffer pool.","chapter-8","File Processing and External Sorting"
"When a buffer’s contents are to be replaced in the buffer pool, we only want","chapter-8","File Processing and External Sorting"
"to write the contents to disk if it is necessary. That would be necessary only if the","chapter-8","File Processing and External Sorting"
"contents have changed since the block was read in originally from the file. The way","chapter-8","File Processing and External Sorting"
"to insure that the block is written when necessary, but only when necessary, is to","chapter-8","File Processing and External Sorting"
"maintain a Boolean variable with the buffer (often referred to as the dirty bit) that","chapter-8","File Processing and External Sorting"
"is turned on when the buffer’s contents are modified by the client. At the time when","chapter-8","File Processing and External Sorting"
"the block is flushed from the buffer pool, it is written to disk if and only if the dirty","chapter-8","File Processing and External Sorting"
"bit has been turned on.","chapter-8","File Processing and External Sorting"
"Modern operating systems support virtual memory. Virtual memory is a tech-","chapter-8","File Processing and External Sorting"
"nique that allows the programmer to write programs as though there is more of the","chapter-8","File Processing and External Sorting"
"faster main memory (such as RAM) than actually exists. Virtual memory makes use","chapter-8","File Processing and External Sorting"
"Sec. 8.3 Buffers and Buffer Pools 277","chapter-8","File Processing and External Sorting"
"of a buffer pool to store data read from blocks on slower, secondary memory (such","chapter-8","File Processing and External Sorting"
"as on the disk drive). The disk stores the complete contents of the virtual memory.","chapter-8","File Processing and External Sorting"
"Blocks are read into main memory as demanded by memory accesses. Naturally,","chapter-8","File Processing and External Sorting"
"programs using virtual memory techniques are slower than programs whose data","chapter-8","File Processing and External Sorting"
"are stored completely in main memory. The advantage is reduced programmer ef-","chapter-8","File Processing and External Sorting"
"fort because a good virtual memory system provides the appearance of larger main","chapter-8","File Processing and External Sorting"
"memory without modifying the program.","chapter-8","File Processing and External Sorting"
"Example 8.2 Consider a virtual memory whose size is ten sectors, and","chapter-8","File Processing and External Sorting"
"which has a buffer pool of five buffers (each one sector in size) associated","chapter-8","File Processing and External Sorting"
"with it. We will use a LRU replacement scheme. The following series of","chapter-8","File Processing and External Sorting"
"memory requests occurs.","chapter-8","File Processing and External Sorting"
"9017668135171","chapter-8","File Processing and External Sorting"
"After the first five requests, the buffer pool will store the sectors in the order","chapter-8","File Processing and External Sorting"
"6, 7, 1, 0, 9. Because Sector 6 is already at the front, the next request can be","chapter-8","File Processing and External Sorting"
"answered without reading new data from disk or reordering the buffers. The","chapter-8","File Processing and External Sorting"
"request to Sector 8 requires emptying the contents of the least recently used","chapter-8","File Processing and External Sorting"
"buffer, which contains Sector 9. The request to Sector 1 brings the buffer","chapter-8","File Processing and External Sorting"
"holding Sector 1’s contents back to the front. Processing the remaining","chapter-8","File Processing and External Sorting"
"requests results in the buffer pool as shown in Figure 8.5.","chapter-8","File Processing and External Sorting"
"Example 8.3 Figure 8.5 illustrates a buffer pool of five blocks mediating","chapter-8","File Processing and External Sorting"
"a virtual memory of ten blocks. At any given moment, up to five sectors of","chapter-8","File Processing and External Sorting"
"information can be in main memory. Assume that Sectors 1, 7, 5, 3, and 8","chapter-8","File Processing and External Sorting"
"are currently in the buffer pool, stored in this order, and that we use the","chapter-8","File Processing and External Sorting"
"LRU buffer replacement strategy. If a request for Sector 9 is then received,","chapter-8","File Processing and External Sorting"
"then one sector currently in the buffer pool must be replaced. Because the","chapter-8","File Processing and External Sorting"
"buffer containing Sector 8 is the least recently used buffer, its contents will","chapter-8","File Processing and External Sorting"
"be copied back to disk at Sector 8. The contents of Sector 9 are then copied","chapter-8","File Processing and External Sorting"
"into this buffer, and it is moved to the front of the buffer pool (leaving the","chapter-8","File Processing and External Sorting"
"buffer containing Sector 3 as the new least-recently used buffer). If the next","chapter-8","File Processing and External Sorting"
"memory request were to Sector 5, no data would need to be read from disk.","chapter-8","File Processing and External Sorting"
"Instead, the buffer already containing Sector 5 would be moved to the front","chapter-8","File Processing and External Sorting"
"of the buffer pool.","chapter-8","File Processing and External Sorting"
"When implementing buffer pools, there are two basic approaches that can be","chapter-8","File Processing and External Sorting"
"taken regarding the transfer of information between the user of the buffer pool and","chapter-8","File Processing and External Sorting"
"the buffer pool class itself. The first approach is to pass “messages” between the","chapter-8","File Processing and External Sorting"
"two. This approach is illustrated by the following abstract class:","chapter-8","File Processing and External Sorting"
"278 Chap. 8 File Processing and External Sorting","chapter-8","File Processing and External Sorting"
"(on disk)","chapter-8","File Processing and External Sorting"
"Secondary Storage","chapter-8","File Processing and External Sorting"
"2","chapter-8","File Processing and External Sorting"
"3","chapter-8","File Processing and External Sorting"
"4","chapter-8","File Processing and External Sorting"
"5","chapter-8","File Processing and External Sorting"
"6","chapter-8","File Processing and External Sorting"
"7","chapter-8","File Processing and External Sorting"
"8","chapter-8","File Processing and External Sorting"
"9","chapter-8","File Processing and External Sorting"
"8","chapter-8","File Processing and External Sorting"
"3","chapter-8","File Processing and External Sorting"
"5","chapter-8","File Processing and External Sorting"
"0","chapter-8","File Processing and External Sorting"
"1 7","chapter-8","File Processing and External Sorting"
"1","chapter-8","File Processing and External Sorting"
"Main Memory","chapter-8","File Processing and External Sorting"
"(in RAM)","chapter-8","File Processing and External Sorting"
"Figure 8.5 An illustration of virtual memory. The complete collection of infor-","chapter-8","File Processing and External Sorting"
"mation resides in the slower, secondary storage (on disk). Those sectors recently","chapter-8","File Processing and External Sorting"
"accessed are held in the fast main memory (in RAM). In this example, copies of","chapter-8","File Processing and External Sorting"
"Sectors 1, 7, 5, 3, and 8 from secondary storage are currently stored in the main","chapter-8","File Processing and External Sorting"
"memory. If a memory access to Sector 9 is received, one of the sectors currently","chapter-8","File Processing and External Sorting"
"in main memory must be replaced.","chapter-8","File Processing and External Sorting"
"/** ADT for buffer pools using the message-passing style */","chapter-8","File Processing and External Sorting"
"public interface BufferPoolADT {","chapter-8","File Processing and External Sorting"
"/** Copy "sz" bytes from "space" to position "pos" in the","chapter-8","File Processing and External Sorting"
"buffered storage */","chapter-8","File Processing and External Sorting"
"public void insert(byte[] space, int sz, int pos);","chapter-8","File Processing and External Sorting"
"/** Copy "sz" bytes from position "pos" of the buffered","chapter-8","File Processing and External Sorting"
"storage to "space". */","chapter-8","File Processing and External Sorting"
"public void getbytes(byte[] space, int sz, int pos);","chapter-8","File Processing and External Sorting"
"}","chapter-8","File Processing and External Sorting"
"This simple class provides an interface with two member functions, insert","chapter-8","File Processing and External Sorting"
"and getbytes. The information is passed between the buffer pool user and the","chapter-8","File Processing and External Sorting"
"buffer pool through the space parameter. This is storage space, provided by the","chapter-8","File Processing and External Sorting"
"bufferpool client and at least sz bytes long, which the buffer pool can take in-","chapter-8","File Processing and External Sorting"
"formation from (the insert function) or put information into (the getbytes","chapter-8","File Processing and External Sorting"
"function). Parameter pos indicates where the information will be placed in the","chapter-8","File Processing and External Sorting"
"buffer pool’s logical storage space. Physically, it will actually be copied to the ap-","chapter-8","File Processing and External Sorting"
"propriate byte position in some buffer in the buffer pool. This ADT is similar to","chapter-8","File Processing and External Sorting"
"the read and write methods of the RandomAccessFile class discussed in","chapter-8","File Processing and External Sorting"
"Section 8.4.","chapter-8","File Processing and External Sorting"
"Sec. 8.3 Buffers and Buffer Pools 279","chapter-8","File Processing and External Sorting"
"Example 8.4 Assume each sector of the disk file (and thus each block in","chapter-8","File Processing and External Sorting"
"the buffer pool) stores 1024 bytes. Assume that the buffer pool is in the","chapter-8","File Processing and External Sorting"
"state shown in Figure 8.5. If the next request is to copy 40 bytes begin-","chapter-8","File Processing and External Sorting"
"ning at position 6000 of the file, these bytes should be placed into Sector 5","chapter-8","File Processing and External Sorting"
"(whose bytes go from position 5120 to position 6143). Because Sector 5","chapter-8","File Processing and External Sorting"
"is currently in the buffer pool, we simply copy the 40 bytes contained in","chapter-8","File Processing and External Sorting"
"space to byte positions 880-919. The buffer containing Sector 5 is then","chapter-8","File Processing and External Sorting"
"moved to the buffer pool ahead of the buffer containing Sector 1.","chapter-8","File Processing and External Sorting"
"An alternative interface is to have the buffer pool provide to the user a direct","chapter-8","File Processing and External Sorting"
"pointer to a buffer that contains the requested information. Such an interface might","chapter-8","File Processing and External Sorting"
"look as follows:","chapter-8","File Processing and External Sorting"
"/** ADT for buffer pools using the buffer-passing style */","chapter-8","File Processing and External Sorting"
"public interface BufferPoolADT {","chapter-8","File Processing and External Sorting"
"/** Return pointer to the requested block */","chapter-8","File Processing and External Sorting"
"public byte[] getblock(int block);","chapter-8","File Processing and External Sorting"
"/** Set the dirty bit for the buffer holding "block" */","chapter-8","File Processing and External Sorting"
"public void dirtyblock(int block);","chapter-8","File Processing and External Sorting"
"// Tell the size of a buffer","chapter-8","File Processing and External Sorting"
"public int blocksize();","chapter-8","File Processing and External Sorting"
"};","chapter-8","File Processing and External Sorting"
"In this approach, the buffer pool user is made aware that the storage space is","chapter-8","File Processing and External Sorting"
"divided into blocks of a given size, where each block is the size of a buffer. The user","chapter-8","File Processing and External Sorting"
"requests specific blocks from the buffer pool, with a pointer to the buffer holding","chapter-8","File Processing and External Sorting"
"the requested block being returned to the user. The user might then read from or","chapter-8","File Processing and External Sorting"
"write to this space. If the user writes to the space, the buffer pool must be informed","chapter-8","File Processing and External Sorting"
"of this fact. The reason is that, when a given block is to be removed from the buffer","chapter-8","File Processing and External Sorting"
"pool, the contents of that block must be written to the backing storage if it has been","chapter-8","File Processing and External Sorting"
"modified. If the block has not been modified, then it is unnecessary to write it out.","chapter-8","File Processing and External Sorting"
"Example 8.5 We wish to write 40 bytes beginning at logical position","chapter-8","File Processing and External Sorting"
"6000 in the file. Assume that the buffer pool is in the state shown in Fig-","chapter-8","File Processing and External Sorting"
"ure 8.5. Using the second ADT, the client would need to know that blocks","chapter-8","File Processing and External Sorting"
"(buffers) are of size 1024, and therefore would request access to Sector 5.","chapter-8","File Processing and External Sorting"
"A pointer to the buffer containing Sector 5 would be returned by the call to","chapter-8","File Processing and External Sorting"
"getblock. The client would then copy 40 bytes to positions 880-919 of","chapter-8","File Processing and External Sorting"
"the buffer, and call dirtyblock to warn the buffer pool that the contents","chapter-8","File Processing and External Sorting"
"of this block have been modified.","chapter-8","File Processing and External Sorting"
"280 Chap. 8 File Processing and External Sorting","chapter-8","File Processing and External Sorting"
"A variation on this approach is to have the getblock function take another","chapter-8","File Processing and External Sorting"
"parameter to indicate the “mode” of use for the information. If the mode is READ","chapter-8","File Processing and External Sorting"
"then the buffer pool assumes that no changes will be made to the buffer’s contents","chapter-8","File Processing and External Sorting"
"(and so no write operation need be done when the buffer is reused to store another","chapter-8","File Processing and External Sorting"
"block). If the mode is WRITE then the buffer pool assumes that the client will not","chapter-8","File Processing and External Sorting"
"look at the contents of the buffer and so no read from the file is necessary. If the","chapter-8","File Processing and External Sorting"
"mode is READ AND WRITE then the buffer pool would read the existing contents","chapter-8","File Processing and External Sorting"
"of the block in from disk, and write the contents of the buffer to disk when the","chapter-8","File Processing and External Sorting"
"buffer is to be reused. Using the “mode” approach, the dirtyblock method is","chapter-8","File Processing and External Sorting"
"avoided.","chapter-8","File Processing and External Sorting"
"One problem with the buffer-passing ADT is the risk of stale pointers. When","chapter-8","File Processing and External Sorting"
"the buffer pool user is given a pointer to some buffer space at time T1, that pointer","chapter-8","File Processing and External Sorting"
"does indeed refer to the desired data at that time. As further requests are made to","chapter-8","File Processing and External Sorting"
"the buffer pool, it is possible that the data in any given buffer will be removed and","chapter-8","File Processing and External Sorting"
"replaced with new data. If the buffer pool user at a later time T2 then refers to the","chapter-8","File Processing and External Sorting"
"data referred to by the pointer given at time T1, it is possible that the data are no","chapter-8","File Processing and External Sorting"
"longer valid because the buffer contents have been replaced in the meantime. Thus","chapter-8","File Processing and External Sorting"
"the pointer into the buffer pool’s memory has become “stale.” To guarantee that a","chapter-8","File Processing and External Sorting"
"pointer is not stale, it should not be used if intervening requests to the buffer pool","chapter-8","File Processing and External Sorting"
"have taken place.","chapter-8","File Processing and External Sorting"
"We can solve this problem by introducing the concept of a user (or possibly","chapter-8","File Processing and External Sorting"
"multiple users) gaining access to a buffer, and then releasing the buffer when done.","chapter-8","File Processing and External Sorting"
"We will add method acquireBuffer and releaseBuffer for this purpose.","chapter-8","File Processing and External Sorting"
"Method acquireBuffer takes a block ID as input and returns a pointer to the","chapter-8","File Processing and External Sorting"
"buffer that will be used to store this block. The buffer pool will keep a count of the","chapter-8","File Processing and External Sorting"
"number of requests currently active for this block. Method releaseBuffer will","chapter-8","File Processing and External Sorting"
"reduce the count of active users for the associated block. Buffers associated with","chapter-8","File Processing and External Sorting"
"active blocks will not be eligible for flushing from the buffer pool. This will lead","chapter-8","File Processing and External Sorting"
"to a problem if the client neglects to release active blocks when they are no longer","chapter-8","File Processing and External Sorting"
"needed. There would also be a problem if there were more total active blocks than","chapter-8","File Processing and External Sorting"
"buffers in the buffer pool. However, the buffer pool should always be initialized to","chapter-8","File Processing and External Sorting"
"include more buffers than should ever be active at one time.","chapter-8","File Processing and External Sorting"
"An additional problem with both ADTs presented so far comes when the user","chapter-8","File Processing and External Sorting"
"intends to completely overwrite the contents of a block, and does not need to read","chapter-8","File Processing and External Sorting"
"in the old contents already on disk. However, the buffer pool cannot in general","chapter-8","File Processing and External Sorting"
"know whether the user wishes to use the old contents or not. This is especially true","chapter-8","File Processing and External Sorting"
"with the message-passing approach where a given message might overwrite only","chapter-8","File Processing and External Sorting"
"part of the block. In this case, the block will be read into memory even when not","chapter-8","File Processing and External Sorting"
"needed, and then its contents will be overwritten.","chapter-8","File Processing and External Sorting"
"This inefficiency can be avoided (at least in the buffer-passing version) by sep-","chapter-8","File Processing and External Sorting"
"arating the assignment of blocks to buffers from actually reading in data for the","chapter-8","File Processing and External Sorting"
"Sec. 8.3 Buffers and Buffer Pools 281","chapter-8","File Processing and External Sorting"
"block. In particular, the following revised buffer-passing ADT does not actually","chapter-8","File Processing and External Sorting"
"read data in the acquireBuffer method. Users who wish to see the old con-","chapter-8","File Processing and External Sorting"
"tents must then issue a readBlock request to read the data from disk into the","chapter-8","File Processing and External Sorting"
"buffer, and then a getDataPointer request to gain direct access to the buffer’s","chapter-8","File Processing and External Sorting"
"data contents.","chapter-8","File Processing and External Sorting"
"/** Improved ADT for buffer pools using the buffer-passing","chapter-8","File Processing and External Sorting"
"style. Most user functionality is in the buffer class,","chapter-8","File Processing and External Sorting"
"not the buffer pool itself. */","chapter-8","File Processing and External Sorting"
"/** A single buffer in the buffer pool */","chapter-8","File Processing and External Sorting"
"public interface BufferADT {","chapter-8","File Processing and External Sorting"
"/** Read the associated block from disk (if necessary)","chapter-8","File Processing and External Sorting"
"and return a pointer to the data */","chapter-8","File Processing and External Sorting"
"public byte[] readBlock();","chapter-8","File Processing and External Sorting"
"/** Return a pointer to the buffer’s data array","chapter-8","File Processing and External Sorting"
"(without reading from disk) */","chapter-8","File Processing and External Sorting"
"public byte[] getDataPointer();","chapter-8","File Processing and External Sorting"
"/** Flag buffer’s contents as having changed, so that","chapter-8","File Processing and External Sorting"
"flushing the block will write it back to disk */","chapter-8","File Processing and External Sorting"
"public void markDirty();","chapter-8","File Processing and External Sorting"
"/** Release the block’s access to this buffer. Further","chapter-8","File Processing and External Sorting"
"accesses to this buffer are illegal. */","chapter-8","File Processing and External Sorting"
"public void releaseBuffer();","chapter-8","File Processing and External Sorting"
"}","chapter-8","File Processing and External Sorting"
"/** The bufferpool */","chapter-8","File Processing and External Sorting"
"public interface BufferPoolADT {","chapter-8","File Processing and External Sorting"
"/** Relate a block to a buffer, returning a pointer to","chapter-8","File Processing and External Sorting"
"a buffer object */","chapter-8","File Processing and External Sorting"
"Buffer acquireBuffer(int block);","chapter-8","File Processing and External Sorting"
"}","chapter-8","File Processing and External Sorting"
"Again, a mode parameter could be added to the acquireBuffer method,","chapter-8","File Processing and External Sorting"
"eliminating the need for the readBlock and markDirty methods.","chapter-8","File Processing and External Sorting"
"Clearly, the buffer-passing approach places more obligations on the user of the","chapter-8","File Processing and External Sorting"
"buffer pool. These obligations include knowing the size of a block, not corrupting","chapter-8","File Processing and External Sorting"
"the buffer pool’s storage space, and informing the buffer pool both when a block","chapter-8","File Processing and External Sorting"
"has been modified and when it is no longer needed. So many obligations make this","chapter-8","File Processing and External Sorting"
"approach prone to error. An advantage is that there is no need to do an extra copy","chapter-8","File Processing and External Sorting"
"step when getting information from the user to the buffer. If the size of the records","chapter-8","File Processing and External Sorting"
"stored is small, this is not an important consideration. If the size of the records is","chapter-8","File Processing and External Sorting"
"large (especially if the record size and the buffer size are the same, as typically is the","chapter-8","File Processing and External Sorting"
"case when implementing B-trees, see Section 10.5), then this efficiency issue might","chapter-8","File Processing and External Sorting"
"282 Chap. 8 File Processing and External Sorting","chapter-8","File Processing and External Sorting"
"become important. Note however that the in-memory copy time will always be far","chapter-8","File Processing and External Sorting"
"less than the time required to write the contents of a buffer to disk. For applications","chapter-8","File Processing and External Sorting"
"where disk I/O is the bottleneck for the program, even the time to copy lots of","chapter-8","File Processing and External Sorting"
"information between the buffer pool user and the buffer might be inconsequential.","chapter-8","File Processing and External Sorting"
"Another advantage to buffer passing is the reduction in unnecessary read operations","chapter-8","File Processing and External Sorting"
"for data that will be overwritten anyway.","chapter-8","File Processing and External Sorting"
"You should note that these implementations for the buffer pool ADT do not use","chapter-8","File Processing and External Sorting"
"generics. Instead, the space parameter and the buffer pointer are declared to be","chapter-8","File Processing and External Sorting"
"byte[]. When a class uses a generic, that means that the record type is arbitrary,","chapter-8","File Processing and External Sorting"
"but that the class knows what the record type is. In contrast, using byte[] for the","chapter-8","File Processing and External Sorting"
"space means that not only is the record type arbitrary, but also the buffer pool does","chapter-8","File Processing and External Sorting"
"not even know what the user’s record type is. In fact, a given buffer pool might","chapter-8","File Processing and External Sorting"
"have many users who store many types of records.","chapter-8","File Processing and External Sorting"
"In a buffer pool, the user decides where a given record will be stored but has","chapter-8","File Processing and External Sorting"
"no control over the precise mechanism by which data are transferred to the backing","chapter-8","File Processing and External Sorting"
"storage. This is in contrast to the memory manager described in Section 12.3 in","chapter-8","File Processing and External Sorting"
"which the user passes a record to the manager and has no control at all over where","chapter-8","File Processing and External Sorting"
"the record is stored.","chapter-8","File Processing and External Sorting"
"8.4 The Programmer’s View of Files","chapter-8","File Processing and External Sorting"
"The Java programmer’s logical view of a random access file is a single stream","chapter-8","File Processing and External Sorting"
"of bytes. Interaction with a file can be viewed as a communications channel for","chapter-8","File Processing and External Sorting"
"issuing one of three instructions: read bytes from the current position in the file,","chapter-8","File Processing and External Sorting"
"write bytes to the current position in the file, and move the current position within","chapter-8","File Processing and External Sorting"
"the file. You do not normally see how the bytes are stored in sectors, clusters, and","chapter-8","File Processing and External Sorting"
"so forth. The mapping from logical to physical addresses is done by the file system,","chapter-8","File Processing and External Sorting"
"and sector-level buffering is done automatically by the disk controller.","chapter-8","File Processing and External Sorting"
"When processing records in a disk file, the order of access can have a great","chapter-8","File Processing and External Sorting"
"effect on I/O time. A random access procedure processes records in an order","chapter-8","File Processing and External Sorting"
"independent of their logical order within the file. Sequential access processes","chapter-8","File Processing and External Sorting"
"records in order of their logical appearance within the file. Sequential processing","chapter-8","File Processing and External Sorting"
"requires less seek time if the physical layout of the disk file matches its logical","chapter-8","File Processing and External Sorting"
"layout, as would be expected if the file were created on a disk with a high percentage","chapter-8","File Processing and External Sorting"
"of free space.","chapter-8","File Processing and External Sorting"
"Java provides several mechanisms for manipulating disk files. One of the most","chapter-8","File Processing and External Sorting"
"commonly used is the RandomAccessFile class. The following methods can","chapter-8","File Processing and External Sorting"
"be used to manipulate information in the file.","chapter-8","File Processing and External Sorting"
"• RandomAccessFile(String name, String mode): Class con-","chapter-8","File Processing and External Sorting"
"structor, opens a disk file for processing.","chapter-8","File Processing and External Sorting"
"Sec. 8.5 External Sorting 283","chapter-8","File Processing and External Sorting"
"• read(byte[] b): Read some bytes from the current position in the file.","chapter-8","File Processing and External Sorting"
"The current position moves forward as the bytes are read.","chapter-8","File Processing and External Sorting"
"• write(byte[] b): Write some bytes at the current position in the file","chapter-8","File Processing and External Sorting"
"(overwriting the bytes already at that position). The current position moves","chapter-8","File Processing and External Sorting"
"forward as the bytes are written.","chapter-8","File Processing and External Sorting"
"• seek(long pos): Move the current position in the file to pos. This","chapter-8","File Processing and External Sorting"
"allows bytes at arbitrary places within the file to be read or written.","chapter-8","File Processing and External Sorting"
"• close(): Close a file at the end of processing.","chapter-8","File Processing and External Sorting"
"8.5 External Sorting","chapter-8","File Processing and External Sorting"
"We now consider the problem of sorting collections of records too large to fit in","chapter-8","File Processing and External Sorting"
"main memory. Because the records must reside in peripheral or external memory,","chapter-8","File Processing and External Sorting"
"such sorting methods are called external sorts. This is in contrast to the internal","chapter-8","File Processing and External Sorting"
"sorts discussed in Chapter 7 which assume that the records to be sorted are stored in","chapter-8","File Processing and External Sorting"
"main memory. Sorting large collections of records is central to many applications,","chapter-8","File Processing and External Sorting"
"such as processing payrolls and other large business databases. As a consequence,","chapter-8","File Processing and External Sorting"
"many external sorting algorithms have been devised. Years ago, sorting algorithm","chapter-8","File Processing and External Sorting"
"designers sought to optimize the use of specific hardware configurations, such as","chapter-8","File Processing and External Sorting"
"multiple tape or disk drives. Most computing today is done on personal computers","chapter-8","File Processing and External Sorting"
"and low-end workstations with relatively powerful CPUs, but only one or at most","chapter-8","File Processing and External Sorting"
"two disk drives. The techniques presented here are geared toward optimized pro-","chapter-8","File Processing and External Sorting"
"cessing on a single disk drive. This approach allows us to cover the most important","chapter-8","File Processing and External Sorting"
"issues in external sorting while skipping many less important machine-dependent","chapter-8","File Processing and External Sorting"
"details. Readers who have a need to implement efficient external sorting algorithms","chapter-8","File Processing and External Sorting"
"that take advantage of more sophisticated hardware configurations should consult","chapter-8","File Processing and External Sorting"
"the references in Section 8.6.","chapter-8","File Processing and External Sorting"
"When a collection of records is too large to fit in main memory, the only prac-","chapter-8","File Processing and External Sorting"
"tical way to sort it is to read some records from disk, do some rearranging, then","chapter-8","File Processing and External Sorting"
"write them back to disk. This process is repeated until the file is sorted, with each","chapter-8","File Processing and External Sorting"
"record read perhaps many times. Given the high cost of disk I/O, it should come as","chapter-8","File Processing and External Sorting"
"no surprise that the primary goal of an external sorting algorithm is to minimize the","chapter-8","File Processing and External Sorting"
"number of times information must be read from or written to disk. A certain amount","chapter-8","File Processing and External Sorting"
"of additional CPU processing can profitably be traded for reduced disk access.","chapter-8","File Processing and External Sorting"
"Before discussing external sorting techniques, consider again the basic model","chapter-8","File Processing and External Sorting"
"for accessing information from disk. The file to be sorted is viewed by the program-","chapter-8","File Processing and External Sorting"
"mer as a sequential series of fixed-size blocks. Assume (for simplicity) that each","chapter-8","File Processing and External Sorting"
"block contains the same number of fixed-size data records. Depending on the ap-","chapter-8","File Processing and External Sorting"
"plication, a record might be only a few bytes — composed of little or nothing more","chapter-8","File Processing and External Sorting"
"than the key — or might be hundreds of bytes with a relatively small key field.","chapter-8","File Processing and External Sorting"
"Records are assumed not to cross block boundaries. These assumptions can be","chapter-8","File Processing and External Sorting"
"284 Chap. 8 File Processing and External Sorting","chapter-8","File Processing and External Sorting"
"relaxed for special-purpose sorting applications, but ignoring such complications","chapter-8","File Processing and External Sorting"
"makes the principles clearer.","chapter-8","File Processing and External Sorting"
"As explained in Section 8.2, a sector is the basic unit of I/O. In other words,","chapter-8","File Processing and External Sorting"
"all disk reads and writes are for one or more complete sectors. Sector sizes are","chapter-8","File Processing and External Sorting"
"typically a power of two, in the range 512 to 16K bytes, depending on the operating","chapter-8","File Processing and External Sorting"
"system and the size and speed of the disk drive. The block size used for external","chapter-8","File Processing and External Sorting"
"sorting algorithms should be equal to or a multiple of the sector size.","chapter-8","File Processing and External Sorting"
"Under this model, a sorting algorithm reads a block of data into a buffer in main","chapter-8","File Processing and External Sorting"
"memory, performs some processing on it, and at some future time writes it back to","chapter-8","File Processing and External Sorting"
"disk. From Section 8.1 we see that reading or writing a block from disk takes on","chapter-8","File Processing and External Sorting"
"the order of one million times longer than a memory access. Based on this fact, we","chapter-8","File Processing and External Sorting"
"can reasonably expect that the records contained in a single block can be sorted by","chapter-8","File Processing and External Sorting"
"an internal sorting algorithm such as Quicksort in less time than is required to read","chapter-8","File Processing and External Sorting"
"or write the block.","chapter-8","File Processing and External Sorting"
"Under good conditions, reading from a file in sequential order is more efficient","chapter-8","File Processing and External Sorting"
"than reading blocks in random order. Given the significant impact of seek time on","chapter-8","File Processing and External Sorting"
"disk access, it might seem obvious that sequential processing is faster. However,","chapter-8","File Processing and External Sorting"
"it is important to understand precisely under what circumstances sequential file","chapter-8","File Processing and External Sorting"
"processing is actually faster than random access, because it affects our approach to","chapter-8","File Processing and External Sorting"
"designing an external sorting algorithm.","chapter-8","File Processing and External Sorting"
"Efficient sequential access relies on seek time being kept to a minimum. The","chapter-8","File Processing and External Sorting"
"first requirement is that the blocks making up a file are in fact stored on disk in","chapter-8","File Processing and External Sorting"
"sequential order and close together, preferably filling a small number of contiguous","chapter-8","File Processing and External Sorting"
"tracks. At the very least, the number of extents making up the file should be small.","chapter-8","File Processing and External Sorting"
"Users typically do not have much control over the layout of their file on disk, but","chapter-8","File Processing and External Sorting"
"writing a file all at once in sequential order to a disk drive with a high percentage","chapter-8","File Processing and External Sorting"
"of free space increases the likelihood of such an arrangement.","chapter-8","File Processing and External Sorting"
"The second requirement is that the disk drive’s I/O head remain positioned","chapter-8","File Processing and External Sorting"
"over the file throughout sequential processing. This will not happen if there is","chapter-8","File Processing and External Sorting"
"competition of any kind for the I/O head. For example, on a multi-user time-shared","chapter-8","File Processing and External Sorting"
"computer the sorting process might compete for the I/O head with the processes","chapter-8","File Processing and External Sorting"
"of other users. Even when the sorting process has sole control of the I/O head, it","chapter-8","File Processing and External Sorting"
"is still likely that sequential processing will not be efficient. Imagine the situation","chapter-8","File Processing and External Sorting"
"where all processing is done on a single disk drive, with the typical arrangement","chapter-8","File Processing and External Sorting"
"of a single bank of read/write heads that move together over a stack of platters. If","chapter-8","File Processing and External Sorting"
"the sorting process involves reading from an input file, alternated with writing to an","chapter-8","File Processing and External Sorting"
"output file, then the I/O head will continuously seek between the input file and the","chapter-8","File Processing and External Sorting"
"output file. Similarly, if two input files are being processed simultaneously (such","chapter-8","File Processing and External Sorting"
"as during a merge process), then the I/O head will continuously seek between these","chapter-8","File Processing and External Sorting"
"two files.","chapter-8","File Processing and External Sorting"
"Sec. 8.5 External Sorting 285","chapter-8","File Processing and External Sorting"
"The moral is that, with a single disk drive, there often is no such thing as effi-","chapter-8","File Processing and External Sorting"
"cient sequential processing of a data file. Thus, a sorting algorithm might be more","chapter-8","File Processing and External Sorting"
"efficient if it performs a smaller number of non-sequential disk operations rather","chapter-8","File Processing and External Sorting"
"than a larger number of logically sequential disk operations that require a large","chapter-8","File Processing and External Sorting"
"number of seeks in practice.","chapter-8","File Processing and External Sorting"
"As mentioned previously, the record size might be quite large compared to the","chapter-8","File Processing and External Sorting"
"size of the key. For example, payroll entries for a large business might each store","chapter-8","File Processing and External Sorting"
"hundreds of bytes of information including the name, ID, address, and job title for","chapter-8","File Processing and External Sorting"
"each employee. The sort key might be the ID number, requiring only a few bytes.","chapter-8","File Processing and External Sorting"
"The simplest sorting algorithm might be to process such records as a whole, reading","chapter-8","File Processing and External Sorting"
"the entire record whenever it is processed. However, this will greatly increase the","chapter-8","File Processing and External Sorting"
"amount of I/O required, because only a relatively few records will fit into a single","chapter-8","File Processing and External Sorting"
"disk block. Another alternative is to do a key sort. Under this method, the keys are","chapter-8","File Processing and External Sorting"
"all read and stored together in an index file, where each key is stored along with a","chapter-8","File Processing and External Sorting"
"pointer indicating the position of the corresponding record in the original data file.","chapter-8","File Processing and External Sorting"
"The key and pointer combination should be substantially smaller than the size of","chapter-8","File Processing and External Sorting"
"the original record; thus, the index file will be much smaller than the complete data","chapter-8","File Processing and External Sorting"
"file. The index file will then be sorted, requiring much less I/O because the index","chapter-8","File Processing and External Sorting"
"records are smaller than the complete records.","chapter-8","File Processing and External Sorting"
"Once the index file is sorted, it is possible to reorder the records in the original","chapter-8","File Processing and External Sorting"
"database file. This is typically not done for two reasons. First, reading the records","chapter-8","File Processing and External Sorting"
"in sorted order from the record file requires a random access for each record. This","chapter-8","File Processing and External Sorting"
"can take a substantial amount of time and is only of value if the complete collection","chapter-8","File Processing and External Sorting"
"of records needs to be viewed or processed in sorted order (as opposed to a search","chapter-8","File Processing and External Sorting"
"for selected records). Second, database systems typically allow searches to be done","chapter-8","File Processing and External Sorting"
"on multiple keys. For example, today’s processing might be done in order of ID","chapter-8","File Processing and External Sorting"
"numbers. Tomorrow, the boss might want information sorted by salary. Thus, there","chapter-8","File Processing and External Sorting"
"might be no single “sorted” order for the full record. Instead, multiple index files","chapter-8","File Processing and External Sorting"
"are often maintained, one for each sort key. These ideas are explored further in","chapter-8","File Processing and External Sorting"
"Chapter 10.","chapter-8","File Processing and External Sorting"
"8.5.1 Simple Approaches to External Sorting","chapter-8","File Processing and External Sorting"
"If your operating system supports virtual memory, the simplest “external” sort is","chapter-8","File Processing and External Sorting"
"to read the entire file into virtual memory and run an internal sorting method such","chapter-8","File Processing and External Sorting"
"as Quicksort. This approach allows the virtual memory manager to use its normal","chapter-8","File Processing and External Sorting"
"buffer pool mechanism to control disk accesses. Unfortunately, this might not al-","chapter-8","File Processing and External Sorting"
"ways be a viable option. One potential drawback is that the size of virtual memory","chapter-8","File Processing and External Sorting"
"is usually limited to something much smaller than the disk space available. Thus,","chapter-8","File Processing and External Sorting"
"your input file might not fit into virtual memory. Limited virtual memory can be","chapter-8","File Processing and External Sorting"
"overcome by adapting an internal sorting method to make use of your own buffer","chapter-8","File Processing and External Sorting"
"pool.","chapter-8","File Processing and External Sorting"
"286 Chap. 8 File Processing and External Sorting","chapter-8","File Processing and External Sorting"
"Runs of length 2 Runs of length 4","chapter-8","File Processing and External Sorting"
"36","chapter-8","File Processing and External Sorting"
"15 23","chapter-8","File Processing and External Sorting"
"20","chapter-8","File Processing and External Sorting"
"13","chapter-8","File Processing and External Sorting"
"14","chapter-8","File Processing and External Sorting"
"15","chapter-8","File Processing and External Sorting"
"36 17 28 23","chapter-8","File Processing and External Sorting"
"20 13 14 14","chapter-8","File Processing and External Sorting"
"13","chapter-8","File Processing and External Sorting"
"Runs of length 1","chapter-8","File Processing and External Sorting"
"15","chapter-8","File Processing and External Sorting"
"36 28","chapter-8","File Processing and External Sorting"
"17 23","chapter-8","File Processing and External Sorting"
"17 20","chapter-8","File Processing and External Sorting"
"28","chapter-8","File Processing and External Sorting"
"Figure 8.6 A simple external Mergesort algorithm. Input records are divided","chapter-8","File Processing and External Sorting"
"equally between two input files. The first runs from each input file are merged and","chapter-8","File Processing and External Sorting"
"placed into the first output file. The second runs from each input file are merged","chapter-8","File Processing and External Sorting"
"and placed in the second output file. Merging alternates between the two output","chapter-8","File Processing and External Sorting"
"files until the input files are empty. The roles of input and output files are then","chapter-8","File Processing and External Sorting"
"reversed, allowing the runlength to be doubled with each pass.","chapter-8","File Processing and External Sorting"
"A more general problem with adapting an internal sorting algorithm to exter-","chapter-8","File Processing and External Sorting"
"nal sorting is that it is not likely to be as efficient as designing a new algorithm","chapter-8","File Processing and External Sorting"
"with the specific goal of minimizing disk I/O. Consider the simple adaptation of","chapter-8","File Processing and External Sorting"
"Quicksort to use a buffer pool. Quicksort begins by processing the entire array of","chapter-8","File Processing and External Sorting"
"records, with the first partition step moving indices inward from the two ends. This","chapter-8","File Processing and External Sorting"
"can be implemented efficiently using a buffer pool. However, the next step is to","chapter-8","File Processing and External Sorting"
"process each of the subarrays, followed by processing of sub-subarrays, and so on.","chapter-8","File Processing and External Sorting"
"As the subarrays get smaller, processing quickly approaches random access to the","chapter-8","File Processing and External Sorting"
"disk drive. Even with maximum use of the buffer pool, Quicksort still must read","chapter-8","File Processing and External Sorting"
"and write each record log n times on average. We can do much better. Finally,","chapter-8","File Processing and External Sorting"
"even if the virtual memory manager can give good performance using a standard","chapter-8","File Processing and External Sorting"
"Quicksort, this will come at the cost of using a lot of the system’s working mem-","chapter-8","File Processing and External Sorting"
"ory, which will mean that the system cannot use this space for other work. Better","chapter-8","File Processing and External Sorting"
"methods can save time while also using less memory.","chapter-8","File Processing and External Sorting"
"Our approach to external sorting is derived from the Mergesort algorithm. The","chapter-8","File Processing and External Sorting"
"simplest form of external Mergesort performs a series of sequential passes over","chapter-8","File Processing and External Sorting"
"the records, merging larger and larger sublists on each pass. The first pass merges","chapter-8","File Processing and External Sorting"
"sublists of size 1 into sublists of size 2; the second pass merges the sublists of size","chapter-8","File Processing and External Sorting"
"2 into sublists of size 4; and so on. A sorted sublist is called a run. Thus, each pass","chapter-8","File Processing and External Sorting"
"is merging pairs of runs to form longer runs. Each pass copies the contents of the","chapter-8","File Processing and External Sorting"
"file to another file. Here is a sketch of the algorithm, as illustrated by Figure 8.6.","chapter-8","File Processing and External Sorting"
"1. Split the original file into two equal-sized run files.","chapter-8","File Processing and External Sorting"
"2. Read one block from each run file into input buffers.","chapter-8","File Processing and External Sorting"
"3. Take the first record from each input buffer, and write a run of length two to","chapter-8","File Processing and External Sorting"
"an output buffer in sorted order.","chapter-8","File Processing and External Sorting"
"4. Take the next record from each input buffer, and write a run of length two to","chapter-8","File Processing and External Sorting"
"a second output buffer in sorted order.","chapter-8","File Processing and External Sorting"
"5. Repeat until finished, alternating output between the two output run buffers.","chapter-8","File Processing and External Sorting"
"Whenever the end of an input block is reached, read the next block from the","chapter-8","File Processing and External Sorting"
"Sec. 8.5 External Sorting 287","chapter-8","File Processing and External Sorting"
"appropriate input file. When an output buffer is full, write it to the appropriate","chapter-8","File Processing and External Sorting"
"output file.","chapter-8","File Processing and External Sorting"
"6. Repeat steps 2 through 5, using the original output files as input files. On the","chapter-8","File Processing and External Sorting"
"second pass, the first two records of each input run file are already in sorted","chapter-8","File Processing and External Sorting"
"order. Thus, these two runs may be merged and output as a single run of four","chapter-8","File Processing and External Sorting"
"elements.","chapter-8","File Processing and External Sorting"
"7. Each pass through the run files provides larger and larger runs until only one","chapter-8","File Processing and External Sorting"
"run remains.","chapter-8","File Processing and External Sorting"
"Example 8.6 Using the input of Figure 8.6, we first create runs of length","chapter-8","File Processing and External Sorting"
"one split between two input files. We then process these two input files","chapter-8","File Processing and External Sorting"
"sequentially, making runs of length two. The first run has the values 20 and","chapter-8","File Processing and External Sorting"
"36, which are output to the first output file. The next run has 13 and 17,","chapter-8","File Processing and External Sorting"
"which is output to the second file. The run 14, 28 is sent to the first file,","chapter-8","File Processing and External Sorting"
"then run 15, 23 is sent to the second file, and so on. Once this pass has","chapter-8","File Processing and External Sorting"
"completed, the roles of the input files and output files are reversed. The","chapter-8","File Processing and External Sorting"
"next pass will merge runs of length two into runs of length four. Runs 20,","chapter-8","File Processing and External Sorting"
"36 and 13, 17 are merged to send 13, 17, 20, 36 to the first output file. Then","chapter-8","File Processing and External Sorting"
"runs 14, 28 and 15, 23 are merged to send run 14, 15, 23, 28 to the second","chapter-8","File Processing and External Sorting"
"output file. In the final pass, these runs are merged to form the final run 13,","chapter-8","File Processing and External Sorting"
"14, 15, 17, 20, 23, 28, 36.","chapter-8","File Processing and External Sorting"
"This algorithm can easily take advantage of the double buffering techniques","chapter-8","File Processing and External Sorting"
"described in Section 8.3. Note that the various passes read the input run files se-","chapter-8","File Processing and External Sorting"
"quentially and write the output run files sequentially. For sequential processing and","chapter-8","File Processing and External Sorting"
"double buffering to be effective, however, it is necessary that there be a separate","chapter-8","File Processing and External Sorting"
"I/O head available for each file. This typically means that each of the input and","chapter-8","File Processing and External Sorting"
"output files must be on separate disk drives, requiring a total of four disk drives for","chapter-8","File Processing and External Sorting"
"maximum efficiency.","chapter-8","File Processing and External Sorting"
"The external Mergesort algorithm just described requires that log n passes be","chapter-8","File Processing and External Sorting"
"made to sort a file of n records. Thus, each record must be read from disk and","chapter-8","File Processing and External Sorting"
"written to disk log n times. The number of passes can be significantly reduced by","chapter-8","File Processing and External Sorting"
"observing that it is not necessary to use Mergesort on small runs. A simple modi-","chapter-8","File Processing and External Sorting"
"fication is to read in a block of data, sort it in memory (perhaps using Quicksort),","chapter-8","File Processing and External Sorting"
"and then output it as a single sorted run.","chapter-8","File Processing and External Sorting"
"Example 8.7 Assume that we have blocks of size 4KB, and records are","chapter-8","File Processing and External Sorting"
"eight bytes with four bytes of data and a 4-byte key. Thus, each block con-","chapter-8","File Processing and External Sorting"
"tains 512 records. Standard Mergesort would require nine passes to gener-","chapter-8","File Processing and External Sorting"
"ate runs of 512 records, whereas processing each block as a unit can be done","chapter-8","File Processing and External Sorting"
"288 Chap. 8 File Processing and External Sorting","chapter-8","File Processing and External Sorting"
"in one pass with an internal sort. These runs can then be merged by Merge-","chapter-8","File Processing and External Sorting"
"sort. Standard Mergesort requires eighteen passes to process 256K records.","chapter-8","File Processing and External Sorting"
"Using an internal sort to create initial runs of 512 records reduces this to","chapter-8","File Processing and External Sorting"
"one initial pass to create the runs and nine merge passes to put them all","chapter-8","File Processing and External Sorting"
"together, approximately half as many passes.","chapter-8","File Processing and External Sorting"
"We can extend this concept to improve performance even further. Available","chapter-8","File Processing and External Sorting"
"main memory is usually much more than one block in size. If we process larger","chapter-8","File Processing and External Sorting"
"initial runs, then the number of passes required by Mergesort is further reduced. For","chapter-8","File Processing and External Sorting"
"example, most modern computers can provide tens or even hundreds of megabytes","chapter-8","File Processing and External Sorting"
"of RAM to the sorting program. If all of this memory (excepting a small amount for","chapter-8","File Processing and External Sorting"
"buffers and local variables) is devoted to building initial runs as large as possible,","chapter-8","File Processing and External Sorting"
"then quite large files can be processed in few passes. The next section presents a","chapter-8","File Processing and External Sorting"
"technique for producing large runs, typically twice as large as could fit directly into","chapter-8","File Processing and External Sorting"
"main memory.","chapter-8","File Processing and External Sorting"
"Another way to reduce the number of passes required is to increase the number","chapter-8","File Processing and External Sorting"
"of runs that are merged together during each pass. While the standard Mergesort","chapter-8","File Processing and External Sorting"
"algorithm merges two runs at a time, there is no reason why merging needs to be","chapter-8","File Processing and External Sorting"
"limited in this way. Section 8.5.3 discusses the technique of multiway merging.","chapter-8","File Processing and External Sorting"
"Over the years, many variants on external sorting have been presented, but all","chapter-8","File Processing and External Sorting"
"are based on the following two steps:","chapter-8","File Processing and External Sorting"
"1. Break the file into large initial runs.","chapter-8","File Processing and External Sorting"
"2. Merge the runs together to form a single sorted file.","chapter-8","File Processing and External Sorting"
"8.5.2 Replacement Selection","chapter-8","File Processing and External Sorting"
"This section treats the problem of creating initial runs as large as possible from a","chapter-8","File Processing and External Sorting"
"disk file, assuming a fixed amount of RAM is available for processing. As men-","chapter-8","File Processing and External Sorting"
"tioned previously, a simple approach is to allocate as much RAM as possible to a","chapter-8","File Processing and External Sorting"
"large array, fill this array from disk, and sort the array using Quicksort. Thus, if","chapter-8","File Processing and External Sorting"
"the size of memory available for the array is M records, then the input file can be","chapter-8","File Processing and External Sorting"
"broken into initial runs of length M. A better approach is to use an algorithm called","chapter-8","File Processing and External Sorting"
"replacement selection that, on average, creates runs of 2M records in length. Re-","chapter-8","File Processing and External Sorting"
"placement selection is actually a slight variation on the Heapsort algorithm. The","chapter-8","File Processing and External Sorting"
"fact that Heapsort is slower than Quicksort is irrelevant in this context because I/O","chapter-8","File Processing and External Sorting"
"time will dominate the total running time of any reasonable external sorting alg-","chapter-8","File Processing and External Sorting"
"orithm. Building longer initial runs will reduce the total I/O time required.","chapter-8","File Processing and External Sorting"
"Replacement selection views RAM as consisting of an array of size M in ad-","chapter-8","File Processing and External Sorting"
"dition to an input buffer and an output buffer. (Additional I/O buffers might be","chapter-8","File Processing and External Sorting"
"desirable if the operating system supports double buffering, because replacement","chapter-8","File Processing and External Sorting"
"selection does sequential processing on both its input and its output.) Imagine that","chapter-8","File Processing and External Sorting"
"Sec. 8.5 External Sorting 289","chapter-8","File Processing and External Sorting"
"File Input Buffer Output Buffer","chapter-8","File Processing and External Sorting"
"Input","chapter-8","File Processing and External Sorting"
"Run File","chapter-8","File Processing and External Sorting"
"Output","chapter-8","File Processing and External Sorting"
"RAM","chapter-8","File Processing and External Sorting"
"Figure 8.7 Overview of replacement selection. Input records are processed se-","chapter-8","File Processing and External Sorting"
"quentially. Initially RAM is filled with M records. As records are processed, they","chapter-8","File Processing and External Sorting"
"are written to an output buffer. When this buffer becomes full, it is written to disk.","chapter-8","File Processing and External Sorting"
"Meanwhile, as replacement selection needs records, it reads them from the input","chapter-8","File Processing and External Sorting"
"buffer. Whenever this buffer becomes empty, the next block of records is read","chapter-8","File Processing and External Sorting"
"from disk.","chapter-8","File Processing and External Sorting"
"the input and output files are streams of records. Replacement selection takes the","chapter-8","File Processing and External Sorting"
"next record in sequential order from the input stream when needed, and outputs","chapter-8","File Processing and External Sorting"
"runs one record at a time to the output stream. Buffering is used so that disk I/O is","chapter-8","File Processing and External Sorting"
"performed one block at a time. A block of records is initially read and held in the","chapter-8","File Processing and External Sorting"
"input buffer. Replacement selection removes records from the input buffer one at","chapter-8","File Processing and External Sorting"
"a time until the buffer is empty. At this point the next block of records is read in.","chapter-8","File Processing and External Sorting"
"Output to a buffer is similar: Once the buffer fills up it is written to disk as a unit.","chapter-8","File Processing and External Sorting"
"This process is illustrated by Figure 8.7.","chapter-8","File Processing and External Sorting"
"Replacement selection works as follows. Assume that the main processing is","chapter-8","File Processing and External Sorting"
"done in an array of size M records.","chapter-8","File Processing and External Sorting"
"1. Fill the array from disk. Set LAST = M − 1.","chapter-8","File Processing and External Sorting"
"2. Build a min-heap. (Recall that a min-heap is defined such that the record at","chapter-8","File Processing and External Sorting"
"each node has a key value less than the key values of its children.)","chapter-8","File Processing and External Sorting"
"3. Repeat until the array is empty:","chapter-8","File Processing and External Sorting"
"(a) Send the record with the minimum key value (the root) to the output","chapter-8","File Processing and External Sorting"
"buffer.","chapter-8","File Processing and External Sorting"
"(b) Let R be the next record in the input buffer. If R’s key value is greater","chapter-8","File Processing and External Sorting"
"than the key value just output ...","chapter-8","File Processing and External Sorting"
"i. Then place R at the root.","chapter-8","File Processing and External Sorting"
"ii. Else replace the root with the record in array position LAST, and","chapter-8","File Processing and External Sorting"
"place R at position LAST. Set LAST = LAST − 1.","chapter-8","File Processing and External Sorting"
"(c) Sift down the root to reorder the heap.","chapter-8","File Processing and External Sorting"
"When the test at step 3(b) is successful, a new record is added to the heap,","chapter-8","File Processing and External Sorting"
"eventually to be output as part of the run. As long as records coming from the input","chapter-8","File Processing and External Sorting"
"file have key values greater than the last key value output to the run, they can be","chapter-8","File Processing and External Sorting"
"safely added to the heap. Records with smaller key values cannot be output as part","chapter-8","File Processing and External Sorting"
"of the current run because they would not be in sorted order. Such values must be","chapter-8","File Processing and External Sorting"
"290 Chap. 8 File Processing and External Sorting","chapter-8","File Processing and External Sorting"
"stored somewhere for future processing as part of another run. However, because","chapter-8","File Processing and External Sorting"
"the heap will shrink by one element in this case, there is now a free space where the","chapter-8","File Processing and External Sorting"
"last element of the heap used to be! Thus, replacement selection will slowly shrink","chapter-8","File Processing and External Sorting"
"the heap and at the same time use the discarded heap space to store records for the","chapter-8","File Processing and External Sorting"
"next run. Once the first run is complete (i.e., the heap becomes empty), the array","chapter-8","File Processing and External Sorting"
"will be filled with records ready to be processed for the second run. Figure 8.8","chapter-8","File Processing and External Sorting"
"illustrates part of a run being created by replacement selection.","chapter-8","File Processing and External Sorting"
"It should be clear that the minimum length of a run will be M records if the size","chapter-8","File Processing and External Sorting"
"of the heap is M, because at least those records originally in the heap will be part of","chapter-8","File Processing and External Sorting"
"the run. Under good conditions (e.g., if the input is sorted), then an arbitrarily long","chapter-8","File Processing and External Sorting"
"run is possible. In fact, the entire file could be processed as one run. If conditions","chapter-8","File Processing and External Sorting"
"are bad (e.g., if the input is reverse sorted), then runs of only size M result.","chapter-8","File Processing and External Sorting"
"What is the expected length of a run generated by replacement selection? It","chapter-8","File Processing and External Sorting"
"can be deduced from an analogy called the snowplow argument. Imagine that a","chapter-8","File Processing and External Sorting"
"snowplow is going around a circular track during a heavy, but steady, snowstorm.","chapter-8","File Processing and External Sorting"
"After the plow has been around at least once, snow on the track must be as follows.","chapter-8","File Processing and External Sorting"
"Immediately behind the plow, the track is empty because it was just plowed. The","chapter-8","File Processing and External Sorting"
"greatest level of snow on the track is immediately in front of the plow, because","chapter-8","File Processing and External Sorting"
"this is the place least recently plowed. At any instant, there is a certain amount of","chapter-8","File Processing and External Sorting"
"snow S on the track. Snow is constantly falling throughout the track at a steady","chapter-8","File Processing and External Sorting"
"rate, with some snow falling “in front” of the plow and some “behind” the plow.","chapter-8","File Processing and External Sorting"
"(On a circular track, everything is actually “in front” of the plow, but Figure 8.9","chapter-8","File Processing and External Sorting"
"illustrates the idea.) During the next revolution of the plow, all snow S on the track","chapter-8","File Processing and External Sorting"
"is removed, plus half of what falls. Because everything is assumed to be in steady","chapter-8","File Processing and External Sorting"
"state, after one revolution S snow is still on the track, so 2S snow must fall during","chapter-8","File Processing and External Sorting"
"a revolution, and 2S snow is removed during a revolution (leaving S snow behind).","chapter-8","File Processing and External Sorting"
"At the beginning of replacement selection, nearly all values coming from the","chapter-8","File Processing and External Sorting"
"input file are greater (i.e., “in front of the plow”) than the latest key value output for","chapter-8","File Processing and External Sorting"
"this run, because the run’s initial key values should be small. As the run progresses,","chapter-8","File Processing and External Sorting"
"the latest key value output becomes greater and so new key values coming from the","chapter-8","File Processing and External Sorting"
"input file are more likely to be too small (i.e., “after the plow”); such records go to","chapter-8","File Processing and External Sorting"
"the bottom of the array. The total length of the run is expected to be twice the size of","chapter-8","File Processing and External Sorting"
"the array. Of course, this assumes that incoming key values are evenly distributed","chapter-8","File Processing and External Sorting"
"within the key range (in terms of the snowplow analogy, we assume that snow falls","chapter-8","File Processing and External Sorting"
"evenly throughout the track). Sorted and reverse sorted inputs do not meet this","chapter-8","File Processing and External Sorting"
"expectation and so change the length of the run.","chapter-8","File Processing and External Sorting"
"8.5.3 Multiway Merging","chapter-8","File Processing and External Sorting"
"The second stage of a typical external sorting algorithm merges the runs created by","chapter-8","File Processing and External Sorting"
"the first stage. Assume that we have R runs to merge. If a simple two-way merge","chapter-8","File Processing and External Sorting"
"is used, then R runs (regardless of their sizes) will require log R passes through","chapter-8","File Processing and External Sorting"
"Sec. 8.5 External Sorting 291","chapter-8","File Processing and External Sorting"
"Input Memory Output","chapter-8","File Processing and External Sorting"
"16 12","chapter-8","File Processing and External Sorting"
"29 16","chapter-8","File Processing and External Sorting"
"14 19","chapter-8","File Processing and External Sorting"
"21","chapter-8","File Processing and External Sorting"
"25 29 56","chapter-8","File Processing and External Sorting"
"31","chapter-8","File Processing and External Sorting"
"14","chapter-8","File Processing and External Sorting"
"35 25 31 21","chapter-8","File Processing and External Sorting"
"40 29 56","chapter-8","File Processing and External Sorting"
"21","chapter-8","File Processing and External Sorting"
"40","chapter-8","File Processing and External Sorting"
"25 21 56 40","chapter-8","File Processing and External Sorting"
"31","chapter-8","File Processing and External Sorting"
"29","chapter-8","File Processing and External Sorting"
"16","chapter-8","File Processing and External Sorting"
"12","chapter-8","File Processing and External Sorting"
"56 40","chapter-8","File Processing and External Sorting"
"31","chapter-8","File Processing and External Sorting"
"25","chapter-8","File Processing and External Sorting"
"19","chapter-8","File Processing and External Sorting"
"21","chapter-8","File Processing and External Sorting"
"25 21 56","chapter-8","File Processing and External Sorting"
"31","chapter-8","File Processing and External Sorting"
"40","chapter-8","File Processing and External Sorting"
"19","chapter-8","File Processing and External Sorting"
"19","chapter-8","File Processing and External Sorting"
"19","chapter-8","File Processing and External Sorting"
"21","chapter-8","File Processing and External Sorting"
"25","chapter-8","File Processing and External Sorting"
"31","chapter-8","File Processing and External Sorting"
"29 56 40","chapter-8","File Processing and External Sorting"
"14","chapter-8","File Processing and External Sorting"
"Figure 8.8 Replacement selection example. After building the heap, root","chapter-8","File Processing and External Sorting"
"value 12 is output and incoming value 16 replaces it. Value 16 is output next,","chapter-8","File Processing and External Sorting"
"replaced with incoming value 29. The heap is reordered, with 19 rising to the","chapter-8","File Processing and External Sorting"
"root. Value 19 is output next. Incoming value 14 is too small for this run and","chapter-8","File Processing and External Sorting"
"is placed at end of the array, moving value 40 to the root. Reordering the heap","chapter-8","File Processing and External Sorting"
"results in 21 rising to the root, which is output next.","chapter-8","File Processing and External Sorting"
"292 Chap. 8 File Processing and External Sorting","chapter-8","File Processing and External Sorting"
"Existing snow","chapter-8","File Processing and External Sorting"
"Future snow","chapter-8","File Processing and External Sorting"
"Falling Snow","chapter-8","File Processing and External Sorting"
"Snowplow Movement","chapter-8","File Processing and External Sorting"
"Start time T","chapter-8","File Processing and External Sorting"
"Figure 8.9 The snowplow analogy showing the action during one revolution of","chapter-8","File Processing and External Sorting"
"the snowplow. A circular track is laid out straight for purposes of illustration, and","chapter-8","File Processing and External Sorting"
"is shown in cross section. At any time T, the most snow is directly in front of","chapter-8","File Processing and External Sorting"
"the snowplow. As the plow moves around the track, the same amount of snow is","chapter-8","File Processing and External Sorting"
"always in front of the plow. As the plow moves forward, less of this is snow that","chapter-8","File Processing and External Sorting"
"was in the track at time T; more is snow that has fallen since.","chapter-8","File Processing and External Sorting"
"the file. While R should be much less than the total number of records (because","chapter-8","File Processing and External Sorting"
"the initial runs should each contain many records), we would like to reduce still","chapter-8","File Processing and External Sorting"
"further the number of passes required to merge the runs together. Note that two-","chapter-8","File Processing and External Sorting"
"way merging does not make good use of available memory. Because merging is a","chapter-8","File Processing and External Sorting"
"sequential process on the two runs, only one block of records per run need be in","chapter-8","File Processing and External Sorting"
"memory at a time. Keeping more than one block of a run in memory at any time","chapter-8","File Processing and External Sorting"
"will not reduce the disk I/O required by the merge process (though if several blocks","chapter-8","File Processing and External Sorting"
"are read from a file at once time, at least they take advantage of sequential access).","chapter-8","File Processing and External Sorting"
"Thus, most of the space just used by the heap for replacement selection (typically","chapter-8","File Processing and External Sorting"
"many blocks in length) is not being used by the merge process.","chapter-8","File Processing and External Sorting"
"We can make better use of this space and at the same time greatly reduce the","chapter-8","File Processing and External Sorting"
"number of passes needed to merge the runs if we merge several runs at a time.","chapter-8","File Processing and External Sorting"
"Multiway merging is similar to two-way merging. If we have B runs to merge,","chapter-8","File Processing and External Sorting"
"with a block from each run available in memory, then the B-way merge algorithm","chapter-8","File Processing and External Sorting"
"simply looks at B values (the front-most value for each input run) and selects the","chapter-8","File Processing and External Sorting"
"smallest one to output. This value is removed from its run, and the process is","chapter-8","File Processing and External Sorting"
"repeated. When the current block for any run is exhausted, the next block from that","chapter-8","File Processing and External Sorting"
"run is read from disk. Figure 8.10 illustrates a multiway merge.","chapter-8","File Processing and External Sorting"
"Conceptually, multiway merge assumes that each run is stored in a separate file.","chapter-8","File Processing and External Sorting"
"However, this is not necessary in practice. We only need to know the position of","chapter-8","File Processing and External Sorting"
"each run within a single file, and use seek to move to the appropriate block when-","chapter-8","File Processing and External Sorting"
"ever we need new data from a particular run. Naturally, this approach destroys the","chapter-8","File Processing and External Sorting"
"ability to do sequential processing on the input file. However, if all runs were stored","chapter-8","File Processing and External Sorting"
"on a single disk drive, then processing would not be truly sequential anyway be-","chapter-8","File Processing and External Sorting"
"cause the I/O head would be alternating between the runs. Thus, multiway merging","chapter-8","File Processing and External Sorting"
"replaces several (potentially) sequential passes with a single random access pass. If","chapter-8","File Processing and External Sorting"
"Sec. 8.5 External Sorting 293","chapter-8","File Processing and External Sorting"
"Input Runs","chapter-8","File Processing and External Sorting"
"12 18 20 ...","chapter-8","File Processing and External Sorting"
"6 7 23 ...","chapter-8","File Processing and External Sorting"
"5 10 15 ...","chapter-8","File Processing and External Sorting"
"5 6 7 10 12 ...","chapter-8","File Processing and External Sorting"
"Output Buffer","chapter-8","File Processing and External Sorting"
"Figure 8.10 Illustration of multiway merge. The first value in each input run","chapter-8","File Processing and External Sorting"
"is examined and the smallest sent to the output. This value is removed from the","chapter-8","File Processing and External Sorting"
"input and the process repeated. In this example, values 5, 6, and 12 are compared","chapter-8","File Processing and External Sorting"
"first. Value 5 is removed from the first run and sent to the output. Values 10, 6,","chapter-8","File Processing and External Sorting"
"and 12 will be compared next. After the first five values have been output, the","chapter-8","File Processing and External Sorting"
"“current” value of each block is the one underlined.","chapter-8","File Processing and External Sorting"
"the processing would not be sequential anyway (such as when all processing is on","chapter-8","File Processing and External Sorting"
"a single disk drive), no time is lost by doing so.","chapter-8","File Processing and External Sorting"
"Multiway merging can greatly reduce the number of passes required. If there","chapter-8","File Processing and External Sorting"
"is room in memory to store one block for each run, then all runs can be merged","chapter-8","File Processing and External Sorting"
"in a single pass. Thus, replacement selection can build initial runs in one pass,","chapter-8","File Processing and External Sorting"
"and multiway merging can merge all runs in one pass, yielding a total cost of two","chapter-8","File Processing and External Sorting"
"passes. However, for truly large files, there might be too many runs for each to get","chapter-8","File Processing and External Sorting"
"a block in memory. If there is room to allocate B blocks for a B-way merge, and","chapter-8","File Processing and External Sorting"
"the number of runs R is greater than B, then it will be necessary to do multiple","chapter-8","File Processing and External Sorting"
"merge passes. In other words, the first B runs are merged, then the next B, and","chapter-8","File Processing and External Sorting"
"so on. These super-runs are then merged by subsequent passes, B super-runs at a","chapter-8","File Processing and External Sorting"
"time.","chapter-8","File Processing and External Sorting"
"How big a file can be merged in one pass? Assuming B blocks were allocated to","chapter-8","File Processing and External Sorting"
"the heap for replacement selection (resulting in runs of average length 2B blocks),","chapter-8","File Processing and External Sorting"
"followed by a B-way merge, we can process on average a file of size 2B2 blocks","chapter-8","File Processing and External Sorting"
"in a single multiway merge. 2Bk+1 blocks on average can be processed in k B-","chapter-8","File Processing and External Sorting"
"way merges. To gain some appreciation for how quickly this grows, assume that","chapter-8","File Processing and External Sorting"
"we have available 0.5MB of working memory, and that a block is 4KB, yielding","chapter-8","File Processing and External Sorting"
"128 blocks in working memory. The average run size is 1MB (twice the working","chapter-8","File Processing and External Sorting"
"memory size). In one pass, 128 runs can be merged. Thus, a file of size 128MB","chapter-8","File Processing and External Sorting"
"can, on average, be processed in two passes (one to build the runs, one to do the","chapter-8","File Processing and External Sorting"
"merge) with only 0.5MB of working memory. As another example, assume blocks","chapter-8","File Processing and External Sorting"
"are 1KB long and working memory is 1MB = 1024 blocks. Then 1024 runs of","chapter-8","File Processing and External Sorting"
"average length 2MB (which is about 2GB) can be combined in a single merge","chapter-8","File Processing and External Sorting"
"pass. A larger block size would reduce the size of the file that can be processed","chapter-8","File Processing and External Sorting"
"294 Chap. 8 File Processing and External Sorting","chapter-8","File Processing and External Sorting"
"File Sort 1 Sort 2 Sort 3","chapter-8","File Processing and External Sorting"
"Size Memory size (in blocks) Memory size (in blocks)","chapter-8","File Processing and External Sorting"
"(Mb) 2 4 16 256 2 4 16","chapter-8","File Processing and External Sorting"
"1 0.61 0.27 0.24 0.19 0.10 0.21 0.15 0.13","chapter-8","File Processing and External Sorting"
"4,864 2,048 1,792 1,280 256 2,048 1,024 512","chapter-8","File Processing and External Sorting"
"4 2.56 1.30 1.19 0.96 0.61 1.15 0.68 0.66*","chapter-8","File Processing and External Sorting"
"21,504 10,240 9,216 7,168 3,072 10,240 5,120 2,048","chapter-8","File Processing and External Sorting"
"16 11.28 6.12 5.63 4.78 3.36 5.42 3.19 3.10","chapter-8","File Processing and External Sorting"
"94,208 49,152 45,056 36,864 20,480 49,152 24,516 12,288","chapter-8","File Processing and External Sorting"
"256 220.39 132.47 123.68 110.01 86.66 115.73 69.31 68.71","chapter-8","File Processing and External Sorting"
"1,769K 1,048K 983K 852K 589K 1,049K 524K 262K","chapter-8","File Processing and External Sorting"
"Figure 8.11 A comparison of three external sorts on a collection of small records","chapter-8","File Processing and External Sorting"
"for files of various sizes. Each entry in the table shows time in seconds and total","chapter-8","File Processing and External Sorting"
"number of blocks read and written by the program. File sizes are in Megabytes.","chapter-8","File Processing and External Sorting"
"For the third sorting algorithm, on a file size of 4MB, the time and blocks shown","chapter-8","File Processing and External Sorting"
"in the last column are for a 32-way merge (marked with an asterisk). 32 is used","chapter-8","File Processing and External Sorting"
"instead of 16 because 32 is a root of the number of blocks in the file (while 16 is","chapter-8","File Processing and External Sorting"
"not), thus allowing the same number of runs to be merged at every pass.","chapter-8","File Processing and External Sorting"
"in one merge pass for a fixed-size working memory; a smaller block size or larger","chapter-8","File Processing and External Sorting"
"working memory would increase the file size that can be processed in one merge","chapter-8","File Processing and External Sorting"
"pass. Two merge passes allow much bigger files to be processed. With 0.5MB of","chapter-8","File Processing and External Sorting"
"working memory and 4KB blocks, a file of size 16 gigabytes could be processed in","chapter-8","File Processing and External Sorting"
"two merge passes, which is big enough for most applications. Thus, this is a very","chapter-8","File Processing and External Sorting"
"effective algorithm for single disk drive external sorting.","chapter-8","File Processing and External Sorting"
"Figure 8.11 shows a comparison of the running time to sort various-sized files","chapter-8","File Processing and External Sorting"
"for the following implementations: (1) standard Mergesort with two input runs and","chapter-8","File Processing and External Sorting"
"two output runs, (2) two-way Mergesort with large initial runs (limited by the size","chapter-8","File Processing and External Sorting"
"of available memory), and (3) R-way Mergesort performed after generating large","chapter-8","File Processing and External Sorting"
"initial runs. In each case, the file was composed of a series of four-byte records","chapter-8","File Processing and External Sorting"
"(a two-byte key and a two-byte data value), or 256K records per megabyte of file","chapter-8","File Processing and External Sorting"
"size. We can see from this table that using even a modest memory size (two blocks)","chapter-8","File Processing and External Sorting"
"to create initial runs results in a tremendous savings in time. Doing 4-way merges","chapter-8","File Processing and External Sorting"
"of the runs provides another considerable speedup, however large-scale multi-way","chapter-8","File Processing and External Sorting"
"merges for R beyond about 4 or 8 runs does not help much because a lot of time is","chapter-8","File Processing and External Sorting"
"spent determining which is the next smallest element among the R runs.","chapter-8","File Processing and External Sorting"
"We see from this experiment that building large initial runs reduces the running","chapter-8","File Processing and External Sorting"
"time to slightly more than one third that of standard Mergesort, depending on file","chapter-8","File Processing and External Sorting"
"and memory sizes. Using a multiway merge further cuts the time nearly in half.","chapter-8","File Processing and External Sorting"
"In summary, a good external sorting algorithm will seek to do the following:","chapter-8","File Processing and External Sorting"
"• Make the initial runs as long as possible.","chapter-8","File Processing and External Sorting"
"• At all stages, overlap input, processing, and output as much as possible.","chapter-8","File Processing and External Sorting"
"Sec. 8.6 Further Reading 295","chapter-8","File Processing and External Sorting"
"• Use as much working memory as possible. Applying more memory usually","chapter-8","File Processing and External Sorting"
"speeds processing. In fact, more memory will have a greater effect than a","chapter-8","File Processing and External Sorting"
"faster disk. A faster CPU is unlikely to yield much improvement in running","chapter-8","File Processing and External Sorting"
"time for external sorting, because disk I/O speed is the limiting factor.","chapter-8","File Processing and External Sorting"
"• If possible, use additional disk drives for more overlapping of processing","chapter-8","File Processing and External Sorting"
"with I/O, and to allow for sequential file processing.","chapter-8","File Processing and External Sorting"
"8.6 Further Reading","chapter-8","File Processing and External Sorting"
"A good general text on file processing is Folk and Zoellick’s File Structures: A","chapter-8","File Processing and External Sorting"
"Conceptual Toolkit [FZ98]. A somewhat more advanced discussion on key issues in","chapter-8","File Processing and External Sorting"
"file processing is Betty Salzberg’s File Structures: An Analytical Approach [Sal88].","chapter-8","File Processing and External Sorting"
"A great discussion on external sorting methods can be found in Salzberg’s book.","chapter-8","File Processing and External Sorting"
"The presentation in this chapter is similar in spirit to Salzberg’s.","chapter-8","File Processing and External Sorting"
"For details on disk drive modeling and measurement, see the article by Ruemm-","chapter-8","File Processing and External Sorting"
"ler and Wilkes, “An Introduction to Disk Drive Modeling” [RW94]. See Andrew","chapter-8","File Processing and External Sorting"
"S. Tanenbaum’s Structured Computer Organization [Tan06] for an introduction to","chapter-8","File Processing and External Sorting"
"computer hardware and organization. An excellent, detailed description of mem-","chapter-8","File Processing and External Sorting"
"ory and hard disk drives can be found online at “The PC Guide,” by Charles M.","chapter-8","File Processing and External Sorting"
"Kozierok [Koz05] (www.pcguide.com). The PC Guide also gives detailed de-","chapter-8","File Processing and External Sorting"
"scriptions of the Microsoft Windows and UNIX (Linux) file systems.","chapter-8","File Processing and External Sorting"
"See “Outperforming LRU with an Adaptive Replacement Cache Algorithm”","chapter-8","File Processing and External Sorting"
"by Megiddo and Modha [MM04] for an example of a more sophisticated algorithm","chapter-8","File Processing and External Sorting"
"than LRU for managing buffer pools.","chapter-8","File Processing and External Sorting"
"The snowplow argument comes from Donald E. Knuth’s Sorting and Searching","chapter-8","File Processing and External Sorting"
"[Knu98], which also contains a wide variety of external sorting algorithms.","chapter-8","File Processing and External Sorting"
"8.7 Exercises","chapter-8","File Processing and External Sorting"
"8.1 Computer memory and storage prices change rapidly. Find out what the","chapter-8","File Processing and External Sorting"
"current prices are for the media listed in Figure 8.1. Does your information","chapter-8","File Processing and External Sorting"
"change any of the basic conclusions regarding disk processing?","chapter-8","File Processing and External Sorting"
"8.2 Assume a disk drive from the late 1990s is configured as follows. The to-","chapter-8","File Processing and External Sorting"
"tal storage is approximately 675MB divided among 15 surfaces. Each sur-","chapter-8","File Processing and External Sorting"
"face has 612 tracks; there are 144 sectors/track, 512 bytes/sector, and 8 sec-","chapter-8","File Processing and External Sorting"
"tors/cluster. The disk turns at 3600 rpm. The track-to-track seek time is","chapter-8","File Processing and External Sorting"
"20 ms, and the average seek time is 80 ms. Now assume that there is a","chapter-8","File Processing and External Sorting"
"360KB file on the disk. On average, how long does it take to read all of the","chapter-8","File Processing and External Sorting"
"data in the file? Assume that the first track of the file is randomly placed on","chapter-8","File Processing and External Sorting"
"the disk, that the entire file lies on adjacent tracks, and that the file completely","chapter-8","File Processing and External Sorting"
"fills each track on which it is found. A seek must be performed each time the","chapter-8","File Processing and External Sorting"
"I/O head moves to a new track. Show your calculations.","chapter-8","File Processing and External Sorting"
"296 Chap. 8 File Processing and External Sorting","chapter-8","File Processing and External Sorting"
"8.3 Using the specifications for the disk drive given in Exercise 8.2, calculate the","chapter-8","File Processing and External Sorting"
"expected time to read one entire track, one sector, and one byte. Show your","chapter-8","File Processing and External Sorting"
"calculations.","chapter-8","File Processing and External Sorting"
"8.4 Using the disk drive specifications given in Exercise 8.2, calculate the time","chapter-8","File Processing and External Sorting"
"required to read a 10MB file assuming","chapter-8","File Processing and External Sorting"
"(a) The file is stored on a series of contiguous tracks, as few tracks as pos-","chapter-8","File Processing and External Sorting"
"sible.","chapter-8","File Processing and External Sorting"
"(b) The file is spread randomly across the disk in 4KB clusters.","chapter-8","File Processing and External Sorting"
"Show your calculations.","chapter-8","File Processing and External Sorting"
"8.5 Assume that a disk drive is configured as follows. The total storage is ap-","chapter-8","File Processing and External Sorting"
"proximately 1033MB divided among 15 surfaces. Each surface has 2100","chapter-8","File Processing and External Sorting"
"tracks, there are 64 sectors/track, 512 bytes/sector, and 8 sectors/cluster. The","chapter-8","File Processing and External Sorting"
"disk turns at 7200 rpm. The track-to-track seek time is 3 ms, and the average","chapter-8","File Processing and External Sorting"
"seek time is 20 ms. Now assume that there is a 512KB file on the disk. On","chapter-8","File Processing and External Sorting"
"average, how long does it take to read all of the data on the file? Assume that","chapter-8","File Processing and External Sorting"
"the first track of the file is randomly placed on the disk, that the entire file lies","chapter-8","File Processing and External Sorting"
"on contiguous tracks, and that the file completely fills each track on which it","chapter-8","File Processing and External Sorting"
"is found. Show your calculations.","chapter-8","File Processing and External Sorting"
"8.6 Using the specifications for the disk drive given in Exercise 8.5, calculate the","chapter-8","File Processing and External Sorting"
"expected time to read one entire track, one sector, and one byte. Show your","chapter-8","File Processing and External Sorting"
"calculations.","chapter-8","File Processing and External Sorting"
"8.7 Using the disk drive specifications given in Exercise 8.5, calculate the time","chapter-8","File Processing and External Sorting"
"required to read a 10MB file assuming","chapter-8","File Processing and External Sorting"
"(a) The file is stored on a series of contiguous tracks, as few tracks as pos-","chapter-8","File Processing and External Sorting"
"sible.","chapter-8","File Processing and External Sorting"
"(b) The file is spread randomly across the disk in 4KB clusters.","chapter-8","File Processing and External Sorting"
"Show your calculations.","chapter-8","File Processing and External Sorting"
"8.8 A typical disk drive from 2004 has the following specifications.3 The total","chapter-8","File Processing and External Sorting"
"storage is approximately 120GB on 6 platter surfaces or 20GB/platter. Each","chapter-8","File Processing and External Sorting"
"platter has 16K tracks with 2560 sectors/track (a sector holds 512 bytes) and","chapter-8","File Processing and External Sorting"
"16 sectors/cluster. The disk turns at 7200 rpm. The track-to-track seek time","chapter-8","File Processing and External Sorting"
"is 2.0 ms, and the average seek time is 10.0 ms. Now assume that there is a","chapter-8","File Processing and External Sorting"
"6MB file on the disk. On average, how long does it take to read all of the data","chapter-8","File Processing and External Sorting"
"on the file? Assume that the first track of the file is randomly placed on the","chapter-8","File Processing and External Sorting"
"disk, that the entire file lies on contiguous tracks, and that the file completely","chapter-8","File Processing and External Sorting"
"fills each track on which it is found. Show your calculations.","chapter-8","File Processing and External Sorting"
"3To make the exercise doable, this specification is completely fictitious with respect to the track","chapter-8","File Processing and External Sorting"
"and sector layout. While sectors do have 512 bytes, and while the number of platters and amount of","chapter-8","File Processing and External Sorting"
"data per track is plausible, the reality is that all modern drives use a zoned organization to keep the","chapter-8","File Processing and External Sorting"
"data density from inside to outside of the disk reasonably high. The rest of the numbers are typical","chapter-8","File Processing and External Sorting"
"for a drive from 2004.","chapter-8","File Processing and External Sorting"
"Sec. 8.7 Exercises 297","chapter-8","File Processing and External Sorting"
"8.9 Using the specifications for the disk drive given in Exercise 8.8, calculate the","chapter-8","File Processing and External Sorting"
"expected time to read one entire track, one sector, and one byte. Show your","chapter-8","File Processing and External Sorting"
"calculations.","chapter-8","File Processing and External Sorting"
"8.10 Using the disk drive specifications given in Exercise 8.8, calculate the time","chapter-8","File Processing and External Sorting"
"required to read a 10MB file assuming","chapter-8","File Processing and External Sorting"
"(a) The file is stored on a series of contiguous tracks, as few tracks as pos-","chapter-8","File Processing and External Sorting"
"sible.","chapter-8","File Processing and External Sorting"
"(b) The file is spread randomly across the disk in 8KB clusters.","chapter-8","File Processing and External Sorting"
"Show your calculations.","chapter-8","File Processing and External Sorting"
"8.11 At the end of 2004, the fastest disk drive I could find specifications for was","chapter-8","File Processing and External Sorting"
"the Maxtor Atlas. This drive had a nominal capacity of 73.4GB using 4 plat-","chapter-8","File Processing and External Sorting"
"ters (8 surfaces) or 9.175GB/surface. Assume there are 16,384 tracks with an","chapter-8","File Processing and External Sorting"
"average of 1170 sectors/track and 512 bytes/sector.4 The disk turns at 15,000","chapter-8","File Processing and External Sorting"
"rpm. The track-to-track seek time is 0.4 ms and the average seek time is 3.6","chapter-8","File Processing and External Sorting"
"ms. How long will it take on average to read a 6MB file, assuming that the","chapter-8","File Processing and External Sorting"
"first track of the file is randomly placed on the disk, that the entire file lies on","chapter-8","File Processing and External Sorting"
"contiguous tracks, and that the file completely fills each track on which it is","chapter-8","File Processing and External Sorting"
"found. Show your calculations.","chapter-8","File Processing and External Sorting"
"8.12 Using the specifications for the disk drive given in Exercise 8.11, calculate","chapter-8","File Processing and External Sorting"
"the expected time to read one entire track, one sector, and one byte. Show","chapter-8","File Processing and External Sorting"
"your calculations.","chapter-8","File Processing and External Sorting"
"8.13 Using the disk drive specifications given in Exercise 8.11, calculate the time","chapter-8","File Processing and External Sorting"
"required to read a 10MB file assuming","chapter-8","File Processing and External Sorting"
"(a) The file is stored on a series of contiguous tracks, as few tracks as pos-","chapter-8","File Processing and External Sorting"
"sible.","chapter-8","File Processing and External Sorting"
"(b) The file is spread randomly across the disk in 8KB clusters.","chapter-8","File Processing and External Sorting"
"Show your calculations.","chapter-8","File Processing and External Sorting"
"8.14 Prove that two tracks selected at random from a disk are separated on average","chapter-8","File Processing and External Sorting"
"by one third the number of tracks on the disk.","chapter-8","File Processing and External Sorting"
"8.15 Assume that a file contains one million records sorted by key value. A query","chapter-8","File Processing and External Sorting"
"to the file returns a single record containing the requested key value. Files","chapter-8","File Processing and External Sorting"
"are stored on disk in sectors each containing 100 records. Assume that the","chapter-8","File Processing and External Sorting"
"average time to read a sector selected at random is 10.0 ms. In contrast, it","chapter-8","File Processing and External Sorting"
"takes only 2.0 ms to read the sector adjacent to the current position of the I/O","chapter-8","File Processing and External Sorting"
"head. The “batch” algorithm for processing queries is to first sort the queries","chapter-8","File Processing and External Sorting"
"by order of appearance in the file, and then read the entire file sequentially,","chapter-8","File Processing and External Sorting"
"processing all queries in sequential order as the file is read. This algorithm","chapter-8","File Processing and External Sorting"
"implies that the queries must all be available before processing begins. The","chapter-8","File Processing and External Sorting"
"“interactive” algorithm is to process each query in order of its arrival, search-","chapter-8","File Processing and External Sorting"
"ing for the requested sector each time (unless by chance two queries in a row","chapter-8","File Processing and External Sorting"
"4Again, this track layout does does not account for the zoned arrangement on modern disk drives.","chapter-8","File Processing and External Sorting"
"298 Chap. 8 File Processing and External Sorting","chapter-8","File Processing and External Sorting"
"are to the same sector). Carefully define under what conditions the batch","chapter-8","File Processing and External Sorting"
"method is more efficient than the interactive method.","chapter-8","File Processing and External Sorting"
"8.16 Assume that a virtual memory is managed using a buffer pool. The buffer","chapter-8","File Processing and External Sorting"
"pool contains five buffers and each buffer stores one block of data. Memory","chapter-8","File Processing and External Sorting"
"accesses are by block ID. Assume the following series of memory accesses","chapter-8","File Processing and External Sorting"
"takes place:","chapter-8","File Processing and External Sorting"
"5 2 5 12 3 6 5 9 3 2 4 1 5 9 8 15 3 7 2 5 9 10 4 6 8 5","chapter-8","File Processing and External Sorting"
"For each of the following buffer pool replacement strategies, show the con-","chapter-8","File Processing and External Sorting"
"tents of the buffer pool at the end of the series, and indicate how many times","chapter-8","File Processing and External Sorting"
"a block was found in the buffer pool (instead of being read into memory).","chapter-8","File Processing and External Sorting"
"Assume that the buffer pool is initially empty.","chapter-8","File Processing and External Sorting"
"(a) First-in, first out.","chapter-8","File Processing and External Sorting"
"(b) Least frequently used (with counts kept only for blocks currently in","chapter-8","File Processing and External Sorting"
"memory, counts for a page are lost when that page is removed, and the","chapter-8","File Processing and External Sorting"
"oldest item with the smallest count is removed when there is a tie).","chapter-8","File Processing and External Sorting"
"(c) Least frequently used (with counts kept for all blocks, and the oldest","chapter-8","File Processing and External Sorting"
"item with the smallest count is removed when there is a tie).","chapter-8","File Processing and External Sorting"
"(d) Least recently used.","chapter-8","File Processing and External Sorting"
"(e) Most recently used (replace the block that was most recently accessed).","chapter-8","File Processing and External Sorting"
"8.17 Suppose that a record is 32 bytes, a block is 1024 bytes (thus, there are","chapter-8","File Processing and External Sorting"
"32 records per block), and that working memory is 1MB (there is also addi-","chapter-8","File Processing and External Sorting"
"tional space available for I/O buffers, program variables, etc.). What is the","chapter-8","File Processing and External Sorting"
"expected size for the largest file that can be merged using replacement selec-","chapter-8","File Processing and External Sorting"
"tion followed by a single pass of multiway merge? Explain how you got your","chapter-8","File Processing and External Sorting"
"answer.","chapter-8","File Processing and External Sorting"
"8.18 Assume that working memory size is 256KB broken into blocks of 8192","chapter-8","File Processing and External Sorting"
"bytes (there is also additional space available for I/O buffers, program vari-","chapter-8","File Processing and External Sorting"
"ables, etc.). What is the expected size for the largest file that can be merged","chapter-8","File Processing and External Sorting"
"using replacement selection followed by two passes of multiway merge? Ex-","chapter-8","File Processing and External Sorting"
"plain how you got your answer.","chapter-8","File Processing and External Sorting"
"8.19 Prove or disprove the following proposition: Given space in memory for a","chapter-8","File Processing and External Sorting"
"heap of M records, replacement selection will completely sort a file if no","chapter-8","File Processing and External Sorting"
"record in the file is preceded by M or more keys of greater value.","chapter-8","File Processing and External Sorting"
"8.20 Imagine a database containing ten million records, with each record being","chapter-8","File Processing and External Sorting"
"100 bytes long. Provide an estimate of the time it would take (in seconds) to","chapter-8","File Processing and External Sorting"
"sort the database on a typical desktop or laptop computer.","chapter-8","File Processing and External Sorting"
"8.21 Assume that a company has a computer configuration satisfactory for pro-","chapter-8","File Processing and External Sorting"
"cessing their monthly payroll. Further assume that the bottleneck in payroll","chapter-8","File Processing and External Sorting"
"processing is a sorting operation on all of the employee records, and that","chapter-8","File Processing and External Sorting"
"Sec. 8.8 Projects 299","chapter-8","File Processing and External Sorting"
"an external sorting algorithm is used. The company’s payroll program is so","chapter-8","File Processing and External Sorting"
"good that it plans to hire out its services to do payroll processing for other","chapter-8","File Processing and External Sorting"
"companies. The president has an offer from a second company with 100","chapter-8","File Processing and External Sorting"
"times as many employees. She realizes that her computer is not up to the","chapter-8","File Processing and External Sorting"
"job of sorting 100 times as many records in an acceptable amount of time.","chapter-8","File Processing and External Sorting"
"Describe what impact each of the following modifications to the computing","chapter-8","File Processing and External Sorting"
"system is likely to have in terms of reducing the time required to process the","chapter-8","File Processing and External Sorting"
"larger payroll database.","chapter-8","File Processing and External Sorting"
"(a) A factor of two speedup to the CPU.","chapter-8","File Processing and External Sorting"
"(b) A factor of two speedup to disk I/O time.","chapter-8","File Processing and External Sorting"
"(c) A factor of two speedup to main memory access time.","chapter-8","File Processing and External Sorting"
"(d) A factor of two increase to main memory size.","chapter-8","File Processing and External Sorting"
"8.22 How can the external sorting algorithm described in this chapter be extended","chapter-8","File Processing and External Sorting"
"to handle variable-length records?","chapter-8","File Processing and External Sorting"
"8.8 Projects","chapter-8","File Processing and External Sorting"
"8.1 For a database application, assume it takes 10 ms to read a block from disk,","chapter-8","File Processing and External Sorting"
"1 ms to search for a record in a block stored in memory, and that there is","chapter-8","File Processing and External Sorting"
"room in memory for a buffer pool of 5 blocks. Requests come in for records,","chapter-8","File Processing and External Sorting"
"with the request specifying which block contains the record. If a block is","chapter-8","File Processing and External Sorting"
"accessed, there is a 10% probability for each of the next ten requests that the","chapter-8","File Processing and External Sorting"
"request will be to the same block. What will be the expected performance","chapter-8","File Processing and External Sorting"
"improvement for each of the following modifications to the system?","chapter-8","File Processing and External Sorting"
"(a) Get a CPU that is twice as fast.","chapter-8","File Processing and External Sorting"
"(b) Get a disk drive that is twice as fast.","chapter-8","File Processing and External Sorting"
"(c) Get enough memory to double the buffer pool size.","chapter-8","File Processing and External Sorting"
"Write a simulation to analyze this problem.","chapter-8","File Processing and External Sorting"
"8.2 Pictures are typically stored as an array, row by row, on disk. Consider the","chapter-8","File Processing and External Sorting"
"case where the picture has 16 colors. Thus, each pixel can be represented us-","chapter-8","File Processing and External Sorting"
"ing 4 bits. If you allow 8 bits per pixel, no processing is required to unpack","chapter-8","File Processing and External Sorting"
"the pixels (because a pixel corresponds to a byte, the lowest level of address-","chapter-8","File Processing and External Sorting"
"ing on most machines). If you pack two pixels per byte, space is saved but","chapter-8","File Processing and External Sorting"
"the pixels must be unpacked. Which takes more time to read from disk and","chapter-8","File Processing and External Sorting"
"access every pixel of the image: 8 bits per pixel, or 4 bits per pixel with","chapter-8","File Processing and External Sorting"
"2 pixels per byte? Program both and compare the times.","chapter-8","File Processing and External Sorting"
"8.3 Implement a disk-based buffer pool class based on the LRU buffer pool re-","chapter-8","File Processing and External Sorting"
"placement strategy. Disk blocks are numbered consecutively from the begin-","chapter-8","File Processing and External Sorting"
"ning of the file with the first block numbered as 0. Assume that blocks are","chapter-8","File Processing and External Sorting"
"300 Chap. 8 File Processing and External Sorting","chapter-8","File Processing and External Sorting"
"4096 bytes in size, with the first 4 bytes used to store the block ID corre-","chapter-8","File Processing and External Sorting"
"sponding to that buffer. Use the first BufferPool abstract class given in","chapter-8","File Processing and External Sorting"
"Section 8.3 as the basis for your implementation.","chapter-8","File Processing and External Sorting"
"8.4 Implement an external sort based on replacement selection and multiway","chapter-8","File Processing and External Sorting"
"merging as described in this chapter. Test your program both on files with","chapter-8","File Processing and External Sorting"
"small records and on files with large records. For what size record do you","chapter-8","File Processing and External Sorting"
"find that key sorting would be worthwhile?","chapter-8","File Processing and External Sorting"
"8.5 Implement a Quicksort for large files on disk by replacing all array access in","chapter-8","File Processing and External Sorting"
"the normal Quicksort application with access to a virtual array implemented","chapter-8","File Processing and External Sorting"
"using a buffer pool. That is, whenever a record in the array would be read or","chapter-8","File Processing and External Sorting"
"written by Quicksort, use a call to a buffer pool function instead. Compare","chapter-8","File Processing and External Sorting"
"the running time of this implementation with implementations for external","chapter-8","File Processing and External Sorting"
"sorting based on mergesort as described in this chapter.","chapter-8","File Processing and External Sorting"
"8.6 Section 8.5.1 suggests that an easy modification to the basic 2-way mergesort","chapter-8","File Processing and External Sorting"
"is to read in a large chunk of data into main memory, sort it with Quicksort,","chapter-8","File Processing and External Sorting"
"and write it out for initial runs. Then, a standard 2-way merge is used in","chapter-8","File Processing and External Sorting"
"a series of passes to merge the runs together. However, this makes use of","chapter-8","File Processing and External Sorting"
"only two blocks of working memory at a time. Each block read is essentially","chapter-8","File Processing and External Sorting"
"random access, because the various files are read in an unknown order, even","chapter-8","File Processing and External Sorting"
"though each of the input and output files is processed sequentially on each","chapter-8","File Processing and External Sorting"
"pass. A possible improvement would be, on the merge passes, to divide","chapter-8","File Processing and External Sorting"
"working memory into four equal sections. One section is allocated to each","chapter-8","File Processing and External Sorting"
"of the two input files and two output files. All reads during merge passes","chapter-8","File Processing and External Sorting"
"would be in full sections, rather than single blocks. While the total number","chapter-8","File Processing and External Sorting"
"of blocks read and written would be the same as a regular 2-way Mergesort, it","chapter-8","File Processing and External Sorting"
"is possible that this would speed processing because a series of blocks that are","chapter-8","File Processing and External Sorting"
"logically adjacent in the various input and output files would be read/written","chapter-8","File Processing and External Sorting"
"each time. Implement this variation, and compare its running time against","chapter-8","File Processing and External Sorting"
"a standard series of 2-way merge passes that read/write only a single block","chapter-8","File Processing and External Sorting"
"at a time. Before beginning implementation, write down your hypothesis on","chapter-8","File Processing and External Sorting"
"how the running time will be affected by this change. After implementing,","chapter-8","File Processing and External Sorting"
"did you find that this change has any meaningful effect on performance?","chapter-8","File Processing and External Sorting"
"Organizing and retrieving information is at the heart of most computer applica-","chapter-9","Searching"
"tions, and searching is surely the most frequently performed of all computing tasks.","chapter-9","Searching"
"Search can be viewed abstractly as a process to determine if an element with a par-","chapter-9","Searching"
"ticular value is a member of a particular set. The more common view of searching","chapter-9","Searching"
"is an attempt to find the record within a collection of records that has a particular","chapter-9","Searching"
"key value, or those records in a collection whose key values meet some criterion","chapter-9","Searching"
"such as falling within a range of values.","chapter-9","Searching"
"We can define searching formally as follows. Suppose that we have a collection","chapter-9","Searching"
"L of n records of the form","chapter-9","Searching"
"(k1, I1),(k2, I2), ...,(kn, In)","chapter-9","Searching"
"where Ij is information associated with key kj from record j for 1 ≤ j ≤ n. Given","chapter-9","Searching"
"a particular key value K, the search problem is to locate a record (kj , Ij ) in L","chapter-9","Searching"
"such that kj = K (if one exists). Searching is a systematic method for locating the","chapter-9","Searching"
"record (or records) with key value kj = K.","chapter-9","Searching"
"A successful search is one in which a record with key kj = K is found. An","chapter-9","Searching"
"unsuccessful search is one in which no record with kj = K is found (and no such","chapter-9","Searching"
"record exists).","chapter-9","Searching"
"An exact-match query is a search for the record whose key value matches a","chapter-9","Searching"
"specified key value. A range query is a search for all records whose key value falls","chapter-9","Searching"
"within a specified range of key values.","chapter-9","Searching"
"We can categorize search algorithms into three general approaches:","chapter-9","Searching"
"1. Sequential and list methods.","chapter-9","Searching"
"2. Direct access by key value (hashing).","chapter-9","Searching"
"3. Tree indexing methods.","chapter-9","Searching"
"This and the following chapter treat these three approaches in turn. Any of","chapter-9","Searching"
"these approaches are potentially suitable for implementing the Dictionary ADT","chapter-9","Searching"
"301","chapter-9","Searching"
"302 Chap. 9 Searching","chapter-9","Searching"
"introduced in Section 4.4. However, each has different performance characteristics","chapter-9","Searching"
"that make it the method of choice in particular circumstances.","chapter-9","Searching"
"The current chapter considers methods for searching data stored in lists. List in","chapter-9","Searching"
"this context means any list implementation including a linked list or an array. Most","chapter-9","Searching"
"of these methods are appropriate for sequences (i.e., duplicate key values are al-","chapter-9","Searching"
"lowed), although special techniques applicable to sets are discussed in Section 9.3.","chapter-9","Searching"
"The techniques from the first three sections of this chapter are most appropriate for","chapter-9","Searching"
"searching a collection of records stored in RAM. Section 9.4 discusses hashing,","chapter-9","Searching"
"a technique for organizing data in an array such that the location of each record","chapter-9","Searching"
"within the array is a function of its key value. Hashing is appropriate when records","chapter-9","Searching"
"are stored either in RAM or on disk.","chapter-9","Searching"
"Chapter 10 discusses tree-based methods for organizing information on disk,","chapter-9","Searching"
"including a commonly used file structure called the B-tree. Nearly all programs that","chapter-9","Searching"
"must organize large collections of records stored on disk use some variant of either","chapter-9","Searching"
"hashing or the B-tree. Hashing is practical for only certain access functions (exact-","chapter-9","Searching"
"match queries) and is generally appropriate only when duplicate key values are","chapter-9","Searching"
"not allowed. B-trees are the method of choice for dynamic disk-based applications","chapter-9","Searching"
"anytime hashing is not appropriate.","chapter-9","Searching"
"9.1 Searching Unsorted and Sorted Arrays","chapter-9","Searching"
"The simplest form of search has already been presented in Example 3.1: the se-","chapter-9","Searching"
"quential search algorithm. Sequential search on an unsorted list requires Θ(n) time","chapter-9","Searching"
"in the worst case.","chapter-9","Searching"
"How many comparisons does linear search do on average? A major consid-","chapter-9","Searching"
"eration is whether K is in list L at all. We can simplify our analysis by ignoring","chapter-9","Searching"
"everything about the input except the position of K if it is found in L. Thus, we have","chapter-9","Searching"
"n + 1 distinct possible events: That K is in one of positions 0 to n − 1 in L (each","chapter-9","Searching"
"position having its own probability), or that it is not in L at all. We can express the","chapter-9","Searching"
"probability that K is not in L as","chapter-9","Searching"
"P(K ∈/ L) = 1 −","chapter-9","Searching"
"Xn","chapter-9","Searching"
"i=1","chapter-9","Searching"
"P(K = L[i])","chapter-9","Searching"
"where P(x) is the probability of event x.","chapter-9","Searching"
"Let pi be the probability that K is in position i of L (indexed from 0 to n − 1.","chapter-9","Searching"
"For any position i in the list, we must look at i + 1 records to reach it. So we say","chapter-9","Searching"
"that the cost when K is in position i is i + 1. When K is not in L, sequential search","chapter-9","Searching"
"will require n comparisons. Let pn be the probability that K is not in L. Then the","chapter-9","Searching"
"average cost T(n) will be","chapter-9","Searching"
"Sec. 9.1 Searching Unsorted and Sorted Arrays 303","chapter-9","Searching"
"T(n) = npn +","chapter-9","Searching"
"nX−1","chapter-9","Searching"
"i=0","chapter-9","Searching"
"(i + 1)pi","chapter-9","Searching"
".","chapter-9","Searching"
"What happens to the equation if we assume all the pi’s are equal (except p0)?","chapter-9","Searching"
"T(n) = pnn +","chapter-9","Searching"
"nX−1","chapter-9","Searching"
"i=0","chapter-9","Searching"
"(i + 1)p","chapter-9","Searching"
"= pnn + p","chapter-9","Searching"
"Xn","chapter-9","Searching"
"i=1","chapter-9","Searching"
"i","chapter-9","Searching"
"= pnn + p","chapter-9","Searching"
"n(n + 1)","chapter-9","Searching"
"2","chapter-9","Searching"
"= pnn +","chapter-9","Searching"
"1 − pn","chapter-9","Searching"
"n","chapter-9","Searching"
"n(n + 1)","chapter-9","Searching"
"2","chapter-9","Searching"
"=","chapter-9","Searching"
"n + 1 + pn(n − 1)","chapter-9","Searching"
"2","chapter-9","Searching"
"Depending on the value of pn,","chapter-9","Searching"
"n+1","chapter-9","Searching"
"2 ≤ T(n) ≤ n.","chapter-9","Searching"
"For large collections of records that are searched repeatedly, sequential search","chapter-9","Searching"
"is unacceptably slow. One way to reduce search time is to preprocess the records","chapter-9","Searching"
"by sorting them. Given a sorted array, an obvious improvement over simple linear","chapter-9","Searching"
"search is to test if the current element in L is greater than K. If it is, then we know","chapter-9","Searching"
"that K cannot appear later in the array, and we can quit the search early. But this","chapter-9","Searching"
"still does not improve the worst-case cost of the algorithm.","chapter-9","Searching"
"We can also observe that if we look first at position 1 in sorted array L and find","chapter-9","Searching"
"that K is bigger, then we rule out position 0 as well as position 1. Because more","chapter-9","Searching"
"is often better, what if we look at position 2 in L and find that K is bigger yet?","chapter-9","Searching"
"This rules out positions 0, 1, and 2 with one comparison. What if we carry this to","chapter-9","Searching"
"the extreme and look first at the last position in L and find that K is bigger? Then","chapter-9","Searching"
"we know in one comparison that K is not in L. This is very useful to know, but","chapter-9","Searching"
"what is wrong with the conclusion that we should always start by looking at the last","chapter-9","Searching"
"position? The problem is that, while we learn a lot sometimes (in one comparison","chapter-9","Searching"
"we might learn that K is not in the list), usually we learn only a little bit (that the","chapter-9","Searching"
"last element is not K).","chapter-9","Searching"
"The question then becomes: What is the right amount to jump? This leads us","chapter-9","Searching"
"to an algorithm known as Jump Search. For some value j, we check every j’th","chapter-9","Searching"
"element in L, that is, we check elements L[j], L[2j], and so on. So long as K is","chapter-9","Searching"
"greater than the values we are checking, we continue on. But when we reach a","chapter-9","Searching"
"304 Chap. 9 Searching","chapter-9","Searching"
"value in L greater than K, we do a linear search on the piece of length j − 1 that","chapter-9","Searching"
"we know brackets K if it is in the list.","chapter-9","Searching"
"If we define m such that mj ≤ n < (m + 1)j, then the total cost of this","chapter-9","Searching"
"algorithm is at most m + j − 1 3-way comparisons. (They are 3-way because at","chapter-9","Searching"
"each comparison of K with some L[i] we need to know if K is less than, equal to,","chapter-9","Searching"
"or greater than L[i].) Therefore, the cost to run the algorithm on n items with a","chapter-9","Searching"
"jump of size j is","chapter-9","Searching"
"T(n, j) = m + j − 1 = ","chapter-9","Searching"
"n","chapter-9","Searching"
"j","chapter-9","Searching"
"+ j − 1.","chapter-9","Searching"
"What is the best value that we can pick for j? We want to minimize the cost:","chapter-9","Searching"
"min","chapter-9","Searching"
"1≤j≤n","chapter-9","Searching"
"n","chapter-9","Searching"
"j","chapter-9","Searching"
"+ j − 1","chapter-9","Searching"
"Take the derivative and solve for f","chapter-9","Searching"
"0","chapter-9","Searching"
"(j) = 0 to find the minimum, which is","chapter-9","Searching"
"j =","chapter-9","Searching"
"√","chapter-9","Searching"
"n. In this case, the worst case cost will be roughly 2","chapter-9","Searching"
"√","chapter-9","Searching"
"n.","chapter-9","Searching"
"This example invokes a basic principle of algorithm design. We want to bal-","chapter-9","Searching"
"ance the work done while selecting a sublist with the work done while searching a","chapter-9","Searching"
"sublist. In general, it is a good strategy to make subproblems of equal effort. This","chapter-9","Searching"
"is an example of a divide and conquer algorithm.","chapter-9","Searching"
"What if we extend this idea to three levels? We would first make jumps of","chapter-9","Searching"
"some size j to find a sublist of size j − 1 whose end values bracket value K. We","chapter-9","Searching"
"would then work through this sublist by making jumps of some smaller size, say","chapter-9","Searching"
"j1. Finally, once we find a bracketed sublist of size j1 − 1, we would do sequential","chapter-9","Searching"
"search to complete the process.","chapter-9","Searching"
"This probably sounds convoluted to do two levels of jumping to be followed by","chapter-9","Searching"
"a sequential search. While it might make sense to do a two-level algorithm (that is,","chapter-9","Searching"
"jump search jumps to find a sublist and then does sequential search on the sublist),","chapter-9","Searching"
"it almost never seems to make sense to do a three-level algorithm. Instead, when","chapter-9","Searching"
"we go beyond two levels, we nearly always generalize by using recursion. This","chapter-9","Searching"
"leads us to the most commonly used search algorithm for sorted arrays, the binary","chapter-9","Searching"
"search described in Section 3.5.","chapter-9","Searching"
"If we know nothing about the distribution of key values, then binary search","chapter-9","Searching"
"is the best algorithm available for searching a sorted array (see Exercise 9.22).","chapter-9","Searching"
"However, sometimes we do know something about the expected key distribution.","chapter-9","Searching"
"Consider the typical behavior of a person looking up a word in a large dictionary.","chapter-9","Searching"
"Most people certainly do not use sequential search! Typically, people use a mod-","chapter-9","Searching"
"ified form of binary search, at least until they get close to the word that they are","chapter-9","Searching"
"looking for. The search generally does not start at the middle of the dictionary. A","chapter-9","Searching"
"person looking for a word starting with ‘S’ generally assumes that entries beginning","chapter-9","Searching"
"with ‘S’ start about three quarters of the way through the dictionary. Thus, he or","chapter-9","Searching"
"Sec. 9.1 Searching Unsorted and Sorted Arrays 305","chapter-9","Searching"
"she will first open the dictionary about three quarters of the way through and then","chapter-9","Searching"
"make a decision based on what is found as to where to look next. In other words,","chapter-9","Searching"
"people typically use some knowledge about the expected distribution of key values","chapter-9","Searching"
"to “compute” where to look next. This form of “computed” binary search is called","chapter-9","Searching"
"a dictionary search or interpolation search. In a dictionary search, we search L","chapter-9","Searching"
"at a position p that is appropriate to the value of K as follows.","chapter-9","Searching"
"p =","chapter-9","Searching"
"K − L[1]","chapter-9","Searching"
"L[n] − L[1]","chapter-9","Searching"
"This equation is computing the position of K as a fraction of the distance be-","chapter-9","Searching"
"tween the smallest and largest key values. This will next be translated into that","chapter-9","Searching"
"position which is the same fraction of the way through the array, and this position","chapter-9","Searching"
"is checked first. As with binary search, the value of the key found eliminates all","chapter-9","Searching"
"records either above or below that position. The actual value of the key found can","chapter-9","Searching"
"then be used to compute a new position within the remaining range of the array.","chapter-9","Searching"
"The next check is made based on the new computation. This proceeds until either","chapter-9","Searching"
"the desired record is found, or the array is narrowed until no records are left.","chapter-9","Searching"
"A variation on dictionary search is known as Quadratic Binary Search (QBS),","chapter-9","Searching"
"and we will analyze this in detail because its analysis is easier than that of the","chapter-9","Searching"
"general dictionary search. QBS will first compute p and then examine L[dpne]. If","chapter-9","Searching"
"K < L[dpne] then QBS will sequentially probe to the left by steps of size √","chapter-9","Searching"
"n, that","chapter-9","Searching"
"is, we step through","chapter-9","Searching"
"L[dpn − i","chapter-9","Searching"
"√","chapter-9","Searching"
"ne], i = 1, 2, 3, ...","chapter-9","Searching"
"until we reach a value less than or equal to K. Similarly for K > L[dpne] we will","chapter-9","Searching"
"step to the right by √","chapter-9","Searching"
"n until we reach a value in L that is greater than K. We are","chapter-9","Searching"
"now within √","chapter-9","Searching"
"n positions of K. Assume (for now) that it takes a constant number of","chapter-9","Searching"
"comparisons to bracket K within a sublist of size √","chapter-9","Searching"
"n. We then take this sublist and","chapter-9","Searching"
"repeat the process recursively. That is, at the next level we compute an interpolation","chapter-9","Searching"
"to start somewhere in the subarray. We then step to the left or right (as appropriate)","chapter-9","Searching"
"by steps of size p√","chapter-9","Searching"
"n.","chapter-9","Searching"
"What is the cost for QBS? Note that √","chapter-9","Searching"
"c","chapter-9","Searching"
"n = c","chapter-9","Searching"
"n/2","chapter-9","Searching"
", and we will be repeatedly","chapter-9","Searching"
"taking square roots of the current sublist size until we find the item that we are","chapter-9","Searching"
"looking for. Because n = 2log n","chapter-9","Searching"
"and we can cut log n in half only log log n times,","chapter-9","Searching"
"the cost is Θ(log log n) if the number of probes on jump search is constant.","chapter-9","Searching"
"Say that the number of comparisons needed is i, in which case the cost is i","chapter-9","Searching"
"(since we have to do i comparisons). If Pi","chapter-9","Searching"
"is the probability of needing exactly i","chapter-9","Searching"
"probes, then","chapter-9","Searching"
"√","chapter-9","Searching"
"Xn","chapter-9","Searching"
"i=1","chapter-9","Searching"
"iP(need exactly i probes)","chapter-9","Searching"
"= 1P1 + 2P2 + 3P3 + · · · +","chapter-9","Searching"
"√","chapter-9","Searching"
"nP√","chapter-9","Searching"
"n","chapter-9","Searching"
"306 Chap. 9 Searching","chapter-9","Searching"
"We now show that this is the same as","chapter-9","Searching"
"√","chapter-9","Searching"
"Xn","chapter-9","Searching"
"i=1","chapter-9","Searching"
"P(need at least i probes)","chapter-9","Searching"
"= 1 + (1 − P1) + (1 − P1 − P2) + · · · + P√","chapter-9","Searching"
"n","chapter-9","Searching"
"= (P1 + ... + P√","chapter-9","Searching"
"n","chapter-9","Searching"
") + (P2 + ... + P√","chapter-9","Searching"
"n","chapter-9","Searching"
") +","chapter-9","Searching"
"(P3 + ... + P√","chapter-9","Searching"
"n","chapter-9","Searching"
") + · · ·","chapter-9","Searching"
"= 1P1 + 2P2 + 3P3 + · · · +","chapter-9","Searching"
"√","chapter-9","Searching"
"nP√","chapter-9","Searching"
"n","chapter-9","Searching"
"We require at least two probes to set the bounds, so the cost is","chapter-9","Searching"
"2 +","chapter-9","Searching"
"√","chapter-9","Searching"
"Xn","chapter-9","Searching"
"i=3","chapter-9","Searching"
"P(need at least i probes).","chapter-9","Searching"
"We now make take advantage of a useful fact known as Ceby ˇ sev’s Inequality. ˇ","chapter-9","Searching"
"Ceby ˇ sev’s inequality states that ˇ P(need exactly i probes), or Pi","chapter-9","Searching"
", is","chapter-9","Searching"
"Pi ≤","chapter-9","Searching"
"p(1 − p)n","chapter-9","Searching"
"(i − 2)2n","chapter-9","Searching"
"≤","chapter-9","Searching"
"1","chapter-9","Searching"
"4(i − 2)2","chapter-9","Searching"
"because p(1 − p) ≤ 1/4 for any probability p. This assumes uniformly distributed","chapter-9","Searching"
"data. Thus, the expected number of probes is","chapter-9","Searching"
"2 +","chapter-9","Searching"
"√","chapter-9","Searching"
"Xn","chapter-9","Searching"
"i=3","chapter-9","Searching"
"1","chapter-9","Searching"
"4(i − 2)2","chapter-9","Searching"
"< 2 +","chapter-9","Searching"
"1","chapter-9","Searching"
"4","chapter-9","Searching"
"X∞","chapter-9","Searching"
"i=1","chapter-9","Searching"
"1","chapter-9","Searching"
"i","chapter-9","Searching"
"2","chapter-9","Searching"
"= 2 +","chapter-9","Searching"
"1","chapter-9","Searching"
"4","chapter-9","Searching"
"π","chapter-9","Searching"
"6","chapter-9","Searching"
"≈ 2.4112","chapter-9","Searching"
"Is QBS better than binary search? Theoretically yes, because O(log log n)","chapter-9","Searching"
"grows slower than O(log n). However, we have a situation here which illustrates","chapter-9","Searching"
"the limits to the model of asymptotic complexity in some practical situations. Yes,","chapter-9","Searching"
"c1 log n does grow faster than c2 log log n. In fact, it is exponentially faster! But","chapter-9","Searching"
"even so, for practical input sizes, the absolute cost difference is fairly small. Thus,","chapter-9","Searching"
"the constant factors might play a role. First we compare lg lg n to lg n.","chapter-9","Searching"
"Factor","chapter-9","Searching"
"n lg n lg lg n Difference","chapter-9","Searching"
"16 4 2 2","chapter-9","Searching"
"256 8 3 2.7","chapter-9","Searching"
"2","chapter-9","Searching"
"16 16 4 4","chapter-9","Searching"
"2","chapter-9","Searching"
"32 32 5 6.4","chapter-9","Searching"
"Sec. 9.2 Self-Organizing Lists 307","chapter-9","Searching"
"It is not always practical to reduce an algorithm’s growth rate. There is a “prac-","chapter-9","Searching"
"ticality window” for every problem, in that we have a practical limit to how big an","chapter-9","Searching"
"input we wish to solve for. If our problem size never grows too big, it might not","chapter-9","Searching"
"matter if we can reduce the cost by an extra log factor, because the constant factors","chapter-9","Searching"
"in the two algorithms might differ by more than the log of the log of the input size.","chapter-9","Searching"
"For our two algorithms, let us look further and check the actual number of","chapter-9","Searching"
"comparisons used. For binary search, we need about log n − 1 total comparisons.","chapter-9","Searching"
"Quadratic binary search requires about 2.4 lg lg n comparisons. If we incorporate","chapter-9","Searching"
"this observation into our table, we get a different picture about the relative differ-","chapter-9","Searching"
"ences.","chapter-9","Searching"
"Factor","chapter-9","Searching"
"n lg n − 1 2.4 lg lg n Difference","chapter-9","Searching"
"16 3 4.8 worse","chapter-9","Searching"
"256 7 7.2 ≈ same","chapter-9","Searching"
"64K 15 9.6 1.6","chapter-9","Searching"
"2","chapter-9","Searching"
"32 31 12 2.6","chapter-9","Searching"
"But we still are not done. This is only a count of raw comparisons. Bi-","chapter-9","Searching"
"nary search is inherently much simpler than QBS, because binary search only","chapter-9","Searching"
"needs to calculate the midpoint position of the array before each comparison, while","chapter-9","Searching"
"quadratic binary search must calculate an interpolation point which is more expen-","chapter-9","Searching"
"sive. So the constant factors for QBS are even higher.","chapter-9","Searching"
"Not only are the constant factors worse on average, but QBS is far more depen-","chapter-9","Searching"
"dent than binary search on good data distribution to perform well. For example,","chapter-9","Searching"
"imagine that you are searching a telephone directory for the name “Young.” Nor-","chapter-9","Searching"
"mally you would look near the back of the book. If you found a name beginning","chapter-9","Searching"
"with ‘Z,’ you might look just a little ways toward the front. If the next name you","chapter-9","Searching"
"find also begins with ’Z,‘ you would look a little further toward the front. If this","chapter-9","Searching"
"particular telephone directory were unusual in that half of the entries begin with ‘Z,’","chapter-9","Searching"
"then you would need to move toward the front many times, each time eliminating","chapter-9","Searching"
"relatively few records from the search. In the extreme, the performance of interpo-","chapter-9","Searching"
"lation search might not be much better than sequential search if the distribution of","chapter-9","Searching"
"key values is badly calculated.","chapter-9","Searching"
"While it turns out that QBS is not a practical algorithm, this is not a typical","chapter-9","Searching"
"situation. Fortunately, algorithm growth rates are usually well behaved, so that as-","chapter-9","Searching"
"ymptotic algorithm analysis nearly always gives us a practical indication for which","chapter-9","Searching"
"of two algorithms is better.","chapter-9","Searching"
"9.2 Self-Organizing Lists","chapter-9","Searching"
"While ordering of lists is most commonly done by key value, this is not the only","chapter-9","Searching"
"viable option. Another approach to organizing lists to speed search is to order the","chapter-9","Searching"
"308 Chap. 9 Searching","chapter-9","Searching"
"records by expected frequency of access. While the benefits might not be as great","chapter-9","Searching"
"as when organized by key value, the cost to organize (at least approximately) by","chapter-9","Searching"
"frequency of access can be much cheaper, and thus can speed up sequential search","chapter-9","Searching"
"in some situations.","chapter-9","Searching"
"Assume that we know, for each key ki","chapter-9","Searching"
", the probability pi","chapter-9","Searching"
"that the record with","chapter-9","Searching"
"key ki will be requested. Assume also that the list is ordered so that the most","chapter-9","Searching"
"frequently requested record is first, then the next most frequently requested record,","chapter-9","Searching"
"and so on. Search in the list will be done sequentially, beginning with the first","chapter-9","Searching"
"position. Over the course of many searches, the expected number of comparisons","chapter-9","Searching"
"required for one search is","chapter-9","Searching"
"Cn = 1p0 + 2p1 + ... + npn−1.","chapter-9","Searching"
"In other words, the cost to access the record in L[0] is 1 (because one key value is","chapter-9","Searching"
"looked at), and the probability of this occurring is p0. The cost to access the record","chapter-9","Searching"
"in L[1] is 2 (because we must look at the first and the second records’ key values),","chapter-9","Searching"
"with probability p1, and so on. For n records, assuming that all searches are for","chapter-9","Searching"
"records that actually exist, the probabilities p0 through pn−1 must sum to one.","chapter-9","Searching"
"Certain probability distributions give easily computed results.","chapter-9","Searching"
"Example 9.1 Calculate the expected cost to search a list when each record","chapter-9","Searching"
"has equal chance of being accessed (the classic sequential search through","chapter-9","Searching"
"an unsorted list). Setting pi = 1/n yields","chapter-9","Searching"
"Cn =","chapter-9","Searching"
"Xn","chapter-9","Searching"
"i=1","chapter-9","Searching"
"i/n = (n + 1)/2.","chapter-9","Searching"
"This result matches our expectation that half the records will be accessed on","chapter-9","Searching"
"average by normal sequential search. If the records truly have equal access","chapter-9","Searching"
"probabilities, then ordering records by frequency yields no benefit. We saw","chapter-9","Searching"
"in Section 9.1 the more general case where we must consider the probability","chapter-9","Searching"
"(labeled pn) that the search key does not match that for any record in the","chapter-9","Searching"
"array. In that case, in accordance with our general formula, we get","chapter-9","Searching"
"(1−pn)","chapter-9","Searching"
"n + 1","chapter-9","Searching"
"2","chapter-9","Searching"
"+pnn =","chapter-9","Searching"
"n + 1 − npnn − pn + 2pn","chapter-9","Searching"
"2","chapter-9","Searching"
"=","chapter-9","Searching"
"n + 1 + p0(n − 1)","chapter-9","Searching"
"2","chapter-9","Searching"
".","chapter-9","Searching"
"Thus, n+1","chapter-9","Searching"
"2 ≤ Cn ≤ n, depending on the value of p0.","chapter-9","Searching"
"A geometric probability distribution can yield quite different results.","chapter-9","Searching"
"Sec. 9.2 Self-Organizing Lists 309","chapter-9","Searching"
"Example 9.2 Calculate the expected cost for searching a list ordered by","chapter-9","Searching"
"frequency when the probabilities are defined as","chapter-9","Searching"
"pi =","chapter-9","Searching"
"1/2","chapter-9","Searching"
"i","chapter-9","Searching"
"if 0 ≤ i ≤ n − 2","chapter-9","Searching"
"1/2","chapter-9","Searching"
"n","chapter-9","Searching"
"if i = n − 1.","chapter-9","Searching"
"Then,","chapter-9","Searching"
"Cn ≈","chapter-9","Searching"
"nX−1","chapter-9","Searching"
"i=0","chapter-9","Searching"
"(i + 1)/2","chapter-9","Searching"
"i+1 =","chapter-9","Searching"
"Xn","chapter-9","Searching"
"i=1","chapter-9","Searching"
"(i/2","chapter-9","Searching"
"i","chapter-9","Searching"
") ≈ 2.","chapter-9","Searching"
"For this example, the expected number of accesses is a constant. This is","chapter-9","Searching"
"because the probability for accessing the first record is high (one half), the","chapter-9","Searching"
"second is much lower (one quarter) but still much higher than for the third","chapter-9","Searching"
"record, and so on. This shows that for some probability distributions, or-","chapter-9","Searching"
"dering the list by frequency can yield an efficient search technique.","chapter-9","Searching"
"In many search applications, real access patterns follow a rule of thumb called","chapter-9","Searching"
"the 80/20 rule. The 80/20 rule says that 80% of the record accesses are to 20%","chapter-9","Searching"
"of the records. The values of 80 and 20 are only estimates; every data access pat-","chapter-9","Searching"
"tern has its own values. However, behavior of this nature occurs surprisingly often","chapter-9","Searching"
"in practice (which explains the success of caching techniques widely used by web","chapter-9","Searching"
"browsers for speeding access to web pages, and by disk drive and CPU manufac-","chapter-9","Searching"
"turers for speeding access to data stored in slower memory; see the discussion on","chapter-9","Searching"
"buffer pools in Section 8.3). When the 80/20 rule applies, we can expect consid-","chapter-9","Searching"
"erable improvements to search performance from a list ordered by frequency of","chapter-9","Searching"
"access over standard sequential search in an unordered list.","chapter-9","Searching"
"Example 9.3 The 80/20 rule is an example of a Zipf distribution. Nat-","chapter-9","Searching"
"urally occurring distributions often follow a Zipf distribution. Examples","chapter-9","Searching"
"include the observed frequency for the use of words in a natural language","chapter-9","Searching"
"such as English, and the size of the population for cities (i.e., view the","chapter-9","Searching"
"relative proportions for the populations as equivalent to the “frequency of","chapter-9","Searching"
"use”). Zipf distributions are related to the Harmonic Series defined in Equa-","chapter-9","Searching"
"tion 2.10. Define the Zipf frequency for item i in the distribution for n","chapter-9","Searching"
"records as 1/(iHn) (see Exercise 9.4). The expected cost for the series","chapter-9","Searching"
"whose members follow this Zipf distribution will be","chapter-9","Searching"
"Cn =","chapter-9","Searching"
"Xn","chapter-9","Searching"
"i=1","chapter-9","Searching"
"i/iHn = n/Hn ≈ n/ loge n.","chapter-9","Searching"
"When a frequency distribution follows the 80/20 rule, the average search","chapter-9","Searching"
"looks at about 10-15% of the records in a list ordered by frequency.","chapter-9","Searching"
"310 Chap. 9 Searching","chapter-9","Searching"
"This is potentially a useful observation that typical “real-life” distributions of","chapter-9","Searching"
"record accesses, if the records were ordered by frequency, would require that we","chapter-9","Searching"
"visit on average only 10-15% of the list when doing sequential search. This means","chapter-9","Searching"
"that if we had an application that used sequential search, and we wanted to make it","chapter-9","Searching"
"go a bit faster (by a constant amount), we could do so without a major rewrite to","chapter-9","Searching"
"the system to implement something like a search tree. But that is only true if there","chapter-9","Searching"
"is an easy way to (at least approximately) order the records by frequency.","chapter-9","Searching"
"In most applications, we have no means of knowing in advance the frequencies","chapter-9","Searching"
"of access for the data records. To complicate matters further, certain records might","chapter-9","Searching"
"be accessed frequently for a brief period of time, and then rarely thereafter. Thus,","chapter-9","Searching"
"the probability of access for records might change over time (in most database","chapter-9","Searching"
"systems, this is to be expected). Self-organizing lists seek to solve both of these","chapter-9","Searching"
"problems.","chapter-9","Searching"
"Self-organizing lists modify the order of records within the list based on the","chapter-9","Searching"
"actual pattern of record access. Self-organizing lists use a heuristic for deciding","chapter-9","Searching"
"how to to reorder the list. These heuristics are similar to the rules for managing","chapter-9","Searching"
"buffer pools (see Section 8.3). In fact, a buffer pool is a form of self-organizing","chapter-9","Searching"
"list. Ordering the buffer pool by expected frequency of access is a good strategy,","chapter-9","Searching"
"because typically we must search the contents of the buffers to determine if the","chapter-9","Searching"
"desired information is already in main memory. When ordered by frequency of","chapter-9","Searching"
"access, the buffer at the end of the list will be the one most appropriate for reuse","chapter-9","Searching"
"when a new page of information must be read. Below are three traditional heuristics","chapter-9","Searching"
"for managing self-organizing lists:","chapter-9","Searching"
"1. The most obvious way to keep a list ordered by frequency would be to store","chapter-9","Searching"
"a count of accesses to each record and always maintain records in this or-","chapter-9","Searching"
"der. This method will be referred to as count. Count is similar to the least","chapter-9","Searching"
"frequently used buffer replacement strategy. Whenever a record is accessed,","chapter-9","Searching"
"it might move toward the front of the list if its number of accesses becomes","chapter-9","Searching"
"greater than a record preceding it. Thus, count will store the records in the","chapter-9","Searching"
"order of frequency that has actually occurred so far. Besides requiring space","chapter-9","Searching"
"for the access counts, count does not react well to changing frequency of","chapter-9","Searching"
"access over time. Once a record has been accessed a large number of times","chapter-9","Searching"
"under the frequency count system, it will remain near the front of the list","chapter-9","Searching"
"regardless of further access history.","chapter-9","Searching"
"2. Bring a record to the front of the list when it is found, pushing all the other","chapter-9","Searching"
"records back one position. This is analogous to the least recently used buffer","chapter-9","Searching"
"replacement strategy and is called move-to-front. This heuristic is easy to","chapter-9","Searching"
"implement if the records are stored using a linked list. When records are","chapter-9","Searching"
"stored in an array, bringing a record forward from near the end of the array","chapter-9","Searching"
"will result in a large number of records (slightly) changing position. Move-","chapter-9","Searching"
"to-front’s cost is bounded in the sense that it requires at most twice the num-","chapter-9","Searching"
"Sec. 9.2 Self-Organizing Lists 311","chapter-9","Searching"
"ber of accesses required by the optimal static ordering for n records when","chapter-9","Searching"
"at least n searches are performed. In other words, if we had known the se-","chapter-9","Searching"
"ries of (at least n) searches in advance and had stored the records in order of","chapter-9","Searching"
"frequency so as to minimize the total cost for these accesses, this cost would","chapter-9","Searching"
"be at least half the cost required by the move-to-front heuristic. (This will","chapter-9","Searching"
"be proved using amortized analysis in Section 14.3.) Finally, move-to-front","chapter-9","Searching"
"responds well to local changes in frequency of access, in that if a record is","chapter-9","Searching"
"frequently accessed for a brief period of time it will be near the front of the","chapter-9","Searching"
"list during that period of access. Move-to-front does poorly when the records","chapter-9","Searching"
"are processed in sequential order, especially if that sequential order is then","chapter-9","Searching"
"repeated multiple times.","chapter-9","Searching"
"3. Swap any record found with the record immediately preceding it in the list.","chapter-9","Searching"
"This heuristic is called transpose. Transpose is good for list implementations","chapter-9","Searching"
"based on either linked lists or arrays. Frequently used records will, over time,","chapter-9","Searching"
"move to the front of the list. Records that were once frequently accessed but","chapter-9","Searching"
"are no longer used will slowly drift toward the back. Thus, it appears to have","chapter-9","Searching"
"good properties with respect to changing frequency of access. Unfortunately,","chapter-9","Searching"
"there are some pathological sequences of access that can make transpose","chapter-9","Searching"
"perform poorly. Consider the case where the last record of the list (call it X) is","chapter-9","Searching"
"accessed. This record is then swapped with the next-to-last record (call it Y),","chapter-9","Searching"
"making Y the last record. If Y is now accessed, it swaps with X. A repeated","chapter-9","Searching"
"series of accesses alternating between X and Y will continually search to the","chapter-9","Searching"
"end of the list, because neither record will ever make progress toward the","chapter-9","Searching"
"front. However, such pathological cases are unusual in practice. A variation","chapter-9","Searching"
"on transpose would be to move the accessed record forward in the list by","chapter-9","Searching"
"some fixed number of steps.","chapter-9","Searching"
"Example 9.4 Assume that we have eight records, with key values A to H,","chapter-9","Searching"
"and that they are initially placed in alphabetical order. Now, consider the","chapter-9","Searching"
"result of applying the following access pattern:","chapter-9","Searching"
"F D F G E G F A D F G E.","chapter-9","Searching"
"Assume that when a record’s frequency count goes up, it moves forward in","chapter-9","Searching"
"the list to become the last record with that value for its frequency count.","chapter-9","Searching"
"After the first two accesses, F will be the first record and D will be the","chapter-9","Searching"
"second. The final list resulting from these accesses will be","chapter-9","Searching"
"F G D E A B C H,","chapter-9","Searching"
"and the total cost for the twelve accesses will be 45 comparisons.","chapter-9","Searching"
"If the list is organized by the move-to-front heuristic, then the final list","chapter-9","Searching"
"will be","chapter-9","Searching"
"E G F D A B C H,","chapter-9","Searching"
"312 Chap. 9 Searching","chapter-9","Searching"
"and the total number of comparisons required is 54.","chapter-9","Searching"
"Finally, if the list is organized by the transpose heuristic, then the final","chapter-9","Searching"
"list will be","chapter-9","Searching"
"A B F D G E C H,","chapter-9","Searching"
"and the total number of comparisons required is 62.","chapter-9","Searching"
"While self-organizing lists do not generally perform as well as search trees or a","chapter-9","Searching"
"sorted list, both of which require O(log n) search time, there are many situations in","chapter-9","Searching"
"which self-organizing lists prove a valuable tool. Obviously they have an advantage","chapter-9","Searching"
"over sorted lists in that they need not be sorted. This means that the cost to insert","chapter-9","Searching"
"a new record is low, which could more than make up for the higher search cost","chapter-9","Searching"
"when insertions are frequent. Self-organizing lists are simpler to implement than","chapter-9","Searching"
"search trees and are likely to be more efficient for small lists. Nor do they require","chapter-9","Searching"
"additional space. Finally, in the case of an application where sequential search is","chapter-9","Searching"
"“almost” fast enough, changing an unsorted list to a self-organizing list might speed","chapter-9","Searching"
"the application enough at a minor cost in additional code.","chapter-9","Searching"
"As an example of applying self-organizing lists, consider an algorithm for com-","chapter-9","Searching"
"pressing and transmitting messages. The list is self-organized by the move-to-front","chapter-9","Searching"
"rule. Transmission is in the form of words and numbers, by the following rules:","chapter-9","Searching"
"1. If the word has been seen before, transmit the current position of the word in","chapter-9","Searching"
"the list. Move the word to the front of the list.","chapter-9","Searching"
"2. If the word is seen for the first time, transmit the word. Place the word at the","chapter-9","Searching"
"front of the list.","chapter-9","Searching"
"Both the sender and the receiver keep track of the position of words in the list","chapter-9","Searching"
"in the same way (using the move-to-front rule), so they agree on the meaning of","chapter-9","Searching"
"the numbers that encode repeated occurrences of words. Consider the following","chapter-9","Searching"
"example message to be transmitted (for simplicity, ignore case in letters).","chapter-9","Searching"
"The car on the left hit the car I left.","chapter-9","Searching"
"The first three words have not been seen before, so they must be sent as full","chapter-9","Searching"
"words. The fourth word is the second appearance of “the,” which at this point is","chapter-9","Searching"
"the third word in the list. Thus, we only need to transmit the position value “3.”","chapter-9","Searching"
"The next two words have not yet been seen, so must be sent as full words. The","chapter-9","Searching"
"seventh word is the third appearance of “the,” which coincidentally is again in the","chapter-9","Searching"
"third position. The eighth word is the second appearance of “car,” which is now in","chapter-9","Searching"
"the fifth position of the list. “I” is a new word, and the last word “left” is now in","chapter-9","Searching"
"the fifth position. Thus the entire transmission would be","chapter-9","Searching"
"The car on 3 left hit 3 5 I 5.","chapter-9","Searching"
"Sec. 9.3 Bit Vectors for Representing Sets 313","chapter-9","Searching"
"0 1 2 3 4 5 6 7 8 9 10 11 12 15","chapter-9","Searching"
"0 0 1 1 1 0 1 0 0 0 1 0 1 0 0","chapter-9","Searching"
"13 14","chapter-9","Searching"
"0","chapter-9","Searching"
"Figure 9.1 The bit array for the set of primes in the range 0 to 15. The bit at","chapter-9","Searching"
"position i is set to 1 if and only if i is prime.","chapter-9","Searching"
"This approach to compression is similar in spirit to Ziv-Lempel coding, which","chapter-9","Searching"
"is a class of coding algorithms commonly used in file compression utilities. Ziv-","chapter-9","Searching"
"Lempel coding replaces repeated occurrences of strings with a pointer to the lo-","chapter-9","Searching"
"cation in the file of the first occurrence of the string. The codes are stored in a","chapter-9","Searching"
"self-organizing list in order to speed up the time required to search for a string that","chapter-9","Searching"
"has previously been seen.","chapter-9","Searching"
"9.3 Bit Vectors for Representing Sets","chapter-9","Searching"
"Determining whether a value is a member of a particular set is a special case of","chapter-9","Searching"
"searching for keys in a sequence of records. Thus, any of the search methods","chapter-9","Searching"
"discussed in this book can be used to check for set membership. However, we","chapter-9","Searching"
"can also take advantage of the restricted circumstances imposed by this problem to","chapter-9","Searching"
"develop another representation.","chapter-9","Searching"
"In the case where the set values fall within a limited range, we can represent the","chapter-9","Searching"
"set using a bit array with a bit position allocated for each potential member. Those","chapter-9","Searching"
"members actually in the set store a value of 1 in their corresponding bit; those","chapter-9","Searching"
"members not in the set store a value of 0 in their corresponding bit. For example,","chapter-9","Searching"
"consider the set of primes between 0 and 15. Figure 9.1 shows the corresponding","chapter-9","Searching"
"bit array. To determine if a particular value is prime, we simply check the corre-","chapter-9","Searching"
"sponding bit. This representation scheme is called a bit vector or a bitmap. The","chapter-9","Searching"
"mark array used in several of the graph algorithms of Chapter 11 is an example of","chapter-9","Searching"
"such a set representation.","chapter-9","Searching"
"If the set fits within a single computer word, then set union, intersection, and","chapter-9","Searching"
"difference can be performed by logical bit-wise operations. The union of sets A","chapter-9","Searching"
"and B is the bit-wise OR function (whose symbol is | in Java). The intersection","chapter-9","Searching"
"of sets A and B is the bit-wise AND function (whose symbol is & in Java). For","chapter-9","Searching"
"example, if we would like to compute the set of numbers between 0 and 15 that are","chapter-9","Searching"
"both prime and odd numbers, we need only compute the expression","chapter-9","Searching"
"0011010100010100 & 0101010101010101.","chapter-9","Searching"
"The set difference A − B can be implemented in Java using the expression A& ̃B","chapter-9","Searching"
"( ̃ is the symbol for bit-wise negation). For larger sets that do not fit into a single","chapter-9","Searching"
"computer word, the equivalent operations can be performed in turn on the series of","chapter-9","Searching"
"words making up the entire bit vector.","chapter-9","Searching"
"314 Chap. 9 Searching","chapter-9","Searching"
"This method of computing sets from bit vectors is sometimes applied to doc-","chapter-9","Searching"
"ument retrieval. Consider the problem of picking from a collection of documents","chapter-9","Searching"
"those few which contain selected keywords. For each keyword, the document re-","chapter-9","Searching"
"trieval system stores a bit vector with one bit for each document. If the user wants to","chapter-9","Searching"
"know which documents contain a certain three keywords, the corresponding three","chapter-9","Searching"
"bit vectors are AND’ed together. Those bit positions resulting in a value of 1 cor-","chapter-9","Searching"
"respond to the desired documents. Alternatively, a bit vector can be stored for each","chapter-9","Searching"
"document to indicate those keywords appearing in the document. Such an organiza-","chapter-9","Searching"
"tion is called a signature file. The signatures can be manipulated to find documents","chapter-9","Searching"
"with desired combinations of keywords.","chapter-9","Searching"
"9.4 Hashing","chapter-9","Searching"
"This section presents a completely different approach to searching arrays: by direct","chapter-9","Searching"
"access based on key value. The process of finding a record using some computa-","chapter-9","Searching"
"tion to map its key value to a position in the array is called hashing. Most hash-","chapter-9","Searching"
"ing schemes place records in the array in whatever order satisfies the needs of the","chapter-9","Searching"
"address calculation, thus the records are not ordered by value or frequency. The","chapter-9","Searching"
"function that maps key values to positions is called a hash function and will be","chapter-9","Searching"
"denoted by h. The array that holds the records is called the hash table and will be","chapter-9","Searching"
"denoted by HT. A position in the hash table is also known as a slot. The number","chapter-9","Searching"
"of slots in hash table HT will be denoted by the variable M, with slots numbered","chapter-9","Searching"
"from 0 to M − 1. The goal for a hashing system is to arrange things such that, for","chapter-9","Searching"
"any key value K and some hash function h, i = h(K) is a slot in the table such","chapter-9","Searching"
"that 0 ≤ h(K) < M, and we have the key of the record stored at HT[i] equal to","chapter-9","Searching"
"K.","chapter-9","Searching"
"Hashing is not good for applications where multiple records with the same key","chapter-9","Searching"
"value are permitted. Hashing is not a good method for answering range searches. In","chapter-9","Searching"
"other words, we cannot easily find all records (if any) whose key values fall within","chapter-9","Searching"
"a certain range. Nor can we easily find the record with the minimum or maximum","chapter-9","Searching"
"key value, or visit the records in key order. Hashing is most appropriate for answer-","chapter-9","Searching"
"ing the question, “What record, if any, has key value K?” For applications where","chapter-9","Searching"
"access involves only exact-match queries, hashing is usually the search method of","chapter-9","Searching"
"choice because it is extremely efficient when implemented correctly. As you will","chapter-9","Searching"
"see in this section, however, there are many approaches to hashing and it is easy","chapter-9","Searching"
"to devise an inefficient implementation. Hashing is suitable for both in-memory","chapter-9","Searching"
"and disk-based searching and is one of the two most widely used methods for or-","chapter-9","Searching"
"ganizing large databases stored on disk (the other is the B-tree, which is covered in","chapter-9","Searching"
"Chapter 10).","chapter-9","Searching"
"As a simple (though unrealistic) example of hashing, consider storing n records","chapter-9","Searching"
"each with a unique key value in the range 0 to n − 1. In this simple case, a record","chapter-9","Searching"
"Sec. 9.4 Hashing 315","chapter-9","Searching"
"with key k can be stored in HT[k], and the hash function is simply h(k) = k. To","chapter-9","Searching"
"find the record with key value k, simply look in HT[k].","chapter-9","Searching"
"Typically, there are many more values in the key range than there are slots in","chapter-9","Searching"
"the hash table. For a more realistic example, suppose that the key can take any","chapter-9","Searching"
"value in the range 0 to 65,535 (i.e., the key is a two-byte unsigned integer), and that","chapter-9","Searching"
"we expect to store approximately 1000 records at any given time. It is impractical","chapter-9","Searching"
"in this situation to use a hash table with 65,536 slots, because most of the slots will","chapter-9","Searching"
"be left empty. Instead, we must devise a hash function that allows us to store the","chapter-9","Searching"
"records in a much smaller table. Because the possible key range is larger than the","chapter-9","Searching"
"size of the table, at least some of the slots must be mapped to from multiple key","chapter-9","Searching"
"values. Given a hash function h and two keys k1 and k2, if h(k1) = β = h(k2)","chapter-9","Searching"
"where β is a slot in the table, then we say that k1 and k2 have a collision at slot β","chapter-9","Searching"
"under hash function h.","chapter-9","Searching"
"Finding a record with key value K in a database organized by hashing follows","chapter-9","Searching"
"a two-step procedure:","chapter-9","Searching"
"1. Compute the table location h(K).","chapter-9","Searching"
"2. Starting with slot h(K), locate the record containing key K using (if neces-","chapter-9","Searching"
"sary) a collision resolution policy.","chapter-9","Searching"
"9.4.1 Hash Functions","chapter-9","Searching"
"Hashing generally takes records whose key values come from a large range and","chapter-9","Searching"
"stores those records in a table with a relatively small number of slots. Collisions","chapter-9","Searching"
"occur when two records hash to the same slot in the table. If we are careful—or","chapter-9","Searching"
"lucky—when selecting a hash function, then the actual number of collisions will","chapter-9","Searching"
"be few. Unfortunately, even under the best of circumstances, collisions are nearly","chapter-9","Searching"
"unavoidable.1 For example, consider a classroom full of students. What is the","chapter-9","Searching"
"probability that some pair of students shares the same birthday (i.e., the same day","chapter-9","Searching"
"of the year, not necessarily the same year)? If there are 23 students, then the odds","chapter-9","Searching"
"are about even that two will share a birthday. This is despite the fact that there are","chapter-9","Searching"
"365 days in which students can have birthdays (ignoring leap years), on most of","chapter-9","Searching"
"which no student in the class has a birthday. With more students, the probability","chapter-9","Searching"
"of a shared birthday increases. The mapping of students to days based on their","chapter-9","Searching"
"1The exception to this is perfect hashing. Perfect hashing is a system in which records are","chapter-9","Searching"
"hashed such that there are no collisions. A hash function is selected for the specific set of records","chapter-9","Searching"
"being hashed, which requires that the entire collection of records be available before selecting the","chapter-9","Searching"
"hash function. Perfect hashing is efficient because it always finds the record that we are looking","chapter-9","Searching"
"for exactly where the hash function computes it to be, so only one access is required. Selecting a","chapter-9","Searching"
"perfect hash function can be expensive, but might be worthwhile when extremely efficient search","chapter-9","Searching"
"performance is required. An example is searching for data on a read-only CD. Here the database will","chapter-9","Searching"
"never change, the time for each access is expensive, and the database designer can build the hash","chapter-9","Searching"
"table before issuing the CD.","chapter-9","Searching"
"316 Chap. 9 Searching","chapter-9","Searching"
"birthday is similar to assigning records to slots in a table (of size 365) using the","chapter-9","Searching"
"birthday as a hash function. Note that this observation tells us nothing about which","chapter-9","Searching"
"students share a birthday, or on which days of the year shared birthdays fall.","chapter-9","Searching"
"To be practical, a database organized by hashing must store records in a hash","chapter-9","Searching"
"table that is not so large that it wastes space. Typically, this means that the hash","chapter-9","Searching"
"table will be around half full. Because collisions are extremely likely to occur","chapter-9","Searching"
"under these conditions (by chance, any record inserted into a table that is half full","chapter-9","Searching"
"will have a collision half of the time), does this mean that we need not worry about","chapter-9","Searching"
"the ability of a hash function to avoid collisions? Absolutely not. The difference","chapter-9","Searching"
"between a good hash function and a bad hash function makes a big difference in","chapter-9","Searching"
"practice. Technically, any function that maps all possible key values to a slot in","chapter-9","Searching"
"the hash table is a hash function. In the extreme case, even a function that maps","chapter-9","Searching"
"all records to the same slot is a hash function, but it does nothing to help us find","chapter-9","Searching"
"records during a search operation.","chapter-9","Searching"
"We would like to pick a hash function that stores the actual records in the col-","chapter-9","Searching"
"lection such that each slot in the hash table has equal probability of being filled. Un-","chapter-9","Searching"
"fortunately, we normally have no control over the key values of the actual records,","chapter-9","Searching"
"so how well any particular hash function does this depends on the distribution of","chapter-9","Searching"
"the keys within the allowable key range. In some cases, incoming data are well","chapter-9","Searching"
"distributed across their key range. For example, if the input is a set of random","chapter-9","Searching"
"numbers selected uniformly from the key range, any hash function that assigns the","chapter-9","Searching"
"key range so that each slot in the hash table receives an equal share of the range","chapter-9","Searching"
"will likely also distribute the input records uniformly within the table. However,","chapter-9","Searching"
"in many applications the incoming records are highly clustered or otherwise poorly","chapter-9","Searching"
"distributed. When input records are not well distributed throughout the key range","chapter-9","Searching"
"it can be difficult to devise a hash function that does a good job of distributing the","chapter-9","Searching"
"records throughout the table, especially if the input distribution is not known in","chapter-9","Searching"
"advance.","chapter-9","Searching"
"There are many reasons why data values might be poorly distributed.","chapter-9","Searching"
"1. Natural frequency distributions tend to follow a common pattern where a few","chapter-9","Searching"
"of the entities occur frequently while most entities occur relatively rarely.","chapter-9","Searching"
"For example, consider the populations of the 100 largest cities in the United","chapter-9","Searching"
"States. If you plot these populations on a number line, most of them will be","chapter-9","Searching"
"clustered toward the low side, with a few outliers on the high side. This is an","chapter-9","Searching"
"example of a Zipf distribution (see Section 9.2). Viewed the other way, the","chapter-9","Searching"
"home town for a given person is far more likely to be a particular large city","chapter-9","Searching"
"than a particular small town.","chapter-9","Searching"
"2. Collected data are likely to be skewed in some way. Field samples might be","chapter-9","Searching"
"rounded to, say, the nearest 5 (i.e., all numbers end in 5 or 0).","chapter-9","Searching"
"3. If the input is a collection of common English words, the beginning letter","chapter-9","Searching"
"will be poorly distributed.","chapter-9","Searching"
"Sec. 9.4 Hashing 317","chapter-9","Searching"
"Note that in examples 2 and 3, either high- or low-order bits of the key are poorly","chapter-9","Searching"
"distributed.","chapter-9","Searching"
"When designing hash functions, we are generally faced with one of two situa-","chapter-9","Searching"
"tions.","chapter-9","Searching"
"1. We know nothing about the distribution of the incoming keys. In this case,","chapter-9","Searching"
"we wish to select a hash function that evenly distributes the key range across","chapter-9","Searching"
"the hash table, while avoiding obvious opportunities for clustering such as","chapter-9","Searching"
"hash functions that are sensitive to the high- or low-order bits of the key","chapter-9","Searching"
"value.","chapter-9","Searching"
"2. We know something about the distribution of the incoming keys. In this case,","chapter-9","Searching"
"we should use a distribution-dependent hash function that avoids assigning","chapter-9","Searching"
"clusters of related key values to the same hash table slot. For example, if","chapter-9","Searching"
"hashing English words, we should not hash on the value of the first character","chapter-9","Searching"
"because this is likely to be unevenly distributed.","chapter-9","Searching"
"Below are several examples of hash functions that illustrate these points.","chapter-9","Searching"
"Example 9.5 Consider the following hash function used to hash integers","chapter-9","Searching"
"to a table of sixteen slots:","chapter-9","Searching"
"int h(int x) {","chapter-9","Searching"
"return(x % 16);","chapter-9","Searching"
"}","chapter-9","Searching"
"The value returned by this hash function depends solely on the least","chapter-9","Searching"
"significant four bits of the key. Because these bits are likely to be poorly","chapter-9","Searching"
"distributed (as an example, a high percentage of the keys might be even","chapter-9","Searching"
"numbers, which means that the low order bit is zero), the result will also","chapter-9","Searching"
"be poorly distributed. This example shows that the size of the table M can","chapter-9","Searching"
"have a big effect on the performance of a hash system because this value is","chapter-9","Searching"
"typically used as the modulus to ensure that the hash function produces a","chapter-9","Searching"
"number in the range 0 to M − 1.","chapter-9","Searching"
"Example 9.6 A good hash function for numerical values comes from the","chapter-9","Searching"
"mid-square method. The mid-square method squares the key value, and","chapter-9","Searching"
"then takes the middle r bits of the result, giving a value in the range 0 to","chapter-9","Searching"
"2","chapter-9","Searching"
"r − 1. This works well because most or all bits of the key value contribute","chapter-9","Searching"
"to the result. For example, consider records whose keys are 4-digit numbers","chapter-9","Searching"
"in base 10. The goal is to hash these key values to a table of size 100","chapter-9","Searching"
"(i.e., a range of 0 to 99). This range is equivalent to two digits in base 10.","chapter-9","Searching"
"That is, r = 2. If the input is the number 4567, squaring yields an 8-digit","chapter-9","Searching"
"number, 20857489. The middle two digits of this result are 57. All digits","chapter-9","Searching"
"318 Chap. 9 Searching","chapter-9","Searching"
"4567","chapter-9","Searching"
"4567","chapter-9","Searching"
"31969","chapter-9","Searching"
"27402","chapter-9","Searching"
"22835","chapter-9","Searching"
"18268","chapter-9","Searching"
"20857489","chapter-9","Searching"
"4567","chapter-9","Searching"
"Figure 9.2 An illustration of the mid-square method, showing the details of","chapter-9","Searching"
"long multiplication in the process of squaring the value 4567. The bottom of the","chapter-9","Searching"
"figure indicates which digits of the answer are most influenced by each digit of","chapter-9","Searching"
"the operands.","chapter-9","Searching"
"(equivalently, all bits when the number is viewed in binary) contribute to the","chapter-9","Searching"
"middle two digits of the squared value. Figure 9.2 illustrates the concept.","chapter-9","Searching"
"Thus, the result is not dominated by the distribution of the bottom digit or","chapter-9","Searching"
"the top digit of the original key value.","chapter-9","Searching"
"Example 9.7 Here is a hash function for strings of characters:","chapter-9","Searching"
"int h(String x, int M) {","chapter-9","Searching"
"char ch[];","chapter-9","Searching"
"ch = x.toCharArray();","chapter-9","Searching"
"int xlength = x.length();","chapter-9","Searching"
"int i, sum;","chapter-9","Searching"
"for (sum=0, i=0; i<x.length(); i++)","chapter-9","Searching"
"sum += ch[i];","chapter-9","Searching"
"return sum % M;","chapter-9","Searching"
"}","chapter-9","Searching"
"This function sums the ASCII values of the letters in a string. If the hash","chapter-9","Searching"
"table size M is small, this hash function should do a good job of distributing","chapter-9","Searching"
"strings evenly among the hash table slots, because it gives equal weight to","chapter-9","Searching"
"all characters. This is an example of the folding approach to designing a","chapter-9","Searching"
"hash function. Note that the order of the characters in the string has no","chapter-9","Searching"
"effect on the result. A similar method for integers would add the digits of","chapter-9","Searching"
"the key value, assuming that there are enough digits to (1) keep any one","chapter-9","Searching"
"or two digits with bad distribution from skewing the results of the process","chapter-9","Searching"
"and (2) generate a sum much larger than M. As with many other hash","chapter-9","Searching"
"functions, the final step is to apply the modulus operator to the result, using","chapter-9","Searching"
"table size M to generate a value within the table range. If the sum is not","chapter-9","Searching"
"sufficiently large, then the modulus operator will yield a poor distribution.","chapter-9","Searching"
"For example, because the ASCII value for “A” is 65 and “Z” is 90, sum will","chapter-9","Searching"
"always be in the range 650 to 900 for a string of ten upper case letters. For","chapter-9","Searching"
"Sec. 9.4 Hashing 319","chapter-9","Searching"
"a hash table of size 100 or less, a reasonable distribution results. For a hash","chapter-9","Searching"
"table of size 1000, the distribution is terrible because only slots 650 to 900","chapter-9","Searching"
"can possibly be the home slot for some key value, and the values are not","chapter-9","Searching"
"evenly distributed even within those slots.","chapter-9","Searching"
"Example 9.8 Here is a much better hash function for strings.","chapter-9","Searching"
"long sfold(String s, int M) {","chapter-9","Searching"
"int intLength = s.length() / 4;","chapter-9","Searching"
"long sum = 0;","chapter-9","Searching"
"for (int j = 0; j < intLength; j++) {","chapter-9","Searching"
"char c[] = s.substring(j*4,(j*4)+4).toCharArray();","chapter-9","Searching"
"long mult = 1;","chapter-9","Searching"
"for (int k = 0; k < c.length; k++) {","chapter-9","Searching"
"sum += c[k] * mult;","chapter-9","Searching"
"mult *= 256;","chapter-9","Searching"
"}","chapter-9","Searching"
"}","chapter-9","Searching"
"char c[] = s.substring(intLength * 4).toCharArray();","chapter-9","Searching"
"long mult = 1;","chapter-9","Searching"
"for (int k = 0; k < c.length; k++) {","chapter-9","Searching"
"sum += c[k] * mult;","chapter-9","Searching"
"mult *= 256;","chapter-9","Searching"
"}","chapter-9","Searching"
"return(Math.abs(sum) % M);","chapter-9","Searching"
"}","chapter-9","Searching"
"This function takes a string as input. It processes the string four bytes at","chapter-9","Searching"
"a time, and interprets each of the four-byte chunks as a single long integer","chapter-9","Searching"
"value. The integer values for the four-byte chunks are added together. In","chapter-9","Searching"
"the end, the resulting sum is converted to the range 0 to M − 1 using the","chapter-9","Searching"
"modulus operator.2","chapter-9","Searching"
"For example, if the string “aaaabbbb” is passed to sfold, then the first","chapter-9","Searching"
"four bytes (“aaaa”) will be interpreted as the integer value 1,633,771,873","chapter-9","Searching"
"and the next four bytes (“bbbb”) will be interpreted as the integer value","chapter-9","Searching"
"1,650,614,882. Their sum is 3,284,386,755 (when viewed as an unsigned","chapter-9","Searching"
"integer). If the table size is 101 then the modulus function will cause this","chapter-9","Searching"
"key to hash to slot 75 in the table. Note that for any sufficiently long string,","chapter-9","Searching"
"2Recall from Section 2.2 that the implementation for n mod m on many C++ and Java compilers","chapter-9","Searching"
"will yield a negative number if n is negative. Implementors for hash functions need to be careful that","chapter-9","Searching"
"their hash function does not generate a negative number. This can be avoided either by insuring that","chapter-9","Searching"
"n is positive when computing n mod m, or adding m to the result if n mod m is negative. Here,","chapter-9","Searching"
"sfold takes the absolute value of sum before applying the modulus operator.","chapter-9","Searching"
"320 Chap. 9 Searching","chapter-9","Searching"
"0","chapter-9","Searching"
"1","chapter-9","Searching"
"2","chapter-9","Searching"
"3","chapter-9","Searching"
"4","chapter-9","Searching"
"5","chapter-9","Searching"
"6","chapter-9","Searching"
"7","chapter-9","Searching"
"8","chapter-9","Searching"
"9","chapter-9","Searching"
"9530","chapter-9","Searching"
"2007 1057","chapter-9","Searching"
"1000","chapter-9","Searching"
"3013","chapter-9","Searching"
"9879","chapter-9","Searching"
"9877","chapter-9","Searching"
"Figure 9.3 An illustration of open hashing for seven numbers stored in a ten-slot","chapter-9","Searching"
"hash table using the hash function h(K) = K mod 10. The numbers are inserted","chapter-9","Searching"
"in the order 9877, 2007, 1000, 9530, 3013, 9879, and 1057. Two of the values","chapter-9","Searching"
"hash to slot 0, one value hashes to slot 2, three of the values hash to slot 7, and","chapter-9","Searching"
"one value hashes to slot 9.","chapter-9","Searching"
"the sum for the integer quantities will typically cause a 32-bit integer to","chapter-9","Searching"
"overflow (thus losing some of the high-order bits) because the resulting","chapter-9","Searching"
"values are so large. But this causes no problems when the goal is to compute","chapter-9","Searching"
"a hash function.","chapter-9","Searching"
"9.4.2 Open Hashing","chapter-9","Searching"
"While the goal of a hash function is to minimize collisions, some collisions are","chapter-9","Searching"
"unavoidable in practice. Thus, hashing implementations must include some form of","chapter-9","Searching"
"collision resolution policy. Collision resolution techniques can be broken into two","chapter-9","Searching"
"classes: open hashing (also called separate chaining) and closed hashing (also","chapter-9","Searching"
"called open addressing).3 The difference between the two has to do with whether","chapter-9","Searching"
"collisions are stored outside the table (open hashing), or whether collisions result","chapter-9","Searching"
"in storing one of the records at another slot in the table (closed hashing). Open","chapter-9","Searching"
"hashing is treated in this section, and closed hashing in Section 9.4.3.","chapter-9","Searching"
"The simplest form of open hashing defines each slot in the hash table to be","chapter-9","Searching"
"the head of a linked list. All records that hash to a particular slot are placed on","chapter-9","Searching"
"that slot’s linked list. Figure 9.3 illustrates a hash table where each slot stores one","chapter-9","Searching"
"record and a link pointer to the rest of the list.","chapter-9","Searching"
"3Yes, it is confusing when “open hashing” means the opposite of “open addressing,” but unfortu-","chapter-9","Searching"
"nately, that is the way it is.","chapter-9","Searching"
"Sec. 9.4 Hashing 321","chapter-9","Searching"
"Records within a slot’s list can be ordered in several ways: by insertion order,","chapter-9","Searching"
"by key value order, or by frequency-of-access order. Ordering the list by key value","chapter-9","Searching"
"provides an advantage in the case of an unsuccessful search, because we know to","chapter-9","Searching"
"stop searching the list once we encounter a key that is greater than the one being","chapter-9","Searching"
"searched for. If records on the list are unordered or ordered by frequency, then an","chapter-9","Searching"
"unsuccessful search will need to visit every record on the list.","chapter-9","Searching"
"Given a table of size M storing N records, the hash function will (ideally)","chapter-9","Searching"
"spread the records evenly among the M positions in the table, yielding on average","chapter-9","Searching"
"N/M records for each list. Assuming that the table has more slots than there are","chapter-9","Searching"
"records to be stored, we can hope that few slots will contain more than one record.","chapter-9","Searching"
"In the case where a list is empty or has only one record, a search requires only one","chapter-9","Searching"
"access to the list. Thus, the average cost for hashing should be Θ(1). However, if","chapter-9","Searching"
"clustering causes many records to hash to only a few of the slots, then the cost to","chapter-9","Searching"
"access a record will be much higher because many elements on the linked list must","chapter-9","Searching"
"be searched.","chapter-9","Searching"
"Open hashing is most appropriate when the hash table is kept in main memory,","chapter-9","Searching"
"with the lists implemented by a standard in-memory linked list. Storing an open","chapter-9","Searching"
"hash table on disk in an efficient way is difficult, because members of a given","chapter-9","Searching"
"linked list might be stored on different disk blocks. This would result in multiple","chapter-9","Searching"
"disk accesses when searching for a particular key value, which defeats the purpose","chapter-9","Searching"
"of using hashing.","chapter-9","Searching"
"There are similarities between open hashing and Binsort. One way to view","chapter-9","Searching"
"open hashing is that each record is simply placed in a bin. While multiple records","chapter-9","Searching"
"may hash to the same bin, this initial binning should still greatly reduce the number","chapter-9","Searching"
"of records accessed by a search operation. In a similar fashion, a simple Binsort","chapter-9","Searching"
"reduces the number of records in each bin to a small number that can be sorted in","chapter-9","Searching"
"some other way.","chapter-9","Searching"
"9.4.3 Closed Hashing","chapter-9","Searching"
"Closed hashing stores all records directly in the hash table. Each record R with key","chapter-9","Searching"
"value kR has a home position that is h(kR), the slot computed by the hash function.","chapter-9","Searching"
"If R is to be inserted and another record already occupies R’s home position, then","chapter-9","Searching"
"R will be stored at some other slot in the table. It is the business of the collision","chapter-9","Searching"
"resolution policy to determine which slot that will be. Naturally, the same policy","chapter-9","Searching"
"must be followed during search as during insertion, so that any record not found in","chapter-9","Searching"
"its home position can be recovered by repeating the collision resolution process.","chapter-9","Searching"
"Bucket Hashing","chapter-9","Searching"
"One implementation for closed hashing groups hash table slots into buckets. The","chapter-9","Searching"
"M slots of the hash table are divided into B buckets, with each bucket consisting","chapter-9","Searching"
"322 Chap. 9 Searching","chapter-9","Searching"
"0","chapter-9","Searching"
"1","chapter-9","Searching"
"2","chapter-9","Searching"
"3","chapter-9","Searching"
"4","chapter-9","Searching"
"Table Overflow","chapter-9","Searching"
"Hash","chapter-9","Searching"
"9877","chapter-9","Searching"
"2007","chapter-9","Searching"
"3013","chapter-9","Searching"
"9879","chapter-9","Searching"
"1057","chapter-9","Searching"
"9530","chapter-9","Searching"
"1000","chapter-9","Searching"
"Figure 9.4 An illustration of bucket hashing for seven numbers stored in a five-","chapter-9","Searching"
"bucket hash table using the hash function h(K) = K mod 5. Each bucket con-","chapter-9","Searching"
"tains two slots. The numbers are inserted in the order 9877, 2007, 1000, 9530,","chapter-9","Searching"
"3013, 9879, and 1057. Two of the values hash to bucket 0, three values hash to","chapter-9","Searching"
"bucket 2, one value hashes to bucket 3, and one value hashes to bucket 4. Because","chapter-9","Searching"
"bucket 2 cannot hold three values, the third one ends up in the overflow bucket.","chapter-9","Searching"
"of M/B slots. The hash function assigns each record to the first slot within one","chapter-9","Searching"
"of the buckets. If this slot is already occupied, then the bucket slots are searched","chapter-9","Searching"
"sequentially until an open slot is found. If a bucket is entirely full, then the record","chapter-9","Searching"
"is stored in an overflow bucket of infinite capacity at the end of the table. All","chapter-9","Searching"
"buckets share the same overflow bucket. A good implementation will use a hash","chapter-9","Searching"
"function that distributes the records evenly among the buckets so that as few records","chapter-9","Searching"
"as possible go into the overflow bucket. Figure 9.4 illustrates bucket hashing.","chapter-9","Searching"
"When searching for a record, the first step is to hash the key to determine which","chapter-9","Searching"
"bucket should contain the record. The records in this bucket are then searched. If","chapter-9","Searching"
"the desired key value is not found and the bucket still has free slots, then the search","chapter-9","Searching"
"is complete. If the bucket is full, then it is possible that the desired record is stored","chapter-9","Searching"
"in the overflow bucket. In this case, the overflow bucket must be searched until the","chapter-9","Searching"
"record is found or all records in the overflow bucket have been checked. If many","chapter-9","Searching"
"records are in the overflow bucket, this will be an expensive process.","chapter-9","Searching"
"A simple variation on bucket hashing is to hash a key value to some slot in","chapter-9","Searching"
"the hash table as though bucketing were not being used. If the home position is","chapter-9","Searching"
"full, then the collision resolution process is to move down through the table toward","chapter-9","Searching"
"the end of the bucket while searching for a free slot in which to store the record.","chapter-9","Searching"
"If the bottom of the bucket is reached, then the collision resolution routine wraps","chapter-9","Searching"
"around to the top of the bucket to continue the search for an open slot. For example,","chapter-9","Searching"
"Sec. 9.4 Hashing 323","chapter-9","Searching"
"1","chapter-9","Searching"
"3","chapter-9","Searching"
"5","chapter-9","Searching"
"7","chapter-9","Searching"
"9","chapter-9","Searching"
"0","chapter-9","Searching"
"2","chapter-9","Searching"
"4","chapter-9","Searching"
"8","chapter-9","Searching"
"Table Overflow","chapter-9","Searching"
"Hash","chapter-9","Searching"
"1057","chapter-9","Searching"
"9530","chapter-9","Searching"
"1000","chapter-9","Searching"
"6","chapter-9","Searching"
"9879","chapter-9","Searching"
"9877","chapter-9","Searching"
"3013","chapter-9","Searching"
"2007","chapter-9","Searching"
"Figure 9.5 An variant of bucket hashing for seven numbers stored in a 10-slot","chapter-9","Searching"
"hash table using the hash function h(K) = K mod 10. Each bucket contains two","chapter-9","Searching"
"slots. The numbers are inserted in the order 9877, 2007, 1000, 9530, 3013, 9879,","chapter-9","Searching"
"and 1057. Value 9877 first hashes to slot 7, so when value 2007 attempts to do","chapter-9","Searching"
"likewise, it is placed in the other slot associated with that bucket which is slot 6.","chapter-9","Searching"
"When value 1057 is inserted, there is no longer room in the bucket and it is placed","chapter-9","Searching"
"into overflow. The other collision occurs after value 1000 is inserted to slot 0,","chapter-9","Searching"
"causing 9530 to be moved to slot 1.","chapter-9","Searching"
"assume that buckets contain eight records, with the first bucket consisting of slots 0","chapter-9","Searching"
"through 7. If a record is hashed to slot 5, the collision resolution process will","chapter-9","Searching"
"attempt to insert the record into the table in the order 5, 6, 7, 0, 1, 2, 3, and finally 4.","chapter-9","Searching"
"If all slots in this bucket are full, then the record is assigned to the overflow bucket.","chapter-9","Searching"
"The advantage of this approach is that initial collisions are reduced, Because any","chapter-9","Searching"
"slot can be a home position rather than just the first slot in the bucket. Figure 9.5","chapter-9","Searching"
"shows another example for this form of bucket hashing.","chapter-9","Searching"
"Bucket methods are good for implementing hash tables stored on disk, because","chapter-9","Searching"
"the bucket size can be set to the size of a disk block. Whenever search or insertion","chapter-9","Searching"
"occurs, the entire bucket is read into memory. Because the entire bucket is then","chapter-9","Searching"
"in memory, processing an insert or search operation requires only one disk access,","chapter-9","Searching"
"unless the bucket is full. If the bucket is full, then the overflow bucket must be","chapter-9","Searching"
"retrieved from disk as well. Naturally, overflow should be kept small to minimize","chapter-9","Searching"
"unnecessary disk accesses.","chapter-9","Searching"
"324 Chap. 9 Searching","chapter-9","Searching"
"/** Insert record r with key k into HT */","chapter-9","Searching"
"void hashInsert(Key k, E r) {","chapter-9","Searching"
"int home; // Home position for r","chapter-9","Searching"
"int pos = home = h(k); // Initial position","chapter-9","Searching"
"for (int i=1; HT[pos] != null; i++) {","chapter-9","Searching"
"pos = (home + p(k, i)) % M; // Next pobe slot","chapter-9","Searching"
"assert HT[pos].key().compareTo(k) != 0 :","chapter-9","Searching"
""Duplicates not allowed";","chapter-9","Searching"
"}","chapter-9","Searching"
"HT[pos] = new KVpair<Key,E>(k, r); // Insert R","chapter-9","Searching"
"}","chapter-9","Searching"
"Figure 9.6 Insertion method for a dictionary implemented by a hash table.","chapter-9","Searching"
"Linear Probing","chapter-9","Searching"
"We now turn to the most commonly used form of hashing: closed hashing with no","chapter-9","Searching"
"bucketing, and a collision resolution policy that can potentially use any slot in the","chapter-9","Searching"
"hash table.","chapter-9","Searching"
"During insertion, the goal of collision resolution is to find a free slot in the hash","chapter-9","Searching"
"table when the home position for the record is already occupied. We can view any","chapter-9","Searching"
"collision resolution method as generating a sequence of hash table slots that can","chapter-9","Searching"
"potentially hold the record. The first slot in the sequence will be the home position","chapter-9","Searching"
"for the key. If the home position is occupied, then the collision resolution policy","chapter-9","Searching"
"goes to the next slot in the sequence. If this is occupied as well, then another slot","chapter-9","Searching"
"must be found, and so on. This sequence of slots is known as the probe sequence,","chapter-9","Searching"
"and it is generated by some probe function that we will call p. The insert function","chapter-9","Searching"
"is shown in Figure 9.6.","chapter-9","Searching"
"Method hashInsert first checks to see if the home slot for the key is empty.","chapter-9","Searching"
"If the home slot is occupied, then we use the probe function, p(k, i) to locate a free","chapter-9","Searching"
"slot in the table. Function p has two parameters, the key k and a count i for where","chapter-9","Searching"
"in the probe sequence we wish to be. That is, to get the first position in the probe","chapter-9","Searching"
"sequence after the home slot for key K, we call p(K, 1). For the next slot in the","chapter-9","Searching"
"probe sequence, call p(K, 2). Note that the probe function returns an offset from","chapter-9","Searching"
"the original home position, rather than a slot in the hash table. Thus, the for loop","chapter-9","Searching"
"in hashInsert is computing positions in the table at each iteration by adding","chapter-9","Searching"
"the value returned from the probe function to the home position. The ith call to p","chapter-9","Searching"
"returns the ith offset to be used.","chapter-9","Searching"
"Searching in a hash table follows the same probe sequence that was followed","chapter-9","Searching"
"when inserting records. In this way, a record not in its home position can be recov-","chapter-9","Searching"
"ered. A Java implementation for the search procedure is shown in Figure 9.7.","chapter-9","Searching"
"The insert and search routines assume that at least one slot on the probe se-","chapter-9","Searching"
"quence of every key will be empty. Otherwise, they will continue in an infinite","chapter-9","Searching"
"loop on unsuccessful searches. Thus, the dictionary should keep a count of the","chapter-9","Searching"
"Sec. 9.4 Hashing 325","chapter-9","Searching"
"/** Search in hash table HT for the record with key k */","chapter-9","Searching"
"E hashSearch(Key k) {","chapter-9","Searching"
"int home; // Home position for k","chapter-9","Searching"
"int pos = home = h(k); // Initial position","chapter-9","Searching"
"for (int i = 1; (HT[pos] != null) &&","chapter-9","Searching"
"(HT[pos].key().compareTo(k) != 0); i++)","chapter-9","Searching"
"pos = (home + p(k, i)) % M; // Next probe position","chapter-9","Searching"
"if (HT[pos] == null) return null; // Key not in hash table","chapter-9","Searching"
"else return HT[pos].value(); // Found it","chapter-9","Searching"
"}","chapter-9","Searching"
"Figure 9.7 Search method for a dictionary implemented by a hash table.","chapter-9","Searching"
"number of records stored, and refuse to insert into a table that has only one free","chapter-9","Searching"
"slot.","chapter-9","Searching"
"The discussion on bucket hashing presented a simple method of collision reso-","chapter-9","Searching"
"lution. If the home position for the record is occupied, then move down the bucket","chapter-9","Searching"
"until a free slot is found. This is an example of a technique for collision resolution","chapter-9","Searching"
"known as linear probing. The probe function for simple linear probing is","chapter-9","Searching"
"p(K, i) = i.","chapter-9","Searching"
"That is, the ith offset on the probe sequence is just i, meaning that the ith step is","chapter-9","Searching"
"simply to move down i slots in the table.","chapter-9","Searching"
"Once the bottom of the table is reached, the probe sequence wraps around to","chapter-9","Searching"
"the beginning of the table. Linear probing has the virtue that all slots in the table","chapter-9","Searching"
"will be candidates for inserting a new record before the probe sequence returns to","chapter-9","Searching"
"the home position.","chapter-9","Searching"
"While linear probing is probably the first idea that comes to mind when consid-","chapter-9","Searching"
"ering collision resolution policies, it is not the only one possible. Probe function p","chapter-9","Searching"
"allows us many options for how to do collision resolution. In fact, linear probing is","chapter-9","Searching"
"one of the worst collision resolution methods. The main problem is illustrated by","chapter-9","Searching"
"Figure 9.8. Here, we see a hash table of ten slots used to store four-digit numbers,","chapter-9","Searching"
"with hash function h(K) = K mod 10. In Figure 9.8(a), five numbers have been","chapter-9","Searching"
"placed in the table, leaving five slots remaining.","chapter-9","Searching"
"The ideal behavior for a collision resolution mechanism is that each empty slot","chapter-9","Searching"
"in the table will have equal probability of receiving the next record inserted (assum-","chapter-9","Searching"
"ing that every slot in the table has equal probability of being hashed to initially). In","chapter-9","Searching"
"this example, assume that the hash function gives each slot (roughly) equal proba-","chapter-9","Searching"
"bility of being the home position for the next key. However, consider what happens","chapter-9","Searching"
"to the next record if its key has its home position at slot 0. Linear probing will","chapter-9","Searching"
"send the record to slot 2. The same will happen to records whose home position","chapter-9","Searching"
"is at slot 1. A record with home position at slot 2 will remain in slot 2. Thus, the","chapter-9","Searching"
"probability is 3/10 that the next record inserted will end up in slot 2. In a similar","chapter-9","Searching"
"326 Chap. 9 Searching","chapter-9","Searching"
"0","chapter-9","Searching"
"1","chapter-9","Searching"
"2","chapter-9","Searching"
"4","chapter-9","Searching"
"3","chapter-9","Searching"
"5","chapter-9","Searching"
"6","chapter-9","Searching"
"7","chapter-9","Searching"
"9","chapter-9","Searching"
"0","chapter-9","Searching"
"1","chapter-9","Searching"
"2","chapter-9","Searching"
"3","chapter-9","Searching"
"4","chapter-9","Searching"
"5","chapter-9","Searching"
"6","chapter-9","Searching"
"7","chapter-9","Searching"
"8","chapter-9","Searching"
"9","chapter-9","Searching"
"8","chapter-9","Searching"
"9050","chapter-9","Searching"
"1001","chapter-9","Searching"
"9877","chapter-9","Searching"
"9050","chapter-9","Searching"
"1001","chapter-9","Searching"
"9877","chapter-9","Searching"
"2037","chapter-9","Searching"
"1059","chapter-9","Searching"
"2037","chapter-9","Searching"
"(a) (b)","chapter-9","Searching"
"Figure 9.8 Example of problems with linear probing. (a) Four values are inserted","chapter-9","Searching"
"in the order 1001, 9050, 9877, and 2037 using hash function h(K) = K mod 10.","chapter-9","Searching"
"(b) The value 1059 is added to the hash table.","chapter-9","Searching"
"manner, records hashing to slots 7 or 8 will end up in slot 9. However, only records","chapter-9","Searching"
"hashing to slot 3 will be stored in slot 3, yielding one chance in ten of this happen-","chapter-9","Searching"
"ing. Likewise, there is only one chance in ten that the next record will be stored","chapter-9","Searching"
"in slot 4, one chance in ten for slot 5, and one chance in ten for slot 6. Thus, the","chapter-9","Searching"
"resulting probabilities are not equal.","chapter-9","Searching"
"To make matters worse, if the next record ends up in slot 9 (which already has","chapter-9","Searching"
"a higher than normal chance of happening), then the following record will end up","chapter-9","Searching"
"in slot 2 with probability 6/10. This is illustrated by Figure 9.8(b). This tendency","chapter-9","Searching"
"of linear probing to cluster items together is known as primary clustering. Small","chapter-9","Searching"
"clusters tend to merge into big clusters, making the problem worse. The objection","chapter-9","Searching"
"to primary clustering is that it leads to long probe sequences.","chapter-9","Searching"
"Improved Collision Resolution Methods","chapter-9","Searching"
"How can we avoid primary clustering? One possible improvement might be to use","chapter-9","Searching"
"linear probing, but to skip slots by a constant c other than 1. This would make the","chapter-9","Searching"
"probe function","chapter-9","Searching"
"p(K, i) = ci,","chapter-9","Searching"
"and so the ith slot in the probe sequence will be (h(K) + ic) mod M. In this way,","chapter-9","Searching"
"records with adjacent home positions will not follow the same probe sequence. For","chapter-9","Searching"
"example, if we were to skip by twos, then our offsets from the home slot would","chapter-9","Searching"
"be 2, then 4, then 6, and so on.","chapter-9","Searching"
"Sec. 9.4 Hashing 327","chapter-9","Searching"
"One quality of a good probe sequence is that it will cycle through all slots in","chapter-9","Searching"
"the hash table before returning to the home position. Clearly linear probing (which","chapter-9","Searching"
"“skips” slots by one each time) does this. Unfortunately, not all values for c will","chapter-9","Searching"
"make this happen. For example, if c = 2 and the table contains an even number","chapter-9","Searching"
"of slots, then any key whose home position is in an even slot will have a probe","chapter-9","Searching"
"sequence that cycles through only the even slots. Likewise, the probe sequence","chapter-9","Searching"
"for a key whose home position is in an odd slot will cycle through the odd slots.","chapter-9","Searching"
"Thus, this combination of table size and linear probing constant effectively divides","chapter-9","Searching"
"the records into two sets stored in two disjoint sections of the hash table. So long","chapter-9","Searching"
"as both sections of the table contain the same number of records, this is not really","chapter-9","Searching"
"important. However, just from chance it is likely that one section will become fuller","chapter-9","Searching"
"than the other, leading to more collisions and poorer performance for those records.","chapter-9","Searching"
"The other section would have fewer records, and thus better performance. But the","chapter-9","Searching"
"overall system performance will be degraded, as the additional cost to the side that","chapter-9","Searching"
"is more full outweighs the improved performance of the less-full side.","chapter-9","Searching"
"Constant c must be relatively prime to M to generate a linear probing sequence","chapter-9","Searching"
"that visits all slots in the table (that is, c and M must share no factors). For a hash","chapter-9","Searching"
"table of size M = 10, if c is any one of 1, 3, 7, or 9, then the probe sequence","chapter-9","Searching"
"will visit all slots for any key. When M = 11, any value for c between 1 and 10","chapter-9","Searching"
"generates a probe sequence that visits all slots for every key.","chapter-9","Searching"
"Consider the situation where c = 2 and we wish to insert a record with key k1","chapter-9","Searching"
"such that h(k1) = 3. The probe sequence for k1 is 3, 5, 7, 9, and so on. If another","chapter-9","Searching"
"key k2 has home position at slot 5, then its probe sequence will be 5, 7, 9, and so on.","chapter-9","Searching"
"The probe sequences of k1 and k2 are linked together in a manner that contributes","chapter-9","Searching"
"to clustering. In other words, linear probing with a value of c > 1 does not solve","chapter-9","Searching"
"the problem of primary clustering. We would like to find a probe function that does","chapter-9","Searching"
"not link keys together in this way. We would prefer that the probe sequence for k1","chapter-9","Searching"
"after the first step on the sequence should not be identical to the probe sequence of","chapter-9","Searching"
"k2. Instead, their probe sequences should diverge.","chapter-9","Searching"
"The ideal probe function would select the next position on the probe sequence","chapter-9","Searching"
"at random from among the unvisited slots; that is, the probe sequence should be a","chapter-9","Searching"
"random permutation of the hash table positions. Unfortunately, we cannot actually","chapter-9","Searching"
"select the next position in the probe sequence at random, because then we would not","chapter-9","Searching"
"be able to duplicate this same probe sequence when searching for the key. However,","chapter-9","Searching"
"we can do something similar called pseudo-random probing. In pseudo-random","chapter-9","Searching"
"probing, the ith slot in the probe sequence is (h(K) + ri) mod M where ri","chapter-9","Searching"
"is the","chapter-9","Searching"
"ith value in a random permutation of the numbers from 1 to M − 1. All insertion","chapter-9","Searching"
"and search operations use the same random permutation. The probe function is","chapter-9","Searching"
"p(K, i) = Perm[i − 1],","chapter-9","Searching"
"where Perm is an array of length M − 1 containing a random permutation of the","chapter-9","Searching"
"values from 1 to M − 1.","chapter-9","Searching"
"328 Chap. 9 Searching","chapter-9","Searching"
"Example 9.9 Consider a table of size M = 101, with Perm[1] = 5,","chapter-9","Searching"
"Perm[2] = 2, and Perm[3] = 32. Assume that we have two keys k1 and","chapter-9","Searching"
"k2 where h(k1) = 30 and h(k2) = 35. The probe sequence for k1 is 30,","chapter-9","Searching"
"then 35, then 32, then 62. The probe sequence for k2 is 35, then 40, then","chapter-9","Searching"
"37, then 67. Thus, while k2 will probe to k1’s home position as its second","chapter-9","Searching"
"choice, the two keys’ probe sequences diverge immediately thereafter.","chapter-9","Searching"
"Another probe function that eliminates primary clustering is called quadratic","chapter-9","Searching"
"probing. Here the probe function is some quadratic function","chapter-9","Searching"
"p(K, i) = c1i","chapter-9","Searching"
"2 + c2i + c3","chapter-9","Searching"
"for some choice of constants c1, c2, and c3. The simplest variation is p(K, i) = i","chapter-9","Searching"
"2","chapter-9","Searching"
"(i.e., c1 = 1, c2 = 0, and c3 = 0. Then the ith value in the probe sequence would","chapter-9","Searching"
"be (h(K) + i","chapter-9","Searching"
"2","chapter-9","Searching"
") mod M. Under quadratic probing, two keys with different home","chapter-9","Searching"
"positions will have diverging probe sequences.","chapter-9","Searching"
"Example 9.10 Given a hash table of size M = 101, assume for keys k1","chapter-9","Searching"
"and k2 that h(k1) = 30 and h(k2) = 29. The probe sequence for k1 is 30,","chapter-9","Searching"
"then 31, then 34, then 39. The probe sequence for k2 is 29, then 30, then","chapter-9","Searching"
"33, then 38. Thus, while k2 will probe to k1’s home position as its second","chapter-9","Searching"
"choice, the two keys’ probe sequences diverge immediately thereafter.","chapter-9","Searching"
"Unfortunately, quadratic probing has the disadvantage that typically not all hash","chapter-9","Searching"
"table slots will be on the probe sequence. Using p(K, i) = i","chapter-9","Searching"
"2 gives particularly in-","chapter-9","Searching"
"consistent results. For many hash table sizes, this probe function will cycle through","chapter-9","Searching"
"a relatively small number of slots. If all slots on that cycle happen to be full, then","chapter-9","Searching"
"the record cannot be inserted at all! For example, if our hash table has three slots,","chapter-9","Searching"
"then records that hash to slot 0 can probe only to slots 0 and 1 (that is, the probe","chapter-9","Searching"
"sequence will never visit slot 2 in the table). Thus, if slots 0 and 1 are full, then","chapter-9","Searching"
"the record cannot be inserted even though the table is not full. A more realistic","chapter-9","Searching"
"example is a table with 105 slots. The probe sequence starting from any given slot","chapter-9","Searching"
"will only visit 23 other slots in the table. If all 24 of these slots should happen to","chapter-9","Searching"
"be full, even if other slots in the table are empty, then the record cannot be inserted","chapter-9","Searching"
"because the probe sequence will continually hit only those same 24 slots.","chapter-9","Searching"
"Fortunately, it is possible to get good results from quadratic probing at low","chapter-9","Searching"
"cost. The right combination of probe function and table size will visit many slots","chapter-9","Searching"
"in the table. In particular, if the hash table size is a prime number and the probe","chapter-9","Searching"
"function is p(K, i) = i","chapter-9","Searching"
"2","chapter-9","Searching"
", then at least half the slots in the table will be visited.","chapter-9","Searching"
"Thus, if the table is less than half full, we can be certain that a free slot will be","chapter-9","Searching"
"found. Alternatively, if the hash table size is a power of two and the probe function","chapter-9","Searching"
"Sec. 9.4 Hashing 329","chapter-9","Searching"
"is p(K, i) = (i","chapter-9","Searching"
"2 + i)/2, then every slot in the table will be visited by the probe","chapter-9","Searching"
"function.","chapter-9","Searching"
"Both pseudo-random probing and quadratic probing eliminate primary cluster-","chapter-9","Searching"
"ing, which is the problem of keys sharing substantial segments of a probe sequence.","chapter-9","Searching"
"If two keys hash to the same home position, however, then they will always follow","chapter-9","Searching"
"the same probe sequence for every collision resolution method that we have seen so","chapter-9","Searching"
"far. The probe sequences generated by pseudo-random and quadratic probing (for","chapter-9","Searching"
"example) are entirely a function of the home position, not the original key value.","chapter-9","Searching"
"This is because function p ignores its input parameter K for these collision resolu-","chapter-9","Searching"
"tion methods. If the hash function generates a cluster at a particular home position,","chapter-9","Searching"
"then the cluster remains under pseudo-random and quadratic probing. This problem","chapter-9","Searching"
"is called secondary clustering.","chapter-9","Searching"
"To avoid secondary clustering, we need to have the probe sequence make use of","chapter-9","Searching"
"the original key value in its decision-making process. A simple technique for doing","chapter-9","Searching"
"this is to return to linear probing by a constant step size for the probe function, but","chapter-9","Searching"
"to have that constant be determined by a second hash function, h2. Thus, the probe","chapter-9","Searching"
"sequence would be of the form p(K, i) = i ∗h2(K). This method is called double","chapter-9","Searching"
"hashing.","chapter-9","Searching"
"Example 9.11 Assume a hash table has size M = 101, and that there","chapter-9","Searching"
"are three keys k1, k2, and k3 with h(k1) = 30, h(k2) = 28, h(k3) = 30,","chapter-9","Searching"
"h2(k1) = 2, h2(k2) = 5, and h2(k3) = 5. Then, the probe sequence","chapter-9","Searching"
"for k1 will be 30, 32, 34, 36, and so on. The probe sequence for k2 will","chapter-9","Searching"
"be 28, 33, 38, 43, and so on. The probe sequence for k3 will be 30, 35,","chapter-9","Searching"
"40, 45, and so on. Thus, none of the keys share substantial portions of the","chapter-9","Searching"
"same probe sequence. Of course, if a fourth key k4 has h(k4) = 28 and","chapter-9","Searching"
"h2(k4) = 2, then it will follow the same probe sequence as k1. Pseudo-","chapter-9","Searching"
"random or quadratic probing can be combined with double hashing to solve","chapter-9","Searching"
"this problem.","chapter-9","Searching"
"A good implementation of double hashing should ensure that all of the probe","chapter-9","Searching"
"sequence constants are relatively prime to the table size M. This can be achieved","chapter-9","Searching"
"easily. One way is to select M to be a prime number, and have h2 return a value in","chapter-9","Searching"
"the range 1 ≤ h2(K) ≤ M − 1. Another way is to set M = 2m for some value m","chapter-9","Searching"
"and have h2 return an odd value between 1 and 2","chapter-9","Searching"
"m.","chapter-9","Searching"
"Figure 9.9 shows an implementation of the dictionary ADT by means of a hash","chapter-9","Searching"
"table. The simplest hash function is used, with collision resolution by linear prob-","chapter-9","Searching"
"ing, as the basis for the structure of a hash table implementation. A suggested","chapter-9","Searching"
"project at the end of this chapter asks you to improve the implementation with","chapter-9","Searching"
"other hash functions and collision resolution policies.","chapter-9","Searching"
"330 Chap. 9 Searching","chapter-9","Searching"
"/** Dictionary implemented using hashing. */","chapter-9","Searching"
"class HashDictionary<Key extends Comparable<? super Key>, E>","chapter-9","Searching"
"implements Dictionary<Key, E> {","chapter-9","Searching"
"private static final int defaultSize = 10;","chapter-9","Searching"
"private HashTable<Key,E> T; // The hash table","chapter-9","Searching"
"private int count; // # of records now in table","chapter-9","Searching"
"private int maxsize; // Maximum size of dictionary","chapter-9","Searching"
"HashDictionary() { this(defaultSize); }","chapter-9","Searching"
"HashDictionary(int sz) {","chapter-9","Searching"
"T = new HashTable<Key,E>(sz);","chapter-9","Searching"
"count = 0;","chapter-9","Searching"
"maxsize = sz;","chapter-9","Searching"
"}","chapter-9","Searching"
"public void clear() { /** Reinitialize */","chapter-9","Searching"
"T = new HashTable<Key,E>(maxsize);","chapter-9","Searching"
"count = 0;","chapter-9","Searching"
"}","chapter-9","Searching"
"public void insert(Key k, E e) { /** Insert an element */","chapter-9","Searching"
"assert count < maxsize : "Hash table is full";","chapter-9","Searching"
"T.hashInsert(k, e);","chapter-9","Searching"
"count++;","chapter-9","Searching"
"}","chapter-9","Searching"
"public E remove(Key k) { /** Remove an element */","chapter-9","Searching"
"E temp = T.hashRemove(k);","chapter-9","Searching"
"if (temp != null) count--;","chapter-9","Searching"
"return temp;","chapter-9","Searching"
"}","chapter-9","Searching"
"public E removeAny() { /** Remove some element. */","chapter-9","Searching"
"if (count != 0) {","chapter-9","Searching"
"count--;","chapter-9","Searching"
"return T.hashRemoveAny();","chapter-9","Searching"
"}","chapter-9","Searching"
"else return null;","chapter-9","Searching"
"}","chapter-9","Searching"
"/** Find a record with key value "k" */","chapter-9","Searching"
"public E find(Key k) { return T.hashSearch(k); }","chapter-9","Searching"
"/** Return number of values in the hash table */","chapter-9","Searching"
"public int size() { return count; }","chapter-9","Searching"
"}","chapter-9","Searching"
"Figure 9.9 A partial implementation for the dictionary ADT using a hash ta-","chapter-9","Searching"
"ble. This uses a poor hash function and a poor collision resolution policy (linear","chapter-9","Searching"
"probing), which can easily be replaced. Member functions hashInsert and","chapter-9","Searching"
"hashSearch appear in Figures 9.6 and 9.7, respectively.","chapter-9","Searching"
"Sec. 9.4 Hashing 331","chapter-9","Searching"
"9.4.4 Analysis of Closed Hashing","chapter-9","Searching"
"How efficient is hashing? We can measure hashing performance in terms of the","chapter-9","Searching"
"number of record accesses required when performing an operation. The primary","chapter-9","Searching"
"operations of concern are insertion, deletion, and search. It is useful to distinguish","chapter-9","Searching"
"between successful and unsuccessful searches. Before a record can be deleted, it","chapter-9","Searching"
"must be found. Thus, the number of accesses required to delete a record is equiv-","chapter-9","Searching"
"alent to the number required to successfully search for it. To insert a record, an","chapter-9","Searching"
"empty slot along the record’s probe sequence must be found. This is equivalent to","chapter-9","Searching"
"an unsuccessful search for the record (recall that a successful search for the record","chapter-9","Searching"
"during insertion should generate an error because two records with the same key","chapter-9","Searching"
"are not allowed to be stored in the table).","chapter-9","Searching"
"When the hash table is empty, the first record inserted will always find its home","chapter-9","Searching"
"position free. Thus, it will require only one record access to find a free slot. If all","chapter-9","Searching"
"records are stored in their home positions, then successful searches will also require","chapter-9","Searching"
"only one record access. As the table begins to fill up, the probability that a record","chapter-9","Searching"
"can be inserted into its home position decreases. If a record hashes to an occupied","chapter-9","Searching"
"slot, then the collision resolution policy must locate another slot in which to store","chapter-9","Searching"
"it. Finding records not stored in their home position also requires additional record","chapter-9","Searching"
"accesses as the record is searched for along its probe sequence. As the table fills","chapter-9","Searching"
"up, more and more records are likely to be located ever further from their home","chapter-9","Searching"
"positions.","chapter-9","Searching"
"From this discussion, we see that the expected cost of hashing is a function of","chapter-9","Searching"
"how full the table is. Define the load factor for the table as α = N/M, where N","chapter-9","Searching"
"is the number of records currently in the table.","chapter-9","Searching"
"An estimate of the expected cost for an insertion (or an unsuccessful search)","chapter-9","Searching"
"can be derived analytically as a function of α in the case where we assume that","chapter-9","Searching"
"the probe sequence follows a random permutation of the slots in the hash table.","chapter-9","Searching"
"Assuming that every slot in the table has equal probability of being the home slot","chapter-9","Searching"
"for the next record, the probability of finding the home position occupied is α. The","chapter-9","Searching"
"probability of finding both the home position occupied and the next slot on the","chapter-9","Searching"
"probe sequence occupied is N(N−1)","chapter-9","Searching"
"M(M−1) . The probability of i collisions is","chapter-9","Searching"
"N(N − 1)· · ·(N − i + 1)","chapter-9","Searching"
"M(M − 1)· · ·(M − i + 1).","chapter-9","Searching"
"If N and M are large, then this is approximately (N/M)","chapter-9","Searching"
"i","chapter-9","Searching"
". The expected number","chapter-9","Searching"
"of probes is one plus the sum over i ≥ 1 of the probability of i collisions, which is","chapter-9","Searching"
"approximately","chapter-9","Searching"
"1 +X∞","chapter-9","Searching"
"i=1","chapter-9","Searching"
"(N/M)","chapter-9","Searching"
"i = 1/(1 − α).","chapter-9","Searching"
"332 Chap. 9 Searching","chapter-9","Searching"
"The cost for a successful search (or a deletion) has the same cost as originally","chapter-9","Searching"
"inserting that record. However, the expected value for the insertion cost depends","chapter-9","Searching"
"on the value of α not at the time of deletion, but rather at the time of the original","chapter-9","Searching"
"insertion. We can derive an estimate of this cost (essentially an average over all the","chapter-9","Searching"
"insertion costs) by integrating from 0 to the current value of α, yielding a result of","chapter-9","Searching"
"1","chapter-9","Searching"
"α","chapter-9","Searching"
"Z α","chapter-9","Searching"
"0","chapter-9","Searching"
"1","chapter-9","Searching"
"1 − x","chapter-9","Searching"
"dx =","chapter-9","Searching"
"1","chapter-9","Searching"
"α","chapter-9","Searching"
"loge","chapter-9","Searching"
"1","chapter-9","Searching"
"1 − α","chapter-9","Searching"
".","chapter-9","Searching"
"It is important to realize that these equations represent the expected cost for","chapter-9","Searching"
"operations using the unrealistic assumption that the probe sequence is based on a","chapter-9","Searching"
"random permutation of the slots in the hash table (thus avoiding all expense result-","chapter-9","Searching"
"ing from clustering). Thus, these costs are lower-bound estimates in the average","chapter-9","Searching"
"case. The true average cost under linear probing is 1","chapter-9","Searching"
"2","chapter-9","Searching"
"(1+ 1/(1−α)","chapter-9","Searching"
"2","chapter-9","Searching"
") for insertions","chapter-9","Searching"
"or unsuccessful searches and 1","chapter-9","Searching"
"2","chapter-9","Searching"
"(1+ 1/(1−α)) for deletions or successful searches.","chapter-9","Searching"
"Proofs for these results can be found in the references cited in Section 9.5.","chapter-9","Searching"
"Figure 9.10 shows the graphs of these four equations to help you visualize the","chapter-9","Searching"
"expected performance of hashing based on the load factor. The two solid lines show","chapter-9","Searching"
"the costs in the case of a “random” probe sequence for (1) insertion or unsuccessful","chapter-9","Searching"
"search and (2) deletion or successful search. As expected, the cost for insertion or","chapter-9","Searching"
"unsuccessful search grows faster, because these operations typically search further","chapter-9","Searching"
"down the probe sequence. The two dashed lines show equivalent costs for linear","chapter-9","Searching"
"probing. As expected, the cost of linear probing grows faster than the cost for","chapter-9","Searching"
"“random” probing.","chapter-9","Searching"
"From Figure 9.10 we see that the cost for hashing when the table is not too full","chapter-9","Searching"
"is typically close to one record access. This is extraordinarily efficient, much better","chapter-9","Searching"
"than binary search which requires log n record accesses. As α increases, so does","chapter-9","Searching"
"the expected cost. For small values of α, the expected cost is low. It remains below","chapter-9","Searching"
"two until the hash table is about half full. When the table is nearly empty, adding","chapter-9","Searching"
"a new record to the table does not increase the cost of future search operations","chapter-9","Searching"
"by much. However, the additional search cost caused by each additional insertion","chapter-9","Searching"
"increases rapidly once the table becomes half full. Based on this analysis, the rule","chapter-9","Searching"
"of thumb is to design a hashing system so that the hash table never gets above half","chapter-9","Searching"
"full. Beyond that point performance will degrade rapidly. This requires that the","chapter-9","Searching"
"implementor have some idea of how many records are likely to be in the table at","chapter-9","Searching"
"maximum loading, and select the table size accordingly.","chapter-9","Searching"
"You might notice that a recommendation to never let a hash table become more","chapter-9","Searching"
"than half full contradicts the disk-based space/time tradeoff principle, which strives","chapter-9","Searching"
"to minimize disk space to increase information density. Hashing represents an un-","chapter-9","Searching"
"usual situation in that there is no benefit to be expected from locality of reference.","chapter-9","Searching"
"In a sense, the hashing system implementor does everything possible to eliminate","chapter-9","Searching"
"the effects of locality of reference! Given the disk block containing the last record","chapter-9","Searching"
"Sec. 9.4 Hashing 333","chapter-9","Searching"
"1","chapter-9","Searching"
"2","chapter-9","Searching"
"3","chapter-9","Searching"
"4","chapter-9","Searching"
"5","chapter-9","Searching"
"Insert Delete","chapter-9","Searching"
"0 .2 .4 .6 .8 1.0","chapter-9","Searching"
"Figure 9.10 Growth of expected record accesses with α. The horizontal axis is","chapter-9","Searching"
"the value for α, the vertical axis is the expected number of accesses to the hash","chapter-9","Searching"
"table. Solid lines show the cost for “random” probing (a theoretical lower bound","chapter-9","Searching"
"on the cost), while dashed lines show the cost for linear probing (a relatively poor","chapter-9","Searching"
"collision resolution strategy). The two leftmost lines show the cost for insertion","chapter-9","Searching"
"(equivalently, unsuccessful search); the two rightmost lines show the cost for dele-","chapter-9","Searching"
"tion (equivalently, successful search).","chapter-9","Searching"
"accessed, the chance of the next record access coming to the same disk block is","chapter-9","Searching"
"no better than random chance in a well-designed hash system. This is because a","chapter-9","Searching"
"good hashing implementation breaks up relationships between search keys. Instead","chapter-9","Searching"
"of improving performance by taking advantage of locality of reference, hashing","chapter-9","Searching"
"trades increased hash table space for an improved chance that the record will be","chapter-9","Searching"
"in its home position. Thus, the more space available for the hash table, the more","chapter-9","Searching"
"efficient hashing should be.","chapter-9","Searching"
"Depending on the pattern of record accesses, it might be possible to reduce the","chapter-9","Searching"
"expected cost of access even in the face of collisions. Recall the 80/20 rule: 80%","chapter-9","Searching"
"of the accesses will come to 20% of the data. In other words, some records are","chapter-9","Searching"
"accessed more frequently. If two records hash to the same home position, which","chapter-9","Searching"
"would be better placed in the home position, and which in a slot further down the","chapter-9","Searching"
"probe sequence? The answer is that the record with higher frequency of access","chapter-9","Searching"
"should be placed in the home position, because this will reduce the total number of","chapter-9","Searching"
"record accesses. Ideally, records along a probe sequence will be ordered by their","chapter-9","Searching"
"frequency of access.","chapter-9","Searching"
"One approach to approximating this goal is to modify the order of records along","chapter-9","Searching"
"the probe sequence whenever a record is accessed. If a search is made to a record","chapter-9","Searching"
"334 Chap. 9 Searching","chapter-9","Searching"
"that is not in its home position, a self-organizing list heuristic can be used. For","chapter-9","Searching"
"example, if the linear probing collision resolution policy is used, then whenever a","chapter-9","Searching"
"record is located that is not in its home position, it can be swapped with the record","chapter-9","Searching"
"preceding it in the probe sequence. That other record will now be further from","chapter-9","Searching"
"its home position, but hopefully it will be accessed less frequently. Note that this","chapter-9","Searching"
"approach will not work for the other collision resolution policies presented in this","chapter-9","Searching"
"section, because swapping a pair of records to improve access to one might remove","chapter-9","Searching"
"the other from its probe sequence.","chapter-9","Searching"
"Another approach is to keep access counts for records and periodically rehash","chapter-9","Searching"
"the entire table. The records should be inserted into the hash table in frequency","chapter-9","Searching"
"order, ensuring that records that were frequently accessed during the last series of","chapter-9","Searching"
"requests have the best chance of being near their home positions.","chapter-9","Searching"
"9.4.5 Deletion","chapter-9","Searching"
"When deleting records from a hash table, there are two important considerations.","chapter-9","Searching"
"1. Deleting a record must not hinder later searches. In other words, the search","chapter-9","Searching"
"process must still pass through the newly emptied slot to reach records whose","chapter-9","Searching"
"probe sequence passed through this slot. Thus, the delete process cannot","chapter-9","Searching"
"simply mark the slot as empty, because this will isolate records further down","chapter-9","Searching"
"the probe sequence. For example, in Figure 9.8(a), keys 9877 and 2037 both","chapter-9","Searching"
"hash to slot 7. Key 2037 is placed in slot 8 by the collision resolution policy.","chapter-9","Searching"
"If 9877 is deleted from the table, a search for 2037 must still pass through","chapter-9","Searching"
"Slot 7 as it probes to slot 8.","chapter-9","Searching"
"2. We do not want to make positions in the hash table unusable because of","chapter-9","Searching"
"deletion. The freed slot should be available to a future insertion.","chapter-9","Searching"
"Both of these problems can be resolved by placing a special mark in place","chapter-9","Searching"
"of the deleted record, called a tombstone. The tombstone indicates that a record","chapter-9","Searching"
"once occupied the slot but does so no longer. If a tombstone is encountered when","chapter-9","Searching"
"searching along a probe sequence, the search procedure continues with the search.","chapter-9","Searching"
"When a tombstone is encountered during insertion, that slot can be used to store the","chapter-9","Searching"
"new record. However, to avoid inserting duplicate keys, it will still be necessary for","chapter-9","Searching"
"the search procedure to follow the probe sequence until a truly empty position has","chapter-9","Searching"
"been found, simply to verify that a duplicate is not in the table. However, the new","chapter-9","Searching"
"record would actually be inserted into the slot of the first tombstone encountered.","chapter-9","Searching"
"The use of tombstones allows searches to work correctly and allows reuse of","chapter-9","Searching"
"deleted slots. However, after a series of intermixed insertion and deletion opera-","chapter-9","Searching"
"tions, some slots will contain tombstones. This will tend to lengthen the average","chapter-9","Searching"
"distance from a record’s home position to the record itself, beyond where it could","chapter-9","Searching"
"be if the tombstones did not exist. A typical database application will first load a","chapter-9","Searching"
"collection of records into the hash table and then progress to a phase of intermixed","chapter-9","Searching"
"Sec. 9.5 Further Reading 335","chapter-9","Searching"
"insertions and deletions. After the table is loaded with the initial collection of","chapter-9","Searching"
"records, the first few deletions will lengthen the average probe sequence distance","chapter-9","Searching"
"for records (it will add tombstones). Over time, the average distance will reach","chapter-9","Searching"
"an equilibrium point because insertions will tend to decrease the average distance","chapter-9","Searching"
"by filling in tombstone slots. For example, after initially loading records into the","chapter-9","Searching"
"database, the average path distance might be 1.2 (i.e., an average of 0.2 accesses","chapter-9","Searching"
"per search beyond the home position will be required). After a series of insertions","chapter-9","Searching"
"and deletions, this average distance might increase to 1.6 due to tombstones. This","chapter-9","Searching"
"seems like a small increase, but it is three times longer on average beyond the home","chapter-9","Searching"
"position than before deletions.","chapter-9","Searching"
"Two possible solutions to this problem are","chapter-9","Searching"
"1. Do a local reorganization upon deletion to try to shorten the average path","chapter-9","Searching"
"length. For example, after deleting a key, continue to follow the probe se-","chapter-9","Searching"
"quence of that key and swap records further down the probe sequence into","chapter-9","Searching"
"the slot of the recently deleted record (being careful not to remove any key","chapter-9","Searching"
"from its probe sequence). This will not work for all collision resolution poli-","chapter-9","Searching"
"cies.","chapter-9","Searching"
"2. Periodically rehash the table by reinserting all records into a new hash table.","chapter-9","Searching"
"Not only will this remove the tombstones, but it also provides an opportunity","chapter-9","Searching"
"to place the most frequently accessed records into their home positions.","chapter-9","Searching"
"9.5 Further Reading","chapter-9","Searching"
"For a comparison of the efficiencies for various self-organizing techniques, see","chapter-9","Searching"
"Bentley and McGeoch, “Amortized Analysis of Self-Organizing Sequential Search","chapter-9","Searching"
"Heuristics” [BM85]. The text compression example of Section 9.2 comes from","chapter-9","Searching"
"Bentley et al., “A Locally Adaptive Data Compression Scheme” [BSTW86]. For","chapter-9","Searching"
"more on Ziv-Lempel coding, see Data Compression: Methods and Theory by","chapter-9","Searching"
"James A. Storer [Sto88]. Knuth covers self-organizing lists and Zipf distributions","chapter-9","Searching"
"in Volume 3 of The Art of Computer Programming[Knu98].","chapter-9","Searching"
"Introduction to Modern Information Retrieval by Salton and McGill [SM83] is","chapter-9","Searching"
"an excellent source for more information about document retrieval techniques.","chapter-9","Searching"
"See the paper “Practical Minimal Perfect Hash Functions for Large Databases”","chapter-9","Searching"
"by Fox et al. [FHCD92] for an introduction and a good algorithm for perfect hash-","chapter-9","Searching"
"ing.","chapter-9","Searching"
"For further details on the analysis for various collision resolution policies, see","chapter-9","Searching"
"Knuth, Volume 3 [Knu98] and Concrete Mathematics: A Foundation for Computer","chapter-9","Searching"
"Science by Graham, Knuth, and Patashnik [GKP94].","chapter-9","Searching"
"The model of hashing presented in this chapter has been of a fixed-size hash","chapter-9","Searching"
"table. A problem not addressed is what to do when the hash table gets half full and","chapter-9","Searching"
"more records must be inserted. This is the domain of dynamic hashing methods.","chapter-9","Searching"
"336 Chap. 9 Searching","chapter-9","Searching"
"A good introduction to this topic is “Dynamic Hashing Schemes” by R.J. Enbody","chapter-9","Searching"
"and H.C. Du [ED88].","chapter-9","Searching"
"9.6 Exercises","chapter-9","Searching"
"9.1 Create a graph showing expected cost versus the probability of an unsuc-","chapter-9","Searching"
"cessful search when performing sequential search (see Section 9.1). What","chapter-9","Searching"
"can you say qualitatively about the rate of increase in expected cost as the","chapter-9","Searching"
"probability of unsuccessful search grows?","chapter-9","Searching"
"9.2 Modify the binary search routine of Section 3.5 to implement interpolation","chapter-9","Searching"
"search. Assume that keys are in the range 1 to 10,000, and that all key values","chapter-9","Searching"
"within the range are equally likely to occur.","chapter-9","Searching"
"9.3 Write an algorithm to find the Kth smallest value in an unsorted array of n","chapter-9","Searching"
"numbers (K <= n). Your algorithm should require Θ(n) time in the average","chapter-9","Searching"
"case. Hint: Your algorithm should look similar to Quicksort.","chapter-9","Searching"
"9.4 Example 9.9.3 discusses a distribution where the relative frequencies of the","chapter-9","Searching"
"records match the harmonic series. That is, for every occurrence of the first","chapter-9","Searching"
"record, the second record will appear half as often, the third will appear one","chapter-9","Searching"
"third as often, the fourth one quarter as often, and so on. The actual prob-","chapter-9","Searching"
"ability for the ith record was defined to be 1/(iHn). Explain why this is","chapter-9","Searching"
"correct.","chapter-9","Searching"
"9.5 Graph the equations T(n) = log2 n and T(n) = n/ loge n. Which gives the","chapter-9","Searching"
"better performance, binary search on a sorted list, or sequential search on a","chapter-9","Searching"
"list ordered by frequency where the frequency conforms to a Zipf distribu-","chapter-9","Searching"
"tion? Characterize the difference in running times.","chapter-9","Searching"
"9.6 Assume that the values A through H are stored in a self-organizing list, ini-","chapter-9","Searching"
"tially in ascending order. Consider the three self-organizing list heuristics:","chapter-9","Searching"
"count, move-to-front, and transpose. For count, assume that the record is","chapter-9","Searching"
"moved ahead in the list passing over any other record that its count is now","chapter-9","Searching"
"greater than. For each, show the resulting list and the total number of com-","chapter-9","Searching"
"parisons required resulting from the following series of accesses:","chapter-9","Searching"
"D H H G H E G H G H E C E H G.","chapter-9","Searching"
"9.7 For each of the three self-organizing list heuristics (count, move-to-front, and","chapter-9","Searching"
"transpose), describe a series of record accesses for which it would require the","chapter-9","Searching"
"greatest number of comparisons of the three.","chapter-9","Searching"
"9.8 Write an algorithm to implement the frequency count self-organizing list","chapter-9","Searching"
"heuristic, assuming that the list is implemented using an array. In particu-","chapter-9","Searching"
"lar, write a function FreqCount that takes as input a value to be searched","chapter-9","Searching"
"for and which adjusts the list appropriately. If the value is not already in the","chapter-9","Searching"
"list, add it to the end of the list with a frequency count of one.","chapter-9","Searching"
"Sec. 9.6 Exercises 337","chapter-9","Searching"
"9.9 Write an algorithm to implement the move-to-front self-organizing list heuri-","chapter-9","Searching"
"stic, assuming that the list is implemented using an array. In particular, write","chapter-9","Searching"
"a function MoveToFront that takes as input a value to be searched for and","chapter-9","Searching"
"which adjusts the list appropriately. If the value is not already in the list, add","chapter-9","Searching"
"it to the beginning of the list.","chapter-9","Searching"
"9.10 Write an algorithm to implement the transpose self-organizing list heuristic,","chapter-9","Searching"
"assuming that the list is implemented using an array. In particular, write","chapter-9","Searching"
"a function Transpose that takes as input a value to be searched for and","chapter-9","Searching"
"which adjusts the list appropriately. If the value is not already in the list, add","chapter-9","Searching"
"it to the end of the list.","chapter-9","Searching"
"9.11 Write functions for computing union, intersection, and set difference on ar-","chapter-9","Searching"
"bitrarily long bit vectors used to represent set membership as described in","chapter-9","Searching"
"Section 9.3. Assume that for each operation both vectors are of equal length.","chapter-9","Searching"
"9.12 Compute the probabilities for the following situations. These probabilities","chapter-9","Searching"
"can be computed analytically, or you may write a computer program to gen-","chapter-9","Searching"
"erate the probabilities by simulation.","chapter-9","Searching"
"(a) Out of a group of 23 students, what is the probability that 2 students","chapter-9","Searching"
"share the same birthday?","chapter-9","Searching"
"(b) Out of a group of 100 students, what is the probability that 3 students","chapter-9","Searching"
"share the same birthday?","chapter-9","Searching"
"(c) How many students must be in the class for the probability to be at least","chapter-9","Searching"
"50% that there are 2 who share a birthday in the same month?","chapter-9","Searching"
"9.13 Assume that you are hashing key K to a hash table of n slots (indexed from","chapter-9","Searching"
"0 to n − 1). For each of the following functions h(K), is the function ac-","chapter-9","Searching"
"ceptable as a hash function (i.e., would the hash program work correctly for","chapter-9","Searching"
"both insertions and searches), and if so, is it a good hash function? Function","chapter-9","Searching"
"Random(n) returns a random integer between 0 and n − 1, inclusive.","chapter-9","Searching"
"(a) h(k) = k/n where k and n are integers.","chapter-9","Searching"
"(b) h(k) = 1.","chapter-9","Searching"
"(c) h(k) = (k + Random(n)) mod n.","chapter-9","Searching"
"(d) h(k) = k mod n where n is a prime number.","chapter-9","Searching"
"9.14 Assume that you have a seven-slot closed hash table (the slots are numbered","chapter-9","Searching"
"0 through 6). Show the final hash table that would result if you used the","chapter-9","Searching"
"hash function h(k) = k mod 7 and linear probing on this list of numbers:","chapter-9","Searching"
"3, 12, 9, 2. After inserting the record with key value 2, list for each empty","chapter-9","Searching"
"slot the probability that it will be the next one filled.","chapter-9","Searching"
"9.15 Assume that you have a ten-slot closed hash table (the slots are numbered 0","chapter-9","Searching"
"through 9). Show the final hash table that would result if you used the hash","chapter-9","Searching"
"function h(k) = k mod 10 and quadratic probing on this list of numbers:","chapter-9","Searching"
"3, 12, 9, 2, 79, 46. After inserting the record with key value 46, list for each","chapter-9","Searching"
"empty slot the probability that it will be the next one filled.","chapter-9","Searching"
"338 Chap. 9 Searching","chapter-9","Searching"
"9.16 Assume that you have a ten-slot closed hash table (the slots are numbered","chapter-9","Searching"
"0 through 9). Show the final hash table that would result if you used the","chapter-9","Searching"
"hash function h(k) = k mod 10 and pseudo-random probing on this list of","chapter-9","Searching"
"numbers: 3, 12, 9, 2, 79, 44. The permutation of offsets to be used by the","chapter-9","Searching"
"pseudo-random probing will be: 5, 9, 2, 1, 4, 8, 6, 3, 7. After inserting the","chapter-9","Searching"
"record with key value 44, list for each empty slot the probability that it will","chapter-9","Searching"
"be the next one filled.","chapter-9","Searching"
"9.17 What is the result of running sfold from Section 9.4.1 on the following","chapter-9","Searching"
"strings? Assume a hash table size of 101 slots.","chapter-9","Searching"
"(a) HELLO WORLD","chapter-9","Searching"
"(b) NOW HEAR THIS","chapter-9","Searching"
"(c) HEAR THIS NOW","chapter-9","Searching"
"9.18 Using closed hashing, with double hashing to resolve collisions, insert the","chapter-9","Searching"
"following keys into a hash table of thirteen slots (the slots are numbered","chapter-9","Searching"
"0 through 12). The hash functions to be used are H1 and H2, defined be-","chapter-9","Searching"
"low. You should show the hash table after all eight keys have been inserted.","chapter-9","Searching"
"Be sure to indicate how you are using H1 and H2 to do the hashing. Func-","chapter-9","Searching"
"tion Rev(k) reverses the decimal digits of k, for example, Rev(37) = 73;","chapter-9","Searching"
"Rev(7) = 7.","chapter-9","Searching"
"H1(k) = k mod 13.","chapter-9","Searching"
"H2(k) = (Rev(k + 1) mod 11).","chapter-9","Searching"
"Keys: 2, 8, 31, 20, 19, 18, 53, 27.","chapter-9","Searching"
"9.19 Write an algorithm for a deletion function for hash tables that replaces the","chapter-9","Searching"
"record with a special value indicating a tombstone. Modify the functions","chapter-9","Searching"
"hashInsert and hashSearch to work correctly with tombstones.","chapter-9","Searching"
"9.20 Consider the following permutation for the numbers 1 to 6:","chapter-9","Searching"
"2, 4, 6, 1, 3, 5.","chapter-9","Searching"
"Analyze what will happen if this permutation is used by an implementation of","chapter-9","Searching"
"pseudo-random probing on a hash table of size seven. Will this permutation","chapter-9","Searching"
"solve the problem of primary clustering? What does this say about selecting","chapter-9","Searching"
"a permutation for use when implementing pseudo-random probing?","chapter-9","Searching"
"9.7 Projects","chapter-9","Searching"
"9.1 Implement a binary search and the quadratic binary search of Section 9.1.","chapter-9","Searching"
"Run your implementations over a large range of problem sizes, timing the","chapter-9","Searching"
"results for each algorithm. Graph and compare these timing results.","chapter-9","Searching"
"Sec. 9.7 Projects 339","chapter-9","Searching"
"9.2 Implement the three self-organizing list heuristics count, move-to-front, and","chapter-9","Searching"
"transpose. Compare the cost for running the three heuristics on various input","chapter-9","Searching"
"data. The cost metric should be the total number of comparisons required","chapter-9","Searching"
"when searching the list. It is important to compare the heuristics using input","chapter-9","Searching"
"data for which self-organizing lists are reasonable, that is, on frequency dis-","chapter-9","Searching"
"tributions that are uneven. One good approach is to read text files. The list","chapter-9","Searching"
"should store individual words in the text file. Begin with an empty list, as","chapter-9","Searching"
"was done for the text compression example of Section 9.2. Each time a word","chapter-9","Searching"
"is encountered in the text file, search for it in the self-organizing list. If the","chapter-9","Searching"
"word is found, reorder the list as appropriate. If the word is not in the list,","chapter-9","Searching"
"add it to the end of the list and then reorder as appropriate.","chapter-9","Searching"
"9.3 Implement the text compression system described in Section 9.2.","chapter-9","Searching"
"9.4 Implement a system for managing document retrieval. Your system should","chapter-9","Searching"
"have the ability to insert (abstract references to) documents into the system,","chapter-9","Searching"
"associate keywords with a given document, and to search for documents with","chapter-9","Searching"
"specified keywords.","chapter-9","Searching"
"9.5 Implement a database stored on disk using bucket hashing. Define records to","chapter-9","Searching"
"be 128 bytes long with a 4-byte key and 120 bytes of data. The remaining","chapter-9","Searching"
"4 bytes are available for you to store necessary information to support the","chapter-9","Searching"
"hash table. A bucket in the hash table will be 1024 bytes long, so each bucket","chapter-9","Searching"
"has space for 8 records. The hash table should consist of 27 buckets (total","chapter-9","Searching"
"space for 216 records with slots indexed by positions 0 to 215) followed by","chapter-9","Searching"
"the overflow bucket at record position 216 in the file. The hash function for","chapter-9","Searching"
"key value K should be K mod 213. (Note that this means the last three","chapter-9","Searching"
"slots in the table will not be home positions for any record.) The collision","chapter-9","Searching"
"resolution function should be linear probing with wrap-around within the","chapter-9","Searching"
"bucket. For example, if a record is hashed to slot 5, the collision resolution","chapter-9","Searching"
"process will attempt to insert the record into the table in the order 5, 6, 7, 0,","chapter-9","Searching"
"1, 2, 3, and finally 4. If a bucket is full, the record should be placed in the","chapter-9","Searching"
"overflow section at the end of the file.","chapter-9","Searching"
"Your hash table should implement the dictionary ADT of Section 4.4. When","chapter-9","Searching"
"you do your testing, assume that the system is meant to store about 100 or so","chapter-9","Searching"
"records at a time.","chapter-9","Searching"
"9.6 Implement the dictionary ADT of Section 4.4 by means of a hash table with","chapter-9","Searching"
"linear probing as the collision resolution policy. You might wish to begin","chapter-9","Searching"
"with the code of Figure 9.9. Using empirical simulation, determine the cost","chapter-9","Searching"
"of insert and delete as α grows (i.e., reconstruct the dashed lines of Fig-","chapter-9","Searching"
"ure 9.10). Then, repeat the experiment using quadratic probing and pseudo-","chapter-9","Searching"
"random probing. What can you say about the relative performance of these","chapter-9","Searching"
"three collision resolution policies?","chapter-9","Searching"
"Many large-scale computing applications are centered around data sets that are too","chapter-10","Indexing"
"large to fit into main memory. The classic example is a large database of records","chapter-10","Indexing"
"with multiple search keys, requiring the ability to insert, delete, and search for","chapter-10","Indexing"
"records. Hashing provides outstanding performance for such situations, but only","chapter-10","Indexing"
"in the limited case in which all searches are of the form “find the record with key","chapter-10","Indexing"
"value K.” Many applications require more general search capabilities. One exam-","chapter-10","Indexing"
"ple is a range query search for all records whose key lies within some range. Other","chapter-10","Indexing"
"queries might involve visiting all records in order of their key value, or finding the","chapter-10","Indexing"
"record with the greatest key value. Hash tables are not organized to support any of","chapter-10","Indexing"
"these queries efficiently.","chapter-10","Indexing"
"This chapter introduces file structures used to organize a large collection of","chapter-10","Indexing"
"records stored on disk. Such file structures support efficient insertion, deletion, and","chapter-10","Indexing"
"search operations, for exact-match queries, range queries, and largest/smallest key","chapter-10","Indexing"
"value searches.","chapter-10","Indexing"
"Before discussing such file structures, we must become familiar with some ba-","chapter-10","Indexing"
"sic file-processing terminology. An entry-sequenced file stores records in the order","chapter-10","Indexing"
"that they were added to the file. Entry-sequenced files are the disk-based equivalent","chapter-10","Indexing"
"to an unsorted list and so do not support efficient search. The natural solution is to","chapter-10","Indexing"
"sort the records by order of the search key. However, a typical database, such as a","chapter-10","Indexing"
"collection of employee or customer records maintained by a business, might con-","chapter-10","Indexing"
"tain multiple search keys. To answer a question about a particular customer might","chapter-10","Indexing"
"require a search on the name of the customer. Businesses often wish to sort and","chapter-10","Indexing"
"output the records by zip code order for a bulk mailing. Government paperwork","chapter-10","Indexing"
"might require the ability to search by Social Security number. Thus, there might","chapter-10","Indexing"
"not be a single “correct” order in which to store the records.","chapter-10","Indexing"
"Indexing is the process of associating a key with the location of a correspond-","chapter-10","Indexing"
"ing data record. Section 8.5 discussed the concept of a key sort, in which an index","chapter-10","Indexing"
"file is created whose records consist of key/pointer pairs. Here, each key is asso-","chapter-10","Indexing"
"ciated with a pointer to a complete record in the main database file. The index file","chapter-10","Indexing"
"341","chapter-10","Indexing"
"342 Chap. 10 Indexing","chapter-10","Indexing"
"could be sorted or organized using a tree structure, thereby imposing a logical or-","chapter-10","Indexing"
"der on the records without physically rearranging them. One database might have","chapter-10","Indexing"
"several associated index files, each supporting efficient access through a different","chapter-10","Indexing"
"key field.","chapter-10","Indexing"
"Each record of a database normally has a unique identifier, called the primary","chapter-10","Indexing"
"key. For example, the primary key for a set of personnel records might be the","chapter-10","Indexing"
"Social Security number or ID number for the individual. Unfortunately, the ID","chapter-10","Indexing"
"number is generally an inconvenient value on which to perform a search because","chapter-10","Indexing"
"the searcher is unlikely to know it. Instead, the searcher might know the desired","chapter-10","Indexing"
"employee’s name. Alternatively, the searcher might be interested in finding all","chapter-10","Indexing"
"employees whose salary is in a certain range. If these are typical search requests","chapter-10","Indexing"
"to the database, then the name and salary fields deserve separate indices. However,","chapter-10","Indexing"
"key values in the name and salary indices are not likely to be unique.","chapter-10","Indexing"
"A key field such as salary, where a particular key value might be duplicated in","chapter-10","Indexing"
"multiple records, is called a secondary key. Most searches are performed using a","chapter-10","Indexing"
"secondary key. The secondary key index (or more simply, secondary index) will","chapter-10","Indexing"
"associate a secondary key value with the primary key of each record having that","chapter-10","Indexing"
"secondary key value. At this point, the full database might be searched directly","chapter-10","Indexing"
"for the record with that primary key, or there might be a primary key index (or","chapter-10","Indexing"
"primary index) that relates each primary key value with a pointer to the actual","chapter-10","Indexing"
"record on disk. In the latter case, only the primary index provides the location of","chapter-10","Indexing"
"the actual record on disk, while the secondary indices refer to the primary index.","chapter-10","Indexing"
"Indexing is an important technique for organizing large databases, and many","chapter-10","Indexing"
"indexing methods have been developed. Direct access through hashing is discussed","chapter-10","Indexing"
"in Section 9.4. A simple list sorted by key value can also serve as an index to the","chapter-10","Indexing"
"record file. Indexing disk files by sorted lists are discussed in the following section.","chapter-10","Indexing"
"Unfortunately, a sorted list does not perform well for insert and delete operations.","chapter-10","Indexing"
"A third approach to indexing is the tree index. Trees are typically used to or-","chapter-10","Indexing"
"ganize large databases that must support record insertion, deletion, and key range","chapter-10","Indexing"
"searches. Section 10.2 briefly describes ISAM, a tentative step toward solving the","chapter-10","Indexing"
"problem of storing a large database that must support insertion and deletion of","chapter-10","Indexing"
"records. Its shortcomings help to illustrate the value of tree indexing techniques.","chapter-10","Indexing"
"Section 10.3 introduces the basic issues related to tree indexing. Section 10.4 in-","chapter-10","Indexing"
"troduces the 2-3 tree, a balanced tree structure that is a simple form of the B-tree","chapter-10","Indexing"
"covered in Section 10.5. B-trees are the most widely used indexing method for","chapter-10","Indexing"
"large disk-based databases, and for implementing file systems. Since they have","chapter-10","Indexing"
"such great practical importance, many variations have been invented. Section 10.5","chapter-10","Indexing"
"begins with a discussion of the variant normally referred to simply as a “B-tree.”","chapter-10","Indexing"
"Section 10.5.1 presents the most widely implemented variant, the B+-tree.","chapter-10","Indexing"
"Sec. 10.1 Linear Indexing 343","chapter-10","Indexing"
"Linear Index","chapter-10","Indexing"
"Database Records","chapter-10","Indexing"
"37 42 73 98 52","chapter-10","Indexing"
"73 52 98 37 42","chapter-10","Indexing"
"Figure 10.1 Linear indexing for variable-length records. Each record in the","chapter-10","Indexing"
"index file is of fixed length and contains a pointer to the beginning of the corre-","chapter-10","Indexing"
"sponding record in the database file.","chapter-10","Indexing"
"10.1 Linear Indexing","chapter-10","Indexing"
"A linear index is an index file organized as a sequence of key/pointer pairs where","chapter-10","Indexing"
"the keys are in sorted order and the pointers either (1) point to the position of the","chapter-10","Indexing"
"complete record on disk, (2) point to the position of the primary key in the primary","chapter-10","Indexing"
"index, or (3) are actually the value of the primary key. Depending on its size, a","chapter-10","Indexing"
"linear index might be stored in main memory or on disk. A linear index provides","chapter-10","Indexing"
"a number of advantages. It provides convenient access to variable-length database","chapter-10","Indexing"
"records, because each entry in the index file contains a fixed-length key field and","chapter-10","Indexing"
"a fixed-length pointer to the beginning of a (variable-length) record as shown in","chapter-10","Indexing"
"Figure 10.1. A linear index also allows for efficient search and random access to","chapter-10","Indexing"
"database records, because it is amenable to binary search.","chapter-10","Indexing"
"If the database contains enough records, the linear index might be too large","chapter-10","Indexing"
"to store in main memory. This makes binary search of the index more expensive","chapter-10","Indexing"
"because many disk accesses would typically be required by the search process. One","chapter-10","Indexing"
"solution to this problem is to store a second-level linear index in main memory that","chapter-10","Indexing"
"indicates which disk block in the index file stores a desired key. For example, the","chapter-10","Indexing"
"linear index on disk might reside in a series of 1024-byte blocks. If each key/pointer","chapter-10","Indexing"
"pair in the linear index requires 8 bytes (a 4-byte key and a 4-byte pointer), then","chapter-10","Indexing"
"128 key/pointer pairs are stored per block. The second-level index, stored in main","chapter-10","Indexing"
"memory, consists of a simple table storing the value of the key in the first position","chapter-10","Indexing"
"of each block in the linear index file. This arrangement is shown in Figure 10.2. If","chapter-10","Indexing"
"the linear index requires 1024 disk blocks (1MB), the second-level index contains","chapter-10","Indexing"
"only 1024 entries, one per disk block. To find which disk block contains a desired","chapter-10","Indexing"
"search key value, first search through the 1024-entry table to find the greatest value","chapter-10","Indexing"
"less than or equal to the search key. This directs the search to the proper block in","chapter-10","Indexing"
"the index file, which is then read into memory. At this point, a binary search within","chapter-10","Indexing"
"this block will produce a pointer to the actual record in the database. Because the","chapter-10","Indexing"
"344 Chap. 10 Indexing","chapter-10","Indexing"
"1 2003 5894","chapter-10","Indexing"
"Second Level Index","chapter-10","Indexing"
"1 2001 5894 9942 10528 10984","chapter-10","Indexing"
"Linear Index: Disk Blocks","chapter-10","Indexing"
"2003 5688","chapter-10","Indexing"
"10528","chapter-10","Indexing"
"Figure 10.2 A simple two-level linear index. The linear index is stored on disk.","chapter-10","Indexing"
"The smaller, second-level index is stored in main memory. Each element in the","chapter-10","Indexing"
"second-level index stores the first key value in the corresponding disk block of the","chapter-10","Indexing"
"index file. In this example, the first disk block of the linear index stores keys in","chapter-10","Indexing"
"the range 1 to 2001, and the second disk block stores keys in the range 2003 to","chapter-10","Indexing"
"5688. Thus, the first entry of the second-level index is key value 1 (the first key","chapter-10","Indexing"
"in the first block of the linear index), while the second entry of the second-level","chapter-10","Indexing"
"index is key value 2003.","chapter-10","Indexing"
"second-level index is stored in main memory, accessing a record by this method","chapter-10","Indexing"
"requires two disk reads: one from the index file and one from the database file for","chapter-10","Indexing"
"the actual record.","chapter-10","Indexing"
"Every time a record is inserted to or deleted from the database, all associated","chapter-10","Indexing"
"secondary indices must be updated. Updates to a linear index are expensive, be-","chapter-10","Indexing"
"cause the entire contents of the array might be shifted. Another problem is that","chapter-10","Indexing"
"multiple records with the same secondary key each duplicate that key value within","chapter-10","Indexing"
"the index. When the secondary key field has many duplicates, such as when it has","chapter-10","Indexing"
"a limited range (e.g., a field to indicate job category from among a small number of","chapter-10","Indexing"
"possible job categories), this duplication might waste considerable space.","chapter-10","Indexing"
"One improvement on the simple sorted array is a two-dimensional array where","chapter-10","Indexing"
"each row corresponds to a secondary key value. A row contains the primary keys","chapter-10","Indexing"
"whose records have the indicated secondary key value. Figure 10.3 illustrates this","chapter-10","Indexing"
"approach. Now there is no duplication of secondary key values, possibly yielding a","chapter-10","Indexing"
"considerable space savings. The cost of insertion and deletion is reduced, because","chapter-10","Indexing"
"only one row of the table need be adjusted. Note that a new row is added to the array","chapter-10","Indexing"
"when a new secondary key value is added. This might lead to moving many records,","chapter-10","Indexing"
"but this will happen infrequently in applications suited to using this arrangement.","chapter-10","Indexing"
"A drawback to this approach is that the array must be of fixed size, which","chapter-10","Indexing"
"imposes an upper limit on the number of primary keys that might be associated","chapter-10","Indexing"
"with a particular secondary key. Furthermore, those secondary keys with fewer","chapter-10","Indexing"
"records than the width of the array will waste the remainder of their row. A better","chapter-10","Indexing"
"approach is to have a one-dimensional array of secondary key values, where each","chapter-10","Indexing"
"secondary key is associated with a linked list. This works well if the index is stored","chapter-10","Indexing"
"in main memory, but not so well when it is stored on disk because the linked list","chapter-10","Indexing"
"for a given key might be scattered across several disk blocks.","chapter-10","Indexing"
"Sec. 10.1 Linear Indexing 345","chapter-10","Indexing"
"Jones","chapter-10","Indexing"
"Smith","chapter-10","Indexing"
"Zukowski","chapter-10","Indexing"
"AA10","chapter-10","Indexing"
"AX33","chapter-10","Indexing"
"ZQ99","chapter-10","Indexing"
"AB12","chapter-10","Indexing"
"AX35","chapter-10","Indexing"
"AB39","chapter-10","Indexing"
"ZX45","chapter-10","Indexing"
"FF37","chapter-10","Indexing"
"Figure 10.3 A two-dimensional linear index. Each row lists the primary keys","chapter-10","Indexing"
"associated with a particular secondary key value. In this example, the secondary","chapter-10","Indexing"
"key is a name. The primary key is a unique four-character code.","chapter-10","Indexing"
"Jones","chapter-10","Indexing"
"Smith","chapter-10","Indexing"
"Zukowski","chapter-10","Indexing"
"Primary","chapter-10","Indexing"
"Key","chapter-10","Indexing"
"AA10","chapter-10","Indexing"
"AB12","chapter-10","Indexing"
"AB39","chapter-10","Indexing"
"FF37","chapter-10","Indexing"
"AX33","chapter-10","Indexing"
"AX35","chapter-10","Indexing"
"ZX45","chapter-10","Indexing"
"ZQ99","chapter-10","Indexing"
"Secondary","chapter-10","Indexing"
"Key","chapter-10","Indexing"
"Figure 10.4 Illustration of an inverted list. Each secondary key value is stored","chapter-10","Indexing"
"in the secondary key list. Each secondary key value on the list has a pointer to a","chapter-10","Indexing"
"list of the primary keys whose associated records have that secondary key value.","chapter-10","Indexing"
"Consider a large database of employee records. If the primary key is the em-","chapter-10","Indexing"
"ployee’s ID number and the secondary key is the employee’s name, then each","chapter-10","Indexing"
"record in the name index associates a name with one or more ID numbers. The","chapter-10","Indexing"
"ID number index in turn associates an ID number with a unique pointer to the full","chapter-10","Indexing"
"record on disk. The secondary key index in such an organization is also known","chapter-10","Indexing"
"as an inverted list or inverted file. It is inverted in that searches work backwards","chapter-10","Indexing"
"from the secondary key to the primary key to the actual data record. It is called a","chapter-10","Indexing"
"list because each secondary key value has (conceptually) a list of primary keys as-","chapter-10","Indexing"
"sociated with it. Figure 10.4 illustrates this arrangement. Here, we have last names","chapter-10","Indexing"
"as the secondary key. The primary key is a four-character unique identifier.","chapter-10","Indexing"
"Figure 10.5 shows a better approach to storing inverted lists. An array of sec-","chapter-10","Indexing"
"ondary key values is shown as before. Associated with each secondary key is a","chapter-10","Indexing"
"pointer to an array of primary keys. The primary key array uses a linked-list im-","chapter-10","Indexing"
"plementation. This approach combines the storage for all of the secondary key lists","chapter-10","Indexing"
"into a single array, probably saving space. Each record in this array consists of a","chapter-10","Indexing"
"346 Chap. 10 Indexing","chapter-10","Indexing"
"Index","chapter-10","Indexing"
"0","chapter-10","Indexing"
"1","chapter-10","Indexing"
"3","chapter-10","Indexing"
"Primary","chapter-10","Indexing"
"Key Next","chapter-10","Indexing"
"AA10","chapter-10","Indexing"
"AX33","chapter-10","Indexing"
"ZX45","chapter-10","Indexing"
"ZQ99","chapter-10","Indexing"
"AB12","chapter-10","Indexing"
"AB39","chapter-10","Indexing"
"AX35","chapter-10","Indexing"
"FF37","chapter-10","Indexing"
"4","chapter-10","Indexing"
"6","chapter-10","Indexing"
"5","chapter-10","Indexing"
"7","chapter-10","Indexing"
"2","chapter-10","Indexing"
"Key","chapter-10","Indexing"
"Jones","chapter-10","Indexing"
"Smith","chapter-10","Indexing"
"Zukowski","chapter-10","Indexing"
"0","chapter-10","Indexing"
"1","chapter-10","Indexing"
"2","chapter-10","Indexing"
"3","chapter-10","Indexing"
"4","chapter-10","Indexing"
"5","chapter-10","Indexing"
"6","chapter-10","Indexing"
"7","chapter-10","Indexing"
"Secondary","chapter-10","Indexing"
"Figure 10.5 An inverted list implemented as an array of secondary keys and","chapter-10","Indexing"
"combined lists of primary keys. Each record in the secondary key array contains","chapter-10","Indexing"
"a pointer to a record in the primary key array. The next field of the primary key","chapter-10","Indexing"
"array indicates the next record with that secondary key value.","chapter-10","Indexing"
"primary key value and a pointer to the next element on the list. It is easy to insert","chapter-10","Indexing"
"and delete secondary keys from this array, making this a good implementation for","chapter-10","Indexing"
"disk-based inverted files.","chapter-10","Indexing"
"10.2 ISAM","chapter-10","Indexing"
"How do we handle large databases that require frequent update? The main problem","chapter-10","Indexing"
"with the linear index is that it is a single, large array that does not adjust well to","chapter-10","Indexing"
"updates because a single update can require changing the position of every key in","chapter-10","Indexing"
"the index. Inverted lists reduce this problem, but they are only suitable for sec-","chapter-10","Indexing"
"ondary key indices with many fewer secondary key values than records. The linear","chapter-10","Indexing"
"index would perform well as a primary key index if it could somehow be broken","chapter-10","Indexing"
"into pieces such that individual updates affect only a part of the index. This con-","chapter-10","Indexing"
"cept will be pursued throughout the rest of this chapter, eventually culminating in","chapter-10","Indexing"
"the B+-tree, the most widely used indexing method today. But first, we begin by","chapter-10","Indexing"
"studying ISAM, an early attempt to solve the problem of large databases requiring","chapter-10","Indexing"
"frequent update. Its weaknesses help to illustrate why the B+-tree works so well.","chapter-10","Indexing"
"Before the invention of effective tree indexing schemes, a variety of disk-based","chapter-10","Indexing"
"indexing methods were in use. All were rather cumbersome, largely because no","chapter-10","Indexing"
"adequate method for handling updates was known. Typically, updates would cause","chapter-10","Indexing"
"the index to degrade in performance. ISAM is one example of such an index and","chapter-10","Indexing"
"was widely used by IBM prior to adoption of the B-tree.","chapter-10","Indexing"
"ISAM is based on a modified form of the linear index, as illustrated by Fig-","chapter-10","Indexing"
"ure 10.6. Records are stored in sorted order by primary key. The disk file is divided","chapter-10","Indexing"
"Sec. 10.2 ISAM 347","chapter-10","Indexing"
"Cylinder","chapter-10","Indexing"
"Overflow","chapter-10","Indexing"
"Cylinder","chapter-10","Indexing"
"Overflow","chapter-10","Indexing"
"Index","chapter-10","Indexing"
"Cylinder Keys","chapter-10","Indexing"
"In−memory","chapter-10","Indexing"
"Table of","chapter-10","Indexing"
"Cylinder 1 Cylinder 2","chapter-10","Indexing"
"Records Records","chapter-10","Indexing"
"Cylinder","chapter-10","Indexing"
"Index","chapter-10","Indexing"
"System","chapter-10","Indexing"
"Overflow","chapter-10","Indexing"
"Cylinder","chapter-10","Indexing"
"Figure 10.6 Illustration of the ISAM indexing system.","chapter-10","Indexing"
"among a number of cylinders on disk.1 Each cylinder holds a section of the list in","chapter-10","Indexing"
"sorted order. Initially, each cylinder is not filled to capacity, and the extra space is","chapter-10","Indexing"
"set aside in the cylinder overflow. In memory is a table listing the lowest key value","chapter-10","Indexing"
"stored in each cylinder of the file. Each cylinder contains a table listing the lowest","chapter-10","Indexing"
"key value for each block in that cylinder, called the cylinder index. When new","chapter-10","Indexing"
"records are inserted, they are placed in the correct cylinder’s overflow area (in ef-","chapter-10","Indexing"
"fect, a cylinder acts as a bucket). If a cylinder’s overflow area fills completely, then","chapter-10","Indexing"
"a system-wide overflow area is used. Search proceeds by determining the proper","chapter-10","Indexing"
"cylinder from the system-wide table kept in main memory. The cylinder’s block","chapter-10","Indexing"
"table is brought in from disk and consulted to determine the correct block. If the","chapter-10","Indexing"
"record is found in that block, then the search is complete. Otherwise, the cylin-","chapter-10","Indexing"
"der’s overflow area is searched. If that is full, and the record is not found, then the","chapter-10","Indexing"
"system-wide overflow is searched.","chapter-10","Indexing"
"After initial construction of the database, so long as no new records are inserted","chapter-10","Indexing"
"or deleted, access is efficient because it requires only two disk fetches. The first","chapter-10","Indexing"
"disk fetch recovers the block table for the desired cylinder. The second disk fetch","chapter-10","Indexing"
"recovers the block that, under good conditions, contains the record. After many","chapter-10","Indexing"
"inserts, the overflow list becomes too long, resulting in significant search time as","chapter-10","Indexing"
"the cylinder overflow area fills up. Under extreme conditions, many searches might","chapter-10","Indexing"
"eventually lead to the system overflow area. The “solution” to this problem is to","chapter-10","Indexing"
"periodically reorganize the entire database. This means re-balancing the records","chapter-10","Indexing"
"1Recall from Section 8.2.1 that a cylinder is all of the tracks readable from a particular placement","chapter-10","Indexing"
"of the heads on the multiple platters of a disk drive.","chapter-10","Indexing"
"348 Chap. 10 Indexing","chapter-10","Indexing"
"among the cylinders, sorting the records within each cylinder, and updating both","chapter-10","Indexing"
"the system index table and the within-cylinder block table. Such reorganization","chapter-10","Indexing"
"was typical of database systems during the 1960s and would normally be done","chapter-10","Indexing"
"each night or weekly.","chapter-10","Indexing"
"10.3 Tree-based Indexing","chapter-10","Indexing"
"Linear indexing is efficient when the database is static, that is, when records are","chapter-10","Indexing"
"inserted and deleted rarely or never. ISAM is adequate for a limited number of","chapter-10","Indexing"
"updates, but not for frequent changes. Because it has essentially two levels of","chapter-10","Indexing"
"indexing, ISAM will also break down for a truly large database where the number","chapter-10","Indexing"
"of cylinders is too great for the top-level index to fit in main memory.","chapter-10","Indexing"
"In their most general form, database applications have the following character-","chapter-10","Indexing"
"istics:","chapter-10","Indexing"
"1. Large sets of records that are frequently updated.","chapter-10","Indexing"
"2. Search is by one or a combination of several keys.","chapter-10","Indexing"
"3. Key range queries or min/max queries are used.","chapter-10","Indexing"
"For such databases, a better organization must be found. One approach would","chapter-10","Indexing"
"be to use the binary search tree (BST) to store primary and secondary key indices.","chapter-10","Indexing"
"BSTs can store duplicate key values, they provide efficient insertion and deletion as","chapter-10","Indexing"
"well as efficient search, and they can perform efficient range queries. When there","chapter-10","Indexing"
"is enough main memory, the BST is a viable option for implementing both primary","chapter-10","Indexing"
"and secondary key indices.","chapter-10","Indexing"
"Unfortunately, the BST can become unbalanced. Even under relatively good","chapter-10","Indexing"
"conditions, the depth of leaf nodes can easily vary by a factor of two. This might","chapter-10","Indexing"
"not be a significant concern when the tree is stored in main memory because the","chapter-10","Indexing"
"time required is still Θ(log n) for search and update. When the tree is stored on","chapter-10","Indexing"
"disk, however, the depth of nodes in the tree becomes crucial. Every time a BST","chapter-10","Indexing"
"node B is visited, it is necessary to visit all nodes along the path from the root to B.","chapter-10","Indexing"
"Each node on this path must be retrieved from disk. Each disk access returns a","chapter-10","Indexing"
"block of information. If a node is on the same block as its parent, then the cost to","chapter-10","Indexing"
"find that node is trivial once its parent is in main memory. Thus, it is desirable to","chapter-10","Indexing"
"keep subtrees together on the same block. Unfortunately, many times a node is not","chapter-10","Indexing"
"on the same block as its parent. Thus, each access to a BST node could potentially","chapter-10","Indexing"
"require that another block to be read from disk. Using a buffer pool to store multiple","chapter-10","Indexing"
"blocks in memory can mitigate disk access problems if BST accesses display good","chapter-10","Indexing"
"locality of reference. But a buffer pool cannot eliminate disk I/O entirely. The","chapter-10","Indexing"
"problem becomes greater if the BST is unbalanced, because nodes deep in the tree","chapter-10","Indexing"
"have the potential of causing many disk blocks to be read. Thus, there are two","chapter-10","Indexing"
"significant issues that must be addressed to have efficient search from a disk-based","chapter-10","Indexing"
"Sec. 10.3 Tree-based Indexing 349","chapter-10","Indexing"
"Figure 10.7 Breaking the BST into blocks. The BST is divided among disk","chapter-10","Indexing"
"blocks, each with space for three nodes. The path from the root to any leaf is","chapter-10","Indexing"
"contained on two blocks.","chapter-10","Indexing"
"5","chapter-10","Indexing"
"3","chapter-10","Indexing"
"2 4 6 3 5 7","chapter-10","Indexing"
"(a) (b)","chapter-10","Indexing"
"7","chapter-10","Indexing"
"4","chapter-10","Indexing"
"2 6","chapter-10","Indexing"
"1","chapter-10","Indexing"
"Figure 10.8 An attempt to re-balance a BST after insertion can be expensive.","chapter-10","Indexing"
"(a) A BST with six nodes in the shape of a complete binary tree. (b) A node with","chapter-10","Indexing"
"value 1 is inserted into the BST of (a). To maintain both the complete binary tree","chapter-10","Indexing"
"shape and the BST property, a major reorganization of the tree is required.","chapter-10","Indexing"
"BST. The first is how to keep the tree balanced. The second is how to arrange the","chapter-10","Indexing"
"nodes on blocks so as to keep the number of blocks encountered on any path from","chapter-10","Indexing"
"the root to the leaves at a minimum.","chapter-10","Indexing"
"We could select a scheme for balancing the BST and allocating BST nodes to","chapter-10","Indexing"
"blocks in a way that minimizes disk I/O, as illustrated by Figure 10.7. However,","chapter-10","Indexing"
"maintaining such a scheme in the face of insertions and deletions is difficult. In","chapter-10","Indexing"
"particular, the tree should remain balanced when an update takes place, but doing","chapter-10","Indexing"
"so might require much reorganization. Each update should affect only a few blocks,","chapter-10","Indexing"
"or its cost will be too high. As you can see from Figure 10.8, adopting a rule such","chapter-10","Indexing"
"as requiring the BST to be complete can cause a great deal of rearranging of data","chapter-10","Indexing"
"within the tree.","chapter-10","Indexing"
"We can solve these problems by selecting another tree structure that automat-","chapter-10","Indexing"
"ically remains balanced after updates, and which is amenable to storing in blocks.","chapter-10","Indexing"
"There are a number of balanced tree data structures, and there are also techniques","chapter-10","Indexing"
"for keeping BSTs balanced. Examples are the AVL and splay trees discussed in","chapter-10","Indexing"
"Section 13.2. As an alternative, Section 10.4 presents the 2-3 tree, which has the","chapter-10","Indexing"
"property that its leaves are always at the same level. The main reason for discussing","chapter-10","Indexing"
"the 2-3 tree here in preference to the other balanced search trees is that it naturally","chapter-10","Indexing"
"350 Chap. 10 Indexing","chapter-10","Indexing"
"33","chapter-10","Indexing"
"23 30 48","chapter-10","Indexing"
"18","chapter-10","Indexing"
"12","chapter-10","Indexing"
"10 15 20 21 31 24 45 47 52 50","chapter-10","Indexing"
"Figure 10.9 A 2-3 tree.","chapter-10","Indexing"
"leads to the B-tree of Section 10.5, which is by far the most widely used indexing","chapter-10","Indexing"
"method today.","chapter-10","Indexing"
"10.4 2-3 Trees","chapter-10","Indexing"
"This section presents a data structure called the 2-3 tree. The 2-3 tree is not a binary","chapter-10","Indexing"
"tree, but instead its shape obeys the following definition:","chapter-10","Indexing"
"1. A node contains one or two keys.","chapter-10","Indexing"
"2. Every internal node has either two children (if it contains one key) or three","chapter-10","Indexing"
"children (if it contains two keys). Hence the name.","chapter-10","Indexing"
"3. All leaves are at the same level in the tree, so the tree is always height bal-","chapter-10","Indexing"
"anced.","chapter-10","Indexing"
"In addition to these shape properties, the 2-3 tree has a search tree property","chapter-10","Indexing"
"analogous to that of a BST. For every node, the values of all descendants in the left","chapter-10","Indexing"
"subtree are less than the value of the first key, while values in the center subtree","chapter-10","Indexing"
"are greater than or equal to the value of the first key. If there is a right subtree","chapter-10","Indexing"
"(equivalently, if the node stores two keys), then the values of all descendants in","chapter-10","Indexing"
"the center subtree are less than the value of the second key, while values in the","chapter-10","Indexing"
"right subtree are greater than or equal to the value of the second key. To maintain","chapter-10","Indexing"
"these shape and search properties requires that special action be taken when nodes","chapter-10","Indexing"
"are inserted and deleted. The 2-3 tree has the advantage over the BST in that the","chapter-10","Indexing"
"2-3 tree can be kept height balanced at relatively low cost.","chapter-10","Indexing"
"Figure 10.9 illustrates the 2-3 tree. Nodes are shown as rectangular boxes with","chapter-10","Indexing"
"two key fields. (These nodes actually would contain complete records or pointers","chapter-10","Indexing"
"to complete records, but the figures will show only the keys.) Internal nodes with","chapter-10","Indexing"
"only two children have an empty right key field. Leaf nodes might contain either","chapter-10","Indexing"
"one or two keys. Figure 10.10 is an implementation for the 2-3 tree node.","chapter-10","Indexing"
"Note that this sample declaration does not distinguish between leaf and internal","chapter-10","Indexing"
"nodes and so is space inefficient, because leaf nodes store three pointers each. The","chapter-10","Indexing"
"techniques of Section 5.3.1 can be applied here to implement separate internal and","chapter-10","Indexing"
"leaf node types.","chapter-10","Indexing"
"Sec. 10.4 2-3 Trees 351","chapter-10","Indexing"
"/** 2-3 tree node implementation */","chapter-10","Indexing"
"class TTNode<Key extends Comparable<? super Key>,E> {","chapter-10","Indexing"
"private E lval; // The left record","chapter-10","Indexing"
"private Key lkey; // The node’s left key","chapter-10","Indexing"
"private E rval; // The right record","chapter-10","Indexing"
"private Key rkey; // The node’s right key","chapter-10","Indexing"
"private TTNode<Key,E> left; // Pointer to left child","chapter-10","Indexing"
"private TTNode<Key,E> center; // Pointer to middle child","chapter-10","Indexing"
"private TTNode<Key,E> right; // Pointer to right child","chapter-10","Indexing"
"public TTNode() { center = left = right = null; }","chapter-10","Indexing"
"public TTNode(Key lk, E lv, Key rk, E rv,","chapter-10","Indexing"
"TTNode<Key,E> p1, TTNode<Key,E> p2,","chapter-10","Indexing"
"TTNode<Key,E> p3) {","chapter-10","Indexing"
"lkey = lk; rkey = rk;","chapter-10","Indexing"
"lval = lv; rval = rv;","chapter-10","Indexing"
"left = p1; center = p2; right = p3;","chapter-10","Indexing"
"}","chapter-10","Indexing"
"public boolean isLeaf() { return left == null; }","chapter-10","Indexing"
"public TTNode<Key,E> lchild() { return left; }","chapter-10","Indexing"
"public TTNode<Key,E> rchild() { return right; }","chapter-10","Indexing"
"public TTNode<Key,E> cchild() { return center; }","chapter-10","Indexing"
"public Key lkey() { return lkey; } // Left key","chapter-10","Indexing"
"public E lval() { return lval; } // Left value","chapter-10","Indexing"
"public Key rkey() { return rkey; } // Right key","chapter-10","Indexing"
"public E rval() { return rval; } // Right value","chapter-10","Indexing"
"public void setLeft(Key k, E e) { lkey = k; lval = e; }","chapter-10","Indexing"
"public void setRight(Key k, E e) { rkey = k; rval = e; }","chapter-10","Indexing"
"public void setLeftChild(TTNode<Key,E> it) { left = it; }","chapter-10","Indexing"
"public void setCenterChild(TTNode<Key,E> it)","chapter-10","Indexing"
"{ center = it; }","chapter-10","Indexing"
"public void setRightChild(TTNode<Key,E> it)","chapter-10","Indexing"
"{ right = it; }","chapter-10","Indexing"
"Figure 10.10 The 2-3 tree node implementation.","chapter-10","Indexing"
"From the defining rules for 2-3 trees we can derive relationships between the","chapter-10","Indexing"
"number of nodes in the tree and the depth of the tree. A 2-3 tree of height k has at","chapter-10","Indexing"
"least 2","chapter-10","Indexing"
"k−1","chapter-10","Indexing"
"leaves, because if every internal node has two children it degenerates to","chapter-10","Indexing"
"the shape of a complete binary tree. A 2-3 tree of height k has at most 3","chapter-10","Indexing"
"k−1","chapter-10","Indexing"
"leaves,","chapter-10","Indexing"
"because each internal node can have at most three children.","chapter-10","Indexing"
"Searching for a value in a 2-3 tree is similar to searching in a BST. Search","chapter-10","Indexing"
"begins at the root. If the root does not contain the search key K, then the search","chapter-10","Indexing"
"progresses to the only subtree that can possibly contain K. The value(s) stored in","chapter-10","Indexing"
"the root node determine which is the correct subtree. For example, if searching for","chapter-10","Indexing"
"the value 30 in the tree of Figure 10.9, we begin with the root node. Because 30 is","chapter-10","Indexing"
"between 18 and 33, it can only be in the middle subtree. Searching the middle child","chapter-10","Indexing"
"of the root node yields the desired record. If searching for 15, then the first step is","chapter-10","Indexing"
"352 Chap. 10 Indexing","chapter-10","Indexing"
"private E findhelp(TTNode<Key,E> root, Key k) {","chapter-10","Indexing"
"if (root == null) return null; // val not found","chapter-10","Indexing"
"if (k.compareTo(root.lkey()) == 0) return root.lval();","chapter-10","Indexing"
"if ((root.rkey() != null) && (k.compareTo(root.rkey())","chapter-10","Indexing"
"== 0))","chapter-10","Indexing"
"return root.rval();","chapter-10","Indexing"
"if (k.compareTo(root.lkey()) < 0) // Search left","chapter-10","Indexing"
"return findhelp(root.lchild(), k);","chapter-10","Indexing"
"else if (root.rkey() == null) // Search center","chapter-10","Indexing"
"return findhelp(root.cchild(), k);","chapter-10","Indexing"
"else if (k.compareTo(root.rkey()) < 0) // Search center","chapter-10","Indexing"
"return findhelp(root.cchild(), k);","chapter-10","Indexing"
"else return findhelp(root.rchild(), k); // Search right","chapter-10","Indexing"
"}","chapter-10","Indexing"
"Figure 10.11 Implementation for the 2-3 tree search method.","chapter-10","Indexing"
"12","chapter-10","Indexing"
"10 20 21","chapter-10","Indexing"
"33","chapter-10","Indexing"
"23 30","chapter-10","Indexing"
"24 31 50","chapter-10","Indexing"
"18","chapter-10","Indexing"
"45","chapter-10","Indexing"
"48","chapter-10","Indexing"
"15 15 47 52","chapter-10","Indexing"
"14","chapter-10","Indexing"
"Figure 10.12 Simple insert into the 2-3 tree of Figure 10.9. The value 14 is","chapter-10","Indexing"
"inserted into the tree at the leaf node containing 15. Because there is room in the","chapter-10","Indexing"
"node for a second key, it is simply added to the left position with 15 moved to the","chapter-10","Indexing"
"right position.","chapter-10","Indexing"
"again to search the root node. Because 15 is less than 18, the first (left) branch is","chapter-10","Indexing"
"taken. At the next level, we take the second branch to the leaf node containing 15.","chapter-10","Indexing"
"If the search key were 16, then upon encountering the leaf containing 15 we would","chapter-10","Indexing"
"find that the search key is not in the tree. Figure 10.11 is an implementation for the","chapter-10","Indexing"
"2-3 tree search method.","chapter-10","Indexing"
"Insertion into a 2-3 tree is similar to insertion into a BST to the extent that the","chapter-10","Indexing"
"new record is placed in the appropriate leaf node. Unlike BST insertion, a new","chapter-10","Indexing"
"child is not created to hold the record being inserted, that is, the 2-3 tree does not","chapter-10","Indexing"
"grow downward. The first step is to find the leaf node that would contain the record","chapter-10","Indexing"
"if it were in the tree. If this leaf node contains only one value, then the new record","chapter-10","Indexing"
"can be added to that node with no further modification to the tree, as illustrated in","chapter-10","Indexing"
"Figure 10.12. In this example, a record with key value 14 is inserted. Searching","chapter-10","Indexing"
"from the root, we come to the leaf node that stores 15. We add 14 as the left value","chapter-10","Indexing"
"(pushing the record with key 15 to the rightmost position).","chapter-10","Indexing"
"If we insert the new record into a leaf node L that already contains two records,","chapter-10","Indexing"
"then more space must be created. Consider the two records of node L and the","chapter-10","Indexing"
"Sec. 10.4 2-3 Trees 353","chapter-10","Indexing"
"33","chapter-10","Indexing"
"15","chapter-10","Indexing"
"23 30 48 52","chapter-10","Indexing"
"10 45 47 50 55","chapter-10","Indexing"
"12","chapter-10","Indexing"
"18","chapter-10","Indexing"
"20 21 24 31","chapter-10","Indexing"
"Figure 10.13 A simple node-splitting insert for a 2-3 tree. The value 55 is added","chapter-10","Indexing"
"to the 2-3 tree of Figure 10.9. This makes the node containing values 50 and 52","chapter-10","Indexing"
"split, promoting value 52 to the parent node.","chapter-10","Indexing"
"record to be inserted without further concern for which two were already in L and","chapter-10","Indexing"
"which is the new record. The first step is to split L into two nodes. Thus, a new","chapter-10","Indexing"
"node — call it L","chapter-10","Indexing"
"0 — must be created from free store. L receives the record with","chapter-10","Indexing"
"the least of the three key values. L","chapter-10","Indexing"
"0","chapter-10","Indexing"
"receives the greatest of the three. The record","chapter-10","Indexing"
"with the middle of the three key value is passed up to the parent node along with a","chapter-10","Indexing"
"pointer to L","chapter-10","Indexing"
"0","chapter-10","Indexing"
". This is called a promotion. The promoted key is then inserted into","chapter-10","Indexing"
"the parent. If the parent currently contains only one record (and thus has only two","chapter-10","Indexing"
"children), then the promoted record and the pointer to L","chapter-10","Indexing"
"0","chapter-10","Indexing"
"are simply added to the","chapter-10","Indexing"
"parent node. If the parent is full, then the split-and-promote process is repeated.","chapter-10","Indexing"
"Figure 10.13 illustrates a simple promotion. Figure 10.14 illustrates what happens","chapter-10","Indexing"
"when promotions require the root to split, adding a new level to the tree. In either","chapter-10","Indexing"
"case, all leaf nodes continue to have equal depth. Figures 10.15 and 10.16 present","chapter-10","Indexing"
"an implementation for the insertion process.","chapter-10","Indexing"
"Note that inserthelp of Figure 10.15 takes three parameters. The first is","chapter-10","Indexing"
"a pointer to the root of the current subtree, named rt. The second is the key for","chapter-10","Indexing"
"the record to be inserted, and the third is the record itself. The return value for","chapter-10","Indexing"
"inserthelp is a pointer to a 2-3 tree node. If rt is unchanged, then a pointer to","chapter-10","Indexing"
"rt is returned. If rt is changed (due to the insertion causing the node to split), then","chapter-10","Indexing"
"a pointer to the new subtree root is returned, with the key value and record value in","chapter-10","Indexing"
"the leftmost fields, and a pointer to the (single) subtree in the center pointer field.","chapter-10","Indexing"
"This revised node will then be added to the parent, as illustrated in Figure 10.14.","chapter-10","Indexing"
"When deleting a record from the 2-3 tree, there are three cases to consider. The","chapter-10","Indexing"
"simplest occurs when the record is to be removed from a leaf node containing two","chapter-10","Indexing"
"records. In this case, the record is simply removed, and no other nodes are affected.","chapter-10","Indexing"
"The second case occurs when the only record in a leaf node is to be removed. The","chapter-10","Indexing"
"third case occurs when a record is to be removed from an internal node. In both","chapter-10","Indexing"
"the second and the third cases, the deleted record is replaced with another that can","chapter-10","Indexing"
"take its place while maintaining the correct order, similar to removing a node from","chapter-10","Indexing"
"a BST. If the tree is sparse enough, there is no such record available that will allow","chapter-10","Indexing"
"all nodes to still maintain at least one record. In this situation, sibling nodes are","chapter-10","Indexing"
"354 Chap. 10 Indexing","chapter-10","Indexing"
"23","chapter-10","Indexing"
"20","chapter-10","Indexing"
"(a) (b)","chapter-10","Indexing"
"(c)","chapter-10","Indexing"
"20 30","chapter-10","Indexing"
"19 21 19 24 31 21 24 31","chapter-10","Indexing"
"12","chapter-10","Indexing"
"10 19 24","chapter-10","Indexing"
"30","chapter-10","Indexing"
"31","chapter-10","Indexing"
"33","chapter-10","Indexing"
"45 47 50 52","chapter-10","Indexing"
"23","chapter-10","Indexing"
"18","chapter-10","Indexing"
"20","chapter-10","Indexing"
"21","chapter-10","Indexing"
"48","chapter-10","Indexing"
"15","chapter-10","Indexing"
"23 30","chapter-10","Indexing"
"18 33","chapter-10","Indexing"
"Figure 10.14 Example of inserting a record that causes the 2-3 tree root to split.","chapter-10","Indexing"
"(a) The value 19 is added to the 2-3 tree of Figure 10.9. This causes the node","chapter-10","Indexing"
"containing 20 and 21 to split, promoting 20. (b) This in turn causes the internal","chapter-10","Indexing"
"node containing 23 and 30 to split, promoting 23. (c) Finally, the root node splits,","chapter-10","Indexing"
"promoting 23 to become the left record in the new root. The result is that the tree","chapter-10","Indexing"
"becomes one level higher.","chapter-10","Indexing"
"merged together. The delete operation for the 2-3 tree is excessively complex and","chapter-10","Indexing"
"will not be described further. Instead, a complete discussion of deletion will be","chapter-10","Indexing"
"postponed until the next section, where it can be generalized for a particular variant","chapter-10","Indexing"
"of the B-tree.","chapter-10","Indexing"
"The 2-3 tree insert and delete routines do not add new nodes at the bottom of","chapter-10","Indexing"
"the tree. Instead they cause leaf nodes to split or merge, possibly causing a ripple","chapter-10","Indexing"
"effect moving up the tree to the root. If necessary the root will split, causing a new","chapter-10","Indexing"
"root node to be created and making the tree one level deeper. On deletion, if the","chapter-10","Indexing"
"last two children of the root merge, then the root node is removed and the tree will","chapter-10","Indexing"
"lose a level. In either case, all leaf nodes are always at the same level. When all","chapter-10","Indexing"
"leaf nodes are at the same level, we say that a tree is height balanced. Because the","chapter-10","Indexing"
"2-3 tree is height balanced, and every internal node has at least two children, we","chapter-10","Indexing"
"know that the maximum depth of the tree is log n. Thus, all 2-3 tree insert, find,","chapter-10","Indexing"
"and delete operations require Θ(log n) time.","chapter-10","Indexing"
"Sec. 10.5 B-Trees 355","chapter-10","Indexing"
"private TTNode<Key,E> inserthelp(TTNode<Key,E> rt,","chapter-10","Indexing"
"Key k, E e) {","chapter-10","Indexing"
"TTNode<Key,E> retval;","chapter-10","Indexing"
"if (rt == null) // Empty tree: create a leaf node for root","chapter-10","Indexing"
"return new TTNode<Key,E>(k, e, null, null,","chapter-10","Indexing"
"null, null, null);","chapter-10","Indexing"
"if (rt.isLeaf()) // At leaf node: insert here","chapter-10","Indexing"
"return rt.add(new TTNode<Key,E>(k, e, null, null,","chapter-10","Indexing"
"null, null, null));","chapter-10","Indexing"
"// Add to internal node","chapter-10","Indexing"
"if (k.compareTo(rt.lkey()) < 0) { // Insert left","chapter-10","Indexing"
"retval = inserthelp(rt.lchild(), k, e);","chapter-10","Indexing"
"if (retval == rt.lchild()) return rt;","chapter-10","Indexing"
"else return rt.add(retval);","chapter-10","Indexing"
"}","chapter-10","Indexing"
"else if((rt.rkey() == null) ||","chapter-10","Indexing"
"(k.compareTo(rt.rkey()) < 0)) {","chapter-10","Indexing"
"retval = inserthelp(rt.cchild(), k, e);","chapter-10","Indexing"
"if (retval == rt.cchild()) return rt;","chapter-10","Indexing"
"else return rt.add(retval);","chapter-10","Indexing"
"}","chapter-10","Indexing"
"else { // Insert right","chapter-10","Indexing"
"retval = inserthelp(rt.rchild(), k, e);","chapter-10","Indexing"
"if (retval == rt.rchild()) return rt;","chapter-10","Indexing"
"else return rt.add(retval);","chapter-10","Indexing"
"}","chapter-10","Indexing"
"}","chapter-10","Indexing"
"Figure 10.15 The 2-3 tree insert routine.","chapter-10","Indexing"
"10.5 B-Trees","chapter-10","Indexing"
"This section presents the B-tree. B-trees are usually attributed to R. Bayer and","chapter-10","Indexing"
"E. McCreight who described the B-tree in a 1972 paper. By 1979, B-trees had re-","chapter-10","Indexing"
"placed virtually all large-file access methods other than hashing. B-trees, or some","chapter-10","Indexing"
"variant of B-trees, are the standard file organization for applications requiring inser-","chapter-10","Indexing"
"tion, deletion, and key range searches. They are used to implement most modern","chapter-10","Indexing"
"file systems. B-trees address effectively all of the major problems encountered","chapter-10","Indexing"
"when implementing disk-based search trees:","chapter-10","Indexing"
"1. B-trees are always height balanced, with all leaf nodes at the same level.","chapter-10","Indexing"
"2. Update and search operations affect only a few disk blocks. The fewer the","chapter-10","Indexing"
"number of disk blocks affected, the less disk I/O is required.","chapter-10","Indexing"
"3. B-trees keep related records (that is, records with similar key values) on the","chapter-10","Indexing"
"same disk block, which helps to minimize disk I/O on searches due to locality","chapter-10","Indexing"
"of reference.","chapter-10","Indexing"
"4. B-trees guarantee that every node in the tree will be full at least to a certain","chapter-10","Indexing"
"minimum percentage. This improves space efficiency while reducing the","chapter-10","Indexing"
"typical number of disk fetches necessary during a search or update operation.","chapter-10","Indexing"
"356 Chap. 10 Indexing","chapter-10","Indexing"
"/** Add a new key/value pair to the node. There might be a","chapter-10","Indexing"
"subtree associated with the record being added. This","chapter-10","Indexing"
"information comes in the form of a 2-3 tree node with","chapter-10","Indexing"
"one key and a (possibly null) subtree through the","chapter-10","Indexing"
"center pointer field. */","chapter-10","Indexing"
"public TTNode<Key,E> add(TTNode<Key,E> it) {","chapter-10","Indexing"
"if (rkey == null) { // Only one key, add here","chapter-10","Indexing"
"if (lkey.compareTo(it.lkey()) < 0) {","chapter-10","Indexing"
"rkey = it.lkey(); rval = it.lval();","chapter-10","Indexing"
"right = center; center = it.cchild();","chapter-10","Indexing"
"}","chapter-10","Indexing"
"else {","chapter-10","Indexing"
"rkey = lkey; rval = lval; right = center;","chapter-10","Indexing"
"lkey = it.lkey(); lval = it.lval();","chapter-10","Indexing"
"center = it.cchild();","chapter-10","Indexing"
"}","chapter-10","Indexing"
"return this;","chapter-10","Indexing"
"}","chapter-10","Indexing"
"else if (lkey.compareTo(it.lkey()) >= 0) { // Add left","chapter-10","Indexing"
"center = new TTNode<Key,E>(rkey, rval, null, null,","chapter-10","Indexing"
"center, right, null);","chapter-10","Indexing"
"rkey = null; rval = null; right = null;","chapter-10","Indexing"
"it.setLeftChild(left); left = it;","chapter-10","Indexing"
"return this;","chapter-10","Indexing"
"}","chapter-10","Indexing"
"else if (rkey.compareTo(it.lkey()) < 0) { // Add center","chapter-10","Indexing"
"it.setCenterChild(new TTNode<Key,E>(rkey, rval, null,","chapter-10","Indexing"
"null, it.cchild(), right, null));","chapter-10","Indexing"
"it.setLeftChild(this);","chapter-10","Indexing"
"rkey = null; rval = null; right = null;","chapter-10","Indexing"
"return it;","chapter-10","Indexing"
"}","chapter-10","Indexing"
"else { // Add right","chapter-10","Indexing"
"TTNode<Key,E> N1 = new TTNode<Key,E>(rkey, rval, null,","chapter-10","Indexing"
"null, this, it, null);","chapter-10","Indexing"
"it.setLeftChild(right);","chapter-10","Indexing"
"right = null; rkey = null; rval = null;","chapter-10","Indexing"
"return N1;","chapter-10","Indexing"
"}","chapter-10","Indexing"
"}","chapter-10","Indexing"
"Figure 10.16 The 2-3 tree node add method.","chapter-10","Indexing"
"Sec. 10.5 B-Trees 357","chapter-10","Indexing"
"20","chapter-10","Indexing"
"10 12 18 21 23 30 31 38 47","chapter-10","Indexing"
"15","chapter-10","Indexing"
"24","chapter-10","Indexing"
"33 45 48","chapter-10","Indexing"
"50 52 60","chapter-10","Indexing"
"Figure 10.17 A B-tree of order four.","chapter-10","Indexing"
"A B-tree of order m is defined to have the following shape properties:","chapter-10","Indexing"
"• The root is either a leaf or has at least two children.","chapter-10","Indexing"
"• Each internal node, except for the root, has between dm/2e and m children.","chapter-10","Indexing"
"• All leaves are at the same level in the tree, so the tree is always height bal-","chapter-10","Indexing"
"anced.","chapter-10","Indexing"
"The B-tree is a generalization of the 2-3 tree. Put another way, a 2-3 tree is a","chapter-10","Indexing"
"B-tree of order three. Normally, the size of a node in the B-tree is chosen to fill a","chapter-10","Indexing"
"disk block. A B-tree node implementation typically allows 100 or more children.","chapter-10","Indexing"
"Thus, a B-tree node is equivalent to a disk block, and a “pointer” value stored","chapter-10","Indexing"
"in the tree is actually the number of the block containing the child node (usually","chapter-10","Indexing"
"interpreted as an offset from the beginning of the corresponding disk file). In a","chapter-10","Indexing"
"typical application, the B-tree’s access to the disk file will be managed using a","chapter-10","Indexing"
"buffer pool and a block-replacement scheme such as LRU (see Section 8.3).","chapter-10","Indexing"
"Figure 10.17 shows a B-tree of order four. Each node contains up to three keys,","chapter-10","Indexing"
"and internal nodes have up to four children.","chapter-10","Indexing"
"Search in a B-tree is a generalization of search in a 2-3 tree. It is an alternating","chapter-10","Indexing"
"two-step process, beginning with the root node of the B-tree.","chapter-10","Indexing"
"1. Perform a binary search on the records in the current node. If a record with","chapter-10","Indexing"
"the search key is found, then return that record. If the current node is a leaf","chapter-10","Indexing"
"node and the key is not found, then report an unsuccessful search.","chapter-10","Indexing"
"2. Otherwise, follow the proper branch and repeat the process.","chapter-10","Indexing"
"For example, consider a search for the record with key value 47 in the tree of","chapter-10","Indexing"
"Figure 10.17. The root node is examined and the second (right) branch taken. After","chapter-10","Indexing"
"examining the node at level 1, the third branch is taken to the next level to arrive at","chapter-10","Indexing"
"the leaf node containing a record with key value 47.","chapter-10","Indexing"
"B-tree insertion is a generalization of 2-3 tree insertion. The first step is to find","chapter-10","Indexing"
"the leaf node that should contain the key to be inserted, space permitting. If there","chapter-10","Indexing"
"is room in this node, then insert the key. If there is not, then split the node into two","chapter-10","Indexing"
"and promote the middle key to the parent. If the parent becomes full, then it is split","chapter-10","Indexing"
"in turn, and its middle key promoted.","chapter-10","Indexing"
"358 Chap. 10 Indexing","chapter-10","Indexing"
"Note that this insertion process is guaranteed to keep all nodes at least half full.","chapter-10","Indexing"
"For example, when we attempt to insert into a full internal node of a B-tree of order","chapter-10","Indexing"
"four, there will now be five children that must be dealt with. The node is split into","chapter-10","Indexing"
"two nodes containing two keys each, thus retaining the B-tree property. The middle","chapter-10","Indexing"
"of the five children is promoted to its parent.","chapter-10","Indexing"
"10.5.1 B+-Trees","chapter-10","Indexing"
"The previous section mentioned that B-trees are universally used to implement","chapter-10","Indexing"
"large-scale disk-based systems. Actually, the B-tree as described in the previ-","chapter-10","Indexing"
"ous section is almost never implemented, nor is the 2-3 tree as described in Sec-","chapter-10","Indexing"
"tion 10.4. What is most commonly implemented is a variant of the B-tree, called","chapter-10","Indexing"
"the B+-tree. When greater efficiency is required, a more complicated variant known","chapter-10","Indexing"
"as the B∗","chapter-10","Indexing"
"-tree is used.","chapter-10","Indexing"
"When data are static, a linear index provides an extremely efficient way to","chapter-10","Indexing"
"search. The problem is how to handle those pesky inserts and deletes. We could try","chapter-10","Indexing"
"to keep the core idea of storing a sorted array-based list, but make it more flexible","chapter-10","Indexing"
"by breaking the list into manageable chunks that are more easily updated. How","chapter-10","Indexing"
"might we do that? First, we need to decide how big the chunks should be. Since","chapter-10","Indexing"
"the data are on disk, it seems reasonable to store a chunk that is the size of a disk","chapter-10","Indexing"
"block, or a small multiple of the disk block size. If the next record to be inserted","chapter-10","Indexing"
"belongs to a chunk that hasn’t filled its block then we can just insert it there. The","chapter-10","Indexing"
"fact that this might cause other records in that chunk to move a little bit in the array","chapter-10","Indexing"
"is not important, since this does not cause any extra disk accesses so long as we","chapter-10","Indexing"
"move data within that chunk. But what if the chunk fills up the entire block that","chapter-10","Indexing"
"contains it? We could just split it in half. What if we want to delete a record? We","chapter-10","Indexing"
"could just take the deleted record out of the chunk, but we might not want a lot of","chapter-10","Indexing"
"near-empty chunks. So we could put adjacent chunks together if they have only","chapter-10","Indexing"
"a small amount of data between them. Or we could shuffle data between adjacent","chapter-10","Indexing"
"chunks that together contain more data. The big problem would be how to find","chapter-10","Indexing"
"the desired chunk when processing a record with a given key. Perhaps some sort","chapter-10","Indexing"
"of tree-like structure could be used to locate the appropriate chunk. These ideas","chapter-10","Indexing"
"are exactly what motivate the B+-tree. The B+-tree is essentially a mechanism for","chapter-10","Indexing"
"managing a sorted array-based list, where the list is broken into chunks.","chapter-10","Indexing"
"The most significant difference between the B+-tree and the BST or the stan-","chapter-10","Indexing"
"dard B-tree is that the B+-tree stores records only at the leaf nodes. Internal nodes","chapter-10","Indexing"
"store key values, but these are used solely as placeholders to guide the search. This","chapter-10","Indexing"
"means that internal nodes are significantly different in structure from leaf nodes.","chapter-10","Indexing"
"Internal nodes store keys to guide the search, associating each key with a pointer","chapter-10","Indexing"
"to a child B+-tree node. Leaf nodes store actual records, or else keys and pointers","chapter-10","Indexing"
"to actual records in a separate disk file if the B+-tree is being used purely as an","chapter-10","Indexing"
"index. Depending on the size of a record as compared to the size of a key, a leaf","chapter-10","Indexing"
"Sec. 10.5 B-Trees 359","chapter-10","Indexing"
"23","chapter-10","Indexing"
"30 31 33 45 47","chapter-10","Indexing"
"48","chapter-10","Indexing"
"10 12 15 18 19 20 21 22 48 50 52","chapter-10","Indexing"
"33","chapter-10","Indexing"
"18","chapter-10","Indexing"
"23","chapter-10","Indexing"
"Figure 10.18 Example of a B+-tree of order four. Internal nodes must store","chapter-10","Indexing"
"between two and four children. For this example, the record size is assumed to be","chapter-10","Indexing"
"such that leaf nodes store between three and five records.","chapter-10","Indexing"
"node in a B+-tree of order m might have enough room to store more or less than","chapter-10","Indexing"
"m records. The requirement is simply that the leaf nodes store enough records to","chapter-10","Indexing"
"remain at least half full. The leaf nodes of a B+-tree are normally linked together","chapter-10","Indexing"
"to form a doubly linked list. Thus, the entire collection of records can be traversed","chapter-10","Indexing"
"in sorted order by visiting all the leaf nodes on the linked list. Here is a Java-like","chapter-10","Indexing"
"pseudocode representation for the B+-tree node interface. Leaf node and internal","chapter-10","Indexing"
"node subclasses would implement this interface.","chapter-10","Indexing"
"/** Interface for B+ Tree nodes */","chapter-10","Indexing"
"public interface BPNode<Key,E> {","chapter-10","Indexing"
"public boolean isLeaf();","chapter-10","Indexing"
"public int numrecs();","chapter-10","Indexing"
"public Key[] keys();","chapter-10","Indexing"
"}","chapter-10","Indexing"
"An important implementation detail to note is that while Figure 10.17 shows","chapter-10","Indexing"
"internal nodes containing three keys and four pointers, class BPNode is slightly","chapter-10","Indexing"
"different in that it stores key/pointer pairs. Figure 10.17 shows the B+-tree as it","chapter-10","Indexing"
"is traditionally drawn. To simplify implementation in practice, nodes really do","chapter-10","Indexing"
"associate a key with each pointer. Each internal node should be assumed to hold","chapter-10","Indexing"
"in the leftmost position an additional key that is less than or equal to any possible","chapter-10","Indexing"
"key value in the node’s leftmost subtree. B+-tree implementations typically store","chapter-10","Indexing"
"an additional dummy record in the leftmost leaf node whose key value is less than","chapter-10","Indexing"
"any legal key value.","chapter-10","Indexing"
"B","chapter-10","Indexing"
"+-trees are exceptionally good for range queries. Once the first record in","chapter-10","Indexing"
"the range has been found, the rest of the records with keys in the range can be","chapter-10","Indexing"
"accessed by sequential processing of the remaining records in the first node, and","chapter-10","Indexing"
"then continuing down the linked list of leaf nodes as far as necessary. Figure 10.18","chapter-10","Indexing"
"illustrates the B+-tree.","chapter-10","Indexing"
"Search in a B+-tree is nearly identical to search in a regular B-tree, except that","chapter-10","Indexing"
"the search must always continue to the proper leaf node. Even if the search-key","chapter-10","Indexing"
"value is found in an internal node, this is only a placeholder and does not provide","chapter-10","Indexing"
"360 Chap. 10 Indexing","chapter-10","Indexing"
"private E findhelp(BPNode<Key,E> rt, Key k) {","chapter-10","Indexing"
"int currec = binaryle(rt.keys(), rt.numrecs(), k);","chapter-10","Indexing"
"if (rt.isLeaf())","chapter-10","Indexing"
"if ((((BPLeaf<Key,E>)rt).keys())[currec] == k)","chapter-10","Indexing"
"return ((BPLeaf<Key,E>)rt).recs(currec);","chapter-10","Indexing"
"else return null;","chapter-10","Indexing"
"else","chapter-10","Indexing"
"return findhelp(((BPInternal<Key,E>)rt).","chapter-10","Indexing"
"pointers(currec), k);","chapter-10","Indexing"
"}","chapter-10","Indexing"
"Figure 10.19 Implementation for the B+-tree search method.","chapter-10","Indexing"
"access to the actual record. To find a record with key value 33 in the B+-tree of","chapter-10","Indexing"
"Figure 10.18, search begins at the root. The value 33 stored in the root merely","chapter-10","Indexing"
"serves as a placeholder, indicating that keys with values greater than or equal to 33","chapter-10","Indexing"
"are found in the second subtree. From the second child of the root, the first branch","chapter-10","Indexing"
"is taken to reach the leaf node containing the actual record (or a pointer to the actual","chapter-10","Indexing"
"record) with key value 33. Figure 10.19 shows a pseudocode sketch of the B+-tree","chapter-10","Indexing"
"search algorithm.","chapter-10","Indexing"
"B","chapter-10","Indexing"
"+-tree insertion is similar to B-tree insertion. First, the leaf L that should","chapter-10","Indexing"
"contain the record is found. If L is not full, then the new record is added, and no","chapter-10","Indexing"
"other B+-tree nodes are affected. If L is already full, split it in two (dividing the","chapter-10","Indexing"
"records evenly among the two nodes) and promote a copy of the least-valued key","chapter-10","Indexing"
"in the newly formed right node. As with the 2-3 tree, promotion might cause the","chapter-10","Indexing"
"parent to split in turn, perhaps eventually leading to splitting the root and causing","chapter-10","Indexing"
"the B+-tree to gain a new level. B+-tree insertion keeps all leaf nodes at equal","chapter-10","Indexing"
"depth. Figure 10.20 illustrates the insertion process through several examples. Fig-","chapter-10","Indexing"
"ure 10.21 shows a Java-like pseudocode sketch of the B+-tree insert algorithm.","chapter-10","Indexing"
"To delete record R from the B+-tree, first locate the leaf L that contains R. If L","chapter-10","Indexing"
"is more than half full, then we need only remove R, leaving L still at least half full.","chapter-10","Indexing"
"This is demonstrated by Figure 10.22.","chapter-10","Indexing"
"If deleting a record reduces the number of records in the node below the min-","chapter-10","Indexing"
"imum threshold (called an underflow), then we must do something to keep the","chapter-10","Indexing"
"node sufficiently full. The first choice is to look at the node’s adjacent siblings to","chapter-10","Indexing"
"determine if they have a spare record that can be used to fill the gap. If so, then","chapter-10","Indexing"
"enough records are transferred from the sibling so that both nodes have about the","chapter-10","Indexing"
"same number of records. This is done so as to delay as long as possible the next","chapter-10","Indexing"
"time when a delete causes this node to underflow again. This process might require","chapter-10","Indexing"
"that the parent node has its placeholder key value revised to reflect the true first key","chapter-10","Indexing"
"value in each node. Figure 10.23 illustrates the process.","chapter-10","Indexing"
"If neither sibling can lend a record to the under-full node (call it N), then N","chapter-10","Indexing"
"must give its records to a sibling and be removed from the tree. There is certainly","chapter-10","Indexing"
"room to do this, because the sibling is at most half full (remember that it had no","chapter-10","Indexing"
"Sec. 10.5 B-Trees 361","chapter-10","Indexing"
"33","chapter-10","Indexing"
"(a) (b)","chapter-10","Indexing"
"1012 233348 10 23 33 50 12","chapter-10","Indexing"
"18 33 48","chapter-10","Indexing"
"(c)","chapter-10","Indexing"
"33","chapter-10","Indexing"
"18 23 48","chapter-10","Indexing"
"(d)","chapter-10","Indexing"
"48","chapter-10","Indexing"
"1012 18 20 2123 31 33 45 47 48 50 15 52","chapter-10","Indexing"
"10 15 12 18 20 21 23 30 31 33 45 47 48 50 52","chapter-10","Indexing"
"Figure 10.20 Examples of B+-tree insertion. (a) A B+-tree containing five","chapter-10","Indexing"
"records. (b) The result of inserting a record with key value 50 into the tree of (a).","chapter-10","Indexing"
"The leaf node splits, causing creation of the first internal node. (c) The B+-tree of","chapter-10","Indexing"
"(b) after further insertions. (d) The result of inserting a record with key value 30","chapter-10","Indexing"
"into the tree of (c). The second leaf node splits, which causes the internal node to","chapter-10","Indexing"
"split in turn, creating a new root.","chapter-10","Indexing"
"private BPNode<Key,E> inserthelp(BPNode<Key,E> rt,","chapter-10","Indexing"
"Key k, E e) {","chapter-10","Indexing"
"BPNode<Key,E> retval;","chapter-10","Indexing"
"if (rt.isLeaf()) // At leaf node: insert here","chapter-10","Indexing"
"return ((BPLeaf<Key,E>)rt).add(k, e);","chapter-10","Indexing"
"// Add to internal node","chapter-10","Indexing"
"int currec = binaryle(rt.keys(), rt.numrecs(), k);","chapter-10","Indexing"
"BPNode<Key,E> temp = inserthelp(","chapter-10","Indexing"
"((BPInternal<Key,E>)root).pointers(currec), k, e);","chapter-10","Indexing"
"if (temp != ((BPInternal<Key,E>)rt).pointers(currec))","chapter-10","Indexing"
"return ((BPInternal<Key,E>)rt).","chapter-10","Indexing"
"add((BPInternal<Key,E>)temp);","chapter-10","Indexing"
"else","chapter-10","Indexing"
"return rt;","chapter-10","Indexing"
"}","chapter-10","Indexing"
"Figure 10.21 A Java-like pseudocode sketch of the B+-tree insert algorithm.","chapter-10","Indexing"
"362 Chap. 10 Indexing","chapter-10","Indexing"
"33","chapter-10","Indexing"
"18 23 48","chapter-10","Indexing"
"101215 23 30 31 19 2021 22 47 33 45 48 50 52","chapter-10","Indexing"
"Figure 10.22 Simple deletion from a B+-tree. The record with key value 18 is","chapter-10","Indexing"
"removed from the tree of Figure 10.18. Note that even though 18 is also a place-","chapter-10","Indexing"
"holder used to direct search in the parent node, that value need not be removed","chapter-10","Indexing"
"from internal nodes even if no record in the tree has key value 18. Thus, the","chapter-10","Indexing"
"leftmost node at level one in this example retains the key with value 18 after the","chapter-10","Indexing"
"record with key value 18 has been removed from the second leaf node.","chapter-10","Indexing"
"33","chapter-10","Indexing"
"19 23 48","chapter-10","Indexing"
"101518 19 20 21 22 33 45 47 23 30 31 48 50 52","chapter-10","Indexing"
"Figure 10.23 Deletion from the B+-tree of Figure 10.18 via borrowing from a","chapter-10","Indexing"
"sibling. The key with value 12 is deleted from the leftmost leaf, causing the record","chapter-10","Indexing"
"with key value 18 to shift to the leftmost leaf to take its place. Note that the parent","chapter-10","Indexing"
"must be updated to properly indicate the key range within the subtrees. In this","chapter-10","Indexing"
"example, the parent node has its leftmost key value changed to 19.","chapter-10","Indexing"
"records to contribute to the current node), and N has become less than half full","chapter-10","Indexing"
"because it is under-flowing. This merge process combines two subtrees of the par-","chapter-10","Indexing"
"ent, which might cause it to underflow in turn. If the last two children of the root","chapter-10","Indexing"
"merge together, then the tree loses a level. Figure 10.24 illustrates the node-merge","chapter-10","Indexing"
"deletion process. Figure 10.25 shows Java-like pseudocode for the B+-tree delete","chapter-10","Indexing"
"algorithm.","chapter-10","Indexing"
"The B+-tree requires that all nodes be at least half full (except for the root).","chapter-10","Indexing"
"Thus, the storage utilization must be at least 50%. This is satisfactory for many","chapter-10","Indexing"
"implementations, but note that keeping nodes fuller will result both in less space","chapter-10","Indexing"
"required (because there is less empty space in the disk file) and in more efficient","chapter-10","Indexing"
"processing (fewer blocks on average will be read into memory because the amount","chapter-10","Indexing"
"of information in each block is greater). Because B-trees have become so popular,","chapter-10","Indexing"
"many algorithm designers have tried to improve B-tree performance. One method","chapter-10","Indexing"
"for doing so is to use the B+-tree variant known as the B∗","chapter-10","Indexing"
"-tree. The B∗","chapter-10","Indexing"
"-tree is","chapter-10","Indexing"
"identical to the B+-tree, except for the rules used to split and merge nodes. Instead","chapter-10","Indexing"
"of splitting a node in half when it overflows, the B∗","chapter-10","Indexing"
"-tree gives some records to its","chapter-10","Indexing"
"Sec. 10.5 B-Trees 363","chapter-10","Indexing"
"48","chapter-10","Indexing"
"(a)","chapter-10","Indexing"
"45 4748 50 52","chapter-10","Indexing"
"23","chapter-10","Indexing"
"18 33","chapter-10","Indexing"
"(b)","chapter-10","Indexing"
"101215 22 18 19 20 21 23 30 31 4547485052","chapter-10","Indexing"
"Figure 10.24 Deleting the record with key value 33 from the B+-tree of Fig-","chapter-10","Indexing"
"ure 10.18 via collapsing siblings. (a) The two leftmost leaf nodes merge together","chapter-10","Indexing"
"to form a single leaf. Unfortunately, the parent node now has only one child.","chapter-10","Indexing"
"(b) Because the left subtree has a spare leaf node, that node is passed to the right","chapter-10","Indexing"
"subtree. The placeholder values of the root and the right internal node are updated","chapter-10","Indexing"
"to reflect the changes. Value 23 moves to the root, and old root value 33 moves to","chapter-10","Indexing"
"the rightmost internal node.","chapter-10","Indexing"
"/** Delete a record with the given key value, and","chapter-10","Indexing"
"return true if the root underflows */","chapter-10","Indexing"
"private boolean removehelp(BPNode<Key,E> rt, Key k) {","chapter-10","Indexing"
"int currec = binaryle(rt.keys(), rt.numrecs(), k);","chapter-10","Indexing"
"if (rt.isLeaf())","chapter-10","Indexing"
"if (((BPLeaf<Key,E>)rt).keys()[currec] == k)","chapter-10","Indexing"
"return ((BPLeaf<Key,E>)rt).delete(currec);","chapter-10","Indexing"
"else return false;","chapter-10","Indexing"
"else // Process internal node","chapter-10","Indexing"
"if (removehelp(((BPInternal<Key,E>)rt).pointers(currec),","chapter-10","Indexing"
"k))","chapter-10","Indexing"
"// Child will merge if necessary","chapter-10","Indexing"
"return ((BPInternal<Key,E>)rt).underflow(currec);","chapter-10","Indexing"
"else return false;","chapter-10","Indexing"
"}","chapter-10","Indexing"
"Figure 10.25 Java-like pseudocode for the B+-tree delete algorithm.","chapter-10","Indexing"
"364 Chap. 10 Indexing","chapter-10","Indexing"
"neighboring sibling, if possible. If the sibling is also full, then these two nodes split","chapter-10","Indexing"
"into three. Similarly, when a node underflows, it is combined with its two siblings,","chapter-10","Indexing"
"and the total reduced to two nodes. Thus, the nodes are always at least two thirds","chapter-10","Indexing"
"full.2","chapter-10","Indexing"
"10.5.2 B-Tree Analysis","chapter-10","Indexing"
"The asymptotic cost of search, insertion, and deletion of records from B-trees,","chapter-10","Indexing"
"B","chapter-10","Indexing"
"+-trees, and B∗","chapter-10","Indexing"
"-trees is Θ(log n) where n is the total number of records in the","chapter-10","Indexing"
"tree. However, the base of the log is the (average) branching factor of the tree.","chapter-10","Indexing"
"Typical database applications use extremely high branching factors, perhaps 100 or","chapter-10","Indexing"
"more. Thus, in practice the B-tree and its variants are extremely shallow.","chapter-10","Indexing"
"As an illustration, consider a B+-tree of order 100 and leaf nodes that contain","chapter-10","Indexing"
"up to 100 records. A B+-tree with height one (that is, just a single leaf node) can","chapter-10","Indexing"
"have at most 100 records. A B+-tree with height two (a root internal node whose","chapter-10","Indexing"
"children are leaves) must have at least 100 records (2 leaves with 50 records each).","chapter-10","Indexing"
"It has at most 10,000 records (100 leaves with 100 records each). A B+-tree with","chapter-10","Indexing"
"height three must have at least 5000 records (two second-level nodes with 50 chil-","chapter-10","Indexing"
"dren containing 50 records each) and at most one million records (100 second-level","chapter-10","Indexing"
"nodes with 100 full children each). A B+-tree with height four must have at least","chapter-10","Indexing"
"250,000 records and at most 100 million records. Thus, it would require an ex-","chapter-10","Indexing"
"tremely large database to generate a B+-tree of more than height four.","chapter-10","Indexing"
"The B+-tree split and insert rules guarantee that every node (except perhaps the","chapter-10","Indexing"
"root) is at least half full. So they are on average about 3/4 full. But the internal","chapter-10","Indexing"
"nodes are purely overhead, since the keys stored there are used only by the tree to","chapter-10","Indexing"
"direct search, rather than store actual data. Does this overhead amount to a signifi-","chapter-10","Indexing"
"cant use of space? No, because once again the high fan-out rate of the tree structure","chapter-10","Indexing"
"means that the vast majority of nodes are leaf nodes. Recall (from Section 6.4) that","chapter-10","Indexing"
"a full K-ary tree has approximately 1/K of its nodes as internal nodes. This means","chapter-10","Indexing"
"that while half of a full binary tree’s nodes are internal nodes, in a B+-tree of order","chapter-10","Indexing"
"100 probably only about 1/75 of its nodes are internal nodes. This means that the","chapter-10","Indexing"
"overhead associated with internal nodes is very low.","chapter-10","Indexing"
"We can reduce the number of disk fetches required for the B-tree even more","chapter-10","Indexing"
"by using the following methods. First, the upper levels of the tree can be stored in","chapter-10","Indexing"
"main memory at all times. Because the tree branches so quickly, the top two levels","chapter-10","Indexing"
"(levels 0 and 1) require relatively little space. If the B-tree is only height four, then","chapter-10","Indexing"
"2 This concept can be extended further if higher space utilization is required. However, the update","chapter-10","Indexing"
"routines become much more complicated. I once worked on a project where we implemented 3-for-4","chapter-10","Indexing"
"node split and merge routines. This gave better performance than the 2-for-3 node split and merge","chapter-10","Indexing"
"routines of the B∗","chapter-10","Indexing"
"-tree. However, the spitting and merging routines were so complicated that even","chapter-10","Indexing"
"their author could no longer understand them once they were completed!","chapter-10","Indexing"
"Sec. 10.6 Further Reading 365","chapter-10","Indexing"
"at most two disk fetches (internal nodes at level two and leaves at level three) are","chapter-10","Indexing"
"required to reach the pointer to any given record.","chapter-10","Indexing"
"A buffer pool could be used to manage nodes of the B-tree. Several nodes of","chapter-10","Indexing"
"the tree would typically be in main memory at one time. The most straightforward","chapter-10","Indexing"
"approach is to use a standard method such as LRU to do node replacement. How-","chapter-10","Indexing"
"ever, sometimes it might be desirable to “lock” certain nodes such as the root into","chapter-10","Indexing"
"the buffer pool. In general, if the buffer pool is even of modest size (say at least","chapter-10","Indexing"
"twice the depth of the tree), no special techniques for node replacement will be","chapter-10","Indexing"
"required because the upper-level nodes will naturally be accessed frequently.","chapter-10","Indexing"
"10.6 Further Reading","chapter-10","Indexing"
"For an expanded discussion of the issues touched on in this chapter, see a gen-","chapter-10","Indexing"
"eral file processing text such as File Structures: A Conceptual Toolkit by Folk and","chapter-10","Indexing"
"Zoellick [FZ98]. In particular, Folk and Zoellick provide a good discussion of","chapter-10","Indexing"
"the relationship between primary and secondary indices. The most thorough dis-","chapter-10","Indexing"
"cussion on various implementations for the B-tree is the survey article by Comer","chapter-10","Indexing"
"[Com79]. Also see [Sal88] for further details on implementing B-trees. See Shaf-","chapter-10","Indexing"
"fer and Brown [SB93] for a discussion of buffer pool management strategies for","chapter-10","Indexing"
"B","chapter-10","Indexing"
"+-tree-like data structures.","chapter-10","Indexing"
"10.7 Exercises","chapter-10","Indexing"
"10.1 Assume that a computer system has disk blocks of 1024 bytes, and that you","chapter-10","Indexing"
"are storing records that have 4-byte keys and 4-byte data fields. The records","chapter-10","Indexing"
"are sorted and packed sequentially into the disk file.","chapter-10","Indexing"
"(a) Assume that a linear index uses 4 bytes to store the key and 4 bytes","chapter-10","Indexing"
"to store the block ID for the associated records. What is the greatest","chapter-10","Indexing"
"number of records that can be stored in the file if a linear index of size","chapter-10","Indexing"
"256KB is used?","chapter-10","Indexing"
"(b) What is the greatest number of records that can be stored in the file if","chapter-10","Indexing"
"the linear index is also stored on disk (and thus its size is limited only","chapter-10","Indexing"
"by the second-level index) when using a second-level index of 1024","chapter-10","Indexing"
"bytes (i.e., 256 key values) as illustrated by Figure 10.2? Each element","chapter-10","Indexing"
"of the second-level index references the smallest key value for a disk","chapter-10","Indexing"
"block of the linear index.","chapter-10","Indexing"
"10.2 Assume that a computer system has disk blocks of 4096 bytes, and that you","chapter-10","Indexing"
"are storing records that have 4-byte keys and 64-byte data fields. The records","chapter-10","Indexing"
"are sorted and packed sequentially into the disk file.","chapter-10","Indexing"
"(a) Assume that a linear index uses 4 bytes to store the key and 4 bytes","chapter-10","Indexing"
"to store the block ID for the associated records. What is the greatest","chapter-10","Indexing"
"366 Chap. 10 Indexing","chapter-10","Indexing"
"number of records that can be stored in the file if a linear index of size","chapter-10","Indexing"
"2MB is used?","chapter-10","Indexing"
"(b) What is the greatest number of records that can be stored in the file if","chapter-10","Indexing"
"the linear index is also stored on disk (and thus its size is limited only by","chapter-10","Indexing"
"the second-level index) when using a second-level index of 4096 bytes","chapter-10","Indexing"
"(i.e., 1024 key values) as illustrated by Figure 10.2? Each element of","chapter-10","Indexing"
"the second-level index references the smallest key value for a disk block","chapter-10","Indexing"
"of the linear index.","chapter-10","Indexing"
"10.3 Modify the function binary of Section 3.5 so as to support variable-length","chapter-10","Indexing"
"records with fixed-length keys indexed by a simple linear index as illustrated","chapter-10","Indexing"
"by Figure 10.1.","chapter-10","Indexing"
"10.4 Assume that a database stores records consisting of a 2-byte integer key and","chapter-10","Indexing"
"a variable-length data field consisting of a string. Show the linear index (as","chapter-10","Indexing"
"illustrated by Figure 10.1) for the following collection of records:","chapter-10","Indexing"
"397 Hello world!","chapter-10","Indexing"
"82 XYZ","chapter-10","Indexing"
"1038 This string is rather long","chapter-10","Indexing"
"1037 This is shorter","chapter-10","Indexing"
"42 ABC","chapter-10","Indexing"
"2222 Hello new world!","chapter-10","Indexing"
"10.5 Each of the following series of records consists of a four-digit primary key","chapter-10","Indexing"
"(with no duplicates) and a four-character secondary key (with many dupli-","chapter-10","Indexing"
"cates).","chapter-10","Indexing"
"3456 DEER","chapter-10","Indexing"
"2398 DEER","chapter-10","Indexing"
"2926 DUCK","chapter-10","Indexing"
"9737 DEER","chapter-10","Indexing"
"7739 GOAT","chapter-10","Indexing"
"9279 DUCK","chapter-10","Indexing"
"1111 FROG","chapter-10","Indexing"
"8133 DEER","chapter-10","Indexing"
"7183 DUCK","chapter-10","Indexing"
"7186 FROG","chapter-10","Indexing"
"(a) Show the inverted list (as illustrated by Figure 10.4) for this collection","chapter-10","Indexing"
"of records.","chapter-10","Indexing"
"(b) Show the improved inverted list (as illustrated by Figure 10.5) for this","chapter-10","Indexing"
"collection of records.","chapter-10","Indexing"
"10.6 Under what conditions will ISAM be more efficient than a B+-tree imple-","chapter-10","Indexing"
"mentation?","chapter-10","Indexing"
"Sec. 10.8 Projects 367","chapter-10","Indexing"
"10.7 Prove that the number of leaf nodes in a 2-3 tree with height k is between","chapter-10","Indexing"
"2","chapter-10","Indexing"
"k−1","chapter-10","Indexing"
"and 3","chapter-10","Indexing"
"k−1","chapter-10","Indexing"
".","chapter-10","Indexing"
"10.8 Show the result of inserting the values 55 and 46 into the 2-3 tree of Fig-","chapter-10","Indexing"
"ure 10.9.","chapter-10","Indexing"
"10.9 You are given a series of records whose keys are letters. The records arrive","chapter-10","Indexing"
"in the following order: C, S, D, T, A, M, P, I, B, W, N, G, U, R, K, E, H, O,","chapter-10","Indexing"
"L, J. Show the 2-3 tree that results from inserting these records.","chapter-10","Indexing"
"10.10 You are given a series of records whose keys are letters. The records are","chapter-10","Indexing"
"inserted in the following order: C, S, D, T, A, M, P, I, B, W, N, G, U, R, K,","chapter-10","Indexing"
"E, H, O, L, J. Show the tree that results from inserting these records when","chapter-10","Indexing"
"the 2-3 tree is modified to be a 2-3+ tree, that is, the internal nodes act only","chapter-10","Indexing"
"as placeholders. Assume that the leaf nodes are capable of holding up to two","chapter-10","Indexing"
"records.","chapter-10","Indexing"
"10.11 Show the result of inserting the value 55 into the B-tree of Figure 10.17.","chapter-10","Indexing"
"10.12 Show the result of inserting the values 1, 2, 3, 4, 5, and 6 (in that order) into","chapter-10","Indexing"
"the B+-tree of Figure 10.18.","chapter-10","Indexing"
"10.13 Show the result of deleting the values 18, 19, and 20 (in that order) from the","chapter-10","Indexing"
"B","chapter-10","Indexing"
"+-tree of Figure 10.24b.","chapter-10","Indexing"
"10.14 You are given a series of records whose keys are letters. The records are","chapter-10","Indexing"
"inserted in the following order: C, S, D, T, A, M, P, I, B, W, N, G, U, R,","chapter-10","Indexing"
"K, E, H, O, L, J. Show the B+-tree of order four that results from inserting","chapter-10","Indexing"
"these records. Assume that the leaf nodes are capable of storing up to three","chapter-10","Indexing"
"records.","chapter-10","Indexing"
"10.15 Assume that you have a B+-tree whose internal nodes can store up to 100","chapter-10","Indexing"
"children and whose leaf nodes can store up to 15 records. What are the","chapter-10","Indexing"
"minimum and maximum number of records that can be stored by the B+-tree","chapter-10","Indexing"
"with heights 1, 2, 3, 4, and 5?","chapter-10","Indexing"
"10.16 Assume that you have a B+-tree whose internal nodes can store up to 50","chapter-10","Indexing"
"children and whose leaf nodes can store up to 50 records. What are the","chapter-10","Indexing"
"minimum and maximum number of records that can be stored by the B+-tree","chapter-10","Indexing"
"with heights 1, 2, 3, 4, and 5?","chapter-10","Indexing"
"10.8 Projects","chapter-10","Indexing"
"10.1 Implement a two-level linear index for variable-length records as illustrated","chapter-10","Indexing"
"by Figures 10.1 and 10.2. Assume that disk blocks are 1024 bytes in length.","chapter-10","Indexing"
"Records in the database file should typically range between 20 and 200 bytes,","chapter-10","Indexing"
"including a 4-byte key value. Each record of the index file should store a","chapter-10","Indexing"
"key value and the byte offset in the database file for the first byte of the","chapter-10","Indexing"
"corresponding record. The top-level index (stored in memory) should be a","chapter-10","Indexing"
"simple array storing the lowest key value on the corresponding block in the","chapter-10","Indexing"
"index file.","chapter-10","Indexing"
"368 Chap. 10 Indexing","chapter-10","Indexing"
"10.2 Implement the 2-3+ tree, that is, a 2-3 tree where the internal nodes act only","chapter-10","Indexing"
"as placeholders. Your 2-3+ tree should implement the dictionary interface of","chapter-10","Indexing"
"Section 4.4.","chapter-10","Indexing"
"10.3 Implement the dictionary ADT of Section 4.4 for a large file stored on disk","chapter-10","Indexing"
"by means of the B+-tree of Section 10.5. Assume that disk blocks are","chapter-10","Indexing"
"1024 bytes, and thus both leaf nodes and internal nodes are also 1024 bytes.","chapter-10","Indexing"
"Records should store a 4-byte (int) key value and a 60-byte data field. Inter-","chapter-10","Indexing"
"nal nodes should store key value/pointer pairs where the “pointer” is actually","chapter-10","Indexing"
"the block number on disk for the child node. Both internal nodes and leaf","chapter-10","Indexing"
"nodes will need room to store various information such as a count of the","chapter-10","Indexing"
"records stored on that node, and a pointer to the next node on that level.","chapter-10","Indexing"
"Thus, leaf nodes will store 15 records, and internal nodes will have room to","chapter-10","Indexing"
"store about 120 to 125 children depending on how you implement them. Use","chapter-10","Indexing"
"a buffer pool (Section 8.3) to manage access to the nodes stored on disk.","chapter-10","Indexing"
"Graphs provide the ultimate in data structure flexibility. Graphs can model both","chapter-11","Graphs"
"real-world systems and abstract problems, so they are used in hundreds of applica-","chapter-11","Graphs"
"tions. Here is a small sampling of the range of problems that graphs are routinely","chapter-11","Graphs"
"applied to.","chapter-11","Graphs"
"1. Modeling connectivity in computer and communications networks.","chapter-11","Graphs"
"2. Representing a map as a set of locations with distances between locations;","chapter-11","Graphs"
"used to compute shortest routes between locations.","chapter-11","Graphs"
"3. Modeling flow capacities in transportation networks.","chapter-11","Graphs"
"4. Finding a path from a starting condition to a goal condition; for example, in","chapter-11","Graphs"
"artificial intelligence problem solving.","chapter-11","Graphs"
"5. Modeling computer algorithms, showing transitions from one program state","chapter-11","Graphs"
"to another.","chapter-11","Graphs"
"6. Finding an acceptable order for finishing subtasks in a complex activity, such","chapter-11","Graphs"
"as constructing large buildings.","chapter-11","Graphs"
"7. Modeling relationships such as family trees, business or military organiza-","chapter-11","Graphs"
"tions, and scientific taxonomies.","chapter-11","Graphs"
"We begin in Section 11.1 with some basic graph terminology and then define","chapter-11","Graphs"
"two fundamental representations for graphs, the adjacency matrix and adjacency","chapter-11","Graphs"
"list. Section 11.2 presents a graph ADT and simple implementations based on the","chapter-11","Graphs"
"adjacency matrix and adjacency list. Section 11.3 presents the two most commonly","chapter-11","Graphs"
"used graph traversal algorithms, called depth-first and breadth-first search, with","chapter-11","Graphs"
"application to topological sorting. Section 11.4 presents algorithms for solving","chapter-11","Graphs"
"some problems related to finding shortest routes in a graph. Finally, Section 11.5","chapter-11","Graphs"
"presents algorithms for finding the minimum-cost spanning tree, useful for deter-","chapter-11","Graphs"
"mining lowest-cost connectivity in a network. Besides being useful and interesting","chapter-11","Graphs"
"in their own right, these algorithms illustrate the use of some data structures pre-","chapter-11","Graphs"
"sented in earlier chapters.","chapter-11","Graphs"
"371","chapter-11","Graphs"
"372 Chap. 11 Graphs","chapter-11","Graphs"
"(b) (c)","chapter-11","Graphs"
"0","chapter-11","Graphs"
"3","chapter-11","Graphs"
"4","chapter-11","Graphs"
"1","chapter-11","Graphs"
"2","chapter-11","Graphs"
"7","chapter-11","Graphs"
"1","chapter-11","Graphs"
"2","chapter-11","Graphs"
"3","chapter-11","Graphs"
"4","chapter-11","Graphs"
"(a)","chapter-11","Graphs"
"1","chapter-11","Graphs"
"Figure 11.1 Examples of graphs and terminology. (a) A graph. (b) A directed","chapter-11","Graphs"
"graph (digraph). (c) A labeled (directed) graph with weights associated with the","chapter-11","Graphs"
"edges. In this example, there is a simple path from Vertex 0 to Vertex 3 containing","chapter-11","Graphs"
"Vertices 0, 1, and 3. Vertices 0, 1, 3, 2, 4, and 1 also form a path, but not a simple","chapter-11","Graphs"
"path because Vertex 1 appears twice. Vertices 1, 3, 2, 4, and 1 form a simple cycle.","chapter-11","Graphs"
"11.1 Terminology and Representations","chapter-11","Graphs"
"A graph G = (V, E) consists of a set of vertices V and a set of edges E, such","chapter-11","Graphs"
"that each edge in E is a connection between a pair of vertices in V.","chapter-11","Graphs"
"1 The number","chapter-11","Graphs"
"of vertices is written |V|, and the number of edges is written |E|. |E| can range","chapter-11","Graphs"
"from zero to a maximum of |V|","chapter-11","Graphs"
"2 − |V|. A graph with relatively few edges is called","chapter-11","Graphs"
"sparse, while a graph with many edges is called dense. A graph containing all","chapter-11","Graphs"
"possible edges is said to be complete.","chapter-11","Graphs"
"A graph with edges directed from one vertex to another (as in Figure 11.1(b))","chapter-11","Graphs"
"is called a directed graph or digraph. A graph whose edges are not directed is","chapter-11","Graphs"
"called an undirected graph (as illustrated by Figure 11.1(a)). A graph with labels","chapter-11","Graphs"
"associated with its vertices (as in Figure 11.1(c)) is called a labeled graph. Two","chapter-11","Graphs"
"vertices are adjacent if they are joined by an edge. Such vertices are also called","chapter-11","Graphs"
"neighbors. An edge connecting Vertices U and V is written (U, V). Such an edge","chapter-11","Graphs"
"is said to be incident on Vertices U and V. Associated with each edge may be a","chapter-11","Graphs"
"cost or weight. Graphs whose edges have weights (as in Figure 11.1(c)) are said to","chapter-11","Graphs"
"be weighted.","chapter-11","Graphs"
"A sequence of vertices v1, v2, ..., vn forms a path of length n − 1 if there exist","chapter-11","Graphs"
"edges from vi","chapter-11","Graphs"
"to vi+1 for 1 ≤ i < n. A path is simple if all vertices on the path are","chapter-11","Graphs"
"distinct. The length of a path is the number of edges it contains. A cycle is a path","chapter-11","Graphs"
"of length three or more that connects some vertex v1 to itself. A cycle is simple if","chapter-11","Graphs"
"the path is simple, except for the first and last vertices being the same.","chapter-11","Graphs"
"1","chapter-11","Graphs"
"Some graph applications require that a given pair of vertices can have multiple or parallel edges","chapter-11","Graphs"
"connecting them, or that a vertex can have an edge to itself. However, the applications discussed","chapter-11","Graphs"
"in this book do not require either of these special cases, so for simplicity we will assume that they","chapter-11","Graphs"
"cannot occur.","chapter-11","Graphs"
"Sec. 11.1 Terminology and Representations 373","chapter-11","Graphs"
"0 2","chapter-11","Graphs"
"4","chapter-11","Graphs"
"1 3","chapter-11","Graphs"
"6","chapter-11","Graphs"
"5","chapter-11","Graphs"
"7","chapter-11","Graphs"
"Figure 11.2 An undirected graph with three connected components. Vertices 0,","chapter-11","Graphs"
"1, 2, 3, and 4 form one connected component. Vertices 5 and 6 form a second","chapter-11","Graphs"
"connected component. Vertex 7 by itself forms a third connected component.","chapter-11","Graphs"
"A subgraph S is formed from graph G by selecting a subset Vs of G’s vertices","chapter-11","Graphs"
"and a subset Es of G’s edges such that for every edge E in Es, both of E’s vertices","chapter-11","Graphs"
"are in Vs.","chapter-11","Graphs"
"An undirected graph is connected if there is at least one path from any vertex","chapter-11","Graphs"
"to any other. The maximally connected subgraphs of an undirected graph are called","chapter-11","Graphs"
"connected components. For example, Figure 11.2 shows an undirected graph with","chapter-11","Graphs"
"three connected components.","chapter-11","Graphs"
"A graph without cycles is called acyclic. Thus, a directed graph without cycles","chapter-11","Graphs"
"is called a directed acyclic graph or DAG.","chapter-11","Graphs"
"A free tree is a connected, undirected graph with no simple cycles. An equiv-","chapter-11","Graphs"
"alent definition is that a free tree is connected and has |V| − 1 edges.","chapter-11","Graphs"
"There are two commonly used methods for representing graphs. The adja-","chapter-11","Graphs"
"cency matrix is illustrated by Figure 11.3(b). The adjacency matrix for a graph","chapter-11","Graphs"
"is a |V| × |V| array. Assume that |V| = n and that the vertices are labeled from","chapter-11","Graphs"
"v0 through vn−1. Row i of the adjacency matrix contains entries for Vertex vi","chapter-11","Graphs"
".","chapter-11","Graphs"
"Column j in row i is marked if there is an edge from vi","chapter-11","Graphs"
"to vj and is not marked oth-","chapter-11","Graphs"
"erwise. Thus, the adjacency matrix requires one bit at each position. Alternatively,","chapter-11","Graphs"
"if we wish to associate a number with each edge, such as the weight or distance","chapter-11","Graphs"
"between two vertices, then each matrix position must store that number. In either","chapter-11","Graphs"
"case, the space requirements for the adjacency matrix are Θ(|V|","chapter-11","Graphs"
"2","chapter-11","Graphs"
").","chapter-11","Graphs"
"The second common representation for graphs is the adjacency list, illustrated","chapter-11","Graphs"
"by Figure 11.3(c). The adjacency list is an array of linked lists. The array is","chapter-11","Graphs"
"|V| items long, with position i storing a pointer to the linked list of edges for Ver-","chapter-11","Graphs"
"tex vi","chapter-11","Graphs"
". This linked list represents the edges by the vertices that are adjacent to","chapter-11","Graphs"
"Vertex vi","chapter-11","Graphs"
". The adjacency list is therefore a generalization of the “list of children”","chapter-11","Graphs"
"representation for trees described in Section 6.3.1.","chapter-11","Graphs"
"Example 11.1 The entry for Vertex 0 in Figure 11.3(c) stores 1 and 4","chapter-11","Graphs"
"because there are two edges in the graph leaving Vertex 0, with one going","chapter-11","Graphs"
"374 Chap. 11 Graphs","chapter-11","Graphs"
"(a) (b)","chapter-11","Graphs"
"0","chapter-11","Graphs"
"4","chapter-11","Graphs"
"2","chapter-11","Graphs"
"3","chapter-11","Graphs"
"0","chapter-11","Graphs"
"1","chapter-11","Graphs"
"2","chapter-11","Graphs"
"3","chapter-11","Graphs"
"4","chapter-11","Graphs"
"0 1 2 3 4","chapter-11","Graphs"
"1 1","chapter-11","Graphs"
"1","chapter-11","Graphs"
"1","chapter-11","Graphs"
"1","chapter-11","Graphs"
"1 1","chapter-11","Graphs"
"(c)","chapter-11","Graphs"
"0","chapter-11","Graphs"
"1","chapter-11","Graphs"
"2","chapter-11","Graphs"
"3","chapter-11","Graphs"
"4","chapter-11","Graphs"
"1","chapter-11","Graphs"
"3","chapter-11","Graphs"
"4","chapter-11","Graphs"
"2","chapter-11","Graphs"
"1","chapter-11","Graphs"
"4","chapter-11","Graphs"
"Figure 11.3 Two graph representations. (a) A directed graph. (b) The adjacency","chapter-11","Graphs"
"matrix for the graph of (a). (c) The adjacency list for the graph of (a).","chapter-11","Graphs"
"to Vertex 1 and one going to Vertex 4. The list for Vertex 2 stores an entry","chapter-11","Graphs"
"for Vertex 4 because there is an edge from Vertex 2 to Vertex 4, but no entry","chapter-11","Graphs"
"for Vertex 3 because this edge comes into Vertex 2 rather than going out.","chapter-11","Graphs"
"The storage requirements for the adjacency list depend on both the number of","chapter-11","Graphs"
"edges and the number of vertices in the graph. There must be an array entry for","chapter-11","Graphs"
"each vertex (even if the vertex is not adjacent to any other vertex and thus has no","chapter-11","Graphs"
"elements on its linked list), and each edge must appear on one of the lists. Thus,","chapter-11","Graphs"
"the cost is Θ(|V| + |E|).","chapter-11","Graphs"
"Both the adjacency matrix and the adjacency list can be used to store directed","chapter-11","Graphs"
"or undirected graphs. Each edge of an undirected graph connecting Vertices U","chapter-11","Graphs"
"and V is represented by two directed edges: one from U to V and one from V to","chapter-11","Graphs"
"U. Figure 11.4 illustrates the use of the adjacency matrix and the adjacency list for","chapter-11","Graphs"
"undirected graphs.","chapter-11","Graphs"
"Which graph representation is more space efficient depends on the number of","chapter-11","Graphs"
"edges in the graph. The adjacency list stores information only for those edges that","chapter-11","Graphs"
"actually appear in the graph, while the adjacency matrix requires space for each","chapter-11","Graphs"
"potential edge, whether it exists or not. However, the adjacency matrix requires","chapter-11","Graphs"
"no overhead for pointers, which can be a substantial cost, especially if the only","chapter-11","Graphs"
"Sec. 11.1 Terminology and Representations 375","chapter-11","Graphs"
"(a) (b)","chapter-11","Graphs"
"(c)","chapter-11","Graphs"
"0","chapter-11","Graphs"
"1","chapter-11","Graphs"
"2","chapter-11","Graphs"
"3","chapter-11","Graphs"
"0","chapter-11","Graphs"
"1","chapter-11","Graphs"
"2","chapter-11","Graphs"
"3","chapter-11","Graphs"
"4","chapter-11","Graphs"
"0 1 2 3 4","chapter-11","Graphs"
"1 1","chapter-11","Graphs"
"1 1 1","chapter-11","Graphs"
"1 1","chapter-11","Graphs"
"1 1","chapter-11","Graphs"
"1 1 1","chapter-11","Graphs"
"0","chapter-11","Graphs"
"1","chapter-11","Graphs"
"3","chapter-11","Graphs"
"4","chapter-11","Graphs"
"1","chapter-11","Graphs"
"0","chapter-11","Graphs"
"1","chapter-11","Graphs"
"0","chapter-11","Graphs"
"4","chapter-11","Graphs"
"3","chapter-11","Graphs"
"4","chapter-11","Graphs"
"2","chapter-11","Graphs"
"1","chapter-11","Graphs"
"4","chapter-11","Graphs"
"2","chapter-11","Graphs"
"4","chapter-11","Graphs"
"2 3","chapter-11","Graphs"
"Figure 11.4 Using the graph representations for undirected graphs. (a) An undi-","chapter-11","Graphs"
"rected graph. (b) The adjacency matrix for the graph of (a). (c) The adjacency list","chapter-11","Graphs"
"for the graph of (a).","chapter-11","Graphs"
"information stored for an edge is one bit to indicate its existence. As the graph be-","chapter-11","Graphs"
"comes denser, the adjacency matrix becomes relatively more space efficient. Sparse","chapter-11","Graphs"
"graphs are likely to have their adjacency list representation be more space efficient.","chapter-11","Graphs"
"Example 11.2 Assume that a vertex index requires two bytes, a pointer","chapter-11","Graphs"
"requires four bytes, and an edge weight requires two bytes. Then the adja-","chapter-11","Graphs"
"cency matrix for the graph of Figure 11.3 requires 2|V","chapter-11","Graphs"
"2","chapter-11","Graphs"
"| = 50 bytes while","chapter-11","Graphs"
"the adjacency list requires 4|V| + 6|E| = 56 bytes. For the graph of Fig-","chapter-11","Graphs"
"ure 11.4, the adjacency matrix requires the same space as before, while the","chapter-11","Graphs"
"adjacency list requires 4|V| + 6|E| = 92 bytes (because there are now 12","chapter-11","Graphs"
"edges instead of 6).","chapter-11","Graphs"
"The adjacency matrix often requires a higher asymptotic cost for an algorithm","chapter-11","Graphs"
"than would result if the adjacency list were used. The reason is that it is common","chapter-11","Graphs"
"for a graph algorithm to visit each neighbor of each vertex. Using the adjacency list,","chapter-11","Graphs"
"only the actual edges connecting a vertex to its neighbors are examined. However,","chapter-11","Graphs"
"the adjacency matrix must look at each of its |V| potential edges, yielding a total","chapter-11","Graphs"
"cost of Θ(|V","chapter-11","Graphs"
"2","chapter-11","Graphs"
"|) time when the algorithm might otherwise require only Θ(|V|+|E|)","chapter-11","Graphs"
"376 Chap. 11 Graphs","chapter-11","Graphs"
"time. This is a considerable disadvantage when the graph is sparse, but not when","chapter-11","Graphs"
"the graph is closer to full.","chapter-11","Graphs"
"11.2 Graph Implementations","chapter-11","Graphs"
"We next turn to the problem of implementing a general-purpose graph class. Fig-","chapter-11","Graphs"
"ure 11.5 shows an abstract class defining an ADT for graphs. Vertices are defined","chapter-11","Graphs"
"by an integer index value. In other words, there is a Vertex 0, Vertex 1, and so","chapter-11","Graphs"
"on. We can assume that a graph application stores any additional information of","chapter-11","Graphs"
"interest about a given vertex elsewhere, such as a name or application-dependent","chapter-11","Graphs"
"value. Note that this ADT is not implemented using a generic, because it is the","chapter-11","Graphs"
"Graph class users’ responsibility to maintain information related to the vertices","chapter-11","Graphs"
"themselves. The Graph class need have no knowledge of the type or content of","chapter-11","Graphs"
"the information associated with a vertex, only the index number for that vertex.","chapter-11","Graphs"
"Abstract class Graph has methods to return the number of vertices and edges","chapter-11","Graphs"
"(methods n and e, respectively). Function weight returns the weight of a given","chapter-11","Graphs"
"edge, with that edge identified by its two incident vertices. For example, calling","chapter-11","Graphs"
"weight(0, 4) on the graph of Figure 11.1 (c) would return 4. If no such edge","chapter-11","Graphs"
"exists, the weight is defined to be 0. So calling weight(0, 2) on the graph of","chapter-11","Graphs"
"Figure 11.1 (c) would return 0.","chapter-11","Graphs"
"Functions setEdge and delEdge set the weight of an edge and remove an","chapter-11","Graphs"
"edge from the graph, respectively. Again, an edge is identified by its two incident","chapter-11","Graphs"
"vertices. setEdge does not permit the user to set the weight to be 0, because this","chapter-11","Graphs"
"value is used to indicate a non-existent edge, nor are negative edge weights per-","chapter-11","Graphs"
"mitted. Functions getMark and setMark get and set, respectively, a requested","chapter-11","Graphs"
"value in the Mark array (described below) for Vertex V.","chapter-11","Graphs"
"Nearly every graph algorithm presented in this chapter will require visits to all","chapter-11","Graphs"
"neighbors of a given vertex. Two methods are provided to support this. They work","chapter-11","Graphs"
"in a manner similar to linked list access functions. Function first takes as input","chapter-11","Graphs"
"a vertex V, and returns the edge to the first neighbor for V (we assume the neighbor","chapter-11","Graphs"
"list is sorted by vertex number). Function next takes as input Vertices V1 and V2","chapter-11","Graphs"
"and returns the index for the vertex forming the next edge with V1 after V2 on V1’s","chapter-11","Graphs"
"edge list. Function next will return a value of n = |V| once the end of the edge","chapter-11","Graphs"
"list for V1 has been reached. The following line appears in many graph algorithms:","chapter-11","Graphs"
"for (w = G=>first(v); w < G->n(); w = G->next(v,w))","chapter-11","Graphs"
"This for loop gets the first neighbor of v, then works through the remaining neigh-","chapter-11","Graphs"
"bors of v until a value equal to G->n() is returned, signaling that all neighbors","chapter-11","Graphs"
"of v have been visited. For example, first(1) in Figure 11.4 would return 0.","chapter-11","Graphs"
"next(1, 0) would return 3. next(0, 3) would return 4. next(1, 4)","chapter-11","Graphs"
"would return 5, which is not a vertex in the graph.","chapter-11","Graphs"
"Sec. 11.2 Graph Implementations 377","chapter-11","Graphs"
"/** Graph ADT */","chapter-11","Graphs"
"public interface Graph { // Graph class ADT","chapter-11","Graphs"
"/** Initialize the graph","chapter-11","Graphs"
"@param n The number of vertices */","chapter-11","Graphs"
"public void Init(int n);","chapter-11","Graphs"
"/** @return The number of vertices */","chapter-11","Graphs"
"public int n();","chapter-11","Graphs"
"/** @return The current number of edges */","chapter-11","Graphs"
"public int e();","chapter-11","Graphs"
"/** @return v’s first neighbor */","chapter-11","Graphs"
"public int first(int v);","chapter-11","Graphs"
"/** @return v’s next neighbor after w */","chapter-11","Graphs"
"public int next(int v, int w);","chapter-11","Graphs"
"/** Set the weight for an edge","chapter-11","Graphs"
"@param i,j The vertices","chapter-11","Graphs"
"@param wght Edge weight */","chapter-11","Graphs"
"public void setEdge(int i, int j, int wght);","chapter-11","Graphs"
"/** Delete an edge","chapter-11","Graphs"
"@param i,j The vertices */","chapter-11","Graphs"
"public void delEdge(int i, int j);","chapter-11","Graphs"
"/** Determine if an edge is in the graph","chapter-11","Graphs"
"@param i,j The vertices","chapter-11","Graphs"
"@return true if edge i,j has non-zero weight */","chapter-11","Graphs"
"public boolean isEdge(int i, int j);","chapter-11","Graphs"
"/** @return The weight of edge i,j, or zero","chapter-11","Graphs"
"@param i,j The vertices */","chapter-11","Graphs"
"public int weight(int i, int j);","chapter-11","Graphs"
"/** Set the mark value for a vertex","chapter-11","Graphs"
"@param v The vertex","chapter-11","Graphs"
"@param val The value to set */","chapter-11","Graphs"
"public void setMark(int v, int val);","chapter-11","Graphs"
"/** Get the mark value for a vertex","chapter-11","Graphs"
"@param v The vertex","chapter-11","Graphs"
"@return The value of the mark */","chapter-11","Graphs"
"public int getMark(int v);","chapter-11","Graphs"
"}","chapter-11","Graphs"
"Figure 11.5 A graph ADT. This ADT assumes that the number of vertices is","chapter-11","Graphs"
"fixed when the graph is created, but that edges can be added and removed. It also","chapter-11","Graphs"
"supports a mark array to aid graph traversal algorithms.","chapter-11","Graphs"
"378 Chap. 11 Graphs","chapter-11","Graphs"
"It is reasonably straightforward to implement our graph and edge ADTs using","chapter-11","Graphs"
"either the adjacency list or adjacency matrix. The sample implementations pre-","chapter-11","Graphs"
"sented here do not address the issue of how the graph is actually created. The user","chapter-11","Graphs"
"of these implementations must add functionality for this purpose, perhaps reading","chapter-11","Graphs"
"the graph description from a file. The graph can be built up by using the setEdge","chapter-11","Graphs"
"function provided by the ADT.","chapter-11","Graphs"
"Figure 11.6 shows an implementation for the adjacency matrix. Array Mark","chapter-11","Graphs"
"stores the information manipulated by the setMark and getMark functions. The","chapter-11","Graphs"
"edge matrix is implemented as an integer array of size n × n for a graph of n ver-","chapter-11","Graphs"
"tices. Position (i, j) in the matrix stores the weight for edge (i, j) if it exists. A","chapter-11","Graphs"
"weight of zero for edge (i, j) is used to indicate that no edge connects Vertices i","chapter-11","Graphs"
"and j.","chapter-11","Graphs"
"Given a vertex V, function first locates the position in matrix of the first","chapter-11","Graphs"
"edge (if any) of V by beginning with edge (V, 0) and scanning through row V until","chapter-11","Graphs"
"an edge is found. If no edge is incident on V, then first returns n.","chapter-11","Graphs"
"Function next locates the edge following edge (i, j) (if any) by continuing","chapter-11","Graphs"
"down the row of Vertex i starting at position j + 1, looking for an edge. If no","chapter-11","Graphs"
"such edge exists, next returns n. Functions setEdge and delEdge adjust the","chapter-11","Graphs"
"appropriate value in the array. Function weight returns the value stored in the","chapter-11","Graphs"
"appropriate position in the array.","chapter-11","Graphs"
"Figure 11.7 presents an implementation of the adjacency list representation for","chapter-11","Graphs"
"graphs. Its main data structure is an array of linked lists, one linked list for each","chapter-11","Graphs"
"vertex. These linked lists store objects of type Edge, which merely stores the index","chapter-11","Graphs"
"for the vertex pointed to by the edge, along with the weight of the edge.","chapter-11","Graphs"
"/** Edge class for Adjacency List graph representation */","chapter-11","Graphs"
"class Edge {","chapter-11","Graphs"
"private int vert, wt;","chapter-11","Graphs"
"public Edge(int v, int w) // Constructor","chapter-11","Graphs"
"{ vert = v; wt = w; }","chapter-11","Graphs"
"public int vertex() { return vert; }","chapter-11","Graphs"
"public int weight() { return wt; }","chapter-11","Graphs"
"}","chapter-11","Graphs"
"Implementation for Graphl member functions is straightforward in principle,","chapter-11","Graphs"
"with the key functions being setEdge, delEdge, and weight. They simply","chapter-11","Graphs"
"start at the beginning of the adjacency list and move along it until the desired vertex","chapter-11","Graphs"
"has been found. Note that isEdge checks to see if j is already the current neighbor","chapter-11","Graphs"
"in i’s adjacency list, since this will often be true when processing the neighbors of","chapter-11","Graphs"
"each vertex in turn.","chapter-11","Graphs"
"Sec. 11.2 Graph Implementations 379","chapter-11","Graphs"
"/** Graph: Adjacency matrix */","chapter-11","Graphs"
"class Graphm implements Graph {","chapter-11","Graphs"
"private int[][] matrix; // The edge matrix","chapter-11","Graphs"
"private int numEdge; // Number of edges","chapter-11","Graphs"
"private int[] Mark; // The mark array","chapter-11","Graphs"
"public Graphm() {} // Constructors","chapter-11","Graphs"
"public Graphm(int n) {","chapter-11","Graphs"
"Init(n);","chapter-11","Graphs"
"}","chapter-11","Graphs"
"public void Init(int n) {","chapter-11","Graphs"
"Mark = new int[n];","chapter-11","Graphs"
"matrix = new int[n][n];","chapter-11","Graphs"
"numEdge = 0;","chapter-11","Graphs"
"}","chapter-11","Graphs"
"public int n() { return Mark.length; } // # of vertices","chapter-11","Graphs"
"public int e() { return numEdge; } // # of edges","chapter-11","Graphs"
"/** @return v’s first neighbor */","chapter-11","Graphs"
"public int first(int v) {","chapter-11","Graphs"
"for (int i=0; i<Mark.length; i++)","chapter-11","Graphs"
"if (matrix[v][i] != 0) return i;","chapter-11","Graphs"
"return Mark.length; // No edge for this vertex","chapter-11","Graphs"
"}","chapter-11","Graphs"
"/** @return v’s next neighbor after w */","chapter-11","Graphs"
"public int next(int v, int w) {","chapter-11","Graphs"
"for (int i=w+1; i<Mark.length; i++)","chapter-11","Graphs"
"if (matrix[v][i] != 0)","chapter-11","Graphs"
"return i;","chapter-11","Graphs"
"return Mark.length; // No next edge;","chapter-11","Graphs"
"}","chapter-11","Graphs"
"/** Set the weight for an edge */","chapter-11","Graphs"
"public void setEdge(int i, int j, int wt) {","chapter-11","Graphs"
"assert wt!=0 : "Cannot set weight to 0";","chapter-11","Graphs"
"if (matrix[i][j] == 0) numEdge++;","chapter-11","Graphs"
"matrix[i][j] = wt;","chapter-11","Graphs"
"}","chapter-11","Graphs"
"/** Delete an edge */","chapter-11","Graphs"
"public void delEdge(int i, int j) { // Delete edge (i, j)","chapter-11","Graphs"
"if (matrix[i][j] != 0) numEdge--;","chapter-11","Graphs"
"matrix[i][j] = 0;","chapter-11","Graphs"
"}","chapter-11","Graphs"
"/** Determine if an edge is in the graph */","chapter-11","Graphs"
"public boolean isEdge(int i, int j)","chapter-11","Graphs"
"{ return matrix[i][j] != 0; }","chapter-11","Graphs"
"Figure 11.6 An implementation for the adjacency matrix implementation.","chapter-11","Graphs"
"380 Chap. 11 Graphs","chapter-11","Graphs"
"/** @return an edge’s weight */","chapter-11","Graphs"
"public int weight(int i, int j) {","chapter-11","Graphs"
"return matrix[i][j];","chapter-11","Graphs"
"}","chapter-11","Graphs"
"/** Set/Get the mark value for a vertex */","chapter-11","Graphs"
"public void setMark(int v, int val) { Mark[v] = val; }","chapter-11","Graphs"
"public int getMark(int v) { return Mark[v]; }","chapter-11","Graphs"
"}","chapter-11","Graphs"
"Figure 11.6 (continued)","chapter-11","Graphs"
"11.3 Graph Traversals","chapter-11","Graphs"
"Often it is useful to visit the vertices of a graph in some specific order based on the","chapter-11","Graphs"
"graph’s topology. This is known as a graph traversal and is similar in concept to","chapter-11","Graphs"
"a tree traversal. Recall that tree traversals visit every node exactly once, in some","chapter-11","Graphs"
"specified order such as preorder, inorder, or postorder. Multiple tree traversals exist","chapter-11","Graphs"
"because various applications require the nodes to be visited in a particular order.","chapter-11","Graphs"
"For example, to print a BST’s nodes in ascending order requires an inorder traver-","chapter-11","Graphs"
"sal as opposed to some other traversal. Standard graph traversal orders also exist.","chapter-11","Graphs"
"Each is appropriate for solving certain problems. For example, many problems in","chapter-11","Graphs"
"artificial intelligence programming are modeled using graphs. The problem domain","chapter-11","Graphs"
"may consist of a large collection of states, with connections between various pairs","chapter-11","Graphs"
"of states. Solving the problem may require getting from a specified start state to a","chapter-11","Graphs"
"specified goal state by moving between states only through the connections. Typi-","chapter-11","Graphs"
"cally, the start and goal states are not directly connected. To solve this problem, the","chapter-11","Graphs"
"vertices of the graph must be searched in some organized manner.","chapter-11","Graphs"
"Graph traversal algorithms typically begin with a start vertex and attempt to","chapter-11","Graphs"
"visit the remaining vertices from there. Graph traversals must deal with a number","chapter-11","Graphs"
"of troublesome cases. First, it may not be possible to reach all vertices from the","chapter-11","Graphs"
"start vertex. This occurs when the graph is not connected. Second, the graph may","chapter-11","Graphs"
"contain cycles, and we must make sure that cycles do not cause the algorithm to go","chapter-11","Graphs"
"into an infinite loop.","chapter-11","Graphs"
"Graph traversal algorithms can solve both of these problems by maintaining a","chapter-11","Graphs"
"mark bit for each vertex on the graph. At the beginning of the algorithm, the mark","chapter-11","Graphs"
"bit for all vertices is cleared. The mark bit for a vertex is set when the vertex is first","chapter-11","Graphs"
"visited during the traversal. If a marked vertex is encountered during traversal, it is","chapter-11","Graphs"
"not visited a second time. This keeps the program from going into an infinite loop","chapter-11","Graphs"
"when it encounters a cycle.","chapter-11","Graphs"
"Once the traversal algorithm completes, we can check to see if all vertices have","chapter-11","Graphs"
"been processed by checking the mark bit array. If not all vertices are marked, we","chapter-11","Graphs"
"can continue the traversal from another unmarked vertex. Note that this process","chapter-11","Graphs"
"Sec. 11.3 Graph Traversals 381","chapter-11","Graphs"
"/** Adjacency list graph implementation */","chapter-11","Graphs"
"class Graphl implements Graph {","chapter-11","Graphs"
"private GraphList[] vertex; // The vertex list","chapter-11","Graphs"
"private int numEdge; // Number of edges","chapter-11","Graphs"
"private int[] Mark; // The mark array","chapter-11","Graphs"
"public Graphl() {}","chapter-11","Graphs"
"public Graphl(int n) // Constructor","chapter-11","Graphs"
"{ Init(n); }","chapter-11","Graphs"
"public void Init(int n) {","chapter-11","Graphs"
"Mark = new int[n];","chapter-11","Graphs"
"vertex = new GraphList[n];","chapter-11","Graphs"
"for (int i=0; i<n; i++)","chapter-11","Graphs"
"vertex[i] = new GraphList();","chapter-11","Graphs"
"numEdge = 0;","chapter-11","Graphs"
"}","chapter-11","Graphs"
"public int n() { return Mark.length; } // # of vertices","chapter-11","Graphs"
"public int e() { return numEdge; } // # of edges","chapter-11","Graphs"
"/** @return v’s first neighbor */","chapter-11","Graphs"
"public int first(int v) {","chapter-11","Graphs"
"if (vertex[v].length() == 0)","chapter-11","Graphs"
"return Mark.length; // No neighbor","chapter-11","Graphs"
"vertex[v].moveToStart();","chapter-11","Graphs"
"Edge it = vertex[v].getValue();","chapter-11","Graphs"
"return it.vertex();","chapter-11","Graphs"
"}","chapter-11","Graphs"
"/** @return v’s next neighbor after w */","chapter-11","Graphs"
"public int next(int v, int w) {","chapter-11","Graphs"
"Edge it = null;","chapter-11","Graphs"
"if (isEdge(v, w)) {","chapter-11","Graphs"
"vertex[v].next();","chapter-11","Graphs"
"it = vertex[v].getValue();","chapter-11","Graphs"
"}","chapter-11","Graphs"
"if (it != null)","chapter-11","Graphs"
"return it.vertex();","chapter-11","Graphs"
"return Mark.length; // No neighbor","chapter-11","Graphs"
"}","chapter-11","Graphs"
"Figure 11.7 An implementation for the adjacency list.","chapter-11","Graphs"
"382 Chap. 11 Graphs","chapter-11","Graphs"
"/** Set the weight for an edge */","chapter-11","Graphs"
"public void setEdge(int i, int j, int weight) {","chapter-11","Graphs"
"assert weight != 0 : "May not set weight to 0";","chapter-11","Graphs"
"Edge currEdge = new Edge(j, weight);","chapter-11","Graphs"
"if (isEdge(i, j)) { // Edge already exists in graph","chapter-11","Graphs"
"vertex[i].remove();","chapter-11","Graphs"
"vertex[i].insert(currEdge);","chapter-11","Graphs"
"}","chapter-11","Graphs"
"else { // Keep neighbors sorted by vertex index","chapter-11","Graphs"
"numEdge++;","chapter-11","Graphs"
"for (vertex[i].moveToStart();","chapter-11","Graphs"
"vertex[i].currPos() < vertex[i].length();","chapter-11","Graphs"
"vertex[i].next())","chapter-11","Graphs"
"if (vertex[i].getValue().vertex() > j) break;","chapter-11","Graphs"
"vertex[i].insert(currEdge);","chapter-11","Graphs"
"}","chapter-11","Graphs"
"}","chapter-11","Graphs"
"/** Delete an edge */","chapter-11","Graphs"
"public void delEdge(int i, int j)","chapter-11","Graphs"
"{ if (isEdge(i, j)) { vertex[i].remove(); numEdge--; } }","chapter-11","Graphs"
"/** Determine if an edge is in the graph */","chapter-11","Graphs"
"public boolean isEdge(int v, int w) {","chapter-11","Graphs"
"Edge it = vertex[v].getValue();","chapter-11","Graphs"
"// Check if j is the current neighbor in the list","chapter-11","Graphs"
"if ((it != null) && (it.vertex() == w)) return true;","chapter-11","Graphs"
"for (vertex[v].moveToStart();","chapter-11","Graphs"
"vertex[v].currPos() < vertex[v].length();","chapter-11","Graphs"
"vertex[v].next()) // Check whole list","chapter-11","Graphs"
"if (vertex[v].getValue().vertex() == w) return true;","chapter-11","Graphs"
"return false;","chapter-11","Graphs"
"}","chapter-11","Graphs"
"/** @return an edge’s weight */","chapter-11","Graphs"
"public int weight(int i, int j) {","chapter-11","Graphs"
"if (isEdge(i, j)) return vertex[i].getValue().weight();","chapter-11","Graphs"
"return 0;","chapter-11","Graphs"
"}","chapter-11","Graphs"
"/** Set/Get the mark value for a vertex */","chapter-11","Graphs"
"public void setMark(int v, int val) { Mark[v] = val; }","chapter-11","Graphs"
"public int getMark(int v) { return Mark[v]; }","chapter-11","Graphs"
"}","chapter-11","Graphs"
"Figure 11.7 (continued)","chapter-11","Graphs"
"Sec. 11.3 Graph Traversals 383","chapter-11","Graphs"
"works regardless of whether the graph is directed or undirected. To ensure visiting","chapter-11","Graphs"
"all vertices, graphTraverse could be called as follows on a graph G:","chapter-11","Graphs"
"void graphTraverse(Graph G) {","chapter-11","Graphs"
"int v;","chapter-11","Graphs"
"for (v=0; v<G.n(); v++)","chapter-11","Graphs"
"G.setMark(v, UNVISITED); // Initialize","chapter-11","Graphs"
"for (v=0; v<G.n(); v++)","chapter-11","Graphs"
"if (G.getMark(v) == UNVISITED)","chapter-11","Graphs"
"doTraverse(G, v);","chapter-11","Graphs"
"}","chapter-11","Graphs"
"Function “doTraverse” might be implemented by using one of the graph traver-","chapter-11","Graphs"
"sals described in this section.","chapter-11","Graphs"
"11.3.1 Depth-First Search","chapter-11","Graphs"
"The first method of organized graph traversal is called depth-first search (DFS).","chapter-11","Graphs"
"Whenever a vertex V is visited during the search, DFS will recursively visit all","chapter-11","Graphs"
"of V’s unvisited neighbors. Equivalently, DFS will add all edges leading out of v","chapter-11","Graphs"
"to a stack. The next vertex to be visited is determined by popping the stack and","chapter-11","Graphs"
"following that edge. The effect is to follow one branch through the graph to its","chapter-11","Graphs"
"conclusion, then it will back up and follow another branch, and so on. The DFS","chapter-11","Graphs"
"process can be used to define a depth-first search tree. This tree is composed of","chapter-11","Graphs"
"the edges that were followed to any new (unvisited) vertex during the traversal, and","chapter-11","Graphs"
"leaves out the edges that lead to already visited vertices. DFS can be applied to","chapter-11","Graphs"
"directed or undirected graphs. Here is an implementation for the DFS algorithm:","chapter-11","Graphs"
"/** Depth first search */","chapter-11","Graphs"
"static void DFS(Graph G, int v) {","chapter-11","Graphs"
"PreVisit(G, v); // Take appropriate action","chapter-11","Graphs"
"G.setMark(v, VISITED);","chapter-11","Graphs"
"for (int w = G.first(v); w < G.n() ; w = G.next(v, w))","chapter-11","Graphs"
"if (G.getMark(w) == UNVISITED)","chapter-11","Graphs"
"DFS(G, w);","chapter-11","Graphs"
"PostVisit(G, v); // Take appropriate action","chapter-11","Graphs"
"}","chapter-11","Graphs"
"This implementation contains calls to functions PreVisit and PostVisit.","chapter-11","Graphs"
"These functions specify what activity should take place during the search. Just","chapter-11","Graphs"
"as a preorder tree traversal requires action before the subtrees are visited, some","chapter-11","Graphs"
"graph traversals require that a vertex be processed before ones further along in the","chapter-11","Graphs"
"DFS. Alternatively, some applications require activity after the remaining vertices","chapter-11","Graphs"
"are processed; hence the call to function PostVisit. This would be a natural","chapter-11","Graphs"
"opportunity to make use of the visitor design pattern described in Section 1.3.2.","chapter-11","Graphs"
"Figure 11.8 shows a graph and its corresponding depth-first search tree. Fig-","chapter-11","Graphs"
"ure 11.9 illustrates the DFS process for the graph of Figure 11.8(a).","chapter-11","Graphs"
"384 Chap. 11 Graphs","chapter-11","Graphs"
"(a) (b)","chapter-11","Graphs"
"A B","chapter-11","Graphs"
"D","chapter-11","Graphs"
"F","chapter-11","Graphs"
"A B","chapter-11","Graphs"
"C","chapter-11","Graphs"
"D","chapter-11","Graphs"
"F","chapter-11","Graphs"
"E","chapter-11","Graphs"
"C","chapter-11","Graphs"
"E","chapter-11","Graphs"
"Figure 11.8 (a) A graph. (b) The depth-first search tree for the graph when","chapter-11","Graphs"
"starting at Vertex A.","chapter-11","Graphs"
"DFS processes each edge once in a directed graph. In an undirected graph,","chapter-11","Graphs"
"DFS processes each edge from both directions. Each vertex must be visited, but","chapter-11","Graphs"
"only once, so the total cost is Θ(|V| + |E|).","chapter-11","Graphs"
"11.3.2 Breadth-First Search","chapter-11","Graphs"
"Our second graph traversal algorithm is known as a breadth-first search (BFS).","chapter-11","Graphs"
"BFS examines all vertices connected to the start vertex before visiting vertices fur-","chapter-11","Graphs"
"ther away. BFS is implemented similarly to DFS, except that a queue replaces","chapter-11","Graphs"
"the recursion stack. Note that if the graph is a tree and the start vertex is at the","chapter-11","Graphs"
"root, BFS is equivalent to visiting vertices level by level from top to bottom. Fig-","chapter-11","Graphs"
"ure 11.10 provides an implementation for the BFS algorithm. Figure 11.11 shows","chapter-11","Graphs"
"a graph and the corresponding breadth-first search tree. Figure 11.12 illustrates the","chapter-11","Graphs"
"BFS process for the graph of Figure 11.11(a).","chapter-11","Graphs"
"11.3.3 Topological Sort","chapter-11","Graphs"
"Assume that we need to schedule a series of tasks, such as classes or construction","chapter-11","Graphs"
"jobs, where we cannot start one task until after its prerequisites are completed. We","chapter-11","Graphs"
"wish to organize the tasks into a linear order that allows us to complete them one","chapter-11","Graphs"
"at a time without violating any prerequisites. We can model the problem using a","chapter-11","Graphs"
"DAG. The graph is directed because one task is a prerequisite of another — the","chapter-11","Graphs"
"vertices have a directed relationship. It is acyclic because a cycle would indicate","chapter-11","Graphs"
"a conflicting series of prerequisites that could not be completed without violating","chapter-11","Graphs"
"at least one prerequisite. The process of laying out the vertices of a DAG in a","chapter-11","Graphs"
"linear order to meet the prerequisite rules is called a topological sort. Figure 11.14","chapter-11","Graphs"
"Sec. 11.3 Graph Traversals 385","chapter-11","Graphs"
"Call DFS on A","chapter-11","Graphs"
"Mark B","chapter-11","Graphs"
"Process (B, C)","chapter-11","Graphs"
"Process (B, F)","chapter-11","Graphs"
"Print (B, F) and","chapter-11","Graphs"
"call DFS on F","chapter-11","Graphs"
"Process (F, E)","chapter-11","Graphs"
"Print (F, E) and","chapter-11","Graphs"
"call DFS on E","chapter-11","Graphs"
"Done with B","chapter-11","Graphs"
"Pop B","chapter-11","Graphs"
"Mark A","chapter-11","Graphs"
"Process (A, C)","chapter-11","Graphs"
"Print (A, C) and","chapter-11","Graphs"
"call DFS on C","chapter-11","Graphs"
"Mark F","chapter-11","Graphs"
"Process (F, B)","chapter-11","Graphs"
"Process (F, C)","chapter-11","Graphs"
"Process (F, D)","chapter-11","Graphs"
"Print (F, D) and","chapter-11","Graphs"
"call DFS on D","chapter-11","Graphs"
"Mark E","chapter-11","Graphs"
"Process (E, A)","chapter-11","Graphs"
"Process (E, F)","chapter-11","Graphs"
"Pop E","chapter-11","Graphs"
"Continue with C","chapter-11","Graphs"
"Process (C, E)","chapter-11","Graphs"
"Process (C, F)","chapter-11","Graphs"
"Pop C","chapter-11","Graphs"
"Mark C","chapter-11","Graphs"
"Process (C, A)","chapter-11","Graphs"
"Process (C, B)","chapter-11","Graphs"
"Print (C, B) and","chapter-11","Graphs"
"call DFS on B","chapter-11","Graphs"
"Mark D","chapter-11","Graphs"
"Done with F","chapter-11","Graphs"
"Pop F","chapter-11","Graphs"
"Continue with A","chapter-11","Graphs"
"Process (A, E)","chapter-11","Graphs"
"Pop A","chapter-11","Graphs"
"DFS complete","chapter-11","Graphs"
"Pop D","chapter-11","Graphs"
"Process (D, C)","chapter-11","Graphs"
"Process (D, F)","chapter-11","Graphs"
"E","chapter-11","Graphs"
"F","chapter-11","Graphs"
"B","chapter-11","Graphs"
"C","chapter-11","Graphs"
"A","chapter-11","Graphs"
"A","chapter-11","Graphs"
"F","chapter-11","Graphs"
"B","chapter-11","Graphs"
"C","chapter-11","Graphs"
"A","chapter-11","Graphs"
"C","chapter-11","Graphs"
"A","chapter-11","Graphs"
"F","chapter-11","Graphs"
"B","chapter-11","Graphs"
"C","chapter-11","Graphs"
"A","chapter-11","Graphs"
"C","chapter-11","Graphs"
"A","chapter-11","Graphs"
"D","chapter-11","Graphs"
"F","chapter-11","Graphs"
"B","chapter-11","Graphs"
"C","chapter-11","Graphs"
"A","chapter-11","Graphs"
"A","chapter-11","Graphs"
"B","chapter-11","Graphs"
"C","chapter-11","Graphs"
"A","chapter-11","Graphs"
"B","chapter-11","Graphs"
"C","chapter-11","Graphs"
"A","chapter-11","Graphs"
"F","chapter-11","Graphs"
"B","chapter-11","Graphs"
"C","chapter-11","Graphs"
"A","chapter-11","Graphs"
"Figure 11.9 A detailed illustration of the DFS process for the graph of Fig-","chapter-11","Graphs"
"ure 11.8(a) starting at Vertex A. The steps leading to each change in the recursion","chapter-11","Graphs"
"stack are described.","chapter-11","Graphs"
"386 Chap. 11 Graphs","chapter-11","Graphs"
"/** Breadth first (queue-based) search */","chapter-11","Graphs"
"static void BFS(Graph G, int start) {","chapter-11","Graphs"
"Queue<Integer> Q = new AQueue<Integer>(G.n());","chapter-11","Graphs"
"Q.enqueue(start);","chapter-11","Graphs"
"G.setMark(start, VISITED);","chapter-11","Graphs"
"while (Q.length() > 0) { // Process each vertex on Q","chapter-11","Graphs"
"int v = Q.dequeue();","chapter-11","Graphs"
"PreVisit(G, v); // Take appropriate action","chapter-11","Graphs"
"for (int w = G.first(v); w < G.n(); w = G.next(v, w))","chapter-11","Graphs"
"if (G.getMark(w) == UNVISITED) { // Put neighbors on Q","chapter-11","Graphs"
"G.setMark(w, VISITED);","chapter-11","Graphs"
"Q.enqueue(w);","chapter-11","Graphs"
"}","chapter-11","Graphs"
"PostVisit(G, v); // Take appropriate action","chapter-11","Graphs"
"}","chapter-11","Graphs"
"}","chapter-11","Graphs"
"Figure 11.10 Implementation for the breadth-first graph traversal algorithm","chapter-11","Graphs"
"(a) (b)","chapter-11","Graphs"
"B","chapter-11","Graphs"
"C","chapter-11","Graphs"
"A","chapter-11","Graphs"
"C","chapter-11","Graphs"
"B","chapter-11","Graphs"
"D D","chapter-11","Graphs"
"F","chapter-11","Graphs"
"E E","chapter-11","Graphs"
"A","chapter-11","Graphs"
"F","chapter-11","Graphs"
"Figure 11.11 (a) A graph. (b) The breadth-first search tree for the graph when","chapter-11","Graphs"
"starting at Vertex A.","chapter-11","Graphs"
"illustrates the problem. An acceptable topological sort for this example is J1, J2,","chapter-11","Graphs"
"J3, J4, J5, J6, J7.","chapter-11","Graphs"
"A topological sort may be found by performing a DFS on the graph. When a","chapter-11","Graphs"
"vertex is visited, no action is taken (i.e., function PreVisit does nothing). When","chapter-11","Graphs"
"the recursion pops back to that vertex, function PostVisit prints the vertex. This","chapter-11","Graphs"
"yields a topological sort in reverse order. It does not matter where the sort starts, as","chapter-11","Graphs"
"long as all vertices are visited in the end. Figure 11.13 shows an implementation","chapter-11","Graphs"
"for the DFS-based algorithm.","chapter-11","Graphs"
"Using this algorithm starting at J1 and visiting adjacent neighbors in alphabetic","chapter-11","Graphs"
"order, vertices of the graph in Figure 11.14 are printed out in the order J7, J5, J4,","chapter-11","Graphs"
"J6, J2, J3, J1. Reversing this yields the topological sort J1, J3, J2, J6, J4, J5, J7.","chapter-11","Graphs"
"Sec. 11.3 Graph Traversals 387","chapter-11","Graphs"
"Initial call to BFS on A.","chapter-11","Graphs"
"Mark A and put on the queue.","chapter-11","Graphs"
"Dequeue A.","chapter-11","Graphs"
"Process (A, C).","chapter-11","Graphs"
"Mark and enqueue C. Print (A, C).","chapter-11","Graphs"
"Process (A, E).","chapter-11","Graphs"
"Mark and enqueue E. Print(A, E).","chapter-11","Graphs"
"Dequeue C.","chapter-11","Graphs"
"Process (C, A). Ignore.","chapter-11","Graphs"
"Process (C, B).","chapter-11","Graphs"
"Mark and enqueue B. Print (C, B).","chapter-11","Graphs"
"Process (C, D).","chapter-11","Graphs"
"Mark and enqueue D. Print (C, D).","chapter-11","Graphs"
"Process (C, F).","chapter-11","Graphs"
"Mark and enqueue F. Print (C, F).","chapter-11","Graphs"
"Dequeue E.","chapter-11","Graphs"
"Process (E, A). Ignore.","chapter-11","Graphs"
"Process (E, F). Ignore.","chapter-11","Graphs"
"Dequeue B.","chapter-11","Graphs"
"Process (B, C). Ignore.","chapter-11","Graphs"
"Process (B, F). Ignore.","chapter-11","Graphs"
"Dequeue D.","chapter-11","Graphs"
"Process (D, C). Ignore.","chapter-11","Graphs"
"Process (D, F). Ignore.","chapter-11","Graphs"
"Dequeue F.","chapter-11","Graphs"
"Process (F, B). Ignore.","chapter-11","Graphs"
"Process (F, C). Ignore.","chapter-11","Graphs"
"Process (F, D). Ignore.","chapter-11","Graphs"
"BFS is complete.","chapter-11","Graphs"
"A","chapter-11","Graphs"
"E B D F","chapter-11","Graphs"
"D F","chapter-11","Graphs"
"C E","chapter-11","Graphs"
"B D F","chapter-11","Graphs"
"F","chapter-11","Graphs"
"Figure 11.12 A detailed illustration of the BFS process for the graph of Fig-","chapter-11","Graphs"
"ure 11.11(a) starting at Vertex A. The steps leading to each change in the queue","chapter-11","Graphs"
"are described.","chapter-11","Graphs"
"388 Chap. 11 Graphs","chapter-11","Graphs"
"/** Recursive topological sort */","chapter-11","Graphs"
"static void topsort(Graph G) {","chapter-11","Graphs"
"for (int i=0; i<G.n(); i++) // Initialize Mark array","chapter-11","Graphs"
"G.setMark(i, UNVISITED);","chapter-11","Graphs"
"for (int i=0; i<G.n(); i++) // Process all vertices","chapter-11","Graphs"
"if (G.getMark(i) == UNVISITED)","chapter-11","Graphs"
"tophelp(G, i); // Recursive helper function","chapter-11","Graphs"
"}","chapter-11","Graphs"
"/** Topsort helper function */","chapter-11","Graphs"
"static void tophelp(Graph G, int v) {","chapter-11","Graphs"
"G.setMark(v, VISITED);","chapter-11","Graphs"
"for (int w = G.first(v); w < G.n(); w = G.next(v, w))","chapter-11","Graphs"
"if (G.getMark(w) == UNVISITED)","chapter-11","Graphs"
"tophelp(G, w);","chapter-11","Graphs"
"printout(v); // PostVisit for Vertex v","chapter-11","Graphs"
"}","chapter-11","Graphs"
"Figure 11.13 Implementation for the recursive topological sort.","chapter-11","Graphs"
"J1 J2","chapter-11","Graphs"
"J3 J4","chapter-11","Graphs"
"J5 J7","chapter-11","Graphs"
"J6","chapter-11","Graphs"
"Figure 11.14 An example graph for topological sort. Seven tasks have depen-","chapter-11","Graphs"
"dencies as shown by the directed graph.","chapter-11","Graphs"
"We can implement topological sort using a queue instead of recursion, as fol-","chapter-11","Graphs"
"lows. First visit all edges, counting the number of edges that lead to each vertex","chapter-11","Graphs"
"(i.e., count the number of prerequisites for each vertex). All vertices with no pre-","chapter-11","Graphs"
"requisites are placed on the queue. We then begin processing the queue. When","chapter-11","Graphs"
"Vertex V is taken off of the queue, it is printed, and all neighbors of V (that is, all","chapter-11","Graphs"
"vertices that have V as a prerequisite) have their counts decremented by one. Place","chapter-11","Graphs"
"on the queue any neighbor whose count becomes zero. If the queue becomes empty","chapter-11","Graphs"
"without printing all of the vertices, then the graph contains a cycle (i.e., there is no","chapter-11","Graphs"
"possible ordering for the tasks that does not violate some prerequisite). The printed","chapter-11","Graphs"
"order for the vertices of the graph in Figure 11.14 using the queue version of topo-","chapter-11","Graphs"
"logical sort is J1, J2, J3, J6, J4, J5, J7. Figure 11.15 shows an implementation for","chapter-11","Graphs"
"the algorithm.","chapter-11","Graphs"
"11.4 Shortest-Paths Problems","chapter-11","Graphs"
"On a road map, a road connecting two towns is typically labeled with its distance.","chapter-11","Graphs"
"We can model a road network as a directed graph whose edges are labeled with","chapter-11","Graphs"
"Sec. 11.4 Shortest-Paths Problems 389","chapter-11","Graphs"
"static void topsort(Graph G) { // Topological sort: Queue","chapter-11","Graphs"
"Queue<Integer> Q = new AQueue<Integer>(G.n());","chapter-11","Graphs"
"int[] Count = new int[G.n()];","chapter-11","Graphs"
"int v;","chapter-11","Graphs"
"for (v=0; v<G.n(); v++) Count[v] = 0; // Initialize","chapter-11","Graphs"
"for (v=0; v<G.n(); v++) // Process every edge","chapter-11","Graphs"
"for (int w = G.first(v); w < G.n(); w = G.next(v, w))","chapter-11","Graphs"
"Count[w]++; // Add to v’s prereq count","chapter-11","Graphs"
"for (v=0; v<G.n(); v++) // Initialize Queue","chapter-11","Graphs"
"if (Count[v] == 0) // V has no prerequisites","chapter-11","Graphs"
"Q.enqueue(v);","chapter-11","Graphs"
"while (Q.length() > 0) { // Process the vertices","chapter-11","Graphs"
"v = Q.dequeue().intValue();","chapter-11","Graphs"
"printout(v); // PreVisit for Vertex V","chapter-11","Graphs"
"for (int w = G.first(v); w < G.n(); w = G.next(v, w)) {","chapter-11","Graphs"
"Count[w]--; // One less prerequisite","chapter-11","Graphs"
"if (Count[w] == 0) // This vertex is now free","chapter-11","Graphs"
"Q.enqueue(w);","chapter-11","Graphs"
"}","chapter-11","Graphs"
"}","chapter-11","Graphs"
"}","chapter-11","Graphs"
"Figure 11.15 A queue-based topological sort algorithm.","chapter-11","Graphs"
"real numbers. These numbers represent the distance (or other cost metric, such as","chapter-11","Graphs"
"travel time) between two vertices. These labels may be called weights, costs, or","chapter-11","Graphs"
"distances, depending on the application. Given such a graph, a typical problem","chapter-11","Graphs"
"is to find the total length of the shortest path between two specified vertices. This","chapter-11","Graphs"
"is not a trivial problem, because the shortest path may not be along the edge (if","chapter-11","Graphs"
"any) connecting two vertices, but rather may be along a path involving one or more","chapter-11","Graphs"
"intermediate vertices. For example, in Figure 11.16, the cost of the path from A to","chapter-11","Graphs"
"B to D is 15. The cost of the edge directly from A to D is 20. The cost of the path","chapter-11","Graphs"
"from A to C to B to D is 10. Thus, the shortest path from A to D is 10 (not along","chapter-11","Graphs"
"the edge connecting A to D). We use the notation d(A, D) = 10 to indicate that the","chapter-11","Graphs"
"shortest distance from A to D is 10. In Figure 11.16, there is no path from E to B, so","chapter-11","Graphs"
"we set d(E, B) = ∞. We define w(A, D) = 20 to be the weight of edge (A, D), that","chapter-11","Graphs"
"is, the weight of the direct connection from A to D. Because there is no edge from","chapter-11","Graphs"
"E to B, w(E, B) = ∞. Note that w(D, A) = ∞ because the graph of Figure 11.16","chapter-11","Graphs"
"is directed. We assume that all weights are positive.","chapter-11","Graphs"
"11.4.1 Single-Source Shortest Paths","chapter-11","Graphs"
"This section presents an algorithm to solve the single-source shortest-paths prob-","chapter-11","Graphs"
"lem. Given Vertex S in Graph G, find a shortest path from S to every other vertex","chapter-11","Graphs"
"in G. We might want only the shortest path between two vertices, S and T. How-","chapter-11","Graphs"
"ever in the worst case, while finding the shortest path from S to T, we might find","chapter-11","Graphs"
"the shortest paths from S to every other vertex as well. So there is no better alg-","chapter-11","Graphs"
"390 Chap. 11 Graphs","chapter-11","Graphs"
"5","chapter-11","Graphs"
"20","chapter-11","Graphs"
"2","chapter-11","Graphs"
"10 D","chapter-11","Graphs"
"B","chapter-11","Graphs"
"A","chapter-11","Graphs"
"3","chapter-11","Graphs"
"11","chapter-11","Graphs"
"C E","chapter-11","Graphs"
"15","chapter-11","Graphs"
"Figure 11.16 Example graph for shortest-path definitions.","chapter-11","Graphs"
"orithm (in the worst case) for finding the shortest path to a single vertex than to find","chapter-11","Graphs"
"shortest paths to all vertices. The algorithm described here will only compute the","chapter-11","Graphs"
"distance to every such vertex, rather than recording the actual path. Recording the","chapter-11","Graphs"
"path requires modifications to the algorithm that are left as an exercise.","chapter-11","Graphs"
"Computer networks provide an application for the single-source shortest-paths","chapter-11","Graphs"
"problem. The goal is to find the cheapest way for one computer to broadcast a","chapter-11","Graphs"
"message to all other computers on the network. The network can be modeled by a","chapter-11","Graphs"
"graph with edge weights indicating time or cost to send a message to a neighboring","chapter-11","Graphs"
"computer.","chapter-11","Graphs"
"For unweighted graphs (or whenever all edges have the same cost), the single-","chapter-11","Graphs"
"source shortest paths can be found using a simple breadth-first search. When","chapter-11","Graphs"
"weights are added, BFS will not give the correct answer.","chapter-11","Graphs"
"One approach to solving this problem when the edges have differing weights","chapter-11","Graphs"
"might be to process the vertices in a fixed order. Label the vertices v0 to vn−1, with","chapter-11","Graphs"
"S = v0. When processing Vertex v1, we take the edge connecting v0 and v1. When","chapter-11","Graphs"
"processing v2, we consider the shortest distance from v0 to v2 and compare that to","chapter-11","Graphs"
"the shortest distance from v0 to v1 to v2. When processing Vertex vi","chapter-11","Graphs"
", we consider","chapter-11","Graphs"
"the shortest path for Vertices v0 through vi−1 that have already been processed.","chapter-11","Graphs"
"Unfortunately, the true shortest path to vi might go through Vertex vj for j > i.","chapter-11","Graphs"
"Such a path will not be considered by this algorithm. However, the problem would","chapter-11","Graphs"
"not occur if we process the vertices in order of distance from S. Assume that we","chapter-11","Graphs"
"have processed in order of distance from S to the first i − 1 vertices that are closest","chapter-11","Graphs"
"to S; call this set of vertices S. We are now about to process the ith closest vertex;","chapter-11","Graphs"
"call it X. A shortest path from S to X must have its next-to-last vertex in S. Thus,","chapter-11","Graphs"
"d(S, X) = min","chapter-11","Graphs"
"U∈S","chapter-11","Graphs"
"(d(S, U) + w(U, X)).","chapter-11","Graphs"
"In other words, the shortest path from S to X is the minimum over all paths that go","chapter-11","Graphs"
"from S to U, then have an edge from U to X, where U is some vertex in S.","chapter-11","Graphs"
"This solution is usually referred to as Dijkstra’s algorithm. It works by main-","chapter-11","Graphs"
"taining a distance estimate D(X) for all vertices X in V. The elements of D are ini-","chapter-11","Graphs"
"Sec. 11.4 Shortest-Paths Problems 391","chapter-11","Graphs"
"// Compute shortest path distances from s, store them in D","chapter-11","Graphs"
"static void Dijkstra(Graph G, int s, int[] D) {","chapter-11","Graphs"
"for (int i=0; i<G.n(); i++) // Initialize","chapter-11","Graphs"
"D[i] = Integer.MAX VALUE;","chapter-11","Graphs"
"D[s] = 0;","chapter-11","Graphs"
"for (int i=0; i<G.n(); i++) { // Process the vertices","chapter-11","Graphs"
"int v = minVertex(G, D); // Find next-closest vertex","chapter-11","Graphs"
"G.setMark(v, VISITED);","chapter-11","Graphs"
"if (D[v] == Integer.MAX VALUE) return; // Unreachable","chapter-11","Graphs"
"for (int w = G.first(v); w < G.n(); w = G.next(v, w))","chapter-11","Graphs"
"if (D[w] > (D[v] + G.weight(v, w)))","chapter-11","Graphs"
"D[w] = D[v] + G.weight(v, w);","chapter-11","Graphs"
"}","chapter-11","Graphs"
"}","chapter-11","Graphs"
"Figure 11.17 An implementation for Dijkstra’s algorithm.","chapter-11","Graphs"
"tialized to the value INFINITE. Vertices are processed in order of distance from","chapter-11","Graphs"
"S. Whenever a vertex V is processed, D(X) is updated for every neighbor X of V.","chapter-11","Graphs"
"Figure 11.17 shows an implementation for Dijkstra’s algorithm. At the end, array D","chapter-11","Graphs"
"will contain the shortest distance values.","chapter-11","Graphs"
"There are two reasonable solutions to the key issue of finding the unvisited","chapter-11","Graphs"
"vertex with minimum distance value during each pass through the main for loop.","chapter-11","Graphs"
"The first method is simply to scan through the list of |V| vertices searching for the","chapter-11","Graphs"
"minimum value, as follows:","chapter-11","Graphs"
"static int minVertex(Graph G, int[] D) {","chapter-11","Graphs"
"int v = 0; // Initialize v to any unvisited vertex;","chapter-11","Graphs"
"for (int i=0; i<G.n(); i++)","chapter-11","Graphs"
"if (G.getMark(i) == UNVISITED) { v = i; break; }","chapter-11","Graphs"
"for (int i=0; i<G.n(); i++) // Now find smallest value","chapter-11","Graphs"
"if ((G.getMark(i) == UNVISITED) && (D[i] < D[v]))","chapter-11","Graphs"
"v = i;","chapter-11","Graphs"
"return v;","chapter-11","Graphs"
"}","chapter-11","Graphs"
"Because this scan is done |V| times, and because each edge requires a constant-","chapter-11","Graphs"
"time update to D, the total cost for this approach is Θ(|V|","chapter-11","Graphs"
"2 + |E|) = Θ(|V|","chapter-11","Graphs"
"2","chapter-11","Graphs"
"),","chapter-11","Graphs"
"because |E| is in O(|V|","chapter-11","Graphs"
"2","chapter-11","Graphs"
").","chapter-11","Graphs"
"The second method is to store unprocessed vertices in a min-heap ordered by","chapter-11","Graphs"
"distance values. The next-closest vertex can be found in the heap in Θ(log |V|)","chapter-11","Graphs"
"time. Every time we modify D(X), we could reorder X in the heap by deleting","chapter-11","Graphs"
"and reinserting it. This is an example of a priority queue with priority update, as","chapter-11","Graphs"
"described in Section 5.5. To implement true priority updating, we would need to","chapter-11","Graphs"
"store with each vertex its array index within the heap. A simpler approach is to","chapter-11","Graphs"
"add the new (smaller) distance value for a given vertex as a new record in the heap.","chapter-11","Graphs"
"The smallest value for a given vertex currently in the heap will be found first, and","chapter-11","Graphs"
"greater distance values found later will be ignored because the vertex will already","chapter-11","Graphs"
"be marked as VISITED. The only disadvantage to repeatedly inserting distance","chapter-11","Graphs"
"392 Chap. 11 Graphs","chapter-11","Graphs"
"/** Dijkstra’s shortest-paths: priority queue version */","chapter-11","Graphs"
"static void Dijkstra(Graph G, int s, int[] D) {","chapter-11","Graphs"
"int v; // The current vertex","chapter-11","Graphs"
"DijkElem[] E = new DijkElem[G.e()]; // Heap for edges","chapter-11","Graphs"
"E[0] = new DijkElem(s, 0); // Initial vertex","chapter-11","Graphs"
"MinHeap<DijkElem> H = new MinHeap<DijkElem>(E, 1, G.e());","chapter-11","Graphs"
"for (int i=0; i<G.n(); i++) // Initialize distance","chapter-11","Graphs"
"D[i] = Integer.MAX VALUE;","chapter-11","Graphs"
"D[s] = 0;","chapter-11","Graphs"
"for (int i=0; i<G.n(); i++) { // For each vertex","chapter-11","Graphs"
"do { v = (H.removemin()).vertex(); } // Get position","chapter-11","Graphs"
"while (G.getMark(v) == VISITED);","chapter-11","Graphs"
"G.setMark(v, VISITED);","chapter-11","Graphs"
"if (D[v] == Integer.MAX VALUE) return; // Unreachable","chapter-11","Graphs"
"for (int w = G.first(v); w < G.n(); w = G.next(v, w))","chapter-11","Graphs"
"if (D[w] > (D[v] + G.weight(v, w))) { // Update D","chapter-11","Graphs"
"D[w] = D[v] + G.weight(v, w);","chapter-11","Graphs"
"H.insert(new DijkElem(w, D[w]));","chapter-11","Graphs"
"}","chapter-11","Graphs"
"}","chapter-11","Graphs"
"}","chapter-11","Graphs"
"Figure 11.18 An implementation for Dijkstra’s algorithm using a priority queue.","chapter-11","Graphs"
"values is that it will raise the number of elements in the heap from Θ(|V|) to Θ(|E|)","chapter-11","Graphs"
"in the worst case. The time complexity is Θ((|V| + |E|) log |E|), because for each","chapter-11","Graphs"
"edge we must reorder the heap. Because the objects stored on the heap need to","chapter-11","Graphs"
"know both their vertex number and their distance, we create a simple class for the","chapter-11","Graphs"
"purpose called DijkElem, as follows. DijkElem is quite similar to the Edge","chapter-11","Graphs"
"class used by the adjacency list representation.","chapter-11","Graphs"
"class DijkElem implements Comparable<DijkElem> {","chapter-11","Graphs"
"private int vertex;","chapter-11","Graphs"
"private int weight;","chapter-11","Graphs"
"public DijkElem(int inv, int inw)","chapter-11","Graphs"
"{ vertex = inv; weight = inw; }","chapter-11","Graphs"
"public DijkElem() {vertex = 0; weight = 0; }","chapter-11","Graphs"
"public int key() { return weight; }","chapter-11","Graphs"
"public int vertex() { return vertex; }","chapter-11","Graphs"
"public int compareTo(DijkElem that) {","chapter-11","Graphs"
"if (weight < that.key()) return -1;","chapter-11","Graphs"
"else if (weight == that.key()) return 0;","chapter-11","Graphs"
"else return 1;","chapter-11","Graphs"
"}","chapter-11","Graphs"
"}","chapter-11","Graphs"
"Figure 11.18 shows an implementation for Dijkstra’s algorithm using the prior-","chapter-11","Graphs"
"ity queue.","chapter-11","Graphs"
"Using MinVertex to scan the vertex list for the minimum value is more ef-","chapter-11","Graphs"
"ficient when the graph is dense, that is, when |E| approaches |V|","chapter-11","Graphs"
"2","chapter-11","Graphs"
". Using a prior-","chapter-11","Graphs"
"Sec. 11.5 Minimum-Cost Spanning Trees 393","chapter-11","Graphs"
"A B C D E","chapter-11","Graphs"
"Initial 0 ∞ ∞ ∞ ∞","chapter-11","Graphs"
"Process A 0 10 3 20 ∞","chapter-11","Graphs"
"Process C 0 5 3 20 18","chapter-11","Graphs"
"Process B 0 5 3 10 18","chapter-11","Graphs"
"Process D 0 5 3 10 18","chapter-11","Graphs"
"Process E 0 5 3 10 18","chapter-11","Graphs"
"Figure 11.19 A listing for the progress of Dijkstra’s algorithm operating on the","chapter-11","Graphs"
"graph of Figure 11.16. The start vertex is A.","chapter-11","Graphs"
"ity queue is more efficient when the graph is sparse because its cost is Θ((|V| +","chapter-11","Graphs"
"|E|) log |E|). However, when the graph is dense, this cost can become as great as","chapter-11","Graphs"
"Θ(|V|","chapter-11","Graphs"
"2","chapter-11","Graphs"
"log |E|) = Θ(|V |","chapter-11","Graphs"
"2","chapter-11","Graphs"
"log |V |).","chapter-11","Graphs"
"Figure 11.19 illustrates Dijkstra’s algorithm. The start vertex is A. All vertices","chapter-11","Graphs"
"except A have an initial value of ∞. After processing Vertex A, its neighbors have","chapter-11","Graphs"
"their D estimates updated to be the direct distance from A. After processing C","chapter-11","Graphs"
"(the closest vertex to A), Vertices B and E are updated to reflect the shortest path","chapter-11","Graphs"
"through C. The remaining vertices are processed in order B, D, and E.","chapter-11","Graphs"
"11.5 Minimum-Cost Spanning Trees","chapter-11","Graphs"
"The minimum-cost spanning tree (MST) problem takes as input a connected,","chapter-11","Graphs"
"undirected graph G, where each edge has a distance or weight measure attached.","chapter-11","Graphs"
"The MST is the graph containing the vertices of G along with the subset of G’s","chapter-11","Graphs"
"edges that (1) has minimum total cost as measured by summing the values for all of","chapter-11","Graphs"
"the edges in the subset, and (2) keeps the vertices connected. Applications where a","chapter-11","Graphs"
"solution to this problem is useful include soldering the shortest set of wires needed","chapter-11","Graphs"
"to connect a set of terminals on a circuit board, and connecting a set of cities by","chapter-11","Graphs"
"telephone lines in such a way as to require the least amount of cable.","chapter-11","Graphs"
"The MST contains no cycles. If a proposed MST did have a cycle, a cheaper","chapter-11","Graphs"
"MST could be had by removing any one of the edges in the cycle. Thus, the MST","chapter-11","Graphs"
"is a free tree with |V| − 1 edges. The name “minimum-cost spanning tree” comes","chapter-11","Graphs"
"from the fact that the required set of edges forms a tree, it spans the vertices (i.e.,","chapter-11","Graphs"
"it connects them together), and it has minimum cost. Figure 11.20 shows the MST","chapter-11","Graphs"
"for an example graph.","chapter-11","Graphs"
"11.5.1 Prim’s Algorithm","chapter-11","Graphs"
"The first of our two algorithms for finding MSTs is commonly referred to as Prim’s","chapter-11","Graphs"
"algorithm. Prim’s algorithm is very simple. Start with any Vertex N in the graph,","chapter-11","Graphs"
"setting the MST to be N initially. Pick the least-cost edge connected to N. This","chapter-11","Graphs"
"394 Chap. 11 Graphs","chapter-11","Graphs"
"A","chapter-11","Graphs"
"9","chapter-11","Graphs"
"7 5","chapter-11","Graphs"
"B","chapter-11","Graphs"
"C","chapter-11","Graphs"
"1 2","chapter-11","Graphs"
"6","chapter-11","Graphs"
"D 2","chapter-11","Graphs"
"E 1","chapter-11","Graphs"
"F","chapter-11","Graphs"
"Figure 11.20 A graph and its MST. All edges appear in the original graph.","chapter-11","Graphs"
"Those edges drawn with heavy lines indicate the subset making up the MST. Note","chapter-11","Graphs"
"that edge (C, F) could be replaced with edge (D, F) to form a different MST with","chapter-11","Graphs"
"equal cost.","chapter-11","Graphs"
"edge connects N to another vertex; call this M. Add Vertex M and Edge (N, M) to","chapter-11","Graphs"
"the MST. Next, pick the least-cost edge coming from either N or M to any other","chapter-11","Graphs"
"vertex in the graph. Add this edge and the new vertex it reaches to the MST. This","chapter-11","Graphs"
"process continues, at each step expanding the MST by selecting the least-cost edge","chapter-11","Graphs"
"from a vertex currently in the MST to a vertex not currently in the MST.","chapter-11","Graphs"
"Prim’s algorithm is quite similar to Dijkstra’s algorithm for finding the single-","chapter-11","Graphs"
"source shortest paths. The primary difference is that we are seeking not the next","chapter-11","Graphs"
"closest vertex to the start vertex, but rather the next closest vertex to any vertex","chapter-11","Graphs"
"currently in the MST. Thus we replace the lines","chapter-11","Graphs"
"if (D[w] > (D[v] + G.weight(v, w)))","chapter-11","Graphs"
"D[w] = D[v] + G.weight(v, w);","chapter-11","Graphs"
"in Djikstra’s algorithm with the lines","chapter-11","Graphs"
"if (D[w] > G.weight(v, w))","chapter-11","Graphs"
"D[w] = G.weight(v, w);","chapter-11","Graphs"
"in Prim’s algorithm.","chapter-11","Graphs"
"Figure 11.21 shows an implementation for Prim’s algorithm that searches the","chapter-11","Graphs"
"distance matrix for the next closest vertex. For each vertex I, when I is processed","chapter-11","Graphs"
"by Prim’s algorithm, an edge going to I is added to the MST that we are building.","chapter-11","Graphs"
"Array V[I] stores the previously visited vertex that is closest to Vertex I. This","chapter-11","Graphs"
"information lets us know which edge goes into the MST when Vertex I is processed.","chapter-11","Graphs"
"The implementation of Figure 11.21 also contains calls to AddEdgetoMST to","chapter-11","Graphs"
"indicate which edges are actually added to the MST.","chapter-11","Graphs"
"Alternatively, we can implement Prim’s algorithm using a priority queue to find","chapter-11","Graphs"
"the next closest vertex, as shown in Figure 11.22. As with the priority queue version","chapter-11","Graphs"
"of Dijkstra’s algorithm, the heap’s Elem type stores a DijkElem object.","chapter-11","Graphs"
"Sec. 11.5 Minimum-Cost Spanning Trees 395","chapter-11","Graphs"
"/** Compute a minimal-cost spanning tree */","chapter-11","Graphs"
"static void Prim(Graph G, int s, int[] D, int[] V) {","chapter-11","Graphs"
"for (int i=0; i<G.n(); i++) // Initialize","chapter-11","Graphs"
"D[i] = Integer.MAX VALUE;","chapter-11","Graphs"
"D[s] = 0;","chapter-11","Graphs"
"for (int i=0; i<G.n(); i++) { // Process the vertices","chapter-11","Graphs"
"int v = minVertex(G, D);","chapter-11","Graphs"
"G.setMark(v, VISITED);","chapter-11","Graphs"
"if (v != s) AddEdgetoMST(V[v], v);","chapter-11","Graphs"
"if (D[v] == Integer.MAX VALUE) return; // Unreachable","chapter-11","Graphs"
"for (int w = G.first(v); w < G.n(); w = G.next(v, w))","chapter-11","Graphs"
"if (D[w] > G.weight(v, w)) {","chapter-11","Graphs"
"D[w] = G.weight(v, w);","chapter-11","Graphs"
"V[w] = v;","chapter-11","Graphs"
"}","chapter-11","Graphs"
"}","chapter-11","Graphs"
"}","chapter-11","Graphs"
"Figure 11.21 An implementation for Prim’s algorithm.","chapter-11","Graphs"
"/** Prims’s MST algorithm: priority queue version */","chapter-11","Graphs"
"static void Prim(Graph G, int s, int[] D, int[] V) {","chapter-11","Graphs"
"int v; // The current vertex","chapter-11","Graphs"
"DijkElem[] E = new DijkElem[G.e()]; // Heap for edges","chapter-11","Graphs"
"E[0] = new DijkElem(s, 0); // Initial vertex","chapter-11","Graphs"
"MinHeap<DijkElem> H = new MinHeap<DijkElem>(E, 1, G.e());","chapter-11","Graphs"
"for (int i=0; i<G.n(); i++) // Initialize","chapter-11","Graphs"
"D[i] = Integer.MAX VALUE; // distances","chapter-11","Graphs"
"D[s] = 0;","chapter-11","Graphs"
"for (int i=0; i<G.n(); i++) { // Now, get distances","chapter-11","Graphs"
"do { v = (H.removemin()).vertex(); } // Get position","chapter-11","Graphs"
"while (G.getMark(v) == VISITED);","chapter-11","Graphs"
"G.setMark(v, VISITED);","chapter-11","Graphs"
"if (v != s) AddEdgetoMST(V[v], v); // Add edge to MST","chapter-11","Graphs"
"if (D[v] == Integer.MAX VALUE) return; // Unreachable","chapter-11","Graphs"
"for (int w = G.first(v); w < G.n(); w = G.next(v, w))","chapter-11","Graphs"
"if (D[w] > G.weight(v, w)) { // Update D","chapter-11","Graphs"
"D[w] = G.weight(v, w);","chapter-11","Graphs"
"V[w] = v; // Where it came from","chapter-11","Graphs"
"H.insert(new DijkElem(w, D[w]));","chapter-11","Graphs"
"}","chapter-11","Graphs"
"}","chapter-11","Graphs"
"}","chapter-11","Graphs"
"Figure 11.22 An implementation of Prim’s algorithm using a priority queue.","chapter-11","Graphs"
"396 Chap. 11 Graphs","chapter-11","Graphs"
"Prim’s algorithm is an example of a greedy algorithm. At each step in the","chapter-11","Graphs"
"for loop, we select the least-cost edge that connects some marked vertex to some","chapter-11","Graphs"
"unmarked vertex. The algorithm does not otherwise check that the MST really","chapter-11","Graphs"
"should include this least-cost edge. This leads to an important question: Does","chapter-11","Graphs"
"Prim’s algorithm work correctly? Clearly it generates a spanning tree (because","chapter-11","Graphs"
"each pass through the for loop adds one edge and one unmarked vertex to the","chapter-11","Graphs"
"spanning tree until all vertices have been added), but does this tree have minimum","chapter-11","Graphs"
"cost?","chapter-11","Graphs"
"Theorem 11.1 Prim’s algorithm produces a minimum-cost spanning tree.","chapter-11","Graphs"
"Proof: We will use a proof by contradiction. Let G = (V, E) be a graph for which","chapter-11","Graphs"
"Prim’s algorithm does not generate an MST. Define an ordering on the vertices","chapter-11","Graphs"
"according to the order in which they were added by Prim’s algorithm to the MST:","chapter-11","Graphs"
"v0, v1, ..., vn−1. Let edge ei connect (vx, vi) for some x < i and i ≥ 1. Let ej be the","chapter-11","Graphs"
"lowest numbered (first) edge added by Prim’s algorithm such that the set of edges","chapter-11","Graphs"
"selected so far cannot be extended to form an MST for G. In other words, ej is the","chapter-11","Graphs"
"first edge where Prim’s algorithm “went wrong.” Let T be the “true” MST. Call vp","chapter-11","Graphs"
"(p < j) the vertex connected by edge ej , that is, ej = (vp, vj ).","chapter-11","Graphs"
"Because T is a tree, there exists some path in T connecting vp and vj . There","chapter-11","Graphs"
"must be some edge e","chapter-11","Graphs"
"0","chapter-11","Graphs"
"in this path connecting vertices vu and vw, with u < j and","chapter-11","Graphs"
"w ≥ j. Because ej is not part of T, adding edge ej to T forms a cycle. Edge e","chapter-11","Graphs"
"0 must","chapter-11","Graphs"
"be of lower cost than edge ej , because Prim’s algorithm did not generate an MST.","chapter-11","Graphs"
"This situation is illustrated in Figure 11.23. However, Prim’s algorithm would have","chapter-11","Graphs"
"selected the least-cost edge available. It would have selected e","chapter-11","Graphs"
"0","chapter-11","Graphs"
", not ej . Thus, it is a","chapter-11","Graphs"
"contradiction that Prim’s algorithm would have selected the wrong edge, and thus,","chapter-11","Graphs"
"Prim’s algorithm must be correct. ✷","chapter-11","Graphs"
"Example 11.3 For the graph of Figure 11.20, assume that we begin by","chapter-11","Graphs"
"marking Vertex A. From A, the least-cost edge leads to Vertex C. Vertex C","chapter-11","Graphs"
"and edge (A, C) are added to the MST. At this point, our candidate edges","chapter-11","Graphs"
"connecting the MST (Vertices A and C) with the rest of the graph are (A, E),","chapter-11","Graphs"
"(C, B), (C, D), and (C, F). From these choices, the least-cost edge from the","chapter-11","Graphs"
"MST is (C, D). So we add Vertex D to the MST. For the next iteration, our","chapter-11","Graphs"
"edge choices are (A, E), (C, B), (C, F), and (D, F). Because edges (C, F)","chapter-11","Graphs"
"and (D, F) happen to have equal cost, it is an arbitrary decision as to which","chapter-11","Graphs"
"gets selected. Say we pick (C, F). The next step marks Vertex E and adds","chapter-11","Graphs"
"edge (F, E) to the MST. Following in this manner, Vertex B (through edge","chapter-11","Graphs"
"(C, B)) is marked. At this point, the algorithm terminates.","chapter-11","Graphs"
"Sec. 11.5 Minimum-Cost Spanning Trees 397","chapter-11","Graphs"
"j","chapter-11","Graphs"
"i","chapter-11","Graphs"
"u","chapter-11","Graphs"
"p","chapter-11","Graphs"
"i","chapter-11","Graphs"
"u","chapter-11","Graphs"
"j","chapter-11","Graphs"
"Marked Unmarked","chapter-11","Graphs"
"’’correct’’ edge","chapter-11","Graphs"
"e’","chapter-11","Graphs"
"Prim’s edge","chapter-11","Graphs"
"v","chapter-11","Graphs"
"v v","chapter-11","Graphs"
"v","chapter-11","Graphs"
"e","chapter-11","Graphs"
"Vertices v , i < j Vertices v , i >= j","chapter-11","Graphs"
"Figure 11.23 Prim’s MST algorithm proof. The left oval contains that portion of","chapter-11","Graphs"
"the graph where Prim’s MST and the “true” MST T agree. The right oval contains","chapter-11","Graphs"
"the rest of the graph. The two portions of the graph are connected by (at least)","chapter-11","Graphs"
"edges ej (selected by Prim’s algorithm to be in the MST) and e","chapter-11","Graphs"
"0","chapter-11","Graphs"
"(the “correct”","chapter-11","Graphs"
"edge to be placed in the MST). Note that the path from vw to vj cannot include","chapter-11","Graphs"
"any marked vertex vi","chapter-11","Graphs"
", i ≤ j, because to do so would form a cycle.","chapter-11","Graphs"
"11.5.2 Kruskal’s Algorithm","chapter-11","Graphs"
"Our next MST algorithm is commonly referred to as Kruskal’s algorithm. Kruskal’s","chapter-11","Graphs"
"algorithm is also a simple, greedy algorithm. First partition the set of vertices into","chapter-11","Graphs"
"|V| equivalence classes (see Section 6.2), each consisting of one vertex. Then pro-","chapter-11","Graphs"
"cess the edges in order of weight. An edge is added to the MST, and two equiva-","chapter-11","Graphs"
"lence classes combined, if the edge connects two vertices in different equivalence","chapter-11","Graphs"
"classes. This process is repeated until only one equivalence class remains.","chapter-11","Graphs"
"Example 11.4 Figure 11.24 shows the first three steps of Kruskal’s Alg-","chapter-11","Graphs"
"orithm for the graph of Figure 11.20. Edge (C, D) has the least cost, and","chapter-11","Graphs"
"because C and D are currently in separate MSTs, they are combined. We","chapter-11","Graphs"
"next select edge (E, F) to process, and combine these vertices into a single","chapter-11","Graphs"
"MST. The third edge we process is (C, F), which causes the MST contain-","chapter-11","Graphs"
"ing Vertices C and D to merge with the MST containing Vertices E and F.","chapter-11","Graphs"
"The next edge to process is (D, F). But because Vertices D and F are cur-","chapter-11","Graphs"
"rently in the same MST, this edge is rejected. The algorithm will continue","chapter-11","Graphs"
"on to accept edges (B, C) and (A, C) into the MST.","chapter-11","Graphs"
"The edges can be processed in order of weight by using a min-heap. This is","chapter-11","Graphs"
"generally faster than sorting the edges first, because in practice we need only visit","chapter-11","Graphs"
"a small fraction of the edges before completing the MST. This is an example of","chapter-11","Graphs"
"finding only a few smallest elements in a list, as discussed in Section 7.6.","chapter-11","Graphs"
"398 Chap. 11 Graphs","chapter-11","Graphs"
"Initial","chapter-11","Graphs"
"Step 1 A B","chapter-11","Graphs"
"C","chapter-11","Graphs"
"1","chapter-11","Graphs"
"D","chapter-11","Graphs"
"E F","chapter-11","Graphs"
"Step 2","chapter-11","Graphs"
"Process edge (E, F)","chapter-11","Graphs"
"1","chapter-11","Graphs"
"1","chapter-11","Graphs"
"Step 3","chapter-11","Graphs"
"Process edge (C, F)","chapter-11","Graphs"
"B","chapter-11","Graphs"
"1 2","chapter-11","Graphs"
"E 1","chapter-11","Graphs"
"F","chapter-11","Graphs"
"Process edge (C, D)","chapter-11","Graphs"
"A","chapter-11","Graphs"
"A B D E F C","chapter-11","Graphs"
"C","chapter-11","Graphs"
"D","chapter-11","Graphs"
"B","chapter-11","Graphs"
"C","chapter-11","Graphs"
"D","chapter-11","Graphs"
"E","chapter-11","Graphs"
"A","chapter-11","Graphs"
"F","chapter-11","Graphs"
"Figure 11.24 Illustration of the first three steps of Kruskal’s MST algorithm as","chapter-11","Graphs"
"applied to the graph of Figure 11.20.","chapter-11","Graphs"
"The only tricky part to this algorithm is determining if two vertices belong to","chapter-11","Graphs"
"the same equivalence class. Fortunately, the ideal algorithm is available for the","chapter-11","Graphs"
"purpose — the UNION/FIND algorithm based on the parent pointer representation","chapter-11","Graphs"
"for trees described in Section 6.2. Figure 11.25 shows an implementation for the","chapter-11","Graphs"
"algorithm. Class KruskalElem is used to store the edges on the min-heap.","chapter-11","Graphs"
"Kruskal’s algorithm is dominated by the time required to process the edges.","chapter-11","Graphs"
"The differ and UNION functions are nearly constant in time if path compression","chapter-11","Graphs"
"and weighted union is used. Thus, the total cost of the algorithm is Θ(|E| log |E|)","chapter-11","Graphs"
"in the worst case, when nearly all edges must be processed before all the edges of","chapter-11","Graphs"
"the spanning tree are found and the algorithm can stop. More often the edges of the","chapter-11","Graphs"
"spanning tree are the shorter ones,and only about |V| edges must be processed. If","chapter-11","Graphs"
"so, the cost is often close to Θ(|V| log |E|) in the average case.","chapter-11","Graphs"
"Sec. 11.6 Further Reading 399","chapter-11","Graphs"
"/** Heap element implementation for Kruskal’s algorithm */","chapter-11","Graphs"
"class KruskalElem implements Comparable<KruskalElem> {","chapter-11","Graphs"
"private int v, w, weight;","chapter-11","Graphs"
"public KruskalElem(int inweight, int inv, int inw)","chapter-11","Graphs"
"{ weight = inweight; v = inv; w = inw; }","chapter-11","Graphs"
"public int v1() { return v; }","chapter-11","Graphs"
"public int v2() { return w; }","chapter-11","Graphs"
"public int key() { return weight; }","chapter-11","Graphs"
"public int compareTo(KruskalElem that) {","chapter-11","Graphs"
"if (weight < that.key()) return -1;","chapter-11","Graphs"
"else if (weight == that.key()) return 0;","chapter-11","Graphs"
"else return 1;","chapter-11","Graphs"
"}","chapter-11","Graphs"
"}","chapter-11","Graphs"
"/** Kruskal’s MST algorithm */","chapter-11","Graphs"
"static void Kruskal(Graph G) {","chapter-11","Graphs"
"ParPtrTree A = new ParPtrTree(G.n()); // Equivalence array","chapter-11","Graphs"
"KruskalElem[] E = new KruskalElem[G.e()]; // Minheap array","chapter-11","Graphs"
"int edgecnt = 0; // Count of edges","chapter-11","Graphs"
"for (int i=0; i<G.n(); i++) // Put edges in the array","chapter-11","Graphs"
"for (int w = G.first(i); w < G.n(); w = G.next(i, w))","chapter-11","Graphs"
"E[edgecnt++] = new KruskalElem(G.weight(i, w), i, w);","chapter-11","Graphs"
"MinHeap<KruskalElem> H =","chapter-11","Graphs"
"new MinHeap<KruskalElem>(E, edgecnt, edgecnt);","chapter-11","Graphs"
"int numMST = G.n(); // Initially n classes","chapter-11","Graphs"
"for (int i=0; numMST>1; i++) { // Combine equiv classes","chapter-11","Graphs"
"KruskalElem temp = H.removemin(); // Next cheapest","chapter-11","Graphs"
"int v = temp.v1(); int u = temp.v2();","chapter-11","Graphs"
"if (A.differ(v, u)) { // If in different classes","chapter-11","Graphs"
"A.UNION(v, u); // Combine equiv classes","chapter-11","Graphs"
"AddEdgetoMST(v, u); // Add this edge to MST","chapter-11","Graphs"
"numMST--; // One less MST","chapter-11","Graphs"
"}","chapter-11","Graphs"
"}","chapter-11","Graphs"
"}","chapter-11","Graphs"
"Figure 11.25 An implementation for Kruskal’s algorithm.","chapter-11","Graphs"
"11.6 Further Reading","chapter-11","Graphs"
"Many interesting properties of graphs can be investigated by playing with the pro-","chapter-11","Graphs"
"grams in the Stanford Graphbase. This is a collection of benchmark databases and","chapter-11","Graphs"
"graph processing programs. The Stanford Graphbase is documented in [Knu94].","chapter-11","Graphs"
"11.7 Exercises","chapter-11","Graphs"
"11.1 Prove by induction that a graph with n vertices has at most n(n−1)/2 edges.","chapter-11","Graphs"
"11.2 Prove the following implications regarding free trees.","chapter-11","Graphs"
"400 Chap. 11 Graphs","chapter-11","Graphs"
"(a) IF an undirected graph is connected and has no simple cycles, THEN","chapter-11","Graphs"
"the graph has |V| − 1 edges.","chapter-11","Graphs"
"(b) IF an undirected graph has |V| − 1 edges and no cycles, THEN the","chapter-11","Graphs"
"graph is connected.","chapter-11","Graphs"
"11.3 (a) Draw the adjacency matrix representation for the graph of Figure 11.26.","chapter-11","Graphs"
"(b) Draw the adjacency list representation for the same graph.","chapter-11","Graphs"
"(c) If a pointer requires four bytes, a vertex label requires two bytes, and","chapter-11","Graphs"
"an edge weight requires two bytes, which representation requires more","chapter-11","Graphs"
"space for this graph?","chapter-11","Graphs"
"(d) If a pointer requires four bytes, a vertex label requires one byte, and","chapter-11","Graphs"
"an edge weight requires two bytes, which representation requires more","chapter-11","Graphs"
"space for this graph?","chapter-11","Graphs"
"11.4 Show the DFS tree for the graph of Figure 11.26, starting at Vertex 1.","chapter-11","Graphs"
"11.5 Write a pseudocode algorithm to create a DFS tree for an undirected, con-","chapter-11","Graphs"
"nected graph starting at a specified vertex V.","chapter-11","Graphs"
"11.6 Show the BFS tree for the graph of Figure 11.26, starting at Vertex 1.","chapter-11","Graphs"
"11.7 Write a pseudocode algorithm to create a BFS tree for an undirected, con-","chapter-11","Graphs"
"nected graph starting at a specified vertex V.","chapter-11","Graphs"
"11.8 The BFS topological sort algorithm can report the existence of a cycle if one","chapter-11","Graphs"
"is encountered. Modify this algorithm to print the vertices possibly appearing","chapter-11","Graphs"
"in cycles (if there are any cycles).","chapter-11","Graphs"
"11.9 Explain why, in the worst case, Dijkstra’s algorithm is (asymptotically) as","chapter-11","Graphs"
"efficient as any algorithm for finding the shortest path from some vertex I to","chapter-11","Graphs"
"another vertex J.","chapter-11","Graphs"
"11.10 Show the shortest paths generated by running Dijkstra’s shortest-paths alg-","chapter-11","Graphs"
"orithm on the graph of Figure 11.26, beginning at Vertex 4. Show the D","chapter-11","Graphs"
"values as each vertex is processed, as in Figure 11.19.","chapter-11","Graphs"
"11.11 Modify the algorithm for single-source shortest paths to actually store and","chapter-11","Graphs"
"return the shortest paths rather than just compute the distances.","chapter-11","Graphs"
"11.12 The root of a DAG is a vertex R such that every vertex of the DAG can be","chapter-11","Graphs"
"reached by a directed path from R. Write an algorithm that takes a directed","chapter-11","Graphs"
"graph as input and determines the root (if there is one) for the graph. The","chapter-11","Graphs"
"running time of your algorithm should be Θ(|V| + |E|).","chapter-11","Graphs"
"11.13 Write an algorithm to find the longest path in a DAG, where the length of","chapter-11","Graphs"
"the path is measured by the number of edges that it contains. What is the","chapter-11","Graphs"
"asymptotic complexity of your algorithm?","chapter-11","Graphs"
"11.14 Write an algorithm to determine whether a directed graph of |V| vertices","chapter-11","Graphs"
"contains a cycle. Your algorithm should run in Θ(|V| + |E|) time.","chapter-11","Graphs"
"11.15 Write an algorithm to determine whether an undirected graph of |V| vertices","chapter-11","Graphs"
"contains a cycle. Your algorithm should run in Θ(|V|) time.","chapter-11","Graphs"
"Sec. 11.7 Exercises 401","chapter-11","Graphs"
"2 5","chapter-11","Graphs"
"4 20","chapter-11","Graphs"
"10 3","chapter-11","Graphs"
"6","chapter-11","Graphs"
"11","chapter-11","Graphs"
"3 3","chapter-11","Graphs"
"15 5","chapter-11","Graphs"
"10","chapter-11","Graphs"
"2","chapter-11","Graphs"
"1","chapter-11","Graphs"
"Figure 11.26 Example graph for Chapter 11 exercises.","chapter-11","Graphs"
"11.16 The single-destination shortest-paths problem for a directed graph is to find","chapter-11","Graphs"
"the shortest path from every vertex to a specified vertex V. Write an algorithm","chapter-11","Graphs"
"to solve the single-destination shortest-paths problem.","chapter-11","Graphs"
"11.17 List the order in which the edges of the graph in Figure 11.26 are visited","chapter-11","Graphs"
"when running Prim’s MST algorithm starting at Vertex 3. Show the final","chapter-11","Graphs"
"MST.","chapter-11","Graphs"
"11.18 List the order in which the edges of the graph in Figure 11.26 are visited","chapter-11","Graphs"
"when running Kruskal’s MST algorithm. Each time an edge is added to the","chapter-11","Graphs"
"MST, show the result on the equivalence array, (e.g., show the array as in","chapter-11","Graphs"
"Figure 6.7).","chapter-11","Graphs"
"11.19 Write an algorithm to find a maximum cost spanning tree, that is, the span-","chapter-11","Graphs"
"ning tree with highest possible cost.","chapter-11","Graphs"
"11.20 When can Prim’s and Kruskal’s algorithms yield different MSTs?","chapter-11","Graphs"
"11.21 Prove that, if the costs for the edges of Graph G are distinct, then only one","chapter-11","Graphs"
"MST exists for G.","chapter-11","Graphs"
"11.22 Does either Prim’s or Kruskal’s algorithm work if there are negative edge","chapter-11","Graphs"
"weights?","chapter-11","Graphs"
"11.23 Consider the collection of edges selected by Dijkstra’s algorithm as the short-","chapter-11","Graphs"
"est paths to the graph’s vertices from the start vertex. Do these edges form","chapter-11","Graphs"
"a spanning tree (not necessarily of minimum cost)? Do these edges form an","chapter-11","Graphs"
"MST? Explain why or why not.","chapter-11","Graphs"
"11.24 Prove that a tree is a bipartite graph.","chapter-11","Graphs"
"11.25 Prove that any tree (i.e., a connected, undirected graph with no cycles) can","chapter-11","Graphs"
"be two-colored. (A graph can be two colored if every vertex can be assigned","chapter-11","Graphs"
"one of two colors such that no adjacent vertices have the same color.)","chapter-11","Graphs"
"11.26 Write an algorithm that determines if an arbitrary undirected graph is a bipar-","chapter-11","Graphs"
"tite graph. If the graph is bipartite, then your algorithm should also identify","chapter-11","Graphs"
"the vertices as to which of the two partitions each belongs to.","chapter-11","Graphs"
"402 Chap. 11 Graphs","chapter-11","Graphs"
"11.8 Projects","chapter-11","Graphs"
"11.1 Design a format for storing graphs in files. Then implement two functions:","chapter-11","Graphs"
"one to read a graph from a file and the other to write a graph to a file. Test","chapter-11","Graphs"
"your functions by implementing a complete MST program that reads an undi-","chapter-11","Graphs"
"rected graph in from a file, constructs the MST, and then writes to a second","chapter-11","Graphs"
"file the graph representing the MST.","chapter-11","Graphs"
"11.2 An undirected graph need not explicitly store two separate directed edges to","chapter-11","Graphs"
"represent a single undirected edge. An alternative would be to store only a","chapter-11","Graphs"
"single undirected edge (I, J) to connect Vertices I and J. However, what if the","chapter-11","Graphs"
"user asks for edge (J, I)? We can solve this problem by consistently storing","chapter-11","Graphs"
"the edge such that the lesser of I and J always comes first. Thus, if we have","chapter-11","Graphs"
"an edge connecting Vertices 5 and 3, requests for edge (5, 3) and (3, 5) both","chapter-11","Graphs"
"map to (3, 5) because 3 < 5.","chapter-11","Graphs"
"Looking at the adjacency matrix, we notice that only the lower triangle of","chapter-11","Graphs"
"the array is used. Thus we could cut the space required by the adjacency","chapter-11","Graphs"
"matrix from |V|","chapter-11","Graphs"
"2 positions to |V|(|V|−1)/2 positions. Read Section 12.2 on","chapter-11","Graphs"
"triangular matrices. The re-implement the adjacency matrix representation","chapter-11","Graphs"
"of Figure 11.6 to implement undirected graphs using a triangular array.","chapter-11","Graphs"
"11.3 While the underlying implementation (whether adjacency matrix or adja-","chapter-11","Graphs"
"cency list) is hidden behind the graph ADT, these two implementations can","chapter-11","Graphs"
"have an impact on the efficiency of the resulting program. For Dijkstra’s","chapter-11","Graphs"
"shortest paths algorithm, two different implementations were given in Sec-","chapter-11","Graphs"
"tion 11.4.1 that provide different ways for determining the next closest vertex","chapter-11","Graphs"
"at each iteration of the algorithm. The relative costs of these two variants","chapter-11","Graphs"
"depend on who sparse or dense the graph is. They might also depend on","chapter-11","Graphs"
"whether the graph is implemented using an adjacency list or adjacency ma-","chapter-11","Graphs"
"trix.","chapter-11","Graphs"
"Design and implement a study to compare the effects on performance for","chapter-11","Graphs"
"three variables: (i) the two graph representations (adjacency list and adja-","chapter-11","Graphs"
"cency matrix); (ii) the two implementations for Djikstra’s shortest paths alg-","chapter-11","Graphs"
"orithm (searching the table of vertex distances or using a priority queue to","chapter-11","Graphs"
"track the distances), and (iii) sparse versus dense graphs. Be sure to test your","chapter-11","Graphs"
"implementations on a variety of graphs that are sufficiently large to generate","chapter-11","Graphs"
"meaningful times.","chapter-11","Graphs"
"11.4 The example implementations for DFS and BFS show calls to functions","chapter-11","Graphs"
"PreVisit and PostVisit. Re-implement the BFS and DFS functions","chapter-11","Graphs"
"to make use of the visitor design pattern to handle the pre/post visit function-","chapter-11","Graphs"
"ality.","chapter-11","Graphs"
"11.5 Write a program to label the connected components for an undirected graph.","chapter-11","Graphs"
"In other words, all vertices of the first component are given the first com-","chapter-11","Graphs"
"ponent’s label, all vertices of the second component are given the second","chapter-11","Graphs"
"Sec. 11.8 Projects 403","chapter-11","Graphs"
"component’s label, and so on. Your algorithm should work by defining any","chapter-11","Graphs"
"two vertices connected by an edge to be members of the same equivalence","chapter-11","Graphs"
"class. Once all of the edges have been processed, all vertices in a given equiv-","chapter-11","Graphs"
"alence class will be connected. Use the UNION/FIND implementation from","chapter-11","Graphs"
"Section 6.2 to implement equivalence classes.","chapter-11","Graphs"
"Simple lists and arrays are the right tools for the many applications. Other situa-","chapter-12","Lists and Arrays Revisited"
"tions require support for operations that cannot be implemented efficiently by the","chapter-12","Lists and Arrays Revisited"
"standard list representations of Chapter 4. This chapter presents a range of topics,","chapter-12","Lists and Arrays Revisited"
"whose unifying thread is that the data structures included are all list- or array-like.","chapter-12","Lists and Arrays Revisited"
"These structures overcome some of the problems of simple linked list and con-","chapter-12","Lists and Arrays Revisited"
"tiguous array representations. This chapter also seeks to reinforce the concept of","chapter-12","Lists and Arrays Revisited"
"logical representation versus physical implementation, as some of the “list” imple-","chapter-12","Lists and Arrays Revisited"
"mentations have quite different organizations internally.","chapter-12","Lists and Arrays Revisited"
"Section 12.1 describes a series of representations for multilists, which are lists","chapter-12","Lists and Arrays Revisited"
"that may contain sublists. Section 12.2 discusses representations for implementing","chapter-12","Lists and Arrays Revisited"
"sparse matrices, large matrices where most of the elements have zero values. Sec-","chapter-12","Lists and Arrays Revisited"
"tion 12.3 discusses memory management techniques, which are essentially a way","chapter-12","Lists and Arrays Revisited"
"of allocating variable-length sections from a large array.","chapter-12","Lists and Arrays Revisited"
"12.1 Multilists","chapter-12","Lists and Arrays Revisited"
"Recall from Chapter 4 that a list is a finite, ordered sequence of items of the form","chapter-12","Lists and Arrays Revisited"
"hx0, x1, ..., xn−1i where n ≥ 0. We can represent the empty list by null or hi.","chapter-12","Lists and Arrays Revisited"
"In Chapter 4 we assumed that all list elements had the same data type. In this","chapter-12","Lists and Arrays Revisited"
"section, we extend the definition of lists to allow elements to be arbitrary in nature.","chapter-12","Lists and Arrays Revisited"
"In general, list elements are one of two types.","chapter-12","Lists and Arrays Revisited"
"1. An atom, which is a data record of some type such as a number, symbol, or","chapter-12","Lists and Arrays Revisited"
"string.","chapter-12","Lists and Arrays Revisited"
"2. Another list, which is called a sublist.","chapter-12","Lists and Arrays Revisited"
"A list containing sublists will be written as","chapter-12","Lists and Arrays Revisited"
"hx1,hy1,ha1, a2i, y3i,hz1,z2i, x4i.","chapter-12","Lists and Arrays Revisited"
"405","chapter-12","Lists and Arrays Revisited"
"406 Chap. 12 Lists and Arrays Revisited","chapter-12","Lists and Arrays Revisited"
"x1","chapter-12","Lists and Arrays Revisited"
"y1","chapter-12","Lists and Arrays Revisited"
"a1 a2","chapter-12","Lists and Arrays Revisited"
"y3 z1 z2","chapter-12","Lists and Arrays Revisited"
"x4","chapter-12","Lists and Arrays Revisited"
"Figure 12.1 Example of a multilist represented by a tree.","chapter-12","Lists and Arrays Revisited"
"L2","chapter-12","Lists and Arrays Revisited"
"L1","chapter-12","Lists and Arrays Revisited"
"a b","chapter-12","Lists and Arrays Revisited"
"c d e","chapter-12","Lists and Arrays Revisited"
"L3","chapter-12","Lists and Arrays Revisited"
"Figure 12.2 Example of a reentrant multilist. The shape of the structure is a","chapter-12","Lists and Arrays Revisited"
"DAG (all edges point downward).","chapter-12","Lists and Arrays Revisited"
"In this example, the list has four elements. The second element is the sublist","chapter-12","Lists and Arrays Revisited"
"hy1,ha1, a2i, y3i and the third is the sublist hz1,z2i. The sublist hy1,ha1, a2i, y3i","chapter-12","Lists and Arrays Revisited"
"itself contains a sublist. If a list L has one or more sublists, we call L a multi-","chapter-12","Lists and Arrays Revisited"
"list. Lists with no sublists are often referred to as linear lists or chains. Note that","chapter-12","Lists and Arrays Revisited"
"this definition for multilist fits well with our definition of sets from Definition 2.1,","chapter-12","Lists and Arrays Revisited"
"where a set’s members can be either primitive elements or sets.","chapter-12","Lists and Arrays Revisited"
"We can restrict the sublists of a multilist in various ways, depending on whether","chapter-12","Lists and Arrays Revisited"
"the multilist should have the form of a tree, a DAG, or a generic graph. A pure list","chapter-12","Lists and Arrays Revisited"
"is a list structure whose graph corresponds to a tree, such as in Figure 12.1. In other","chapter-12","Lists and Arrays Revisited"
"words, there is exactly one path from the root to any node, which is equivalent to","chapter-12","Lists and Arrays Revisited"
"saying that no object may appear more than once in the list. In the pure list, each","chapter-12","Lists and Arrays Revisited"
"pair of angle brackets corresponds to an internal node of the tree. The members of","chapter-12","Lists and Arrays Revisited"
"the list correspond to the children for the node. Atoms on the list correspond to leaf","chapter-12","Lists and Arrays Revisited"
"nodes.","chapter-12","Lists and Arrays Revisited"
"A reentrant list is a list structure whose graph corresponds to a DAG. Nodes","chapter-12","Lists and Arrays Revisited"
"might be accessible from the root by more than one path, which is equivalent to","chapter-12","Lists and Arrays Revisited"
"saying that objects (including sublists) may appear multiple times in the list as long","chapter-12","Lists and Arrays Revisited"
"as no cycles are formed. All edges point downward, from the node representing a","chapter-12","Lists and Arrays Revisited"
"list or sublist to its elements. Figure 12.2 illustrates a reentrant list. To write out","chapter-12","Lists and Arrays Revisited"
"this list in bracket notation, we can duplicate nodes as necessary. Thus, the bracket","chapter-12","Lists and Arrays Revisited"
"notation for the list of Figure 12.2 could be written","chapter-12","Lists and Arrays Revisited"
"hhha, bii,hha, bi, ci,hc, d, ei,heii.","chapter-12","Lists and Arrays Revisited"
"For convenience, we will adopt a convention of allowing sublists and atoms to be","chapter-12","Lists and Arrays Revisited"
"labeled, such as “L1:”. Whenever a label is repeated, the element corresponding to","chapter-12","Lists and Arrays Revisited"
"Sec. 12.1 Multilists 407","chapter-12","Lists and Arrays Revisited"
"L1","chapter-12","Lists and Arrays Revisited"
"L2","chapter-12","Lists and Arrays Revisited"
"L4","chapter-12","Lists and Arrays Revisited"
"b d","chapter-12","Lists and Arrays Revisited"
"a","chapter-12","Lists and Arrays Revisited"
"c","chapter-12","Lists and Arrays Revisited"
"L3","chapter-12","Lists and Arrays Revisited"
"Figure 12.3 Example of a cyclic list. The shape of the structure is a directed","chapter-12","Lists and Arrays Revisited"
"graph.","chapter-12","Lists and Arrays Revisited"
"that label will be substituted when we write out the list. Thus, the bracket notation","chapter-12","Lists and Arrays Revisited"
"for the list of Figure 12.2 could be written","chapter-12","Lists and Arrays Revisited"
"hhL1:ha, bii,hL1, L2: ci,hL2, d, L3: ei,hL3ii.","chapter-12","Lists and Arrays Revisited"
"A cyclic list is a list structure whose graph corresponds to any directed graph,","chapter-12","Lists and Arrays Revisited"
"possibly containing cycles. Figure 12.3 illustrates such a list. Labels are required to","chapter-12","Lists and Arrays Revisited"
"write this in bracket notation. Here is the bracket notation for the list of Figure 12.3.","chapter-12","Lists and Arrays Revisited"
"hL1:hL2:ha, L1ii,hL2, L3:bi,hL3, c, di, L4:hL4ii.","chapter-12","Lists and Arrays Revisited"
"Multilists can be implemented in a number of ways. Most of these should be","chapter-12","Lists and Arrays Revisited"
"familiar from implementations suggested earlier in the book for list, tree, and graph","chapter-12","Lists and Arrays Revisited"
"data structures.","chapter-12","Lists and Arrays Revisited"
"One simple approach is to use a simple array to represent the list. This works","chapter-12","Lists and Arrays Revisited"
"well for chains with fixed-length elements, equivalent to the simple array-based list","chapter-12","Lists and Arrays Revisited"
"of Chapter 4. We can view nested sublists as variable-length elements. To use this","chapter-12","Lists and Arrays Revisited"
"approach, we require some indication of the beginning and end of each sublist. In","chapter-12","Lists and Arrays Revisited"
"essence, we are using a sequential tree implementation as discussed in Section 6.5.","chapter-12","Lists and Arrays Revisited"
"This should be no surprise, because the pure list is equivalent to a general tree","chapter-12","Lists and Arrays Revisited"
"structure. Unfortunately, as with any sequential representation, access to the nth","chapter-12","Lists and Arrays Revisited"
"sublist must be done sequentially from the beginning of the list.","chapter-12","Lists and Arrays Revisited"
"Because pure lists are equivalent to trees, we can also use linked allocation","chapter-12","Lists and Arrays Revisited"
"methods to support direct access to the list of children. Simple linear lists are","chapter-12","Lists and Arrays Revisited"
"represented by linked lists. Pure lists can be represented as linked lists with an","chapter-12","Lists and Arrays Revisited"
"additional tag field to indicate whether the node is an atom or a sublist. If it is a","chapter-12","Lists and Arrays Revisited"
"sublist, the data field points to the first element on the sublist. This is illustrated by","chapter-12","Lists and Arrays Revisited"
"Figure 12.4.","chapter-12","Lists and Arrays Revisited"
"Another approach is to represent all list elements with link nodes storing two","chapter-12","Lists and Arrays Revisited"
"pointer fields, except for atoms. Atoms just contain data. This is the system used by","chapter-12","Lists and Arrays Revisited"
"the programming language LISP. Figure 12.5 illustrates this representation. Either","chapter-12","Lists and Arrays Revisited"
"the pointer contains a tag bit to identify what it points to, or the object being pointed","chapter-12","Lists and Arrays Revisited"
"to stores a tag bit to identify itself. Tags distinguish atoms from list nodes. This","chapter-12","Lists and Arrays Revisited"
"408 Chap. 12 Lists and Arrays Revisited","chapter-12","Lists and Arrays Revisited"
"root","chapter-12","Lists and Arrays Revisited"
"y1 − + y3","chapter-12","Lists and Arrays Revisited"
"+ a2","chapter-12","Lists and Arrays Revisited"
"+ z1","chapter-12","Lists and Arrays Revisited"
"x4","chapter-12","Lists and Arrays Revisited"
"z2","chapter-12","Lists and Arrays Revisited"
"+","chapter-12","Lists and Arrays Revisited"
"a1","chapter-12","Lists and Arrays Revisited"
"x1 −","chapter-12","Lists and Arrays Revisited"
"+","chapter-12","Lists and Arrays Revisited"
"+","chapter-12","Lists and Arrays Revisited"
"+","chapter-12","Lists and Arrays Revisited"
"+","chapter-12","Lists and Arrays Revisited"
"−","chapter-12","Lists and Arrays Revisited"
"Figure 12.4 Linked representation for the pure list of Figure 12.1. The first field","chapter-12","Lists and Arrays Revisited"
"in each link node stores a tag bit. If the tag bit stores “+,” then the data field stores","chapter-12","Lists and Arrays Revisited"
"an atom. If the tag bit stores “−,” then the data field stores a pointer to a sublist.","chapter-12","Lists and Arrays Revisited"
"root","chapter-12","Lists and Arrays Revisited"
"B C D","chapter-12","Lists and Arrays Revisited"
"A","chapter-12","Lists and Arrays Revisited"
"Figure 12.5 LISP-like linked representation for the cyclic multilist of Fig-","chapter-12","Lists and Arrays Revisited"
"ure 12.3. Each link node stores two pointers. A pointer either points to an atom,","chapter-12","Lists and Arrays Revisited"
"or to another link node. Link nodes are represented by two boxes, and atoms by","chapter-12","Lists and Arrays Revisited"
"circles.","chapter-12","Lists and Arrays Revisited"
"implementation can easily support reentrant and cyclic lists, because non-atoms","chapter-12","Lists and Arrays Revisited"
"can point to any other node.","chapter-12","Lists and Arrays Revisited"
"12.2 Matrix Representations","chapter-12","Lists and Arrays Revisited"
"Sometimes we need to represent a large, two-dimensional matrix where many of","chapter-12","Lists and Arrays Revisited"
"the elements have a value of zero. One example is the lower triangular matrix that","chapter-12","Lists and Arrays Revisited"
"results from solving systems of simultaneous equations. A lower triangular matrix","chapter-12","Lists and Arrays Revisited"
"stores zero values at all positions [r, c] such that r < c, as shown in Figure 12.6(a).","chapter-12","Lists and Arrays Revisited"
"Thus, the upper-right triangle of the matrix is always zero. Another example is","chapter-12","Lists and Arrays Revisited"
"representing undirected graphs in an adjacency matrix (see Project 11.2). Because","chapter-12","Lists and Arrays Revisited"
"all edges between Vertices i and j go in both directions, there is no need to store","chapter-12","Lists and Arrays Revisited"
"both. Instead we can just store one edge going from the higher-indexed vertex to","chapter-12","Lists and Arrays Revisited"
"Sec. 12.2 Matrix Representations 409","chapter-12","Lists and Arrays Revisited"
"a00 0 0 0","chapter-12","Lists and Arrays Revisited"
"a10 a11 0 0","chapter-12","Lists and Arrays Revisited"
"a20 a21 a22 0","chapter-12","Lists and Arrays Revisited"
"a30 a31 a32 a33","chapter-12","Lists and Arrays Revisited"
"(a)","chapter-12","Lists and Arrays Revisited"
"a00 a01 a02 a03","chapter-12","Lists and Arrays Revisited"
"0 a11 a12 a13","chapter-12","Lists and Arrays Revisited"
"0 0 a22 a23","chapter-12","Lists and Arrays Revisited"
"0 0 0 a33","chapter-12","Lists and Arrays Revisited"
"(b)","chapter-12","Lists and Arrays Revisited"
"Figure 12.6 Triangular matrices. (a) A lower triangular matrix. (b) An upper","chapter-12","Lists and Arrays Revisited"
"triangular matrix.","chapter-12","Lists and Arrays Revisited"
"the lower-indexed vertex. In this case, only the lower triangle of the matrix can","chapter-12","Lists and Arrays Revisited"
"have non-zero values.","chapter-12","Lists and Arrays Revisited"
"We can take advantage of this fact to save space. Instead of storing n(n + 1)/2","chapter-12","Lists and Arrays Revisited"
"pieces of information in an n × n array, it would save space to use a list of length","chapter-12","Lists and Arrays Revisited"
"n(n + 1)/2. This is only practical if some means can be found to locate within the","chapter-12","Lists and Arrays Revisited"
"list the element that would correspond to position [r, c] in the original matrix.","chapter-12","Lists and Arrays Revisited"
"We will derive an equation to convert position [r, c] to a position in a one-","chapter-12","Lists and Arrays Revisited"
"dimensional list to store the lower triangular matrix. Note that row 0 of the matrix","chapter-12","Lists and Arrays Revisited"
"has one non-zero value, row 1 has two non-zero values, and so on. Thus, row r","chapter-12","Lists and Arrays Revisited"
"is preceded by r rows with a total of Pr","chapter-12","Lists and Arrays Revisited"
"k=1 k = (r","chapter-12","Lists and Arrays Revisited"
"2 + r)/2 non-zero elements.","chapter-12","Lists and Arrays Revisited"
"Adding c to reach the cth position in the rth row yields the following equation to","chapter-12","Lists and Arrays Revisited"
"convert position [r, c] in the original matrix to the correct position in the list.","chapter-12","Lists and Arrays Revisited"
"matrix[r, c] = list[(r","chapter-12","Lists and Arrays Revisited"
"2 + r)/2 + c].","chapter-12","Lists and Arrays Revisited"
"A similar equation can be used to convert coordinates in an upper triangular matrix,","chapter-12","Lists and Arrays Revisited"
"that is, a matrix with zero values at positions [r, c] such that r > c, as shown in","chapter-12","Lists and Arrays Revisited"
"Figure 12.6(b). For an n × n upper triangular matrix, the equation to convert from","chapter-12","Lists and Arrays Revisited"
"matrix coordinates to list positions would be","chapter-12","Lists and Arrays Revisited"
"matrix[r, c] = list[rn − (r","chapter-12","Lists and Arrays Revisited"
"2 + r)/2 + c].","chapter-12","Lists and Arrays Revisited"
"A more difficult situation arises when the vast majority of values stored in an","chapter-12","Lists and Arrays Revisited"
"n × m matrix are zero, but there is no restriction on which positions are zero and","chapter-12","Lists and Arrays Revisited"
"which are non-zero. This is known as a sparse matrix.","chapter-12","Lists and Arrays Revisited"
"One approach to representing a sparse matrix is to concatenate (or otherwise","chapter-12","Lists and Arrays Revisited"
"combine) the row and column coordinates into a single value and use this as a key","chapter-12","Lists and Arrays Revisited"
"in a hash table. Thus, if we want to know the value of a particular position in the","chapter-12","Lists and Arrays Revisited"
"matrix, we search the hash table for the appropriate key. If a value for this position","chapter-12","Lists and Arrays Revisited"
"is not found, it is assumed to be zero. This is an ideal approach when all queries to","chapter-12","Lists and Arrays Revisited"
"the matrix are in terms of access by specified position. However, if we wish to find","chapter-12","Lists and Arrays Revisited"
"the first non-zero element in a given row, or the next non-zero element below the","chapter-12","Lists and Arrays Revisited"
"current one in a given column, then the hash table requires us to check sequentially","chapter-12","Lists and Arrays Revisited"
"through all possible positions in some row or column.","chapter-12","Lists and Arrays Revisited"
"410 Chap. 12 Lists and Arrays Revisited","chapter-12","Lists and Arrays Revisited"
"Another approach is to implement the matrix as an orthogonal list. Consider","chapter-12","Lists and Arrays Revisited"
"the following sparse matrix:","chapter-12","Lists and Arrays Revisited"
"10 23 0 0 0 0 19","chapter-12","Lists and Arrays Revisited"
"45 5 0 93 0 0 0","chapter-12","Lists and Arrays Revisited"
"0 0 0 0 0 0 0","chapter-12","Lists and Arrays Revisited"
"0 0 0 0 0 0 0","chapter-12","Lists and Arrays Revisited"
"40 0 0 0 0 0 0","chapter-12","Lists and Arrays Revisited"
"0 0 0 0 0 0 0","chapter-12","Lists and Arrays Revisited"
"0 0 0 0 0 0 0","chapter-12","Lists and Arrays Revisited"
"0 32 0 12 0 0 7","chapter-12","Lists and Arrays Revisited"
"The corresponding orthogonal array is shown in Figure 12.7. Here we have a","chapter-12","Lists and Arrays Revisited"
"list of row headers, each of which contains a pointer to a list of matrix records.","chapter-12","Lists and Arrays Revisited"
"A second list of column headers also contains pointers to matrix records. Each","chapter-12","Lists and Arrays Revisited"
"non-zero matrix element stores pointers to its non-zero neighbors in the row, both","chapter-12","Lists and Arrays Revisited"
"following and preceding it. Each non-zero element also stores pointers to its non-","chapter-12","Lists and Arrays Revisited"
"zero neighbors following and preceding it in the column. Thus, each non-zero","chapter-12","Lists and Arrays Revisited"
"element stores its own value, its position within the matrix, and four pointers. Non-","chapter-12","Lists and Arrays Revisited"
"zero elements are found by traversing a row or column list. Note that the first","chapter-12","Lists and Arrays Revisited"
"non-zero element in a given row could be in any column; likewise, the neighboring","chapter-12","Lists and Arrays Revisited"
"non-zero element in any row or column list could be at any (higher) row or column","chapter-12","Lists and Arrays Revisited"
"in the array. Thus, each non-zero element must also store its row and column","chapter-12","Lists and Arrays Revisited"
"position explicitly.","chapter-12","Lists and Arrays Revisited"
"To find if a particular position in the matrix contains a non-zero element, we","chapter-12","Lists and Arrays Revisited"
"traverse the appropriate row or column list. For example, when looking for the","chapter-12","Lists and Arrays Revisited"
"element at Row 7 and Column 1, we can traverse the list either for Row 7 or for","chapter-12","Lists and Arrays Revisited"
"Column 1. When traversing a row or column list, if we come to an element with","chapter-12","Lists and Arrays Revisited"
"the correct position, then its value is non-zero. If we encounter an element with","chapter-12","Lists and Arrays Revisited"
"a higher position, then the element we are looking for is not in the sparse matrix.","chapter-12","Lists and Arrays Revisited"
"In this case, the element’s value is zero. For example, when traversing the list","chapter-12","Lists and Arrays Revisited"
"for Row 7 in the matrix of Figure 12.7, we first reach the element at Row 7 and","chapter-12","Lists and Arrays Revisited"
"Column 1. If this is what we are looking for, then the search can stop. If we are","chapter-12","Lists and Arrays Revisited"
"looking for the element at Row 7 and Column 2, then the search proceeds along the","chapter-12","Lists and Arrays Revisited"
"Row 7 list to next reach the element at Column 3. At this point we know that no","chapter-12","Lists and Arrays Revisited"
"element at Row 7 and Column 2 is stored in the sparse matrix.","chapter-12","Lists and Arrays Revisited"
"Insertion and deletion can be performed by working in a similar way to insert","chapter-12","Lists and Arrays Revisited"
"or delete elements within the appropriate row and column lists.","chapter-12","Lists and Arrays Revisited"
"Each non-zero element stored in the sparse matrix representation takes much","chapter-12","Lists and Arrays Revisited"
"more space than an element stored in a simple n × n matrix. When is the sparse","chapter-12","Lists and Arrays Revisited"
"matrix more space efficient than the standard representation? To calculate this, we","chapter-12","Lists and Arrays Revisited"
"need to determine how much space the standard matrix requires, and how much","chapter-12","Lists and Arrays Revisited"
"Sec. 12.2 Matrix Representations 411","chapter-12","Lists and Arrays Revisited"
"0,0 0,1 0,6","chapter-12","Lists and Arrays Revisited"
"0,3 1,1 1,3","chapter-12","Lists and Arrays Revisited"
"0,4","chapter-12","Lists and Arrays Revisited"
"7,1 7,3 7,6","chapter-12","Lists and Arrays Revisited"
"0","chapter-12","Lists and Arrays Revisited"
"1","chapter-12","Lists and Arrays Revisited"
"4","chapter-12","Lists and Arrays Revisited"
"7","chapter-12","Lists and Arrays Revisited"
"Cols 0 1 3 6","chapter-12","Lists and Arrays Revisited"
"Rows","chapter-12","Lists and Arrays Revisited"
"10 23 19","chapter-12","Lists and Arrays Revisited"
"45 5 93","chapter-12","Lists and Arrays Revisited"
"40","chapter-12","Lists and Arrays Revisited"
"32 12 7","chapter-12","Lists and Arrays Revisited"
"Figure 12.7 The orthogonal list sparse matrix representation.","chapter-12","Lists and Arrays Revisited"
"the sparse matrix requires. The size of the sparse matrix depends on the number","chapter-12","Lists and Arrays Revisited"
"of non-zero elements (we will refer to this value as NNZ), while the size of the","chapter-12","Lists and Arrays Revisited"
"standard matrix representation does not vary. We need to know the (relative) sizes","chapter-12","Lists and Arrays Revisited"
"of a pointer and a data value. For simplicity, our calculation will ignore the space","chapter-12","Lists and Arrays Revisited"
"taken up by the row and column header (which is not much affected by the number","chapter-12","Lists and Arrays Revisited"
"of elements in the sparse array).","chapter-12","Lists and Arrays Revisited"
"As an example, assume that a data value, a row or column index, and a pointer","chapter-12","Lists and Arrays Revisited"
"each require four bytes. An n × m matrix requires 4nm bytes. The sparse matrix","chapter-12","Lists and Arrays Revisited"
"requires 28 bytes per non-zero element (four pointers, two array indices, and one","chapter-12","Lists and Arrays Revisited"
"data value). If we set X to be the percentage of non-zero elements, we can solve","chapter-12","Lists and Arrays Revisited"
"for the value of X below which the sparse matrix representation is more space","chapter-12","Lists and Arrays Revisited"
"efficient. Using the equation","chapter-12","Lists and Arrays Revisited"
"28X = 4mn","chapter-12","Lists and Arrays Revisited"
"and solving for X, we find that the sparse matrix using this implementation is more","chapter-12","Lists and Arrays Revisited"
"space efficient when X < 1/7, that is, when less than about 14% of the elements","chapter-12","Lists and Arrays Revisited"
"412 Chap. 12 Lists and Arrays Revisited","chapter-12","Lists and Arrays Revisited"
"are non-zero. Different values for the relative sizes of data values, pointers, or","chapter-12","Lists and Arrays Revisited"
"matrix indices can lead to a different break-even point for the two implementations.","chapter-12","Lists and Arrays Revisited"
"The time required to process a sparse matrix should ideally depend on NNZ.","chapter-12","Lists and Arrays Revisited"
"When searching for an element, the cost is the number of elements preceding the","chapter-12","Lists and Arrays Revisited"
"desired element on its row or column list. The cost for operations such as adding","chapter-12","Lists and Arrays Revisited"
"two matrices should be Θ(n + m) in the worst case when the one matrix stores n","chapter-12","Lists and Arrays Revisited"
"non-zero elements and the other stores m non-zero elements.","chapter-12","Lists and Arrays Revisited"
"Another representation for sparse matrices is sometimes called the Yale rep-","chapter-12","Lists and Arrays Revisited"
"resentation. Matlab uses a similar representation, with a primary difference being","chapter-12","Lists and Arrays Revisited"
"that the Matlab representation uses column-major order.1 The Matlab representa-","chapter-12","Lists and Arrays Revisited"
"tion stores the sparse matrix using three lists. The first is simply all of the non-zero","chapter-12","Lists and Arrays Revisited"
"element values, in column-major order. The second list stores the start position","chapter-12","Lists and Arrays Revisited"
"within the first list for each column. The third list stores the row positions for each","chapter-12","Lists and Arrays Revisited"
"of the corresponding non-zero values. In the Yale representation, the matrix of","chapter-12","Lists and Arrays Revisited"
"Figure 12.7 would appear as:","chapter-12","Lists and Arrays Revisited"
"Values: 10 45 40 23 5 32 93 12 19 7","chapter-12","Lists and Arrays Revisited"
"Column starts: 0 3 5 5 7 7 7 7","chapter-12","Lists and Arrays Revisited"
"Row positions: 0 1 4 0 1 7 1 7 0 7","chapter-12","Lists and Arrays Revisited"
"If the matrix has c columns, then the total space required will be proportional to","chapter-12","Lists and Arrays Revisited"
"c + 2NNZ. This is good in terms of space. It allows fairly quick access to any","chapter-12","Lists and Arrays Revisited"
"column, and allows for easy processing of the non-zero values along a column.","chapter-12","Lists and Arrays Revisited"
"However, it does not do a good job of providing access to the values along a row,","chapter-12","Lists and Arrays Revisited"
"and is terrible when values need to be added or removed from the representation.","chapter-12","Lists and Arrays Revisited"
"Fortunately, when doing computations such as adding or multiplying two sparse","chapter-12","Lists and Arrays Revisited"
"matrices, the processing of the input matrices and construction of the output matrix","chapter-12","Lists and Arrays Revisited"
"can be done reasonably efficiently.","chapter-12","Lists and Arrays Revisited"
"12.3 Memory Management","chapter-12","Lists and Arrays Revisited"
"Most data structures are designed to store and access objects of uniform size. A","chapter-12","Lists and Arrays Revisited"
"typical example would be an integer stored in a list or a queue. Some applications","chapter-12","Lists and Arrays Revisited"
"require the ability to store variable-length records, such as a string of arbitrary","chapter-12","Lists and Arrays Revisited"
"length. One solution is to store in the list or queue fixed-length pointers to the","chapter-12","Lists and Arrays Revisited"
"variable-length strings. This is fine for data structures stored in main memory.","chapter-12","Lists and Arrays Revisited"
"But if the collection of strings is meant to be stored on disk, then we might need","chapter-12","Lists and Arrays Revisited"
"to worry about where exactly these strings are stored. And even when stored in","chapter-12","Lists and Arrays Revisited"
"main memory, something has to figure out where there are available bytes to hold","chapter-12","Lists and Arrays Revisited"
"the string. We could easily store variable-size records in a queue or stack, where","chapter-12","Lists and Arrays Revisited"
"1","chapter-12","Lists and Arrays Revisited"
"Scientific packages tend to prefer column-oriented representations for matrices since this the","chapter-12","Lists and Arrays Revisited"
"dominant access need for the operations to be performed.","chapter-12","Lists and Arrays Revisited"
"Sec. 12.3 Memory Management 413","chapter-12","Lists and Arrays Revisited"
"/** Memory Manager interface */","chapter-12","Lists and Arrays Revisited"
"interface MemManADT {","chapter-12","Lists and Arrays Revisited"
"/** Store a record and return a handle to it */","chapter-12","Lists and Arrays Revisited"
"public MemHandle insert(byte[] info);","chapter-12","Lists and Arrays Revisited"
"/** Get back a copy of a stored record */","chapter-12","Lists and Arrays Revisited"
"public byte[] get(MemHandle h);","chapter-12","Lists and Arrays Revisited"
"/** Release the space associated with a record */","chapter-12","Lists and Arrays Revisited"
"public void release(MemHandle h);","chapter-12","Lists and Arrays Revisited"
"}","chapter-12","Lists and Arrays Revisited"
"Figure 12.8 A simple ADT for a memory manager.","chapter-12","Lists and Arrays Revisited"
"the restricted order of insertions and deletions makes this easy to deal with. But","chapter-12","Lists and Arrays Revisited"
"in a language like C++ or Java, programmers can allocate and deallocate space in","chapter-12","Lists and Arrays Revisited"
"complex ways through use of new. Where does this space come from? This section","chapter-12","Lists and Arrays Revisited"
"discusses memory management techniques for the general problem of handling","chapter-12","Lists and Arrays Revisited"
"space requests of variable size.","chapter-12","Lists and Arrays Revisited"
"The basic model for memory management is that we have a (large) block of","chapter-12","Lists and Arrays Revisited"
"contiguous memory locations, which we will call the memory pool. Periodically,","chapter-12","Lists and Arrays Revisited"
"memory requests are issued for some amount of space in the pool. The memory","chapter-12","Lists and Arrays Revisited"
"manager has the job of finding a contiguous block of locations of at least the re-","chapter-12","Lists and Arrays Revisited"
"quested size from somewhere within the memory pool. Honoring such a request","chapter-12","Lists and Arrays Revisited"
"is called a memory allocation. The memory manager will typically return some","chapter-12","Lists and Arrays Revisited"
"piece of information that the requester can hold on to so that later it can recover","chapter-12","Lists and Arrays Revisited"
"the record that was just stored by the memory manager. This piece of information","chapter-12","Lists and Arrays Revisited"
"is called a handle. At some point, space that has been requested might no longer","chapter-12","Lists and Arrays Revisited"
"be needed, and this space can be returned to the memory manager so that it can be","chapter-12","Lists and Arrays Revisited"
"reused. This is called a memory deallocation. The memory manager should then","chapter-12","Lists and Arrays Revisited"
"be able to reuse this space to satisfy later memory requests. We can define an ADT","chapter-12","Lists and Arrays Revisited"
"for the memory manager as shown in Figure 12.8.","chapter-12","Lists and Arrays Revisited"
"The user of the MemManager ADT provides a pointer (in parameter info) to","chapter-12","Lists and Arrays Revisited"
"space that holds some record or message to be stored or retrieved. This is similar","chapter-12","Lists and Arrays Revisited"
"to the Java basic file read/write methods presented in Section 8.4. The fundamental","chapter-12","Lists and Arrays Revisited"
"idea is that the client gives messages to the memory manager for safe keeping. The","chapter-12","Lists and Arrays Revisited"
"memory manager returns a “receipt” for the message in the form of a MemHandle","chapter-12","Lists and Arrays Revisited"
"object. Of course to be practical, a MemHandle must be much smaller than the","chapter-12","Lists and Arrays Revisited"
"typical message to be stored. The client holds the MemHandle object until it","chapter-12","Lists and Arrays Revisited"
"wishes to get the message back.","chapter-12","Lists and Arrays Revisited"
"Method insert lets the client tell the memory manager the length and con-","chapter-12","Lists and Arrays Revisited"
"tents of the message to be stored. This ADT assumes that the memory manager will","chapter-12","Lists and Arrays Revisited"
"remember the length of the message associated with a given handle (perhaps in the","chapter-12","Lists and Arrays Revisited"
"414 Chap. 12 Lists and Arrays Revisited","chapter-12","Lists and Arrays Revisited"
"Figure 12.9 Dynamic storage allocation model. Memory is made up of a series","chapter-12","Lists and Arrays Revisited"
"of variable-size blocks, some allocated and some free. In this example, shaded","chapter-12","Lists and Arrays Revisited"
"areas represent memory currently allocated and unshaded areas represent unused","chapter-12","Lists and Arrays Revisited"
"memory available for future allocation.","chapter-12","Lists and Arrays Revisited"
"handle itself), thus method get does not include a length parameter but instead","chapter-12","Lists and Arrays Revisited"
"returns the length of the message actually stored. Method release allows the","chapter-12","Lists and Arrays Revisited"
"client to tell the memory manager to release the space that stores a given message.","chapter-12","Lists and Arrays Revisited"
"When all inserts and releases follow a simple pattern, such as last requested,","chapter-12","Lists and Arrays Revisited"
"first released (stack order), or first requested, first released (queue order), memory","chapter-12","Lists and Arrays Revisited"
"management is fairly easy. We are concerned here with the general case where","chapter-12","Lists and Arrays Revisited"
"blocks of any size might be requested and released in any order. This is known","chapter-12","Lists and Arrays Revisited"
"as dynamic storage allocation. One example of dynamic storage allocation is","chapter-12","Lists and Arrays Revisited"
"managing free store for a compiler’s runtime environment, such as the system-","chapter-12","Lists and Arrays Revisited"
"level new operation in Java. Another example is managing main memory in a","chapter-12","Lists and Arrays Revisited"
"multitasking operating system. Here, a program might require a certain amount","chapter-12","Lists and Arrays Revisited"
"of space, and the memory manager must keep track of which programs are using","chapter-12","Lists and Arrays Revisited"
"which parts of the main memory. Yet another example is the file manager for a","chapter-12","Lists and Arrays Revisited"
"disk drive. When a disk file is created, expanded, or deleted, the file manager must","chapter-12","Lists and Arrays Revisited"
"allocate or deallocate disk space.","chapter-12","Lists and Arrays Revisited"
"A block of memory or disk space managed in this way is sometimes referred to","chapter-12","Lists and Arrays Revisited"
"as a heap. The term “heap” is being used here in a different way than the heap data","chapter-12","Lists and Arrays Revisited"
"structure discussed in Section 5.5. Here “heap” refers to the memory controlled by","chapter-12","Lists and Arrays Revisited"
"a dynamic memory management scheme.","chapter-12","Lists and Arrays Revisited"
"In the rest of this section, we first study techniques for dynamic memory man-","chapter-12","Lists and Arrays Revisited"
"agement. We then tackle the issue of what to do when no single block of memory","chapter-12","Lists and Arrays Revisited"
"in the memory pool is large enough to honor a given request.","chapter-12","Lists and Arrays Revisited"
"12.3.1 Dynamic Storage Allocation","chapter-12","Lists and Arrays Revisited"
"For the purpose of dynamic storage allocation, we view memory as a single array","chapter-12","Lists and Arrays Revisited"
"which, after a series of memory requests and releases tends to become broken into","chapter-12","Lists and Arrays Revisited"
"a series of variable-size blocks, where some of the blocks are free and some are","chapter-12","Lists and Arrays Revisited"
"reserved or already allocated to store messages. The memory manager typically","chapter-12","Lists and Arrays Revisited"
"uses a linked list to keep track of the free blocks, called the freelist, which is used","chapter-12","Lists and Arrays Revisited"
"for servicing future memory requests. Figure 12.9 illustrates the situation that can","chapter-12","Lists and Arrays Revisited"
"arise after a series of memory allocations and deallocations.","chapter-12","Lists and Arrays Revisited"
"When a memory request is received by the memory manager, some block on","chapter-12","Lists and Arrays Revisited"
"the freelist must be found that is large enough to service the request. If no such","chapter-12","Lists and Arrays Revisited"
"Sec. 12.3 Memory Management 415","chapter-12","Lists and Arrays Revisited"
"Small block: External fragmentation","chapter-12","Lists and Arrays Revisited"
"Unused space in allocated block: Internal fragmentation","chapter-12","Lists and Arrays Revisited"
"Figure 12.10 An illustration of internal and external fragmentation. The small","chapter-12","Lists and Arrays Revisited"
"white block labeled ”External fragmentation” is too small to satisfy typical mem-","chapter-12","Lists and Arrays Revisited"
"ory requests. The small grey block labeled ”Internal fragmentation” was allocated","chapter-12","Lists and Arrays Revisited"
"as part of the grey block to its left, but it does not actually store information.","chapter-12","Lists and Arrays Revisited"
"block is found, then the memory manager must resort to a failure policy such as","chapter-12","Lists and Arrays Revisited"
"discussed in Section 12.3.2.","chapter-12","Lists and Arrays Revisited"
"If there is a request for m words, and no block exists of exactly size m, then","chapter-12","Lists and Arrays Revisited"
"a larger block must be used instead. One possibility in this case is that the entire","chapter-12","Lists and Arrays Revisited"
"block is given away to the memory allocation request. This might be desirable","chapter-12","Lists and Arrays Revisited"
"when the size of the block is only slightly larger than the request. This is because","chapter-12","Lists and Arrays Revisited"
"saving a tiny block that is too small to be useful for a future memory request might","chapter-12","Lists and Arrays Revisited"
"not be worthwhile. Alternatively, for a free block of size k, with k > m, up to","chapter-12","Lists and Arrays Revisited"
"k − m space may be retained by the memory manager to form a new free block,","chapter-12","Lists and Arrays Revisited"
"while the rest is used to service the request.","chapter-12","Lists and Arrays Revisited"
"Memory managers can suffer from two types of fragmentation, which refers to","chapter-12","Lists and Arrays Revisited"
"unused space that is too small to be useful. External fragmentation occurs when","chapter-12","Lists and Arrays Revisited"
"a series of memory requests and releases results in small free blocks. Internal","chapter-12","Lists and Arrays Revisited"
"fragmentation occurs when more than m words are allocated to a request for m","chapter-12","Lists and Arrays Revisited"
"words, wasting free storage. This is equivalent to the internal fragmentation that","chapter-12","Lists and Arrays Revisited"
"occurs when files are allocated in multiples of the cluster size. The difference","chapter-12","Lists and Arrays Revisited"
"between internal and external fragmentation is illustrated by Figure 12.10.","chapter-12","Lists and Arrays Revisited"
"Some memory management schemes sacrifice space to internal fragmentation","chapter-12","Lists and Arrays Revisited"
"to make memory management easier (and perhaps reduce external fragmentation).","chapter-12","Lists and Arrays Revisited"
"For example, external fragmentation does not happen in file management systems","chapter-12","Lists and Arrays Revisited"
"that allocate file space in clusters. Another example of sacrificing space to inter-","chapter-12","Lists and Arrays Revisited"
"nal fragmentation so as to simplify memory management is the buddy method","chapter-12","Lists and Arrays Revisited"
"described later in this section.","chapter-12","Lists and Arrays Revisited"
"The process of searching the memory pool for a block large enough to service","chapter-12","Lists and Arrays Revisited"
"the request, possibly reserving the remaining space as a free block, is referred to as","chapter-12","Lists and Arrays Revisited"
"a sequential fit method.","chapter-12","Lists and Arrays Revisited"
"Sequential Fit Methods","chapter-12","Lists and Arrays Revisited"
"Sequential-fit methods attempt to find a “good” block to service a storage request.","chapter-12","Lists and Arrays Revisited"
"The three sequential-fit methods described here assume that the free blocks are","chapter-12","Lists and Arrays Revisited"
"organized into a doubly linked list, as illustrated by Figure 12.11.","chapter-12","Lists and Arrays Revisited"
"416 Chap. 12 Lists and Arrays Revisited","chapter-12","Lists and Arrays Revisited"
"Figure 12.11 A doubly linked list of free blocks as seen by the memory manager.","chapter-12","Lists and Arrays Revisited"
"Shaded areas represent allocated memory. Unshaded areas are part of the freelist.","chapter-12","Lists and Arrays Revisited"
"There are two basic approaches to implementing the freelist. The simpler ap-","chapter-12","Lists and Arrays Revisited"
"proach is to store the freelist separately from the memory pool. In other words,","chapter-12","Lists and Arrays Revisited"
"a simple linked-list implementation such as described in Chapter 4 can be used,","chapter-12","Lists and Arrays Revisited"
"where each node of the linked list contains a pointer to a single free block in the","chapter-12","Lists and Arrays Revisited"
"memory pool. This is fine if there is space available for the linked list itself, sepa-","chapter-12","Lists and Arrays Revisited"
"rate from the memory pool.","chapter-12","Lists and Arrays Revisited"
"The second approach to storing the freelist is more complicated but saves space.","chapter-12","Lists and Arrays Revisited"
"Because the free space is free, it can be used by the memory manager to help it do","chapter-12","Lists and Arrays Revisited"
"its job. That is, the memory manager can temporarily “borrow” space within the","chapter-12","Lists and Arrays Revisited"
"free blocks to maintain its doubly linked list. To do so, each unallocated block must","chapter-12","Lists and Arrays Revisited"
"be large enough to hold these pointers. In addition, it is usually worthwhile to let","chapter-12","Lists and Arrays Revisited"
"the memory manager add a few bytes of space to each reserved block for its own","chapter-12","Lists and Arrays Revisited"
"purposes. In other words, a request for m bytes of space might result in slightly","chapter-12","Lists and Arrays Revisited"
"more than m bytes being allocated by the memory manager, with the extra bytes","chapter-12","Lists and Arrays Revisited"
"used by the memory manager itself rather than the requester. We will assume that","chapter-12","Lists and Arrays Revisited"
"all memory blocks are organized as shown in Figure 12.12, with space for tags and","chapter-12","Lists and Arrays Revisited"
"linked list pointers. Here, free and reserved blocks are distinguished by a tag bit","chapter-12","Lists and Arrays Revisited"
"at both the beginning and the end of the block, for reasons that will be explained.","chapter-12","Lists and Arrays Revisited"
"In addition, both free and reserved blocks have a size indicator immediately after","chapter-12","Lists and Arrays Revisited"
"the tag bit at the beginning of the block to indicate how large the block is. Free","chapter-12","Lists and Arrays Revisited"
"blocks have a second size indicator immediately preceding the tag bit at the end of","chapter-12","Lists and Arrays Revisited"
"the block. Finally, free blocks have left and right pointers to their neighbors in the","chapter-12","Lists and Arrays Revisited"
"free block list.","chapter-12","Lists and Arrays Revisited"
"The information fields associated with each block permit the memory manager","chapter-12","Lists and Arrays Revisited"
"to allocate and deallocate blocks as needed. When a request comes in for m words","chapter-12","Lists and Arrays Revisited"
"of storage, the memory manager searches the linked list of free blocks until it finds","chapter-12","Lists and Arrays Revisited"
"a “suitable” block for allocation. How it determines which block is suitable will","chapter-12","Lists and Arrays Revisited"
"be discussed below. If the block contains exactly m words (plus space for the tag","chapter-12","Lists and Arrays Revisited"
"and size fields), then it is removed from the freelist. If the block (of size k) is large","chapter-12","Lists and Arrays Revisited"
"enough, then the remaining k − m words are reserved as a block on the freelist, in","chapter-12","Lists and Arrays Revisited"
"the current location.","chapter-12","Lists and Arrays Revisited"
"When a block F is freed, it must be merged into the freelist. If we do not","chapter-12","Lists and Arrays Revisited"
"care about merging adjacent free blocks, then this is a simple insertion into the","chapter-12","Lists and Arrays Revisited"
"doubly linked list of free blocks. However, we would like to merge adjacent blocks,","chapter-12","Lists and Arrays Revisited"
"Sec. 12.3 Memory Management 417","chapter-12","Lists and Arrays Revisited"
"+","chapter-12","Lists and Arrays Revisited"
"Tag Llink","chapter-12","Lists and Arrays Revisited"
"Size Tag","chapter-12","Lists and Arrays Revisited"
"(a)","chapter-12","Lists and Arrays Revisited"
"k","chapter-12","Lists and Arrays Revisited"
"Size","chapter-12","Lists and Arrays Revisited"
"(b)","chapter-12","Lists and Arrays Revisited"
"Size Rlink Tag","chapter-12","Lists and Arrays Revisited"
"+","chapter-12","Lists and Arrays Revisited"
"− k","chapter-12","Lists and Arrays Revisited"
"−","chapter-12","Lists and Arrays Revisited"
"Tag","chapter-12","Lists and Arrays Revisited"
"k","chapter-12","Lists and Arrays Revisited"
"Figure 12.12 Blocks as seen by the memory manager. Each block includes","chapter-12","Lists and Arrays Revisited"
"additional information such as freelist link pointers, start and end tags, and a size","chapter-12","Lists and Arrays Revisited"
"field. (a) The layout for a free block. The beginning of the block contains the tag","chapter-12","Lists and Arrays Revisited"
"bit field, the block size field, and two pointers for the freelist. The end of the block","chapter-12","Lists and Arrays Revisited"
"contains a second tag field and a second block size field. (b) A reserved block of","chapter-12","Lists and Arrays Revisited"
"k bytes. The memory manager adds to these k bytes an additional tag bit field and","chapter-12","Lists and Arrays Revisited"
"block size field at the beginning of the block, and a second tag field at the end of","chapter-12","Lists and Arrays Revisited"
"the block.","chapter-12","Lists and Arrays Revisited"
"+","chapter-12","Lists and Arrays Revisited"
"P F S","chapter-12","Lists and Arrays Revisited"
"k","chapter-12","Lists and Arrays Revisited"
"− k","chapter-12","Lists and Arrays Revisited"
"−","chapter-12","Lists and Arrays Revisited"
"Figure 12.13 Adding block F to the freelist. The word immediately preceding","chapter-12","Lists and Arrays Revisited"
"the start of F in the memory pool stores the tag bit of the preceding block P. If P","chapter-12","Lists and Arrays Revisited"
"is free, merge F into P. We find the end of F by using F’s size field. The word","chapter-12","Lists and Arrays Revisited"
"following the end of F is the tag field for block S. If S is free, merge it into F.","chapter-12","Lists and Arrays Revisited"
"because this allows the memory manager to serve requests of the largest possible","chapter-12","Lists and Arrays Revisited"
"size. Merging is easily done due to the tag and size fields stored at the ends of each","chapter-12","Lists and Arrays Revisited"
"block, as illustrated by Figure 12.13. Here, the memory manager first checks the","chapter-12","Lists and Arrays Revisited"
"unit of memory immediately preceding block F to see if the preceding block (call","chapter-12","Lists and Arrays Revisited"
"it P) is also free. If it is, then the memory unit before P’s tag bit stores the size","chapter-12","Lists and Arrays Revisited"
"of P, thus indicating the position for the beginning of the block in memory. P can","chapter-12","Lists and Arrays Revisited"
"then simply have its size extended to include block F. If block P is not free, then","chapter-12","Lists and Arrays Revisited"
"we just add block F to the freelist. Finally, we also check the bit following the end","chapter-12","Lists and Arrays Revisited"
"of block F. If this bit indicates that the following block (call it S) is free, then S is","chapter-12","Lists and Arrays Revisited"
"removed from the freelist and the size of F is extended appropriately.","chapter-12","Lists and Arrays Revisited"
"418 Chap. 12 Lists and Arrays Revisited","chapter-12","Lists and Arrays Revisited"
"We now consider how a “suitable” free block is selected to service a memory","chapter-12","Lists and Arrays Revisited"
"request. To illustrate the process, assume that we have a memory pool with 200","chapter-12","Lists and Arrays Revisited"
"units of storage. After some series of allocation requests and releases, we have","chapter-12","Lists and Arrays Revisited"
"reached a point where there are four free blocks on the freelist of sizes 25, 35, 32,","chapter-12","Lists and Arrays Revisited"
"and 45 (in that order). Assume that a request is made for 30 units of storage. For","chapter-12","Lists and Arrays Revisited"
"our examples, we ignore the overhead imposed for the tag, link, and size fields","chapter-12","Lists and Arrays Revisited"
"discussed above.","chapter-12","Lists and Arrays Revisited"
"The simplest method for selecting a block would be to move down the free","chapter-12","Lists and Arrays Revisited"
"block list until a block of size at least 30 is found. Any remaining space in this","chapter-12","Lists and Arrays Revisited"
"block is left on the freelist. If we begin at the beginning of the list and work down","chapter-12","Lists and Arrays Revisited"
"to the first free block at least as large as 30, we select the block of size 35. 30 units","chapter-12","Lists and Arrays Revisited"
"of storage will be allocated, leaving a free block with 5 units of space. Because this","chapter-12","Lists and Arrays Revisited"
"approach selects the first block with enough space, it is called first fit. A simple","chapter-12","Lists and Arrays Revisited"
"variation that will improve performance is, instead of always beginning at the head","chapter-12","Lists and Arrays Revisited"
"of the freelist, remember the last position reached in the previous search and start","chapter-12","Lists and Arrays Revisited"
"from there. When the end of the freelist is reached, search begins again at the","chapter-12","Lists and Arrays Revisited"
"head of the freelist. This modification reduces the number of unnecessary searches","chapter-12","Lists and Arrays Revisited"
"through small blocks that were passed over by previous requests.","chapter-12","Lists and Arrays Revisited"
"There is a potential disadvantage to first fit: It might “waste” larger blocks","chapter-12","Lists and Arrays Revisited"
"by breaking them up, and so they will not be available for large requests later.","chapter-12","Lists and Arrays Revisited"
"A strategy that avoids using large blocks unnecessarily is called best fit. Best fit","chapter-12","Lists and Arrays Revisited"
"looks at the entire list and picks the smallest block that is at least as large as the","chapter-12","Lists and Arrays Revisited"
"request (i.e., the “best” or closest fit to the request). Continuing with the preceding","chapter-12","Lists and Arrays Revisited"
"example, the best fit for a request of 30 units is the block of size 32, leaving a","chapter-12","Lists and Arrays Revisited"
"remainder of size 2. Best fit has the disadvantage that it requires that the entire list","chapter-12","Lists and Arrays Revisited"
"be searched. Another problem is that the remaining portion of the best-fit block is","chapter-12","Lists and Arrays Revisited"
"likely to be small, and thus useless for future requests. In other words, best fit tends","chapter-12","Lists and Arrays Revisited"
"to maximize problems of external fragmentation while it minimizes the chance of","chapter-12","Lists and Arrays Revisited"
"not being able to service an occasional large request.","chapter-12","Lists and Arrays Revisited"
"A strategy contrary to best fit might make sense because it tends to minimize the","chapter-12","Lists and Arrays Revisited"
"effects of external fragmentation. This is called worst fit, which always allocates","chapter-12","Lists and Arrays Revisited"
"the largest block on the list hoping that the remainder of the block will be useful","chapter-12","Lists and Arrays Revisited"
"for servicing a future request. In our example, the worst fit is the block of size","chapter-12","Lists and Arrays Revisited"
"45, leaving a remainder of size 15. If there are a few unusually large requests,","chapter-12","Lists and Arrays Revisited"
"this approach will have less chance of servicing them. If requests generally tend","chapter-12","Lists and Arrays Revisited"
"to be of the same size, then this might be an effective strategy. Like best fit, worst","chapter-12","Lists and Arrays Revisited"
"fit requires searching the entire freelist at each memory request to find the largest","chapter-12","Lists and Arrays Revisited"
"block. Alternatively, the freelist can be ordered from largest to smallest free block,","chapter-12","Lists and Arrays Revisited"
"possibly by using a priority queue implementation.","chapter-12","Lists and Arrays Revisited"
"Which strategy is best? It depends on the expected types of memory requests.","chapter-12","Lists and Arrays Revisited"
"If the requests are of widely ranging size, best fit might work well. If the requests","chapter-12","Lists and Arrays Revisited"
"Sec. 12.3 Memory Management 419","chapter-12","Lists and Arrays Revisited"
"tend to be of similar size, with rare large and small requests, first or worst fit might","chapter-12","Lists and Arrays Revisited"
"work well. Unfortunately, there are always request patterns that one of the three","chapter-12","Lists and Arrays Revisited"
"sequential fit methods will service, but which the other two will not be able to","chapter-12","Lists and Arrays Revisited"
"service. For example, if the series of requests 600, 650, 900, 500, 100 is made to","chapter-12","Lists and Arrays Revisited"
"a freelist containing blocks 500, 700, 650, 900 (in that order), the requests can all","chapter-12","Lists and Arrays Revisited"
"be serviced by first fit, but not by best fit. Alternatively, the series of requests 600,","chapter-12","Lists and Arrays Revisited"
"500, 700, 900 can be serviced by best fit but not by first fit on this same freelist.","chapter-12","Lists and Arrays Revisited"
"Buddy Methods","chapter-12","Lists and Arrays Revisited"
"Sequential-fit methods rely on a linked list of free blocks, which must be searched","chapter-12","Lists and Arrays Revisited"
"for a suitable block at each memory request. Thus, the time to find a suitable free","chapter-12","Lists and Arrays Revisited"
"block would be Θ(n) in the worst case for a freelist containing n blocks. Merging","chapter-12","Lists and Arrays Revisited"
"adjacent free blocks is somewhat complicated. Finally, we must either use addi-","chapter-12","Lists and Arrays Revisited"
"tional space for the linked list, or use space within the memory pool to support the","chapter-12","Lists and Arrays Revisited"
"memory manager operations. In the second option, both free and reserved blocks","chapter-12","Lists and Arrays Revisited"
"require tag and size fields. Fields in free blocks do not cost any space (because they","chapter-12","Lists and Arrays Revisited"
"are stored in memory that is not otherwise being used), but fields in reserved blocks","chapter-12","Lists and Arrays Revisited"
"create additional overhead.","chapter-12","Lists and Arrays Revisited"
"The buddy system solves most of these problems. Searching for a block of","chapter-12","Lists and Arrays Revisited"
"the proper size is efficient, merging adjacent free blocks is simple, and no tag or","chapter-12","Lists and Arrays Revisited"
"other information fields need be stored within reserved blocks. The buddy system","chapter-12","Lists and Arrays Revisited"
"assumes that memory is of size 2","chapter-12","Lists and Arrays Revisited"
"N for some integer N. Both free and reserved","chapter-12","Lists and Arrays Revisited"
"blocks will always be of size 2","chapter-12","Lists and Arrays Revisited"
"k","chapter-12","Lists and Arrays Revisited"
"for k ≤ N. At any given time, there might be both","chapter-12","Lists and Arrays Revisited"
"free and reserved blocks of various sizes. The buddy system keeps a separate list","chapter-12","Lists and Arrays Revisited"
"for free blocks of each size. There can be at most N such lists, because there can","chapter-12","Lists and Arrays Revisited"
"only be N distinct block sizes.","chapter-12","Lists and Arrays Revisited"
"When a request comes in for m words, we first determine the smallest value of","chapter-12","Lists and Arrays Revisited"
"k such that 2","chapter-12","Lists and Arrays Revisited"
"k ≥ m. A block of size 2","chapter-12","Lists and Arrays Revisited"
"k","chapter-12","Lists and Arrays Revisited"
"is selected from the free list for that block","chapter-12","Lists and Arrays Revisited"
"size if one exists. The buddy system does not worry about internal fragmentation:","chapter-12","Lists and Arrays Revisited"
"The entire block of size 2","chapter-12","Lists and Arrays Revisited"
"k","chapter-12","Lists and Arrays Revisited"
"is allocated.","chapter-12","Lists and Arrays Revisited"
"If no block of size 2","chapter-12","Lists and Arrays Revisited"
"k","chapter-12","Lists and Arrays Revisited"
"exists, the next larger block is located. This block is split","chapter-12","Lists and Arrays Revisited"
"in half (repeatedly if necessary) until the desired block of size 2","chapter-12","Lists and Arrays Revisited"
"k","chapter-12","Lists and Arrays Revisited"
"is created. Any","chapter-12","Lists and Arrays Revisited"
"other blocks generated as a by-product of this splitting process are placed on the","chapter-12","Lists and Arrays Revisited"
"appropriate freelists.","chapter-12","Lists and Arrays Revisited"
"The disadvantage of the buddy system is that it allows internal fragmentation.","chapter-12","Lists and Arrays Revisited"
"For example, a request for 257 words will require a block of size 512. The primary","chapter-12","Lists and Arrays Revisited"
"advantages of the buddy system are (1) there is less external fragmentation; (2)","chapter-12","Lists and Arrays Revisited"
"search for a block of the right size is cheaper than, say, best fit because we need","chapter-12","Lists and Arrays Revisited"
"only find the first available block on the block list for blocks of size 2","chapter-12","Lists and Arrays Revisited"
"k","chapter-12","Lists and Arrays Revisited"
"; and (3)","chapter-12","Lists and Arrays Revisited"
"merging adjacent free blocks is easy.","chapter-12","Lists and Arrays Revisited"
"420 Chap. 12 Lists and Arrays Revisited","chapter-12","Lists and Arrays Revisited"
"(a) (b)","chapter-12","Lists and Arrays Revisited"
"0000","chapter-12","Lists and Arrays Revisited"
"1000","chapter-12","Lists and Arrays Revisited"
"0000","chapter-12","Lists and Arrays Revisited"
"0100","chapter-12","Lists and Arrays Revisited"
"1000","chapter-12","Lists and Arrays Revisited"
"1100","chapter-12","Lists and Arrays Revisited"
"Buddies","chapter-12","Lists and Arrays Revisited"
"Buddies","chapter-12","Lists and Arrays Revisited"
"Buddies","chapter-12","Lists and Arrays Revisited"
"Figure 12.14 Example of the buddy system. (a) Blocks of size 8. (b) Blocks of","chapter-12","Lists and Arrays Revisited"
"size 4.","chapter-12","Lists and Arrays Revisited"
"The reason why this method is called the buddy system is because of the way","chapter-12","Lists and Arrays Revisited"
"that merging takes place. The buddy for any block of size 2","chapter-12","Lists and Arrays Revisited"
"k","chapter-12","Lists and Arrays Revisited"
"is another block","chapter-12","Lists and Arrays Revisited"
"of the same size, and with the same address (i.e., the byte position in memory,","chapter-12","Lists and Arrays Revisited"
"read as a binary value) except that the kth bit is reversed. For example, the block","chapter-12","Lists and Arrays Revisited"
"of size 8 with beginning address 0000 in Figure 12.14(a) has buddy with address","chapter-12","Lists and Arrays Revisited"
"1000. Likewise, in Figure 12.14(b), the block of size 4 with address 0000 has","chapter-12","Lists and Arrays Revisited"
"buddy 0100. If free blocks are sorted by address value, the buddy can be found by","chapter-12","Lists and Arrays Revisited"
"searching the correct block-size list. Merging simply requires that the address for","chapter-12","Lists and Arrays Revisited"
"the combined buddies be moved to the freelist for the next larger block size.","chapter-12","Lists and Arrays Revisited"
"Other Memory Allocation Methods","chapter-12","Lists and Arrays Revisited"
"In addition to sequential-fit and buddy methods, there are many ad hoc approaches","chapter-12","Lists and Arrays Revisited"
"to memory management. If the application is sufficiently complex, it might be","chapter-12","Lists and Arrays Revisited"
"desirable to break available memory into several memory zones, each with a differ-","chapter-12","Lists and Arrays Revisited"
"ent memory management scheme. For example, some zones might have a simple","chapter-12","Lists and Arrays Revisited"
"memory access pattern of first-in, first-out. This zone can therefore be managed ef-","chapter-12","Lists and Arrays Revisited"
"ficiently by using a simple queue. Another zone might allocate only records of fixed","chapter-12","Lists and Arrays Revisited"
"size, and so can be managed with a simple freelist as described in Section 4.1.2.","chapter-12","Lists and Arrays Revisited"
"Other zones might need one of the general-purpose memory allocation methods","chapter-12","Lists and Arrays Revisited"
"discussed in this section. The advantage of zones is that some portions of memory","chapter-12","Lists and Arrays Revisited"
"can be managed more efficiently. The disadvantage is that one zone might fill up","chapter-12","Lists and Arrays Revisited"
"while other zones have excess free memory if the zone sizes are chosen poorly.","chapter-12","Lists and Arrays Revisited"
"Another approach to memory management is to impose a standard size on all","chapter-12","Lists and Arrays Revisited"
"memory requests. We have seen an example of this concept already in disk file","chapter-12","Lists and Arrays Revisited"
"management, where all files are allocated in multiples of the cluster size. This","chapter-12","Lists and Arrays Revisited"
"approach leads to internal fragmentation, but managing files composed of clusters","chapter-12","Lists and Arrays Revisited"
"Sec. 12.3 Memory Management 421","chapter-12","Lists and Arrays Revisited"
"is easier than managing arbitrarily sized files. The cluster scheme also allows us","chapter-12","Lists and Arrays Revisited"
"to relax the restriction that the memory request be serviced by a contiguous block","chapter-12","Lists and Arrays Revisited"
"of memory. Most disk file managers and operating system main memory managers","chapter-12","Lists and Arrays Revisited"
"work on a cluster or page system. Block management is usually done with a buffer","chapter-12","Lists and Arrays Revisited"
"pool to allocate available blocks in main memory efficiently.","chapter-12","Lists and Arrays Revisited"
"12.3.2 Failure Policies and Garbage Collection","chapter-12","Lists and Arrays Revisited"
"At some point when processing a series of requests, a memory manager could en-","chapter-12","Lists and Arrays Revisited"
"counter a request for memory that it cannot satisfy. In some situations, there might","chapter-12","Lists and Arrays Revisited"
"be nothing that can be done: There simply might not be enough free memory to","chapter-12","Lists and Arrays Revisited"
"service the request, and the application may require that the request be serviced im-","chapter-12","Lists and Arrays Revisited"
"mediately. In this case, the memory manager has no option but to return an error,","chapter-12","Lists and Arrays Revisited"
"which could in turn lead to a failure of the application program. However, in many","chapter-12","Lists and Arrays Revisited"
"cases there are alternatives to simply returning an error. The possible options are","chapter-12","Lists and Arrays Revisited"
"referred to collectively as failure policies.","chapter-12","Lists and Arrays Revisited"
"In some cases, there might be sufficient free memory to satisfy the request,","chapter-12","Lists and Arrays Revisited"
"but it is scattered among small blocks. This can happen when using a sequential-","chapter-12","Lists and Arrays Revisited"
"fit memory allocation method, where external fragmentation has led to a series of","chapter-12","Lists and Arrays Revisited"
"small blocks that collectively could service the request. In this case, it might be","chapter-12","Lists and Arrays Revisited"
"possible to compact memory by moving the reserved blocks around so that the","chapter-12","Lists and Arrays Revisited"
"free space is collected into a single block. A problem with this approach is that","chapter-12","Lists and Arrays Revisited"
"the application must somehow be able to deal with the fact that its data have now","chapter-12","Lists and Arrays Revisited"
"been moved to different locations. If the application program relies on the absolute","chapter-12","Lists and Arrays Revisited"
"positions of the data in any way, this would be disastrous. One approach for dealing","chapter-12","Lists and Arrays Revisited"
"with this problem involves the handles returned by the memory manager. A handle","chapter-12","Lists and Arrays Revisited"
"works as a second level of indirection to a memory location. The memory allocation","chapter-12","Lists and Arrays Revisited"
"routine does not return a pointer to the block of storage, but rather a pointer to a the","chapter-12","Lists and Arrays Revisited"
"handle that in turn gives access to the storage. The handle never moves its position,","chapter-12","Lists and Arrays Revisited"
"but the position of the block might be moved and the value of the handle updated.","chapter-12","Lists and Arrays Revisited"
"Of course, this requires that the memory manager keep track of the handles and","chapter-12","Lists and Arrays Revisited"
"how they associate with the stored messages. Figure 12.15 illustrates the concept.","chapter-12","Lists and Arrays Revisited"
"Another failure policy that might work in some applications is to defer the","chapter-12","Lists and Arrays Revisited"
"memory request until sufficient memory becomes available. For example, a multi-","chapter-12","Lists and Arrays Revisited"
"tasking operating system could adopt the strategy of not allowing a process to run","chapter-12","Lists and Arrays Revisited"
"until there is sufficient memory available. While such a delay might be annoying","chapter-12","Lists and Arrays Revisited"
"to the user, it is better than halting the entire system. The assumption here is that","chapter-12","Lists and Arrays Revisited"
"other processes will eventually terminate, freeing memory.","chapter-12","Lists and Arrays Revisited"
"Another option might be to allocate more memory to the memory manager. In","chapter-12","Lists and Arrays Revisited"
"a zoned memory allocation system where the memory manager is part of a larger","chapter-12","Lists and Arrays Revisited"
"system, this might be a viable option. In a Java program that implements its own","chapter-12","Lists and Arrays Revisited"
"422 Chap. 12 Lists and Arrays Revisited","chapter-12","Lists and Arrays Revisited"
"Handle Memory Block","chapter-12","Lists and Arrays Revisited"
"Figure 12.15 Using handles for dynamic memory management. The memory","chapter-12","Lists and Arrays Revisited"
"manager returns the address of the handle in response to a memory request. The","chapter-12","Lists and Arrays Revisited"
"handle stores the address of the actual memory block. In this way, the memory","chapter-12","Lists and Arrays Revisited"
"block might be moved (with its address updated in the handle) without disrupting","chapter-12","Lists and Arrays Revisited"
"the application program.","chapter-12","Lists and Arrays Revisited"
"memory manager, it might be possible to get more memory from the system-level","chapter-12","Lists and Arrays Revisited"
"new operator, such as is done by the freelist of Section 4.1.2.","chapter-12","Lists and Arrays Revisited"
"The last failure policy that we will consider is garbage collection. Consider","chapter-12","Lists and Arrays Revisited"
"the following series of statements.","chapter-12","Lists and Arrays Revisited"
"int[] p = new int[5];","chapter-12","Lists and Arrays Revisited"
"int[] q = new int[10];","chapter-12","Lists and Arrays Revisited"
"p = q;","chapter-12","Lists and Arrays Revisited"
"While in Java this would be no problem (due to automatic garbage collection), in","chapter-12","Lists and Arrays Revisited"
"languages such as C++, this would be considered bad form because the original","chapter-12","Lists and Arrays Revisited"
"space allocated to p is lost as a result of the third assignment. This space cannot","chapter-12","Lists and Arrays Revisited"
"be used again by the program. Such lost memory is referred to as garbage, also","chapter-12","Lists and Arrays Revisited"
"known as a memory leak. When no program variable points to a block of space,","chapter-12","Lists and Arrays Revisited"
"no future access to that space is possible. Of course, if another variable had first","chapter-12","Lists and Arrays Revisited"
"been assigned to point to p’s space, then reassigning p would not create garbage.","chapter-12","Lists and Arrays Revisited"
"Some programming languages take a different view towards garbage. In par-","chapter-12","Lists and Arrays Revisited"
"ticular, the LISP programming language uses the multilist representation of Fig-","chapter-12","Lists and Arrays Revisited"
"ure 12.5, and all storage is in the form either of internal nodes with two pointers","chapter-12","Lists and Arrays Revisited"
"or atoms. Figure 12.16 shows a typical collection of LISP structures, headed by","chapter-12","Lists and Arrays Revisited"
"variables named A, B, and C, along with a freelist.","chapter-12","Lists and Arrays Revisited"
"In LISP, list objects are constantly being put together in various ways as tem-","chapter-12","Lists and Arrays Revisited"
"porary variables, and then all reference to them is lost when the object is no longer","chapter-12","Lists and Arrays Revisited"
"needed. Thus, garbage is normal in LISP, and in fact cannot be avoided during","chapter-12","Lists and Arrays Revisited"
"routine program behavior. When LISP runs out of memory, it resorts to a garbage","chapter-12","Lists and Arrays Revisited"
"collection process to recover the space tied up in garbage. Garbage collection con-","chapter-12","Lists and Arrays Revisited"
"sists of examining the managed memory pool to determine which parts are still","chapter-12","Lists and Arrays Revisited"
"being used and which parts are garbage. In particular, a list is kept of all program","chapter-12","Lists and Arrays Revisited"
"variables, and any memory locations not reachable from one of these variables are","chapter-12","Lists and Arrays Revisited"
"considered to be garbage. When the garbage collector executes, all unused memory","chapter-12","Lists and Arrays Revisited"
"locations are placed in free store for future access. This approach has the advantage","chapter-12","Lists and Arrays Revisited"
"that it allows for easy collection of garbage. It has the disadvantage, from a user’s","chapter-12","Lists and Arrays Revisited"
"Sec. 12.3 Memory Management 423","chapter-12","Lists and Arrays Revisited"
"a","chapter-12","Lists and Arrays Revisited"
"c","chapter-12","Lists and Arrays Revisited"
"d e","chapter-12","Lists and Arrays Revisited"
"f","chapter-12","Lists and Arrays Revisited"
"g h","chapter-12","Lists and Arrays Revisited"
"A","chapter-12","Lists and Arrays Revisited"
"B","chapter-12","Lists and Arrays Revisited"
"C","chapter-12","Lists and Arrays Revisited"
"Freelist","chapter-12","Lists and Arrays Revisited"
"Figure 12.16 Example of LISP list variables, including the system freelist.","chapter-12","Lists and Arrays Revisited"
"point of view, that every so often the system must halt while it performs garbage","chapter-12","Lists and Arrays Revisited"
"collection. For example, garbage collection is noticeable in the Emacs text edi-","chapter-12","Lists and Arrays Revisited"
"tor, which is normally implemented in LISP. Occasionally the user must wait for a","chapter-12","Lists and Arrays Revisited"
"moment while the memory management system performs garbage collection.","chapter-12","Lists and Arrays Revisited"
"The Java programming language also makes use of garbage collection. As in","chapter-12","Lists and Arrays Revisited"
"LISP, it is common practice in Java to allocate dynamic memory as needed, and to","chapter-12","Lists and Arrays Revisited"
"later drop all references to that memory. The garbage collector is responsible for","chapter-12","Lists and Arrays Revisited"
"reclaiming such unused space as necessary. This might require extra time when","chapter-12","Lists and Arrays Revisited"
"running the program, but it makes life considerably easier for the programmer. In","chapter-12","Lists and Arrays Revisited"
"contrast, many large applications written in C++ (even commonly used commercial","chapter-12","Lists and Arrays Revisited"
"software) contain memory leaks that will in time cause the program to fail.","chapter-12","Lists and Arrays Revisited"
"Several algorithms have been used for garbage collection. One is the reference","chapter-12","Lists and Arrays Revisited"
"count algorithm. Here, every dynamically allocated memory block includes space","chapter-12","Lists and Arrays Revisited"
"for a count field. Whenever a pointer is directed to a memory block, the reference","chapter-12","Lists and Arrays Revisited"
"count is increased. Whenever a pointer is directed away from a memory block,","chapter-12","Lists and Arrays Revisited"
"the reference count is decreased. If the count ever becomes zero, then the memory","chapter-12","Lists and Arrays Revisited"
"block is considered garbage and is immediately placed in free store. This approach","chapter-12","Lists and Arrays Revisited"
"has the advantage that it does not require an explicit garbage collection phase, be-","chapter-12","Lists and Arrays Revisited"
"cause information is put in free store immediately when it becomes garbage.","chapter-12","Lists and Arrays Revisited"
"Reference counts are used by the UNIX file system. Files can have multiple","chapter-12","Lists and Arrays Revisited"
"names, called links. The file system keeps a count of the number of links to each","chapter-12","Lists and Arrays Revisited"
"file. Whenever a file is “deleted,” in actuality its link field is simply reduced by","chapter-12","Lists and Arrays Revisited"
"one. If there is another link to the file, then no space is recovered by the file system.","chapter-12","Lists and Arrays Revisited"
"When the number of links goes to zero, the file’s space becomes available for reuse.","chapter-12","Lists and Arrays Revisited"
"424 Chap. 12 Lists and Arrays Revisited","chapter-12","Lists and Arrays Revisited"
"g h","chapter-12","Lists and Arrays Revisited"
"Figure 12.17 Garbage cycle example. All memory elements in the cycle have","chapter-12","Lists and Arrays Revisited"
"non-zero reference counts because each element has one pointer to it, even though","chapter-12","Lists and Arrays Revisited"
"the entire cycle is garbage (i.e., no static variable in the program points to it).","chapter-12","Lists and Arrays Revisited"
"Reference counts have several major disadvantages. First, a reference count","chapter-12","Lists and Arrays Revisited"
"must be maintained for each memory object. This works well when the objects are","chapter-12","Lists and Arrays Revisited"
"large, such as a file. However, it will not work well in a system such as LISP where","chapter-12","Lists and Arrays Revisited"
"the memory objects typically consist of two pointers or a value (an atom). Another","chapter-12","Lists and Arrays Revisited"
"major problem occurs when garbage contains cycles. Consider Figure 12.17. Here","chapter-12","Lists and Arrays Revisited"
"each memory object is pointed to once, but the collection of objects is still garbage","chapter-12","Lists and Arrays Revisited"
"because no pointer points to the collection. Thus, reference counts only work when","chapter-12","Lists and Arrays Revisited"
"the memory objects are linked together without cycles, such as the UNIX file sys-","chapter-12","Lists and Arrays Revisited"
"tem where files can only be organized as a DAG.","chapter-12","Lists and Arrays Revisited"
"Another approach to garbage collection is the mark/sweep strategy. Here, each","chapter-12","Lists and Arrays Revisited"
"memory object needs only a single mark bit rather than a reference counter field.","chapter-12","Lists and Arrays Revisited"
"When free store is exhausted, a separate garbage collection phase takes place as","chapter-12","Lists and Arrays Revisited"
"follows.","chapter-12","Lists and Arrays Revisited"
"1. Clear all mark bits.","chapter-12","Lists and Arrays Revisited"
"2. Perform depth-first search (DFS) following pointers beginning with each","chapter-12","Lists and Arrays Revisited"
"variable on the system’s list of static variables. Each memory element en-","chapter-12","Lists and Arrays Revisited"
"countered during the DFS has its mark bit turned on.","chapter-12","Lists and Arrays Revisited"
"3. A “sweep” is made through the memory pool, visiting all elements. Un-","chapter-12","Lists and Arrays Revisited"
"marked elements are considered garbage and placed in free store.","chapter-12","Lists and Arrays Revisited"
"The advantages of the mark/sweep approach are that it needs less space than is","chapter-12","Lists and Arrays Revisited"
"necessary for reference counts, and it works for cycles. However, there is a major","chapter-12","Lists and Arrays Revisited"
"disadvantage. This is a “hidden” space requirement needed to do the processing.","chapter-12","Lists and Arrays Revisited"
"DFS is a recursive algorithm: Either it must be implemented recursively, in which","chapter-12","Lists and Arrays Revisited"
"case the compiler’s runtime system maintains a stack, or else the memory manager","chapter-12","Lists and Arrays Revisited"
"can maintain its own stack. What happens if all memory is contained in a single","chapter-12","Lists and Arrays Revisited"
"linked list? Then the depth of the recursion (or the size of the stack) is the number","chapter-12","Lists and Arrays Revisited"
"of memory cells! Unfortunately, the space for the DFS stack must be available at","chapter-12","Lists and Arrays Revisited"
"the worst conceivable time, that is, when free memory has been exhausted.","chapter-12","Lists and Arrays Revisited"
"Fortunately, a clever technique allows DFS to be performed without requiring","chapter-12","Lists and Arrays Revisited"
"additional space for a stack. Instead, the structure being traversed is used to hold","chapter-12","Lists and Arrays Revisited"
"the stack. At each step deeper into the traversal, instead of storing a pointer on the","chapter-12","Lists and Arrays Revisited"
"stack, we “borrow” the pointer being followed. This pointer is set to point back","chapter-12","Lists and Arrays Revisited"
"to the node we just came from in the previous step, as illustrated by Figure 12.18.","chapter-12","Lists and Arrays Revisited"
"Each borrowed pointer stores an additional bit to tell us whether we came down","chapter-12","Lists and Arrays Revisited"
"Sec. 12.4 Further Reading 425","chapter-12","Lists and Arrays Revisited"
"(a)","chapter-12","Lists and Arrays Revisited"
"a","chapter-12","Lists and Arrays Revisited"
"b","chapter-12","Lists and Arrays Revisited"
"4","chapter-12","Lists and Arrays Revisited"
"prev","chapter-12","Lists and Arrays Revisited"
"c e","chapter-12","Lists and Arrays Revisited"
"curr","chapter-12","Lists and Arrays Revisited"
"(b)","chapter-12","Lists and Arrays Revisited"
"4","chapter-12","Lists and Arrays Revisited"
"6","chapter-12","Lists and Arrays Revisited"
"6","chapter-12","Lists and Arrays Revisited"
"2","chapter-12","Lists and Arrays Revisited"
"2","chapter-12","Lists and Arrays Revisited"
"3","chapter-12","Lists and Arrays Revisited"
"5","chapter-12","Lists and Arrays Revisited"
"1","chapter-12","Lists and Arrays Revisited"
"3","chapter-12","Lists and Arrays Revisited"
"1","chapter-12","Lists and Arrays Revisited"
"5","chapter-12","Lists and Arrays Revisited"
"a","chapter-12","Lists and Arrays Revisited"
"b","chapter-12","Lists and Arrays Revisited"
"c e","chapter-12","Lists and Arrays Revisited"
"Figure 12.18 Example of the Deutsch-Schorr-Waite garbage collection alg-","chapter-12","Lists and Arrays Revisited"
"orithm. (a) The initial multilist structure. (b) The multilist structure of (a) at","chapter-12","Lists and Arrays Revisited"
"the instant when link node 5 is being processed by the garbage collection alg-","chapter-12","Lists and Arrays Revisited"
"orithm. A chain of pointers stretching from variable prev to the head node of the","chapter-12","Lists and Arrays Revisited"
"structure has been (temporarily) created by the garbage collection algorithm.","chapter-12","Lists and Arrays Revisited"
"the left branch or the right branch of the link node being pointed to. At any given","chapter-12","Lists and Arrays Revisited"
"instant we have passed down only one path from the root, and we can follow the","chapter-12","Lists and Arrays Revisited"
"trail of pointers back up. As we return (equivalent to popping the recursion stack),","chapter-12","Lists and Arrays Revisited"
"we set the pointer back to its original position so as to return the structure to its","chapter-12","Lists and Arrays Revisited"
"original condition. This is known as the Deutsch-Schorr-Waite garbage collection","chapter-12","Lists and Arrays Revisited"
"algorithm.","chapter-12","Lists and Arrays Revisited"
"12.4 Further Reading","chapter-12","Lists and Arrays Revisited"
"For information on LISP, see The Little LISPer by Friedman and Felleisen [FF89].","chapter-12","Lists and Arrays Revisited"
"Another good LISP reference is Common LISP: The Language by Guy L. Steele","chapter-12","Lists and Arrays Revisited"
"[Ste90]. For information on Emacs, which is both an excellent text editor and","chapter-12","Lists and Arrays Revisited"
"a programming environment, see the GNU Emacs Manual by Richard Stallman","chapter-12","Lists and Arrays Revisited"
"[Sta11b]. You can get more information about Java’s garbage collection system","chapter-12","Lists and Arrays Revisited"
"from The Java Programming Language by Ken Arnold and James Gosling [AG06].","chapter-12","Lists and Arrays Revisited"
"For more details on sparse matrix representations, the Yale representation is de-","chapter-12","Lists and Arrays Revisited"
"scribed by Eisenstat, Schultz and Sherman [ESS81]. The MATLAB sparse matrix","chapter-12","Lists and Arrays Revisited"
"representation is described by Gilbert, Moler, and Schreiber [GMS91].","chapter-12","Lists and Arrays Revisited"
"426 Chap. 12 Lists and Arrays Revisited","chapter-12","Lists and Arrays Revisited"
"(a) (b) (c)","chapter-12","Lists and Arrays Revisited"
"a","chapter-12","Lists and Arrays Revisited"
"b","chapter-12","Lists and Arrays Revisited"
"d e","chapter-12","Lists and Arrays Revisited"
"c a","chapter-12","Lists and Arrays Revisited"
"L1","chapter-12","Lists and Arrays Revisited"
"L1","chapter-12","Lists and Arrays Revisited"
"L2","chapter-12","Lists and Arrays Revisited"
"L4","chapter-12","Lists and Arrays Revisited"
"a b d c","chapter-12","Lists and Arrays Revisited"
"L3","chapter-12","Lists and Arrays Revisited"
"Figure 12.19 Some example multilists.","chapter-12","Lists and Arrays Revisited"
"An introductory text on operating systems covers many topics relating to mem-","chapter-12","Lists and Arrays Revisited"
"ory management issues, including layout of files on disk and caching of information","chapter-12","Lists and Arrays Revisited"
"in main memory. All of the topics covered here on memory management, buffer","chapter-12","Lists and Arrays Revisited"
"pools, and paging are relevant to operating system implementation. For example,","chapter-12","Lists and Arrays Revisited"
"see Operating Systems by William Stallings[Sta11a].","chapter-12","Lists and Arrays Revisited"
"12.5 Exercises","chapter-12","Lists and Arrays Revisited"
"12.1 For each of the following bracket notation descriptions, draw the equivalent","chapter-12","Lists and Arrays Revisited"
"multilist in graphical form such as shown in Figure 12.2.","chapter-12","Lists and Arrays Revisited"
"(a) ha, b,hc, d, ei,hf,hgi, hii","chapter-12","Lists and Arrays Revisited"
"(b) ha, b,hc, d, L1:ei, L1i","chapter-12","Lists and Arrays Revisited"
"(c) hL1:a, L1,hL2: bi, L2,hL1ii","chapter-12","Lists and Arrays Revisited"
"12.2 (a) Show the bracket notation for the list of Figure 12.19(a).","chapter-12","Lists and Arrays Revisited"
"(b) Show the bracket notation for the list of Figure 12.19(b).","chapter-12","Lists and Arrays Revisited"
"(c) Show the bracket notation for the list of Figure 12.19(c).","chapter-12","Lists and Arrays Revisited"
"12.3 Given the linked representation of a pure list such as","chapter-12","Lists and Arrays Revisited"
"hx1,hy1, y2,hz1, z2i, y4i,hw1, w2i, x4i,","chapter-12","Lists and Arrays Revisited"
"write an in-place reversal algorithm to reverse the sublists at all levels in-","chapter-12","Lists and Arrays Revisited"
"cluding the topmost level. For this example, the result would be a linked","chapter-12","Lists and Arrays Revisited"
"representation corresponding to","chapter-12","Lists and Arrays Revisited"
"hx4,hw2, w1i,hy4,hz2, z1i, y2, y1i, x1i.","chapter-12","Lists and Arrays Revisited"
"12.4 What fraction of the values in a matrix must be zero for the sparse matrix","chapter-12","Lists and Arrays Revisited"
"representation of Section 12.2 to be more space efficient than the standard","chapter-12","Lists and Arrays Revisited"
"two-dimensional matrix representation when data values require eight bytes,","chapter-12","Lists and Arrays Revisited"
"array indices require two bytes, and pointers require four bytes?","chapter-12","Lists and Arrays Revisited"
"12.5 Write a function to add an element at a given position to the sparse matrix","chapter-12","Lists and Arrays Revisited"
"representation of Section 12.2.","chapter-12","Lists and Arrays Revisited"
"12.6 Write a function to delete an element from a given position in the sparse","chapter-12","Lists and Arrays Revisited"
"matrix representation of Section 12.2.","chapter-12","Lists and Arrays Revisited"
"Sec. 12.6 Projects 427","chapter-12","Lists and Arrays Revisited"
"12.7 Write a function to transpose a sparse matrix as represented in Section 12.2.","chapter-12","Lists and Arrays Revisited"
"12.8 Write a function to add two sparse matrices as represented in Section 12.2.","chapter-12","Lists and Arrays Revisited"
"12.9 Write memory manager allocation and deallocation routines for the situation","chapter-12","Lists and Arrays Revisited"
"where all requests and releases follow a last-requested, first-released (stack)","chapter-12","Lists and Arrays Revisited"
"order.","chapter-12","Lists and Arrays Revisited"
"12.10 Write memory manager allocation and deallocation routines for the situation","chapter-12","Lists and Arrays Revisited"
"where all requests and releases follow a last-requested, last-released (queue)","chapter-12","Lists and Arrays Revisited"
"order.","chapter-12","Lists and Arrays Revisited"
"12.11 Show the result of allocating the following blocks from a memory pool of","chapter-12","Lists and Arrays Revisited"
"size 1000 using first fit for each series of block requests. State if a given","chapter-12","Lists and Arrays Revisited"
"request cannot be satisfied.","chapter-12","Lists and Arrays Revisited"
"(a) Take 300 (call this block A), take 500, release A, take 200, take 300.","chapter-12","Lists and Arrays Revisited"
"(b) Take 200 (call this block A), take 500, release A, take 200, take 300.","chapter-12","Lists and Arrays Revisited"
"(c) Take 500 (call this block A), take 300, release A, take 300, take 200.","chapter-12","Lists and Arrays Revisited"
"12.12 Show the result of allocating the following blocks from a memory pool of","chapter-12","Lists and Arrays Revisited"
"size 1000 using best fit for each series of block requests. State if a given","chapter-12","Lists and Arrays Revisited"
"request cannot be satisfied.","chapter-12","Lists and Arrays Revisited"
"(a) Take 300 (call this block A), take 500, release A, take 200, take 300.","chapter-12","Lists and Arrays Revisited"
"(b) Take 200 (call this block A), take 500, release A, take 200, take 300.","chapter-12","Lists and Arrays Revisited"
"(c) Take 500 (call this block A), take 300, release A, take 300, take 200.","chapter-12","Lists and Arrays Revisited"
"12.13 Show the result of allocating the following blocks from a memory pool of","chapter-12","Lists and Arrays Revisited"
"size 1000 using worst fit for each series of block requests. State if a given","chapter-12","Lists and Arrays Revisited"
"request cannot be satisfied.","chapter-12","Lists and Arrays Revisited"
"(a) Take 300 (call this block A), take 500, release A, take 200, take 300.","chapter-12","Lists and Arrays Revisited"
"(b) Take 200 (call this block A), take 500, release A, take 200, take 300.","chapter-12","Lists and Arrays Revisited"
"(c) Take 500 (call this block A), take 300, release A, take 300, take 200.","chapter-12","Lists and Arrays Revisited"
"12.14 Assume that the memory pool contains three blocks of free storage. Their","chapter-12","Lists and Arrays Revisited"
"sizes are 1300, 2000, and 1000. Give examples of storage requests for which","chapter-12","Lists and Arrays Revisited"
"(a) first-fit allocation will work, but not best fit or worst fit.","chapter-12","Lists and Arrays Revisited"
"(b) best-fit allocation will work, but not first fit or worst fit.","chapter-12","Lists and Arrays Revisited"
"(c) worst-fit allocation will work, but not first fit or best fit.","chapter-12","Lists and Arrays Revisited"
"12.6 Projects","chapter-12","Lists and Arrays Revisited"
"12.1 Implement the orthogonal list sparse matrix representation of Section 12.2.","chapter-12","Lists and Arrays Revisited"
"Your implementation should support the following operations on the matrix:","chapter-12","Lists and Arrays Revisited"
"• insert an element at a given position,","chapter-12","Lists and Arrays Revisited"
"• delete an element from a given position,","chapter-12","Lists and Arrays Revisited"
"• return the value of the element at a given position,","chapter-12","Lists and Arrays Revisited"
"• take the transpose of a matrix,","chapter-12","Lists and Arrays Revisited"
"428 Chap. 12 Lists and Arrays Revisited","chapter-12","Lists and Arrays Revisited"
"• add two matrices, and","chapter-12","Lists and Arrays Revisited"
"• multiply two matrices.","chapter-12","Lists and Arrays Revisited"
"12.2 Implement the Yale model for sparse matrices described at the end of Sec-","chapter-12","Lists and Arrays Revisited"
"tion 12.2. Your implementation should support the following operations on","chapter-12","Lists and Arrays Revisited"
"the matrix:","chapter-12","Lists and Arrays Revisited"
"• insert an element at a given position,","chapter-12","Lists and Arrays Revisited"
"• delete an element from a given position,","chapter-12","Lists and Arrays Revisited"
"• return the value of the element at a given position,","chapter-12","Lists and Arrays Revisited"
"• take the transpose of a matrix,","chapter-12","Lists and Arrays Revisited"
"• add two matrices, and","chapter-12","Lists and Arrays Revisited"
"• multiply two matrices.","chapter-12","Lists and Arrays Revisited"
"12.3 Implement the MemManager ADT shown at the beginning of Section 12.3.","chapter-12","Lists and Arrays Revisited"
"Use a separate linked list to implement the freelist. Your implementation","chapter-12","Lists and Arrays Revisited"
"should work for any of the three sequential-fit methods: first fit, best fit, and","chapter-12","Lists and Arrays Revisited"
"worst fit. Test your system empirically to determine under what conditions","chapter-12","Lists and Arrays Revisited"
"each method performs well.","chapter-12","Lists and Arrays Revisited"
"12.4 Implement the MemManager ADT shown at the beginning of Section 12.3.","chapter-12","Lists and Arrays Revisited"
"Do not use separate memory for the free list, but instead embed the free list","chapter-12","Lists and Arrays Revisited"
"into the memory pool as shown in Figure 12.12. Your implementation should","chapter-12","Lists and Arrays Revisited"
"work for any of the three sequential-fit methods: first fit, best fit, and worst","chapter-12","Lists and Arrays Revisited"
"fit. Test your system empirically to determine under what conditions each","chapter-12","Lists and Arrays Revisited"
"method performs well.","chapter-12","Lists and Arrays Revisited"
"12.5 Implement the MemManager ADT shown at the beginning of Section 12.3","chapter-12","Lists and Arrays Revisited"
"using the buddy method of Section 12.3.1. Your system should support","chapter-12","Lists and Arrays Revisited"
"requests for blocks of a specified size and release of previously requested","chapter-12","Lists and Arrays Revisited"
"blocks.","chapter-12","Lists and Arrays Revisited"
"12.6 Implement the Deutsch-Schorr-Waite garbage collection algorithm that is il-","chapter-12","Lists and Arrays Revisited"
"lustrated by Figure 12.18.","chapter-12","Lists and Arrays Revisited"
"This chapter introduces several tree structures designed for use in specialized ap-","chapter-13","Advanced Tree Structures"
"plications. The trie of Section 13.1 is commonly used to store and retrieve strings.","chapter-13","Advanced Tree Structures"
"It also serves to illustrate the concept of a key space decomposition. The AVL","chapter-13","Advanced Tree Structures"
"tree and splay tree of Section 13.2 are variants on the BST. They are examples of","chapter-13","Advanced Tree Structures"
"self-balancing search trees and have guaranteed good performance regardless of the","chapter-13","Advanced Tree Structures"
"insertion order for records. An introduction to several spatial data structures used","chapter-13","Advanced Tree Structures"
"to organize point data by xy-coordinates is presented in Section 13.3.","chapter-13","Advanced Tree Structures"
"Descriptions of the fundamental operations are given for each data structure.","chapter-13","Advanced Tree Structures"
"One purpose for this chapter is to provide opportunities for class programming","chapter-13","Advanced Tree Structures"
"projects, so detailed implementations are left to the reader.","chapter-13","Advanced Tree Structures"
"13.1 Tries","chapter-13","Advanced Tree Structures"
"Recall that the shape of a BST is determined by the order in which its data records","chapter-13","Advanced Tree Structures"
"are inserted. One permutation of the records might yield a balanced tree while","chapter-13","Advanced Tree Structures"
"another might yield an unbalanced tree, with the extreme case becoming the shape","chapter-13","Advanced Tree Structures"
"of a linked list. The reason is that the value of the key stored in the root node splits","chapter-13","Advanced Tree Structures"
"the key range into two parts: those key values less than the root’s key value, and","chapter-13","Advanced Tree Structures"
"those key values greater than the root’s key value. Depending on the relationship","chapter-13","Advanced Tree Structures"
"between the root node’s key value and the distribution of the key values for the","chapter-13","Advanced Tree Structures"
"other records in the the tree, the resulting BST might be balanced or unbalanced.","chapter-13","Advanced Tree Structures"
"Thus, the BST is an example of a data structure whose organization is based on an","chapter-13","Advanced Tree Structures"
"object space decomposition, so called because the decomposition of the key range","chapter-13","Advanced Tree Structures"
"is driven by the objects (i.e., the key values of the data records) stored in the tree.","chapter-13","Advanced Tree Structures"
"The alternative to object space decomposition is to predefine the splitting posi-","chapter-13","Advanced Tree Structures"
"tion within the key range for each node in the tree. In other words, the root could be","chapter-13","Advanced Tree Structures"
"predefined to split the key range into two equal halves, regardless of the particular","chapter-13","Advanced Tree Structures"
"values or order of insertion for the data records. Those records with keys in the","chapter-13","Advanced Tree Structures"
"lower half of the key range will be stored in the left subtree, while those records","chapter-13","Advanced Tree Structures"
"429","chapter-13","Advanced Tree Structures"
"430 Chap. 13 Advanced Tree Structures","chapter-13","Advanced Tree Structures"
"with keys in the upper half of the key range will be stored in the right subtree.","chapter-13","Advanced Tree Structures"
"While such a decomposition rule will not necessarily result in a balanced tree (the","chapter-13","Advanced Tree Structures"
"tree will be unbalanced if the records are not well distributed within the key range),","chapter-13","Advanced Tree Structures"
"at least the shape of the tree will not depend on the order of key insertion. Further-","chapter-13","Advanced Tree Structures"
"more, the depth of the tree will be limited by the resolution of the key range; that","chapter-13","Advanced Tree Structures"
"is, the depth of the tree can never be greater than the number of bits required to","chapter-13","Advanced Tree Structures"
"store a key value. For example, if the keys are integers in the range 0 to 1023, then","chapter-13","Advanced Tree Structures"
"the resolution for the key is ten bits. Thus, two keys can be identical only until the","chapter-13","Advanced Tree Structures"
"tenth bit. In the worst case, two keys will follow the same path in the tree only until","chapter-13","Advanced Tree Structures"
"the tenth branch. As a result, the tree will never be more than ten levels deep. In","chapter-13","Advanced Tree Structures"
"contrast, a BST containing n records could be as much as n levels deep.","chapter-13","Advanced Tree Structures"
"Splitting based on predetermined subdivisions of the key range is called key","chapter-13","Advanced Tree Structures"
"space decomposition. In computer graphics, the technique is known as image","chapter-13","Advanced Tree Structures"
"space decomposition, and this term is sometimes used to describe the process for","chapter-13","Advanced Tree Structures"
"data structures as well. A data structure based on key space decomposition is called","chapter-13","Advanced Tree Structures"
"a trie. Folklore has it that “trie” comes from “retrieval.” Unfortunately, that would","chapter-13","Advanced Tree Structures"
"imply that the word is pronounced “tree,” which would lead to confusion with reg-","chapter-13","Advanced Tree Structures"
"ular use of the word “tree.” “Trie” is actually pronounced as “try.”","chapter-13","Advanced Tree Structures"
"Like the B+-tree, a trie stores data records only in leaf nodes. Internal nodes","chapter-13","Advanced Tree Structures"
"serve as placeholders to direct the search process. but since the split points are pre-","chapter-13","Advanced Tree Structures"
"determined, internal nodes need not store “traffic-directing” key values. Figure 13.1","chapter-13","Advanced Tree Structures"
"illustrates the trie concept. Upper and lower bounds must be imposed on the key","chapter-13","Advanced Tree Structures"
"values so that we can compute the middle of the key range. Because the largest","chapter-13","Advanced Tree Structures"
"value inserted in this example is 120, a range from 0 to 127 is assumed, as 128 is","chapter-13","Advanced Tree Structures"
"the smallest power of two greater than 120. The binary value of the key determines","chapter-13","Advanced Tree Structures"
"whether to select the left or right branch at any given point during the search. The","chapter-13","Advanced Tree Structures"
"most significant bit determines the branch direction at the root. Figure 13.1 shows","chapter-13","Advanced Tree Structures"
"a binary trie, so called because in this example the trie structure is based on the","chapter-13","Advanced Tree Structures"
"value of the key interpreted as a binary number, which results in a binary tree.","chapter-13","Advanced Tree Structures"
"The Huffman coding tree of Section 5.6 is another example of a binary trie. All","chapter-13","Advanced Tree Structures"
"data values in the Huffman tree are at the leaves, and each branch splits the range","chapter-13","Advanced Tree Structures"
"of possible letter codes in half. The Huffman codes are actually reconstructed from","chapter-13","Advanced Tree Structures"
"the letter positions within the trie.","chapter-13","Advanced Tree Structures"
"These are examples of binary tries, but tries can be built with any branching","chapter-13","Advanced Tree Structures"
"factor. Normally the branching factor is determined by the alphabet used. For","chapter-13","Advanced Tree Structures"
"binary numbers, the alphabet is {0, 1} and a binary trie results. Other alphabets","chapter-13","Advanced Tree Structures"
"lead to other branching factors.","chapter-13","Advanced Tree Structures"
"One application for tries is to store a dictionary of words. Such a trie will be","chapter-13","Advanced Tree Structures"
"referred to as an alphabet trie. For simplicity, our examples will ignore case in","chapter-13","Advanced Tree Structures"
"letters. We add a special character ($) to the 26 standard English letters. The $","chapter-13","Advanced Tree Structures"
"character is used to represent the end of a string. Thus, the branching factor for","chapter-13","Advanced Tree Structures"
"Sec. 13.1 Tries 431","chapter-13","Advanced Tree Structures"
"0","chapter-13","Advanced Tree Structures"
"0","chapter-13","Advanced Tree Structures"
"0","chapter-13","Advanced Tree Structures"
"2 7","chapter-13","Advanced Tree Structures"
"1","chapter-13","Advanced Tree Structures"
"24","chapter-13","Advanced Tree Structures"
"1","chapter-13","Advanced Tree Structures"
"1","chapter-13","Advanced Tree Structures"
"1","chapter-13","Advanced Tree Structures"
"120","chapter-13","Advanced Tree Structures"
"0","chapter-13","Advanced Tree Structures"
"0 1","chapter-13","Advanced Tree Structures"
"1","chapter-13","Advanced Tree Structures"
"32","chapter-13","Advanced Tree Structures"
"0","chapter-13","Advanced Tree Structures"
"0 1","chapter-13","Advanced Tree Structures"
"40 42","chapter-13","Advanced Tree Structures"
"37","chapter-13","Advanced Tree Structures"
"0","chapter-13","Advanced Tree Structures"
"0","chapter-13","Advanced Tree Structures"
"0","chapter-13","Advanced Tree Structures"
"Figure 13.1 The binary trie for the collection of values 2, 7, 24, 31, 37, 40, 42,","chapter-13","Advanced Tree Structures"
"120. All data values are stored in the leaf nodes. Edges are labeled with the value","chapter-13","Advanced Tree Structures"
"of the bit used to determine the branching direction of each node. The binary","chapter-13","Advanced Tree Structures"
"form of the key value determines the path to the record, assuming that each key is","chapter-13","Advanced Tree Structures"
"represented as a 7-bit value representing a number in the range 0 to 127.","chapter-13","Advanced Tree Structures"
"each node is (up to) 27. Once constructed, the alphabet trie is used to determine","chapter-13","Advanced Tree Structures"
"if a given word is in the dictionary. Consider searching for a word in the alphabet","chapter-13","Advanced Tree Structures"
"trie of Figure 13.2. The first letter of the search word determines which branch","chapter-13","Advanced Tree Structures"
"to take from the root, the second letter determines which branch to take at the","chapter-13","Advanced Tree Structures"
"next level, and so on. Only the letters that lead to a word are shown as branches.","chapter-13","Advanced Tree Structures"
"In Figure 13.2(b) the leaf nodes of the trie store a copy of the actual words, while","chapter-13","Advanced Tree Structures"
"in Figure 13.2(a) the word is built up from the letters associated with each branch.","chapter-13","Advanced Tree Structures"
"One way to implement a node of the alphabet trie is as an array of 27 pointers","chapter-13","Advanced Tree Structures"
"indexed by letter. Because most nodes have branches to only a small fraction of the","chapter-13","Advanced Tree Structures"
"possible letters in the alphabet, an alternate implementation is to use a linked list of","chapter-13","Advanced Tree Structures"
"pointers to the child nodes, as in Figure 6.9.","chapter-13","Advanced Tree Structures"
"The depth of a leaf node in the alphabet trie of Figure 13.2(b) has little to do","chapter-13","Advanced Tree Structures"
"with the number of nodes in the trie, or even with the length of the corresponding","chapter-13","Advanced Tree Structures"
"string. Rather, a node’s depth depends on the number of characters required to","chapter-13","Advanced Tree Structures"
"distinguish this node’s word from any other. For example, if the words “anteater”","chapter-13","Advanced Tree Structures"
"and “antelope” are both stored in the trie, it is not until the fifth letter that the two","chapter-13","Advanced Tree Structures"
"words can be distinguished. Thus, these words must be stored at least as deep as","chapter-13","Advanced Tree Structures"
"level five. In general, the limiting factor on the depth of nodes in the alphabet trie","chapter-13","Advanced Tree Structures"
"is the length of the words stored.","chapter-13","Advanced Tree Structures"
"Poor balance and clumping can result when certain prefixes are heavily used.","chapter-13","Advanced Tree Structures"
"For example, an alphabet trie storing the common words in the English language","chapter-13","Advanced Tree Structures"
"would have many words in the “th” branch of the tree, but none in the “zq” branch.","chapter-13","Advanced Tree Structures"
"Any multiway branching trie can be replaced with a binary trie by replacing the","chapter-13","Advanced Tree Structures"
"original trie’s alphabet with an equivalent binary code. Alternatively, we can use","chapter-13","Advanced Tree Structures"
"the techniques of Section 6.3.4 for converting a general tree to a binary tree without","chapter-13","Advanced Tree Structures"
"modifying the alphabet.","chapter-13","Advanced Tree Structures"
"432 Chap. 13 Advanced Tree Structures","chapter-13","Advanced Tree Structures"
"e","chapter-13","Advanced Tree Structures"
"l","chapter-13","Advanced Tree Structures"
"u","chapter-13","Advanced Tree Structures"
"o","chapter-13","Advanced Tree Structures"
"(b)","chapter-13","Advanced Tree Structures"
"ant","chapter-13","Advanced Tree Structures"
"e","chapter-13","Advanced Tree Structures"
"l","chapter-13","Advanced Tree Structures"
"chicken","chapter-13","Advanced Tree Structures"
"d","chapter-13","Advanced Tree Structures"
"u","chapter-13","Advanced Tree Structures"
"deer duck","chapter-13","Advanced Tree Structures"
"g h","chapter-13","Advanced Tree Structures"
"l o","chapter-13","Advanced Tree Structures"
"horse","chapter-13","Advanced Tree Structures"
"goat goldfish goose","chapter-13","Advanced Tree Structures"
"antelope","chapter-13","Advanced Tree Structures"
"(a)","chapter-13","Advanced Tree Structures"
"n","chapter-13","Advanced Tree Structures"
"t","chapter-13","Advanced Tree Structures"
"$","chapter-13","Advanced Tree Structures"
"a","chapter-13","Advanced Tree Structures"
"t","chapter-13","Advanced Tree Structures"
"e","chapter-13","Advanced Tree Structures"
"r","chapter-13","Advanced Tree Structures"
"$","chapter-13","Advanced Tree Structures"
"o","chapter-13","Advanced Tree Structures"
"p","chapter-13","Advanced Tree Structures"
"e","chapter-13","Advanced Tree Structures"
"$","chapter-13","Advanced Tree Structures"
"c","chapter-13","Advanced Tree Structures"
"h","chapter-13","Advanced Tree Structures"
"i","chapter-13","Advanced Tree Structures"
"c","chapter-13","Advanced Tree Structures"
"k","chapter-13","Advanced Tree Structures"
"n","chapter-13","Advanced Tree Structures"
"$","chapter-13","Advanced Tree Structures"
"e","chapter-13","Advanced Tree Structures"
"r","chapter-13","Advanced Tree Structures"
"$","chapter-13","Advanced Tree Structures"
"c","chapter-13","Advanced Tree Structures"
"k","chapter-13","Advanced Tree Structures"
"$","chapter-13","Advanced Tree Structures"
"g","chapter-13","Advanced Tree Structures"
"o","chapter-13","Advanced Tree Structures"
"a","chapter-13","Advanced Tree Structures"
"$","chapter-13","Advanced Tree Structures"
"l","chapter-13","Advanced Tree Structures"
"d","chapter-13","Advanced Tree Structures"
"f","chapter-13","Advanced Tree Structures"
"i","chapter-13","Advanced Tree Structures"
"s","chapter-13","Advanced Tree Structures"
"h","chapter-13","Advanced Tree Structures"
"$","chapter-13","Advanced Tree Structures"
"r","chapter-13","Advanced Tree Structures"
"s","chapter-13","Advanced Tree Structures"
"e","chapter-13","Advanced Tree Structures"
"a d","chapter-13","Advanced Tree Structures"
"t","chapter-13","Advanced Tree Structures"
"o","chapter-13","Advanced Tree Structures"
"h","chapter-13","Advanced Tree Structures"
"$","chapter-13","Advanced Tree Structures"
"e","chapter-13","Advanced Tree Structures"
"e","chapter-13","Advanced Tree Structures"
"s","chapter-13","Advanced Tree Structures"
"e","chapter-13","Advanced Tree Structures"
"$","chapter-13","Advanced Tree Structures"
"a","chapter-13","Advanced Tree Structures"
"n","chapter-13","Advanced Tree Structures"
"t","chapter-13","Advanced Tree Structures"
"$","chapter-13","Advanced Tree Structures"
"a","chapter-13","Advanced Tree Structures"
"c","chapter-13","Advanced Tree Structures"
"e o","chapter-13","Advanced Tree Structures"
"a","chapter-13","Advanced Tree Structures"
"anteater","chapter-13","Advanced Tree Structures"
"Figure 13.2 Two variations on the alphabet trie representation for a set of ten","chapter-13","Advanced Tree Structures"
"words. (a) Each node contains a set of links corresponding to single letters, and","chapter-13","Advanced Tree Structures"
"each letter in the set of words has a corresponding link. “$” is used to indicate","chapter-13","Advanced Tree Structures"
"the end of a word. Internal nodes direct the search and also spell out the word","chapter-13","Advanced Tree Structures"
"one letter per link. The word need not be stored explicitly. “$” is needed to","chapter-13","Advanced Tree Structures"
"recognize the existence of words that are prefixes to other words, such as ‘ant’ in","chapter-13","Advanced Tree Structures"
"this example. (b) Here the trie extends only far enough to discriminate between the","chapter-13","Advanced Tree Structures"
"words. Leaf nodes of the trie each store a complete word; internal nodes merely","chapter-13","Advanced Tree Structures"
"direct the search.","chapter-13","Advanced Tree Structures"
"Sec. 13.1 Tries 433","chapter-13","Advanced Tree Structures"
"1xxxxxx","chapter-13","Advanced Tree Structures"
"0","chapter-13","Advanced Tree Structures"
"120","chapter-13","Advanced Tree Structures"
"00xxxxx 01xxxxx","chapter-13","Advanced Tree Structures"
"2 3","chapter-13","Advanced Tree Structures"
"0101xxx","chapter-13","Advanced Tree Structures"
"4","chapter-13","Advanced Tree Structures"
"24 4 5","chapter-13","Advanced Tree Structures"
"010101x","chapter-13","Advanced Tree Structures"
"2 7 32 37 40 42","chapter-13","Advanced Tree Structures"
"000xxxx","chapter-13","Advanced Tree Structures"
"0xxxxxx","chapter-13","Advanced Tree Structures"
"1","chapter-13","Advanced Tree Structures"
"Figure 13.3 The PAT trie for the collection of values 2, 7, 24, 32, 37, 40, 42,","chapter-13","Advanced Tree Structures"
"120. Contrast this with the binary trie of Figure 13.1. In the PAT trie, all data","chapter-13","Advanced Tree Structures"
"values are stored in the leaf nodes, while internal nodes store the bit position used","chapter-13","Advanced Tree Structures"
"to determine the branching decision, assuming that each key is represented as a 7-","chapter-13","Advanced Tree Structures"
"bit value representing a number in the range 0 to 127. Some of the branches in this","chapter-13","Advanced Tree Structures"
"PAT trie have been labeled to indicate the binary representation for all values in","chapter-13","Advanced Tree Structures"
"that subtree. For example, all values in the left subtree of the node labeled 0 must","chapter-13","Advanced Tree Structures"
"have value 0xxxxxx (where x means that bit can be either a 0 or a 1). All nodes in","chapter-13","Advanced Tree Structures"
"the right subtree of the node labeled 3 must have value 0101xxx. However, we can","chapter-13","Advanced Tree Structures"
"skip branching on bit 2 for this subtree because all values currently stored have a","chapter-13","Advanced Tree Structures"
"value of 0 for that bit.","chapter-13","Advanced Tree Structures"
"The trie implementations illustrated by Figures 13.1 and 13.2 are potentially","chapter-13","Advanced Tree Structures"
"quite inefficient as certain key sets might lead to a large number of nodes with only","chapter-13","Advanced Tree Structures"
"a single child. A variant on trie implementation is known as PATRICIA, which","chapter-13","Advanced Tree Structures"
"stands for “Practical Algorithm To Retrieve Information Coded In Alphanumeric.”","chapter-13","Advanced Tree Structures"
"In the case of a binary alphabet, a PATRICIA trie (referred to hereafter as a PAT","chapter-13","Advanced Tree Structures"
"trie) is a full binary tree that stores data records in the leaf nodes. Internal nodes","chapter-13","Advanced Tree Structures"
"store only the position within the key’s bit pattern that is used to decide on the next","chapter-13","Advanced Tree Structures"
"branching point. In this way, internal nodes with single children (equivalently, bit","chapter-13","Advanced Tree Structures"
"positions within the key that do not distinguish any of the keys within the current","chapter-13","Advanced Tree Structures"
"subtree) are eliminated. A PAT trie corresponding to the values of Figure 13.1 is","chapter-13","Advanced Tree Structures"
"shown in Figure 13.3.","chapter-13","Advanced Tree Structures"
"Example 13.1 When searching for the value 7 (0000111 in binary) in","chapter-13","Advanced Tree Structures"
"the PAT trie of Figure 13.3, the root node indicates that bit position 0 (the","chapter-13","Advanced Tree Structures"
"leftmost bit) is checked first. Because the 0th bit for value 7 is 0, take the","chapter-13","Advanced Tree Structures"
"left branch. At level 1, branch depending on the value of bit 1, which again","chapter-13","Advanced Tree Structures"
"is 0. At level 2, branch depending on the value of bit 2, which again is 0. At","chapter-13","Advanced Tree Structures"
"level 3, the index stored in the node is 4. This means that bit 4 of the key is","chapter-13","Advanced Tree Structures"
"checked next. (The value of bit 3 is irrelevant, because all values stored in","chapter-13","Advanced Tree Structures"
"that subtree have the same value at bit position 3.) Thus, the single branch","chapter-13","Advanced Tree Structures"
"that extends from the equivalent node in Figure 13.1 is just skipped. For","chapter-13","Advanced Tree Structures"
"key value 7, bit 4 has value 1, so the rightmost branch is taken. Because","chapter-13","Advanced Tree Structures"
"434 Chap. 13 Advanced Tree Structures","chapter-13","Advanced Tree Structures"
"this leads to a leaf node, the search key is compared against the key stored","chapter-13","Advanced Tree Structures"
"in that node. If they match, then the desired record has been found.","chapter-13","Advanced Tree Structures"
"Note that during the search process, only a single bit of the search key is com-","chapter-13","Advanced Tree Structures"
"pared at each internal node. This is significant, because the search key could be","chapter-13","Advanced Tree Structures"
"quite large. Search in the PAT trie requires only a single full-key comparison,","chapter-13","Advanced Tree Structures"
"which takes place once a leaf node has been reached.","chapter-13","Advanced Tree Structures"
"Example 13.2 Consider the situation where we need to store a library of","chapter-13","Advanced Tree Structures"
"DNA sequences. A DNA sequence is a series of letters, usually many thou-","chapter-13","Advanced Tree Structures"
"sands of characters long, with the string coming from an alphabet of only","chapter-13","Advanced Tree Structures"
"four letters that stand for the four amino acids making up a DNA strand.","chapter-13","Advanced Tree Structures"
"Similar DNA sequences might have long sections of their string that are","chapter-13","Advanced Tree Structures"
"identical. The PAT trie would avoid making multiple full key comparisons","chapter-13","Advanced Tree Structures"
"when searching for a specific sequence.","chapter-13","Advanced Tree Structures"
"13.2 Balanced Trees","chapter-13","Advanced Tree Structures"
"We have noted several times that the BST has a high risk of becoming unbalanced,","chapter-13","Advanced Tree Structures"
"resulting in excessively expensive search and update operations. One solution to","chapter-13","Advanced Tree Structures"
"this problem is to adopt another search tree structure such as the 2-3 tree or the","chapter-13","Advanced Tree Structures"
"binary trie. An alternative is to modify the BST access functions in some way to","chapter-13","Advanced Tree Structures"
"guarantee that the tree performs well. This is an appealing concept, and it works","chapter-13","Advanced Tree Structures"
"well for heaps, whose access functions maintain the heap in the shape of a complete","chapter-13","Advanced Tree Structures"
"binary tree. Unfortunately, requiring that the BST always be in the shape of a","chapter-13","Advanced Tree Structures"
"complete binary tree requires excessive modification to the tree during update, as","chapter-13","Advanced Tree Structures"
"discussed in Section 10.3.","chapter-13","Advanced Tree Structures"
"If we are willing to weaken the balance requirements, we can come up with","chapter-13","Advanced Tree Structures"
"alternative update routines that perform well both in terms of cost for the update","chapter-13","Advanced Tree Structures"
"and in balance for the resulting tree structure. The AVL tree works in this way,","chapter-13","Advanced Tree Structures"
"using insertion and deletion routines altered from those of the BST to ensure that,","chapter-13","Advanced Tree Structures"
"for every node, the depths of the left and right subtrees differ by at most one. The","chapter-13","Advanced Tree Structures"
"AVL tree is described in Section 13.2.1.","chapter-13","Advanced Tree Structures"
"A different approach to improving the performance of the BST is to not require","chapter-13","Advanced Tree Structures"
"that the tree always be balanced, but rather to expend some effort toward making","chapter-13","Advanced Tree Structures"
"the BST more balanced every time it is accessed. This is a little like the idea of path","chapter-13","Advanced Tree Structures"
"compression used by the UNION/FIND algorithm presented in Section 6.2. One","chapter-13","Advanced Tree Structures"
"example of such a compromise is called the splay tree. The splay tree is described","chapter-13","Advanced Tree Structures"
"in Section 13.2.2.","chapter-13","Advanced Tree Structures"
"Sec. 13.2 Balanced Trees 435","chapter-13","Advanced Tree Structures"
"7","chapter-13","Advanced Tree Structures"
"2","chapter-13","Advanced Tree Structures"
"32","chapter-13","Advanced Tree Structures"
"42","chapter-13","Advanced Tree Structures"
"40","chapter-13","Advanced Tree Structures"
"120","chapter-13","Advanced Tree Structures"
"37","chapter-13","Advanced Tree Structures"
"42","chapter-13","Advanced Tree Structures"
"24","chapter-13","Advanced Tree Structures"
"7","chapter-13","Advanced Tree Structures"
"2","chapter-13","Advanced Tree Structures"
"32","chapter-13","Advanced Tree Structures"
"42","chapter-13","Advanced Tree Structures"
"40","chapter-13","Advanced Tree Structures"
"120","chapter-13","Advanced Tree Structures"
"37","chapter-13","Advanced Tree Structures"
"42","chapter-13","Advanced Tree Structures"
"24","chapter-13","Advanced Tree Structures"
"5","chapter-13","Advanced Tree Structures"
"Figure 13.4 Example of an insert operation that violates the AVL tree balance","chapter-13","Advanced Tree Structures"
"property. Prior to the insert operation, all nodes of the tree are balanced (i.e., the","chapter-13","Advanced Tree Structures"
"depths of the left and right subtrees for every node differ by at most one). After","chapter-13","Advanced Tree Structures"
"inserting the node with value 5, the nodes with values 7 and 24 are no longer","chapter-13","Advanced Tree Structures"
"balanced.","chapter-13","Advanced Tree Structures"
"13.2.1 The AVL Tree","chapter-13","Advanced Tree Structures"
"The AVL tree (named for its inventors Adelson-Velskii and Landis) should be","chapter-13","Advanced Tree Structures"
"viewed as a BST with the following additional property: For every node, the heights","chapter-13","Advanced Tree Structures"
"of its left and right subtrees differ by at most 1. As long as the tree maintains this","chapter-13","Advanced Tree Structures"
"property, if the tree contains n nodes, then it has a depth of at most O(log n). As","chapter-13","Advanced Tree Structures"
"a result, search for any node will cost O(log n), and if the updates can be done in","chapter-13","Advanced Tree Structures"
"time proportional to the depth of the node inserted or deleted, then updates will also","chapter-13","Advanced Tree Structures"
"cost O(log n), even in the worst case.","chapter-13","Advanced Tree Structures"
"The key to making the AVL tree work is to alter the insert and delete routines","chapter-13","Advanced Tree Structures"
"so as to maintain the balance property. Of course, to be practical, we must be able","chapter-13","Advanced Tree Structures"
"to implement the revised update routines in Θ(log n) time.","chapter-13","Advanced Tree Structures"
"Consider what happens when we insert a node with key value 5, as shown in","chapter-13","Advanced Tree Structures"
"Figure 13.4. The tree on the left meets the AVL tree balance requirements. After","chapter-13","Advanced Tree Structures"
"the insertion, two nodes no longer meet the requirements. Because the original tree","chapter-13","Advanced Tree Structures"
"met the balance requirement, nodes in the new tree can only be unbalanced by a","chapter-13","Advanced Tree Structures"
"difference of at most 2 in the subtrees. For the bottommost unbalanced node, call","chapter-13","Advanced Tree Structures"
"it S, there are 4 cases:","chapter-13","Advanced Tree Structures"
"1. The extra node is in the left child of the left child of S.","chapter-13","Advanced Tree Structures"
"2. The extra node is in the right child of the left child of S.","chapter-13","Advanced Tree Structures"
"3. The extra node is in the left child of the right child of S.","chapter-13","Advanced Tree Structures"
"4. The extra node is in the right child of the right child of S.","chapter-13","Advanced Tree Structures"
"Cases 1 and 4 are symmetrical, as are cases 2 and 3. Note also that the unbalanced","chapter-13","Advanced Tree Structures"
"nodes must be on the path from the root to the newly inserted node.","chapter-13","Advanced Tree Structures"
"Our problem now is how to balance the tree in O(log n) time. It turns out that","chapter-13","Advanced Tree Structures"
"we can do this using a series of local operations known as rotations. Cases 1 and","chapter-13","Advanced Tree Structures"
"436 Chap. 13 Advanced Tree Structures","chapter-13","Advanced Tree Structures"
"S","chapter-13","Advanced Tree Structures"
"X C","chapter-13","Advanced Tree Structures"
"X","chapter-13","Advanced Tree Structures"
"S","chapter-13","Advanced Tree Structures"
"B B C","chapter-13","Advanced Tree Structures"
"A","chapter-13","Advanced Tree Structures"
"(a)","chapter-13","Advanced Tree Structures"
"A","chapter-13","Advanced Tree Structures"
"(b)","chapter-13","Advanced Tree Structures"
"Figure 13.5 A single rotation in an AVL tree. This operation occurs when the","chapter-13","Advanced Tree Structures"
"excess node (in subtree A) is in the left child of the left child of the unbalanced","chapter-13","Advanced Tree Structures"
"node labeled S. By rearranging the nodes as shown, we preserve the BST property,","chapter-13","Advanced Tree Structures"
"as well as re-balance the tree to preserve the AVL tree balance property. The case","chapter-13","Advanced Tree Structures"
"where the excess node is in the right child of the right child of the unbalanced","chapter-13","Advanced Tree Structures"
"node is handled in the same way.","chapter-13","Advanced Tree Structures"
"Y","chapter-13","Advanced Tree Structures"
"S","chapter-13","Advanced Tree Structures"
"Y","chapter-13","Advanced Tree Structures"
"X","chapter-13","Advanced Tree Structures"
"A","chapter-13","Advanced Tree Structures"
"B","chapter-13","Advanced Tree Structures"
"S","chapter-13","Advanced Tree Structures"
"C","chapter-13","Advanced Tree Structures"
"D","chapter-13","Advanced Tree Structures"
"X","chapter-13","Advanced Tree Structures"
"C","chapter-13","Advanced Tree Structures"
"A","chapter-13","Advanced Tree Structures"
"(a)","chapter-13","Advanced Tree Structures"
"B","chapter-13","Advanced Tree Structures"
"D","chapter-13","Advanced Tree Structures"
"(b)","chapter-13","Advanced Tree Structures"
"Figure 13.6 A double rotation in an AVL tree. This operation occurs when the","chapter-13","Advanced Tree Structures"
"excess node (in subtree B) is in the right child of the left child of the unbalanced","chapter-13","Advanced Tree Structures"
"node labeled S. By rearranging the nodes as shown, we preserve the BST property,","chapter-13","Advanced Tree Structures"
"as well as re-balance the tree to preserve the AVL tree balance property. The case","chapter-13","Advanced Tree Structures"
"where the excess node is in the left child of the right child of S is handled in the","chapter-13","Advanced Tree Structures"
"same way.","chapter-13","Advanced Tree Structures"
"4 can be fixed using a single rotation, as shown in Figure 13.5. Cases 2 and 3 can","chapter-13","Advanced Tree Structures"
"be fixed using a double rotation, as shown in Figure 13.6.","chapter-13","Advanced Tree Structures"
"The AVL tree insert algorithm begins with a normal BST insert. Then as the","chapter-13","Advanced Tree Structures"
"recursion unwinds up the tree, we perform the appropriate rotation on any node","chapter-13","Advanced Tree Structures"
"Sec. 13.2 Balanced Trees 437","chapter-13","Advanced Tree Structures"
"that is found to be unbalanced. Deletion is similar; however, consideration for","chapter-13","Advanced Tree Structures"
"unbalanced nodes must begin at the level of the deletemin operation.","chapter-13","Advanced Tree Structures"
"Example 13.3 In Figure 13.4 (b), the bottom-most unbalanced node has","chapter-13","Advanced Tree Structures"
"value 7. The excess node (with value 5) is in the right subtree of the left","chapter-13","Advanced Tree Structures"
"child of 7, so we have an example of Case 2. This requires a double rotation","chapter-13","Advanced Tree Structures"
"to fix. After the rotation, 5 becomes the left child of 24, 2 becomes the left","chapter-13","Advanced Tree Structures"
"child of 5, and 7 becomes the right child of 5.","chapter-13","Advanced Tree Structures"
"13.2.2 The Splay Tree","chapter-13","Advanced Tree Structures"
"Like the AVL tree, the splay tree is not actually a distinct data structure, but rather","chapter-13","Advanced Tree Structures"
"reimplements the BST insert, delete, and search methods to improve the perfor-","chapter-13","Advanced Tree Structures"
"mance of a BST. The goal of these revised methods is to provide guarantees on the","chapter-13","Advanced Tree Structures"
"time required by a series of operations, thereby avoiding the worst-case linear time","chapter-13","Advanced Tree Structures"
"behavior of standard BST operations. No single operation in the splay tree is guar-","chapter-13","Advanced Tree Structures"
"anteed to be efficient. Instead, the splay tree access rules guarantee that a series","chapter-13","Advanced Tree Structures"
"of m operations will take O(m log n) time for a tree of n nodes whenever m ≥ n.","chapter-13","Advanced Tree Structures"
"Thus, a single insert or search operation could take O(n) time. However, m such","chapter-13","Advanced Tree Structures"
"operations are guaranteed to require a total of O(m log n) time, for an average cost","chapter-13","Advanced Tree Structures"
"of O(log n) per access operation. This is a desirable performance guarantee for any","chapter-13","Advanced Tree Structures"
"search-tree structure.","chapter-13","Advanced Tree Structures"
"Unlike the AVL tree, the splay tree is not guaranteed to be height balanced.","chapter-13","Advanced Tree Structures"
"What is guaranteed is that the total cost of the entire series of accesses will be","chapter-13","Advanced Tree Structures"
"cheap. Ultimately, it is the cost of the series of operations that matters, not whether","chapter-13","Advanced Tree Structures"
"the tree is balanced. Maintaining balance is really done only for the sake of reaching","chapter-13","Advanced Tree Structures"
"this time efficiency goal.","chapter-13","Advanced Tree Structures"
"The splay tree access functions operate in a manner reminiscent of the move-","chapter-13","Advanced Tree Structures"
"to-front rule for self-organizing lists from Section 9.2, and of the path compres-","chapter-13","Advanced Tree Structures"
"sion technique for managing parent-pointer trees from Section 6.2. These access","chapter-13","Advanced Tree Structures"
"functions tend to make the tree more balanced, but an individual access will not","chapter-13","Advanced Tree Structures"
"necessarily result in a more balanced tree.","chapter-13","Advanced Tree Structures"
"Whenever a node S is accessed (e.g., when S is inserted, deleted, or is the goal","chapter-13","Advanced Tree Structures"
"of a search), the splay tree performs a process called splaying. Splaying moves S","chapter-13","Advanced Tree Structures"
"to the root of the BST. When S is being deleted, splaying moves the parent of S to","chapter-13","Advanced Tree Structures"
"the root. As in the AVL tree, a splay of node S consists of a series of rotations.","chapter-13","Advanced Tree Structures"
"A rotation moves S higher in the tree by adjusting its position with respect to its","chapter-13","Advanced Tree Structures"
"parent and grandparent. A side effect of the rotations is a tendency to balance the","chapter-13","Advanced Tree Structures"
"tree. There are three types of rotation.","chapter-13","Advanced Tree Structures"
"A single rotation is performed only if S is a child of the root node. The single","chapter-13","Advanced Tree Structures"
"rotation is illustrated by Figure 13.7. It basically switches S with its parent in a","chapter-13","Advanced Tree Structures"
"438 Chap. 13 Advanced Tree Structures","chapter-13","Advanced Tree Structures"
"P","chapter-13","Advanced Tree Structures"
"S","chapter-13","Advanced Tree Structures"
"(a)","chapter-13","Advanced Tree Structures"
"C","chapter-13","Advanced Tree Structures"
"S","chapter-13","Advanced Tree Structures"
"A P","chapter-13","Advanced Tree Structures"
"A B B C","chapter-13","Advanced Tree Structures"
"(b)","chapter-13","Advanced Tree Structures"
"Figure 13.7 Splay tree single rotation. This rotation takes place only when","chapter-13","Advanced Tree Structures"
"the node being splayed is a child of the root. Here, node S is promoted to the","chapter-13","Advanced Tree Structures"
"root, rotating with node P. Because the value of S is less than the value of P,","chapter-13","Advanced Tree Structures"
"P must become S’s right child. The positions of subtrees A, B, and C are altered","chapter-13","Advanced Tree Structures"
"as appropriate to maintain the BST property, but the contents of these subtrees","chapter-13","Advanced Tree Structures"
"remains unchanged. (a) The original tree with P as the parent. (b) The tree after","chapter-13","Advanced Tree Structures"
"a rotation takes place. Performing a single rotation a second time will return the","chapter-13","Advanced Tree Structures"
"tree to its original shape. Equivalently, if (b) is the initial configuration of the tree","chapter-13","Advanced Tree Structures"
"(i.e., S is at the root and P is its right child), then (a) shows the result of a single","chapter-13","Advanced Tree Structures"
"rotation to splay P to the root.","chapter-13","Advanced Tree Structures"
"way that retains the BST property. While Figure 13.7 is slightly different from","chapter-13","Advanced Tree Structures"
"Figure 13.5, in fact the splay tree single rotation is identical to the AVL tree single","chapter-13","Advanced Tree Structures"
"rotation.","chapter-13","Advanced Tree Structures"
"Unlike the AVL tree, the splay tree requires two types of double rotation. Dou-","chapter-13","Advanced Tree Structures"
"ble rotations involve S, its parent (call it P), and S’s grandparent (call it G). The","chapter-13","Advanced Tree Structures"
"effect of a double rotation is to move S up two levels in the tree.","chapter-13","Advanced Tree Structures"
"The first double rotation is called a zigzag rotation. It takes place when either","chapter-13","Advanced Tree Structures"
"of the following two conditions are met:","chapter-13","Advanced Tree Structures"
"1. S is the left child of P, and P is the right child of G.","chapter-13","Advanced Tree Structures"
"2. S is the right child of P, and P is the left child of G.","chapter-13","Advanced Tree Structures"
"In other words, a zigzag rotation is used when G, P, and S form a zigzag. The","chapter-13","Advanced Tree Structures"
"zigzag rotation is illustrated by Figure 13.8.","chapter-13","Advanced Tree Structures"
"The other double rotation is known as a zigzig rotation. A zigzig rotation takes","chapter-13","Advanced Tree Structures"
"place when either of the following two conditions are met:","chapter-13","Advanced Tree Structures"
"1. S is the left child of P, which is in turn the left child of G.","chapter-13","Advanced Tree Structures"
"2. S is the right child of P, which is in turn the right child of G.","chapter-13","Advanced Tree Structures"
"Thus, a zigzig rotation takes place in those situations where a zigzag rotation is not","chapter-13","Advanced Tree Structures"
"appropriate. The zigzig rotation is illustrated by Figure 13.9. While Figure 13.9","chapter-13","Advanced Tree Structures"
"appears somewhat different from Figure 13.6, in fact the zigzig rotation is identical","chapter-13","Advanced Tree Structures"
"to the AVL tree double rotation.","chapter-13","Advanced Tree Structures"
"Sec. 13.2 Balanced Trees 439","chapter-13","Advanced Tree Structures"
"(a) (b)","chapter-13","Advanced Tree Structures"
"S","chapter-13","Advanced Tree Structures"
"G","chapter-13","Advanced Tree Structures"
"S","chapter-13","Advanced Tree Structures"
"P","chapter-13","Advanced Tree Structures"
"A B","chapter-13","Advanced Tree Structures"
"G","chapter-13","Advanced Tree Structures"
"C","chapter-13","Advanced Tree Structures"
"P","chapter-13","Advanced Tree Structures"
"C","chapter-13","Advanced Tree Structures"
"D","chapter-13","Advanced Tree Structures"
"A","chapter-13","Advanced Tree Structures"
"B D","chapter-13","Advanced Tree Structures"
"Figure 13.8 Splay tree zigzag rotation. (a) The original tree with S, P, and G","chapter-13","Advanced Tree Structures"
"in zigzag formation. (b) The tree after the rotation takes place. The positions of","chapter-13","Advanced Tree Structures"
"subtrees A, B, C, and D are altered as appropriate to maintain the BST property.","chapter-13","Advanced Tree Structures"
"(a)","chapter-13","Advanced Tree Structures"
"S","chapter-13","Advanced Tree Structures"
"(b)","chapter-13","Advanced Tree Structures"
"C D","chapter-13","Advanced Tree Structures"
"B","chapter-13","Advanced Tree Structures"
"G","chapter-13","Advanced Tree Structures"
"A B","chapter-13","Advanced Tree Structures"
"C","chapter-13","Advanced Tree Structures"
"S","chapter-13","Advanced Tree Structures"
"A P","chapter-13","Advanced Tree Structures"
"G","chapter-13","Advanced Tree Structures"
"P D","chapter-13","Advanced Tree Structures"
"Figure 13.9 Splay tree zigzig rotation. (a) The original tree with S, P, and G","chapter-13","Advanced Tree Structures"
"in zigzig formation. (b) The tree after the rotation takes place. The positions of","chapter-13","Advanced Tree Structures"
"subtrees A, B, C, and D are altered as appropriate to maintain the BST property.","chapter-13","Advanced Tree Structures"
"Note that zigzag rotations tend to make the tree more balanced, because they","chapter-13","Advanced Tree Structures"
"bring subtrees B and C up one level while moving subtree D down one level. The","chapter-13","Advanced Tree Structures"
"result is often a reduction of the tree’s height by one. Zigzig promotions and single","chapter-13","Advanced Tree Structures"
"rotations do not typically reduce the height of the tree; they merely bring the newly","chapter-13","Advanced Tree Structures"
"accessed record toward the root.","chapter-13","Advanced Tree Structures"
"Splaying node S involves a series of double rotations until S reaches either the","chapter-13","Advanced Tree Structures"
"root or the child of the root. Then, if necessary, a single rotation makes S the root.","chapter-13","Advanced Tree Structures"
"This process tends to re-balance the tree. Regardless of balance, splaying will make","chapter-13","Advanced Tree Structures"
"frequently accessed nodes stay near the top of the tree, resulting in reduced access","chapter-13","Advanced Tree Structures"
"cost. Proof that the splay tree meets the guarantee of O(m log n) is beyond the","chapter-13","Advanced Tree Structures"
"scope of this book. Such a proof can be found in the references in Section 13.4.","chapter-13","Advanced Tree Structures"
"440 Chap. 13 Advanced Tree Structures","chapter-13","Advanced Tree Structures"
"Example 13.4 Consider a search for value 89 in the splay tree of Fig-","chapter-13","Advanced Tree Structures"
"ure 13.10(a). The splay tree’s search operation is identical to searching in","chapter-13","Advanced Tree Structures"
"a BST. However, once the value has been found, it is splayed to the root.","chapter-13","Advanced Tree Structures"
"Three rotations are required in this example. The first is a zigzig rotation,","chapter-13","Advanced Tree Structures"
"whose result is shown in Figure 13.10(b). The second is a zigzag rotation,","chapter-13","Advanced Tree Structures"
"whose result is shown in Figure 13.10(c). The final step is a single rotation","chapter-13","Advanced Tree Structures"
"resulting in the tree of Figure 13.10(d). Notice that the splaying process has","chapter-13","Advanced Tree Structures"
"made the tree shallower.","chapter-13","Advanced Tree Structures"
"13.3 Spatial Data Structures","chapter-13","Advanced Tree Structures"
"All of the search trees discussed so far — BSTs, AVL trees, splay trees, 2-3 trees,","chapter-13","Advanced Tree Structures"
"B-trees, and tries — are designed for searching on a one-dimensional key. A typical","chapter-13","Advanced Tree Structures"
"example is an integer key, whose one-dimensional range can be visualized as a","chapter-13","Advanced Tree Structures"
"number line. These various tree structures can be viewed as dividing this one-","chapter-13","Advanced Tree Structures"
"dimensional number line into pieces.","chapter-13","Advanced Tree Structures"
"Some databases require support for multiple keys. In other words, records can","chapter-13","Advanced Tree Structures"
"be searched for using any one of several key fields, such as name or ID number.","chapter-13","Advanced Tree Structures"
"Typically, each such key has its own one-dimensional index, and any given search","chapter-13","Advanced Tree Structures"
"query searches one of these independent indices as appropriate.","chapter-13","Advanced Tree Structures"
"A multidimensional search key presents a rather different concept. Imagine","chapter-13","Advanced Tree Structures"
"that we have a database of city records, where each city has a name and an xy-","chapter-13","Advanced Tree Structures"
"coordinate. A BST or splay tree provides good performance for searches on city","chapter-13","Advanced Tree Structures"
"name, which is a one-dimensional key. Separate BSTs could be used to index the x-","chapter-13","Advanced Tree Structures"
"and y-coordinates. This would allow us to insert and delete cities, and locate them","chapter-13","Advanced Tree Structures"
"by name or by one coordinate. However, search on one of the two coordinates is","chapter-13","Advanced Tree Structures"
"not a natural way to view search in a two-dimensional space. Another option is to","chapter-13","Advanced Tree Structures"
"combine the xy-coordinates into a single key, say by concatenating the two coor-","chapter-13","Advanced Tree Structures"
"dinates, and index cities by the resulting key in a BST. That would allow search by","chapter-13","Advanced Tree Structures"
"coordinate, but would not allow for efficient two-dimensional range queries such","chapter-13","Advanced Tree Structures"
"as searching for all cities within a given distance of a specified point. The problem","chapter-13","Advanced Tree Structures"
"is that the BST only works well for one-dimensional keys, while a coordinate is a","chapter-13","Advanced Tree Structures"
"two-dimensional key where neither dimension is more important than the other.","chapter-13","Advanced Tree Structures"
"Multidimensional range queries are the defining feature of a spatial applica-","chapter-13","Advanced Tree Structures"
"tion. Because a coordinate gives a position in space, it is called a spatial attribute.","chapter-13","Advanced Tree Structures"
"To implement spatial applications efficiently requires the use of spatial data struc-","chapter-13","Advanced Tree Structures"
"tures. Spatial data structures store data objects organized by position and are an","chapter-13","Advanced Tree Structures"
"important class of data structures used in geographic information systems, com-","chapter-13","Advanced Tree Structures"
"puter graphics, robotics, and many other fields.","chapter-13","Advanced Tree Structures"
"Sec. 13.3 Spatial Data Structures 441","chapter-13","Advanced Tree Structures"
"S","chapter-13","Advanced Tree Structures"
"25","chapter-13","Advanced Tree Structures"
"42","chapter-13","Advanced Tree Structures"
"99","chapter-13","Advanced Tree Structures"
"G","chapter-13","Advanced Tree Structures"
"P","chapter-13","Advanced Tree Structures"
"S","chapter-13","Advanced Tree Structures"
"75","chapter-13","Advanced Tree Structures"
"17","chapter-13","Advanced Tree Structures"
"G","chapter-13","Advanced Tree Structures"
"99","chapter-13","Advanced Tree Structures"
"18","chapter-13","Advanced Tree Structures"
"72","chapter-13","Advanced Tree Structures"
"42 75","chapter-13","Advanced Tree Structures"
"(a) (b)","chapter-13","Advanced Tree Structures"
"18","chapter-13","Advanced Tree Structures"
"72","chapter-13","Advanced Tree Structures"
"89","chapter-13","Advanced Tree Structures"
"(c) (d)","chapter-13","Advanced Tree Structures"
"17 P","chapter-13","Advanced Tree Structures"
"S","chapter-13","Advanced Tree Structures"
"25","chapter-13","Advanced Tree Structures"
"18 72","chapter-13","Advanced Tree Structures"
"42","chapter-13","Advanced Tree Structures"
"89","chapter-13","Advanced Tree Structures"
"92","chapter-13","Advanced Tree Structures"
"18 72","chapter-13","Advanced Tree Structures"
"42 75","chapter-13","Advanced Tree Structures"
"99","chapter-13","Advanced Tree Structures"
"89 17","chapter-13","Advanced Tree Structures"
"92 25","chapter-13","Advanced Tree Structures"
"99","chapter-13","Advanced Tree Structures"
"75","chapter-13","Advanced Tree Structures"
"92","chapter-13","Advanced Tree Structures"
"25","chapter-13","Advanced Tree Structures"
"89","chapter-13","Advanced Tree Structures"
"P","chapter-13","Advanced Tree Structures"
"17","chapter-13","Advanced Tree Structures"
"92","chapter-13","Advanced Tree Structures"
"Figure 13.10 Example of splaying after performing a search in a splay tree.","chapter-13","Advanced Tree Structures"
"After finding the node with key value 89, that node is splayed to the root by per-","chapter-13","Advanced Tree Structures"
"forming three rotations. (a) The original splay tree. (b) The result of performing","chapter-13","Advanced Tree Structures"
"a zigzig rotation on the node with key value 89 in the tree of (a). (c) The result","chapter-13","Advanced Tree Structures"
"of performing a zigzag rotation on the node with key value 89 in the tree of (b).","chapter-13","Advanced Tree Structures"
"(d) The result of performing a single rotation on the node with key value 89 in the","chapter-13","Advanced Tree Structures"
"tree of (c). If the search had been for 91, the search would have been unsuccessful","chapter-13","Advanced Tree Structures"
"with the node storing key value 89 being that last one visited. In that case, the","chapter-13","Advanced Tree Structures"
"same splay operations would take place.","chapter-13","Advanced Tree Structures"
"442 Chap. 13 Advanced Tree Structures","chapter-13","Advanced Tree Structures"
"This section presents two spatial data structures for storing point data in two or","chapter-13","Advanced Tree Structures"
"more dimensions. They are the k-d tree and the PR quadtree. The k-d tree is a","chapter-13","Advanced Tree Structures"
"natural extension of the BST to multiple dimensions. It is a binary tree whose split-","chapter-13","Advanced Tree Structures"
"ting decisions alternate among the key dimensions. Like the BST, the k-d tree uses","chapter-13","Advanced Tree Structures"
"object space decomposition. The PR quadtree uses key space decomposition and so","chapter-13","Advanced Tree Structures"
"is a form of trie. It is a binary tree only for one-dimensional keys (in which case it","chapter-13","Advanced Tree Structures"
"is a trie with a binary alphabet). For d dimensions it has 2","chapter-13","Advanced Tree Structures"
"d branches. Thus, in two","chapter-13","Advanced Tree Structures"
"dimensions, the PR quadtree has four branches (hence the name “quadtree”), split-","chapter-13","Advanced Tree Structures"
"ting space into four equal-sized quadrants at each branch. Section 13.3.3 briefly","chapter-13","Advanced Tree Structures"
"mentions two other variations on these data structures, the bintree and the point","chapter-13","Advanced Tree Structures"
"quadtree. These four structures cover all four combinations of object versus key","chapter-13","Advanced Tree Structures"
"space decomposition on the one hand, and multi-level binary versus 2","chapter-13","Advanced Tree Structures"
"d","chapter-13","Advanced Tree Structures"
"-way branch-","chapter-13","Advanced Tree Structures"
"ing on the other. Section 13.3.4 briefly discusses spatial data structures for storing","chapter-13","Advanced Tree Structures"
"other types of spatial data.","chapter-13","Advanced Tree Structures"
"13.3.1 The K-D Tree","chapter-13","Advanced Tree Structures"
"The k-d tree is a modification to the BST that allows for efficient processing of","chapter-13","Advanced Tree Structures"
"multidimensional keys. The k-d tree differs from the BST in that each level of","chapter-13","Advanced Tree Structures"
"the k-d tree makes branching decisions based on a particular search key associated","chapter-13","Advanced Tree Structures"
"with that level, called the discriminator. In principle, the k-d tree could be used to","chapter-13","Advanced Tree Structures"
"unify key searching across any arbitrary set of keys such as name and zipcode. But","chapter-13","Advanced Tree Structures"
"in practice, it is nearly always used to support search on multidimensional coordi-","chapter-13","Advanced Tree Structures"
"nates, such as locations in 2D or 3D space. We define the discriminator at level i","chapter-13","Advanced Tree Structures"
"to be i mod k for k dimensions. For example, assume that we store data organized","chapter-13","Advanced Tree Structures"
"by xy-coordinates. In this case, k is 2 (there are two coordinates), with the x-","chapter-13","Advanced Tree Structures"
"coordinate field arbitrarily designated key 0, and the y-coordinate field designated","chapter-13","Advanced Tree Structures"
"key 1. At each level, the discriminator alternates between x and y. Thus, a node N","chapter-13","Advanced Tree Structures"
"at level 0 (the root) would have in its left subtree only nodes whose x values are less","chapter-13","Advanced Tree Structures"
"than Nx (because x is search key 0, and 0 mod 2 = 0). The right subtree would","chapter-13","Advanced Tree Structures"
"contain nodes whose x values are greater than Nx. A node M at level 1 would","chapter-13","Advanced Tree Structures"
"have in its left subtree only nodes whose y values are less than My. There is no re-","chapter-13","Advanced Tree Structures"
"striction on the relative values of Mx and the x values of M’s descendants, because","chapter-13","Advanced Tree Structures"
"branching decisions made at M are based solely on the y coordinate. Figure 13.11","chapter-13","Advanced Tree Structures"
"shows an example of how a collection of two-dimensional points would be stored","chapter-13","Advanced Tree Structures"
"in a k-d tree.","chapter-13","Advanced Tree Structures"
"In Figure 13.11 the region containing the points is (arbitrarily) restricted to a","chapter-13","Advanced Tree Structures"
"128 × 128 square, and each internal node splits the search space. Each split is","chapter-13","Advanced Tree Structures"
"shown by a line, vertical for nodes with x discriminators and horizontal for nodes","chapter-13","Advanced Tree Structures"
"with y discriminators. The root node splits the space into two parts; its children","chapter-13","Advanced Tree Structures"
"further subdivide the space into smaller parts. The children’s split lines do not","chapter-13","Advanced Tree Structures"
"cross the root’s split line. Thus, each node in the k-d tree helps to decompose the","chapter-13","Advanced Tree Structures"
"Sec. 13.3 Spatial Data Structures 443","chapter-13","Advanced Tree Structures"
"B","chapter-13","Advanced Tree Structures"
"A D","chapter-13","Advanced Tree Structures"
"C","chapter-13","Advanced Tree Structures"
"(a)","chapter-13","Advanced Tree Structures"
"E","chapter-13","Advanced Tree Structures"
"x","chapter-13","Advanced Tree Structures"
"y","chapter-13","Advanced Tree Structures"
"y","chapter-13","Advanced Tree Structures"
"x","chapter-13","Advanced Tree Structures"
"B (15, 70)","chapter-13","Advanced Tree Structures"
"A (40, 45)","chapter-13","Advanced Tree Structures"
"C (70, 10)","chapter-13","Advanced Tree Structures"
"D (69, 50)","chapter-13","Advanced Tree Structures"
"F (85, 90)","chapter-13","Advanced Tree Structures"
"(b)","chapter-13","Advanced Tree Structures"
"F E (66, 85)","chapter-13","Advanced Tree Structures"
"Figure 13.11 Example of a k-d tree. (a) The k-d tree decomposition for a 128 ×","chapter-13","Advanced Tree Structures"
"128-unit region containing seven data points. (b) The k-d tree for the region of (a).","chapter-13","Advanced Tree Structures"
"space into rectangles that show the extent of where nodes can fall in the various","chapter-13","Advanced Tree Structures"
"subtrees.","chapter-13","Advanced Tree Structures"
"Searching a k-d tree for the record with a specified xy-coordinate is like search-","chapter-13","Advanced Tree Structures"
"ing a BST, except that each level of the k-d tree is associated with a particular dis-","chapter-13","Advanced Tree Structures"
"criminator.","chapter-13","Advanced Tree Structures"
"Example 13.5 Consider searching the k-d tree for a record located at P =","chapter-13","Advanced Tree Structures"
"(69, 50). First compare P with the point stored at the root (record A in","chapter-13","Advanced Tree Structures"
"Figure 13.11). If P matches the location of A, then the search is successful.","chapter-13","Advanced Tree Structures"
"In this example the positions do not match (A’s location (40, 45) is not","chapter-13","Advanced Tree Structures"
"the same as (69, 50)), so the search must continue. The x value of A is","chapter-13","Advanced Tree Structures"
"compared with that of P to determine in which direction to branch. Because","chapter-13","Advanced Tree Structures"
"Ax’s value of 40 is less than P’s x value of 69, we branch to the right subtree","chapter-13","Advanced Tree Structures"
"(all cities with x value greater than or equal to 40 are in the right subtree).","chapter-13","Advanced Tree Structures"
"Ay does not affect the decision on which way to branch at this level. At the","chapter-13","Advanced Tree Structures"
"second level, P does not match record C’s position, so another branch must","chapter-13","Advanced Tree Structures"
"be taken. However, at this level we branch based on the relative y values","chapter-13","Advanced Tree Structures"
"of point P and record C (because 1 mod 2 = 1, which corresponds to the","chapter-13","Advanced Tree Structures"
"y-coordinate). Because Cy’s value of 10 is less than Py’s value of 50, we","chapter-13","Advanced Tree Structures"
"branch to the right. At this point, P is compared against the position of D.","chapter-13","Advanced Tree Structures"
"A match is made and the search is successful.","chapter-13","Advanced Tree Structures"
"If the search process reaches a null pointer, then that point is not contained","chapter-13","Advanced Tree Structures"
"in the tree. Here is a k-d tree search implementation, equivalent to the findhelp","chapter-13","Advanced Tree Structures"
"function of the BST class. KD class private member D stores the key’s dimension.","chapter-13","Advanced Tree Structures"
"444 Chap. 13 Advanced Tree Structures","chapter-13","Advanced Tree Structures"
"private E findhelp(KDNode<E> rt, int[] key, int level) {","chapter-13","Advanced Tree Structures"
"if (rt == null) return null;","chapter-13","Advanced Tree Structures"
"E it = rt.element();","chapter-13","Advanced Tree Structures"
"int[] itkey = rt.key();","chapter-13","Advanced Tree Structures"
"if ((itkey[0] == key[0]) && (itkey[1] == key[1]))","chapter-13","Advanced Tree Structures"
"return rt.element();","chapter-13","Advanced Tree Structures"
"if (itkey[level] > key[level])","chapter-13","Advanced Tree Structures"
"return findhelp(rt.left(), key, (level+1)%D);","chapter-13","Advanced Tree Structures"
"else","chapter-13","Advanced Tree Structures"
"return findhelp(rt.right(), key, (level+1)%D);","chapter-13","Advanced Tree Structures"
"}","chapter-13","Advanced Tree Structures"
"Inserting a new node into the k-d tree is similar to BST insertion. The k-d tree","chapter-13","Advanced Tree Structures"
"search procedure is followed until a null pointer is found, indicating the proper","chapter-13","Advanced Tree Structures"
"place to insert the new node.","chapter-13","Advanced Tree Structures"
"Example 13.6 Inserting a record at location (10, 50) in the k-d tree of","chapter-13","Advanced Tree Structures"
"Figure 13.11 first requires a search to the node containing record B. At this","chapter-13","Advanced Tree Structures"
"point, the new record is inserted into B’s left subtree.","chapter-13","Advanced Tree Structures"
"Deleting a node from a k-d tree is similar to deleting from a BST, but slightly","chapter-13","Advanced Tree Structures"
"harder. As with deleting from a BST, the first step is to find the node (call it N)","chapter-13","Advanced Tree Structures"
"to be deleted. It is then necessary to find a descendant of N which can be used to","chapter-13","Advanced Tree Structures"
"replace N in the tree. If N has no children, then N is replaced with a null pointer.","chapter-13","Advanced Tree Structures"
"Note that if N has one child that in turn has children, we cannot simply assign N’s","chapter-13","Advanced Tree Structures"
"parent to point to N’s child as would be done in the BST. To do so would change the","chapter-13","Advanced Tree Structures"
"level of all nodes in the subtree, and thus the discriminator used for a search would","chapter-13","Advanced Tree Structures"
"also change. The result is that the subtree would no longer be a k-d tree because a","chapter-13","Advanced Tree Structures"
"node’s children might now violate the BST property for that discriminator.","chapter-13","Advanced Tree Structures"
"Similar to BST deletion, the record stored in N should be replaced either by the","chapter-13","Advanced Tree Structures"
"record in N’s right subtree with the least value of N’s discriminator, or by the record","chapter-13","Advanced Tree Structures"
"in N’s left subtree with the greatest value for this discriminator. Assume that N was","chapter-13","Advanced Tree Structures"
"at an odd level and therefore y is the discriminator. N could then be replaced by the","chapter-13","Advanced Tree Structures"
"record in its right subtree with the least y value (call it Ymin). The problem is that","chapter-13","Advanced Tree Structures"
"Ymin is not necessarily the leftmost node, as it would be in the BST. A modified","chapter-13","Advanced Tree Structures"
"search procedure to find the least y value in the left subtree must be used to find it","chapter-13","Advanced Tree Structures"
"instead. The implementation for findmin is shown in Figure 13.12. A recursive","chapter-13","Advanced Tree Structures"
"call to the delete routine will then remove Ymin from the tree. Finally, Ymin’s record","chapter-13","Advanced Tree Structures"
"is substituted for the record in node N.","chapter-13","Advanced Tree Structures"
"Note that we can replace the node to be deleted with the least-valued node","chapter-13","Advanced Tree Structures"
"from the right subtree only if the right subtree exists. If it does not, then a suitable","chapter-13","Advanced Tree Structures"
"replacement must be found in the left subtree. Unfortunately, it is not satisfactory","chapter-13","Advanced Tree Structures"
"to replace N’s record with the record having the greatest value for the discriminator","chapter-13","Advanced Tree Structures"
"in the left subtree, because this new value might be duplicated. If so, then we","chapter-13","Advanced Tree Structures"
"Sec. 13.3 Spatial Data Structures 445","chapter-13","Advanced Tree Structures"
"private KDNode<E>","chapter-13","Advanced Tree Structures"
"findmin(KDNode<E> rt, int descrim, int level) {","chapter-13","Advanced Tree Structures"
"KDNode<E> temp1, temp2;","chapter-13","Advanced Tree Structures"
"int[] key1 = null;","chapter-13","Advanced Tree Structures"
"int[] key2 = null;","chapter-13","Advanced Tree Structures"
"if (rt == null) return null;","chapter-13","Advanced Tree Structures"
"temp1 = findmin(rt.left(), descrim, (level+1)%D);","chapter-13","Advanced Tree Structures"
"if (temp1 != null) key1 = temp1.key();","chapter-13","Advanced Tree Structures"
"if (descrim != level) {","chapter-13","Advanced Tree Structures"
"temp2 = findmin(rt.right(), descrim, (level+1)%D);","chapter-13","Advanced Tree Structures"
"if (temp2 != null) key2 = temp2.key();","chapter-13","Advanced Tree Structures"
"if ((temp1 == null) || ((temp2 != null) &&","chapter-13","Advanced Tree Structures"
"(key1[descrim] > key2[descrim])))","chapter-13","Advanced Tree Structures"
"temp1 = temp2;","chapter-13","Advanced Tree Structures"
"key1 = key2;","chapter-13","Advanced Tree Structures"
"} // Now, temp1 has the smaller value","chapter-13","Advanced Tree Structures"
"int[] rtkey = rt.key();","chapter-13","Advanced Tree Structures"
"if ((temp1 == null) || (key1[descrim] > rtkey[descrim]))","chapter-13","Advanced Tree Structures"
"return rt;","chapter-13","Advanced Tree Structures"
"else","chapter-13","Advanced Tree Structures"
"return temp1;","chapter-13","Advanced Tree Structures"
"}","chapter-13","Advanced Tree Structures"
"Figure 13.12 The k-d tree findmin method. On levels using the minimum","chapter-13","Advanced Tree Structures"
"value’s discriminator, branching is to the left. On other levels, both children’s","chapter-13","Advanced Tree Structures"
"subtrees must be visited. Helper function min takes two nodes and a discriminator","chapter-13","Advanced Tree Structures"
"as input, and returns the node with the smaller value in that discriminator.","chapter-13","Advanced Tree Structures"
"would have equal values for the discriminator in N’s left subtree, which violates","chapter-13","Advanced Tree Structures"
"the ordering rules for the k-d tree. Fortunately, there is a simple solution to the","chapter-13","Advanced Tree Structures"
"problem. We first move the left subtree of node N to become the right subtree (i.e.,","chapter-13","Advanced Tree Structures"
"we simply swap the values of N’s left and right child pointers). At this point, we","chapter-13","Advanced Tree Structures"
"proceed with the normal deletion process, replacing the record of N to be deleted","chapter-13","Advanced Tree Structures"
"with the record containing the least value of the discriminator from what is now","chapter-13","Advanced Tree Structures"
"N’s right subtree.","chapter-13","Advanced Tree Structures"
"Assume that we want to print out a list of all records that are within a certain","chapter-13","Advanced Tree Structures"
"distance d of a given point P. We will use Euclidean distance, that is, point P is","chapter-13","Advanced Tree Structures"
"defined to be within distance d of point N if1","chapter-13","Advanced Tree Structures"
"q","chapter-13","Advanced Tree Structures"
"(Px − Nx)","chapter-13","Advanced Tree Structures"
"2 + (Py − Ny)","chapter-13","Advanced Tree Structures"
"2 ≤ d.","chapter-13","Advanced Tree Structures"
"If the search process reaches a node whose key value for the discriminator is","chapter-13","Advanced Tree Structures"
"more than d above the corresponding value in the search key, then it is not possible","chapter-13","Advanced Tree Structures"
"that any record in the right subtree can be within distance d of the search key be-","chapter-13","Advanced Tree Structures"
"cause all key values in that dimension are always too great. Similarly, if the current","chapter-13","Advanced Tree Structures"
"node’s key value in the discriminator is d less than that for the search key value,","chapter-13","Advanced Tree Structures"
"1A more efficient computation is (Px − Nx)","chapter-13","Advanced Tree Structures"
"2 + (Py − Ny)","chapter-13","Advanced Tree Structures"
"2 ≤ d","chapter-13","Advanced Tree Structures"
"2","chapter-13","Advanced Tree Structures"
". This avoids performing a","chapter-13","Advanced Tree Structures"
"square root function.","chapter-13","Advanced Tree Structures"
"446 Chap. 13 Advanced Tree Structures","chapter-13","Advanced Tree Structures"
"A","chapter-13","Advanced Tree Structures"
"C","chapter-13","Advanced Tree Structures"
"Figure 13.13 Function InCircle must check the Euclidean distance between","chapter-13","Advanced Tree Structures"
"a record and the query point. It is possible for a record A to have x- and y-","chapter-13","Advanced Tree Structures"
"coordinates each within the query distance of the query point C, yet have A itself","chapter-13","Advanced Tree Structures"
"lie outside the query circle.","chapter-13","Advanced Tree Structures"
"then no record in the left subtree can be within the radius. In such cases, the sub-","chapter-13","Advanced Tree Structures"
"tree in question need not be searched, potentially saving much time. In the average","chapter-13","Advanced Tree Structures"
"case, the number of nodes that must be visited during a range query is linear on the","chapter-13","Advanced Tree Structures"
"number of data records that fall within the query circle.","chapter-13","Advanced Tree Structures"
"Example 13.7 We will now find all cities in the k-d tree of Figure 13.14","chapter-13","Advanced Tree Structures"
"within 25 units of the point (25, 65). The search begins with the root node,","chapter-13","Advanced Tree Structures"
"which contains record A. Because (40, 45) is exactly 25 units from the","chapter-13","Advanced Tree Structures"
"search point, it will be reported. The search procedure then determines","chapter-13","Advanced Tree Structures"
"which branches of the tree to take. The search circle extends to both the","chapter-13","Advanced Tree Structures"
"left and the right of A’s (vertical) dividing line, so both branches of the","chapter-13","Advanced Tree Structures"
"tree must be searched. The left subtree is processed first. Here, record B is","chapter-13","Advanced Tree Structures"
"checked and found to fall within the search circle. Because the node storing","chapter-13","Advanced Tree Structures"
"B has no children, processing of the left subtree is complete. Processing of","chapter-13","Advanced Tree Structures"
"A’s right subtree now begins. The coordinates of record C are checked","chapter-13","Advanced Tree Structures"
"and found not to fall within the circle. Thus, it should not be reported.","chapter-13","Advanced Tree Structures"
"However, it is possible that cities within C’s subtrees could fall within the","chapter-13","Advanced Tree Structures"
"search circle even if C does not. As C is at level 1, the discriminator at","chapter-13","Advanced Tree Structures"
"this level is the y-coordinate. Because 65 − 25 > 10, no record in C’s left","chapter-13","Advanced Tree Structures"
"subtree (i.e., records above C) could possibly be in the search circle. Thus,","chapter-13","Advanced Tree Structures"
"C’s left subtree (if it had one) need not be searched. However, cities in C’s","chapter-13","Advanced Tree Structures"
"right subtree could fall within the circle. Thus, search proceeds to the node","chapter-13","Advanced Tree Structures"
"containing record D. Again, D is outside the search circle. Because 25 +","chapter-13","Advanced Tree Structures"
"25 < 69, no record in D’s right subtree could be within the search circle.","chapter-13","Advanced Tree Structures"
"Thus, only D’s left subtree need be searched. This leads to comparing","chapter-13","Advanced Tree Structures"
"record E’s coordinates against the search circle. Record E falls outside the","chapter-13","Advanced Tree Structures"
"search circle, and processing is complete. So we see that we only search","chapter-13","Advanced Tree Structures"
"subtrees whose rectangles fall within the search circle.","chapter-13","Advanced Tree Structures"
"Sec. 13.3 Spatial Data Structures 447","chapter-13","Advanced Tree Structures"
"B","chapter-13","Advanced Tree Structures"
"A D","chapter-13","Advanced Tree Structures"
"C","chapter-13","Advanced Tree Structures"
"E","chapter-13","Advanced Tree Structures"
"F","chapter-13","Advanced Tree Structures"
"Figure 13.14 Searching in the k-d treeof Figure 13.11. (a) The k-d tree decom-","chapter-13","Advanced Tree Structures"
"position for a 128×128-unit region containing seven data points. (b) The k-d tree","chapter-13","Advanced Tree Structures"
"for the region of (a).","chapter-13","Advanced Tree Structures"
"private void rshelp(KDNode<E> rt, int[] point,","chapter-13","Advanced Tree Structures"
"int radius, int lev) {","chapter-13","Advanced Tree Structures"
"if (rt == null) return;","chapter-13","Advanced Tree Structures"
"int[] rtkey = rt.key();","chapter-13","Advanced Tree Structures"
"if (InCircle(point, radius, rtkey))","chapter-13","Advanced Tree Structures"
"System.out.println(rt.element());","chapter-13","Advanced Tree Structures"
"if (rtkey[lev] > (point[lev] - radius))","chapter-13","Advanced Tree Structures"
"rshelp(rt.left(), point, radius, (lev+1)%D);","chapter-13","Advanced Tree Structures"
"if (rtkey[lev] < (point[lev] + radius))","chapter-13","Advanced Tree Structures"
"rshelp(rt.right(), point, radius, (lev+1)%D);","chapter-13","Advanced Tree Structures"
"}","chapter-13","Advanced Tree Structures"
"Figure 13.15 The k-d tree region search method.","chapter-13","Advanced Tree Structures"
"Figure 13.15 shows an implementation for the region search method. When","chapter-13","Advanced Tree Structures"
"a node is visited, function InCircle is used to check the Euclidean distance","chapter-13","Advanced Tree Structures"
"between the node’s record and the query point. It is not enough to simply check","chapter-13","Advanced Tree Structures"
"that the differences between the x- and y-coordinates are each less than the query","chapter-13","Advanced Tree Structures"
"distances because the the record could still be outside the search circle, as illustrated","chapter-13","Advanced Tree Structures"
"by Figure 13.13.","chapter-13","Advanced Tree Structures"
"13.3.2 The PR quadtree","chapter-13","Advanced Tree Structures"
"In the Point-Region Quadtree (hereafter referred to as the PR quadtree) each node","chapter-13","Advanced Tree Structures"
"either has exactly four children or is a leaf. That is, the PR quadtree is a full four-","chapter-13","Advanced Tree Structures"
"way branching (4-ary) tree in shape. The PR quadtree represents a collection of","chapter-13","Advanced Tree Structures"
"data points in two dimensions by decomposing the region containing the data points","chapter-13","Advanced Tree Structures"
"into four equal quadrants, subquadrants, and so on, until no leaf node contains more","chapter-13","Advanced Tree Structures"
"than a single point. In other words, if a region contains zero or one data points, then","chapter-13","Advanced Tree Structures"
"448 Chap. 13 Advanced Tree Structures","chapter-13","Advanced Tree Structures"
"(a)","chapter-13","Advanced Tree Structures"
"0","chapter-13","Advanced Tree Structures"
"0","chapter-13","Advanced Tree Structures"
"127","chapter-13","Advanced Tree Structures"
"127","chapter-13","Advanced Tree Structures"
"A","chapter-13","Advanced Tree Structures"
"D","chapter-13","Advanced Tree Structures"
"C","chapter-13","Advanced Tree Structures"
"B","chapter-13","Advanced Tree Structures"
"E","chapter-13","Advanced Tree Structures"
"F","chapter-13","Advanced Tree Structures"
"(b)","chapter-13","Advanced Tree Structures"
"nw se","chapter-13","Advanced Tree Structures"
"(70, 10) (69,50)","chapter-13","Advanced Tree Structures"
"(55,80)(80, 90)","chapter-13","Advanced Tree Structures"
"ne sw","chapter-13","Advanced Tree Structures"
"A","chapter-13","Advanced Tree Structures"
"C D","chapter-13","Advanced Tree Structures"
"B","chapter-13","Advanced Tree Structures"
"E F","chapter-13","Advanced Tree Structures"
"(40,45) (15,70)","chapter-13","Advanced Tree Structures"
"Figure 13.16 Example of a PR quadtree. (a) A map of data points. We de-","chapter-13","Advanced Tree Structures"
"fine the region to be square with origin at the upper-left-hand corner and sides of","chapter-13","Advanced Tree Structures"
"length 128. (b) The PR quadtree for the points in (a). (a) also shows the block","chapter-13","Advanced Tree Structures"
"decomposition imposed by the PR quadtree for this region.","chapter-13","Advanced Tree Structures"
"it is represented by a PR quadtree consisting of a single leaf node. If the region con-","chapter-13","Advanced Tree Structures"
"tains more than a single data point, then the region is split into four equal quadrants.","chapter-13","Advanced Tree Structures"
"The corresponding PR quadtree then contains an internal node and four subtrees,","chapter-13","Advanced Tree Structures"
"each subtree representing a single quadrant of the region, which might in turn be","chapter-13","Advanced Tree Structures"
"split into subquadrants. Each internal node of a PR quadtree represents a single","chapter-13","Advanced Tree Structures"
"split of the two-dimensional region. The four quadrants of the region (or equiva-","chapter-13","Advanced Tree Structures"
"lently, the corresponding subtrees) are designated (in order) NW, NE, SW, and SE.","chapter-13","Advanced Tree Structures"
"Each quadrant containing more than a single point would in turn be recursively di-","chapter-13","Advanced Tree Structures"
"vided into subquadrants until each leaf of the corresponding PR quadtree contains","chapter-13","Advanced Tree Structures"
"at most one point.","chapter-13","Advanced Tree Structures"
"For example, consider the region of Figure 13.16(a) and the corresponding","chapter-13","Advanced Tree Structures"
"PR quadtree in Figure 13.16(b). The decomposition process demands a fixed key","chapter-13","Advanced Tree Structures"
"range. In this example, the region is assumed to be of size 128 × 128. Note that the","chapter-13","Advanced Tree Structures"
"internal nodes of the PR quadtree are used solely to indicate decomposition of the","chapter-13","Advanced Tree Structures"
"region; internal nodes do not store data records. Because the decomposition lines","chapter-13","Advanced Tree Structures"
"are predetermined (i.e, key-space decomposition is used), the PR quadtree is a trie.","chapter-13","Advanced Tree Structures"
"Search for a record matching point Q in the PR quadtree is straightforward.","chapter-13","Advanced Tree Structures"
"Beginning at the root, we continuously branch to the quadrant that contains Q until","chapter-13","Advanced Tree Structures"
"our search reaches a leaf node. If the root is a leaf, then just check to see if the","chapter-13","Advanced Tree Structures"
"node’s data record matches point Q. If the root is an internal node, proceed to","chapter-13","Advanced Tree Structures"
"the child that contains the search coordinate. For example, the NW quadrant of","chapter-13","Advanced Tree Structures"
"Figure 13.16 contains points whose x and y values each fall in the range 0 to 63.","chapter-13","Advanced Tree Structures"
"The NE quadrant contains points whose x value falls in the range 64 to 127, and","chapter-13","Advanced Tree Structures"
"Sec. 13.3 Spatial Data Structures 449","chapter-13","Advanced Tree Structures"
"C","chapter-13","Advanced Tree Structures"
"A","chapter-13","Advanced Tree Structures"
"B","chapter-13","Advanced Tree Structures"
"(a) (b)","chapter-13","Advanced Tree Structures"
"B","chapter-13","Advanced Tree Structures"
"A","chapter-13","Advanced Tree Structures"
"Figure 13.17 PR quadtree insertion example. (a) The initial PR quadtree con-","chapter-13","Advanced Tree Structures"
"taining two data points. (b) The result of inserting point C. The block containing A","chapter-13","Advanced Tree Structures"
"must be decomposed into four sub-blocks. Points A and C would still be in the","chapter-13","Advanced Tree Structures"
"same block if only one subdivision takes place, so a second decomposition is re-","chapter-13","Advanced Tree Structures"
"quired to separate them.","chapter-13","Advanced Tree Structures"
"whose y value falls in the range 0 to 63. If the root’s child is a leaf node, then that","chapter-13","Advanced Tree Structures"
"child is checked to see if Q has been found. If the child is another internal node, the","chapter-13","Advanced Tree Structures"
"search process continues through the tree until a leaf node is found. If this leaf node","chapter-13","Advanced Tree Structures"
"stores a record whose position matches Q then the query is successful; otherwise Q","chapter-13","Advanced Tree Structures"
"is not in the tree.","chapter-13","Advanced Tree Structures"
"Inserting record P into the PR quadtree is performed by first locating the leaf","chapter-13","Advanced Tree Structures"
"node that contains the location of P. If this leaf node is empty, then P is stored","chapter-13","Advanced Tree Structures"
"at this leaf. If the leaf already contains P (or a record with P’s coordinates), then","chapter-13","Advanced Tree Structures"
"a duplicate record should be reported. If the leaf node already contains another","chapter-13","Advanced Tree Structures"
"record, then the node must be repeatedly decomposed until the existing record and","chapter-13","Advanced Tree Structures"
"P fall into different leaf nodes. Figure 13.17 shows an example of such an insertion.","chapter-13","Advanced Tree Structures"
"Deleting a record P is performed by first locating the node N of the PR quadtree","chapter-13","Advanced Tree Structures"
"that contains P. Node N is then changed to be empty. The next step is to look at N’s","chapter-13","Advanced Tree Structures"
"three siblings. N and its siblings must be merged together to form a single node N","chapter-13","Advanced Tree Structures"
"0","chapter-13","Advanced Tree Structures"
"if only one point is contained among them. This merging process continues until","chapter-13","Advanced Tree Structures"
"some level is reached at which at least two points are contained in the subtrees rep-","chapter-13","Advanced Tree Structures"
"resented by node N","chapter-13","Advanced Tree Structures"
"0","chapter-13","Advanced Tree Structures"
"and its siblings. For example, if point C is to be deleted from","chapter-13","Advanced Tree Structures"
"the PR quadtree representing Figure 13.17(b), the resulting node must be merged","chapter-13","Advanced Tree Structures"
"with its siblings, and that larger node again merged with its siblings to restore the","chapter-13","Advanced Tree Structures"
"PR quadtree to the decomposition of Figure 13.17(a).","chapter-13","Advanced Tree Structures"
"Region search is easily performed with the PR quadtree. To locate all points","chapter-13","Advanced Tree Structures"
"within radius r of query point Q, begin at the root. If the root is an empty leaf node,","chapter-13","Advanced Tree Structures"
"then no data points are found. If the root is a leaf containing a data record, then the","chapter-13","Advanced Tree Structures"
"450 Chap. 13 Advanced Tree Structures","chapter-13","Advanced Tree Structures"
"location of the data point is examined to determine if it falls within the circle. If","chapter-13","Advanced Tree Structures"
"the root is an internal node, then the process is performed recursively, but only on","chapter-13","Advanced Tree Structures"
"those subtrees containing some part of the search circle.","chapter-13","Advanced Tree Structures"
"Let us now consider how the structure of the PR quadtree affects the design","chapter-13","Advanced Tree Structures"
"of its node representation. The PR quadtree is actually a trie (as defined in Sec-","chapter-13","Advanced Tree Structures"
"tion 13.1). Decomposition takes place at the mid-points for internal nodes, regard-","chapter-13","Advanced Tree Structures"
"less of where the data points actually fall. The placement of the data points does","chapter-13","Advanced Tree Structures"
"determine whether a decomposition for a node takes place, but not where the de-","chapter-13","Advanced Tree Structures"
"composition for the node takes place. Internal nodes of the PR quadtree are quite","chapter-13","Advanced Tree Structures"
"different from leaf nodes, in that internal nodes have children (leaf nodes do not)","chapter-13","Advanced Tree Structures"
"and leaf nodes have data fields (internal nodes do not). Thus, it is likely to be ben-","chapter-13","Advanced Tree Structures"
"eficial to represent internal nodes differently from leaf nodes. Finally, there is the","chapter-13","Advanced Tree Structures"
"fact that approximately half of the leaf nodes will contain no data field.","chapter-13","Advanced Tree Structures"
"Another issue to consider is: How does a routine traversing the PR quadtree","chapter-13","Advanced Tree Structures"
"get the coordinates for the square represented by the current PR quadtree node?","chapter-13","Advanced Tree Structures"
"One possibility is to store with each node its spatial description (such as upper-left","chapter-13","Advanced Tree Structures"
"corner and width). However, this will take a lot of space — perhaps as much as the","chapter-13","Advanced Tree Structures"
"space needed for the data records, depending on what information is being stored.","chapter-13","Advanced Tree Structures"
"Another possibility is to pass in the coordinates when the recursive call is made.","chapter-13","Advanced Tree Structures"
"For example, consider the search process. Initially, the search visits the root node","chapter-13","Advanced Tree Structures"
"of the tree, which has origin at (0, 0), and whose width is the full size of the space","chapter-13","Advanced Tree Structures"
"being covered. When the appropriate child is visited, it is a simple matter for the","chapter-13","Advanced Tree Structures"
"search routine to determine the origin for the child, and the width of the square is","chapter-13","Advanced Tree Structures"
"simply half that of the parent. Not only does passing in the size and position infor-","chapter-13","Advanced Tree Structures"
"mation for a node save considerable space, but avoiding storing such information","chapter-13","Advanced Tree Structures"
"in the nodes enables a good design choice for empty leaf nodes, as discussed next.","chapter-13","Advanced Tree Structures"
"How should we represent empty leaf nodes? On average, half of the leaf nodes","chapter-13","Advanced Tree Structures"
"in a PR quadtree are empty (i.e., do not store a data point). One implementation","chapter-13","Advanced Tree Structures"
"option is to use a null pointer in internal nodes to represent empty nodes. This","chapter-13","Advanced Tree Structures"
"will solve the problem of excessive space requirements. There is an unfortunate","chapter-13","Advanced Tree Structures"
"side effect that using a null pointer requires the PR quadtree processing meth-","chapter-13","Advanced Tree Structures"
"ods to understand this convention. In other words, you are breaking encapsulation","chapter-13","Advanced Tree Structures"
"on the node representation because the tree now must know things about how the","chapter-13","Advanced Tree Structures"
"nodes are implemented. This is not too horrible for this particular application, be-","chapter-13","Advanced Tree Structures"
"cause the node class can be considered private to the tree class, in which case the","chapter-13","Advanced Tree Structures"
"node implementation is completely invisible to the outside world. However, it is","chapter-13","Advanced Tree Structures"
"undesirable if there is another reasonable alternative.","chapter-13","Advanced Tree Structures"
"Fortunately, there is a good alternative. It is called the Flyweight design pattern.","chapter-13","Advanced Tree Structures"
"In the PR quadtree, a flyweight is a single empty leaf node that is reused in all places","chapter-13","Advanced Tree Structures"
"where an empty leaf node is needed. You simply have all of the internal nodes with","chapter-13","Advanced Tree Structures"
"empty leaf children point to the same node object. This node object is created once","chapter-13","Advanced Tree Structures"
"Sec. 13.3 Spatial Data Structures 451","chapter-13","Advanced Tree Structures"
"at the beginning of the program, and is never removed. The node class recognizes","chapter-13","Advanced Tree Structures"
"from the pointer value that the flyweight is being accessed, and acts accordingly.","chapter-13","Advanced Tree Structures"
"Note that when using the Flyweight design pattern, you cannot store coordi-","chapter-13","Advanced Tree Structures"
"nates for the node in the node. This is an example of the concept of intrinsic versus","chapter-13","Advanced Tree Structures"
"extrinsic state. Intrinsic state for an object is state information stored in the ob-","chapter-13","Advanced Tree Structures"
"ject. If you stored the coordinates for a node in the node object, those coordinates","chapter-13","Advanced Tree Structures"
"would be intrinsic state. Extrinsic state is state information about an object stored","chapter-13","Advanced Tree Structures"
"elsewhere in the environment, such as in global variables or passed to the method.","chapter-13","Advanced Tree Structures"
"If your recursive calls that process the tree pass in the coordinates for the current","chapter-13","Advanced Tree Structures"
"node, then the coordinates will be extrinsic state. A flyweight can have in its intrin-","chapter-13","Advanced Tree Structures"
"sic state only information that is accurate for all instances of the flyweight. Clearly","chapter-13","Advanced Tree Structures"
"coordinates do not qualify, because each empty leaf node has its own location. So,","chapter-13","Advanced Tree Structures"
"if you want to use a flyweight, you must pass in coordinates.","chapter-13","Advanced Tree Structures"
"Another design choice is: Who controls the work, the node class or the tree","chapter-13","Advanced Tree Structures"
"class? For example, on an insert operation, you could have the tree class control","chapter-13","Advanced Tree Structures"
"the flow down the tree, looking at (querying) the nodes to see their type and reacting","chapter-13","Advanced Tree Structures"
"accordingly. This is the approach used by the BST implementation in Section 5.4.","chapter-13","Advanced Tree Structures"
"An alternate approach is to have the node class do the work. That is, you have an","chapter-13","Advanced Tree Structures"
"insert method for the nodes. If the node is internal, it passes the city record to the","chapter-13","Advanced Tree Structures"
"appropriate child (recursively). If the node is a flyweight, it replaces itself with a","chapter-13","Advanced Tree Structures"
"new leaf node. If the node is a full node, it replaces itself with a subtree. This is","chapter-13","Advanced Tree Structures"
"an example of the Composite design pattern, discussed in Section 5.3.1. Use of the","chapter-13","Advanced Tree Structures"
"composite design would be difficult if null pointers are used to represent empty","chapter-13","Advanced Tree Structures"
"leaf nodes. It turns out that the PR quadtree insert and delete methods are easier to","chapter-13","Advanced Tree Structures"
"implement when using the composite design.","chapter-13","Advanced Tree Structures"
"13.3.3 Other Point Data Structures","chapter-13","Advanced Tree Structures"
"The differences between the k-d tree and the PR quadtree illustrate many of the","chapter-13","Advanced Tree Structures"
"design choices encountered when creating spatial data structures. The k-d tree pro-","chapter-13","Advanced Tree Structures"
"vides an object space decomposition of the region, while the PR quadtree provides","chapter-13","Advanced Tree Structures"
"a key space decomposition (thus, it is a trie). The k-d tree stores records at all","chapter-13","Advanced Tree Structures"
"nodes, while the PR quadtree stores records only at the leaf nodes. Finally, the two","chapter-13","Advanced Tree Structures"
"trees have different structures. The k-d tree is a binary tree (and need not be full),","chapter-13","Advanced Tree Structures"
"while the PR quadtree is a full tree with 2","chapter-13","Advanced Tree Structures"
"d branches (in the two-dimensional case,","chapter-13","Advanced Tree Structures"
"2","chapter-13","Advanced Tree Structures"
"2 = 4). Consider the extension of this concept to three dimensions. A k-d tree for","chapter-13","Advanced Tree Structures"
"three dimensions would alternate the discriminator through the x, y, and z dimen-","chapter-13","Advanced Tree Structures"
"sions. The three-dimensional equivalent of the PR quadtree would be a tree with","chapter-13","Advanced Tree Structures"
"2","chapter-13","Advanced Tree Structures"
"3 or eight branches. Such a tree is called an octree.","chapter-13","Advanced Tree Structures"
"We can also devise a binary trie based on a key space decomposition in each","chapter-13","Advanced Tree Structures"
"dimension, or a quadtree that uses the two-dimensional equivalent to an object","chapter-13","Advanced Tree Structures"
"space decomposition. The bintree is a binary trie that uses keyspace decomposition","chapter-13","Advanced Tree Structures"
"452 Chap. 13 Advanced Tree Structures","chapter-13","Advanced Tree Structures"
"x","chapter-13","Advanced Tree Structures"
"y","chapter-13","Advanced Tree Structures"
"A","chapter-13","Advanced Tree Structures"
"x","chapter-13","Advanced Tree Structures"
"B","chapter-13","Advanced Tree Structures"
"y","chapter-13","Advanced Tree Structures"
"C D","chapter-13","Advanced Tree Structures"
"E F","chapter-13","Advanced Tree Structures"
"x","chapter-13","Advanced Tree Structures"
"y","chapter-13","Advanced Tree Structures"
"E","chapter-13","Advanced Tree Structures"
"F","chapter-13","Advanced Tree Structures"
"D","chapter-13","Advanced Tree Structures"
"A","chapter-13","Advanced Tree Structures"
"B","chapter-13","Advanced Tree Structures"
"C","chapter-13","Advanced Tree Structures"
"(a) (b)","chapter-13","Advanced Tree Structures"
"Figure 13.18 An example of the bintree, a binary tree using key space decom-","chapter-13","Advanced Tree Structures"
"position and discriminators rotating among the dimensions. Compare this with","chapter-13","Advanced Tree Structures"
"the k-d tree of Figure 13.11 and the PR quadtree of Figure 13.16.","chapter-13","Advanced Tree Structures"
"127","chapter-13","Advanced Tree Structures"
"0","chapter-13","Advanced Tree Structures"
"0 127","chapter-13","Advanced Tree Structures"
"(a)","chapter-13","Advanced Tree Structures"
"B","chapter-13","Advanced Tree Structures"
"A","chapter-13","Advanced Tree Structures"
"C","chapter-13","Advanced Tree Structures"
"F","chapter-13","Advanced Tree Structures"
"E","chapter-13","Advanced Tree Structures"
"D","chapter-13","Advanced Tree Structures"
"(b)","chapter-13","Advanced Tree Structures"
"C","chapter-13","Advanced Tree Structures"
"nw","chapter-13","Advanced Tree Structures"
"ne sw","chapter-13","Advanced Tree Structures"
"se","chapter-13","Advanced Tree Structures"
"D","chapter-13","Advanced Tree Structures"
"A","chapter-13","Advanced Tree Structures"
"B","chapter-13","Advanced Tree Structures"
"E F","chapter-13","Advanced Tree Structures"
"Figure 13.19 An example of the point quadtree, a 4-ary tree using object space","chapter-13","Advanced Tree Structures"
"decomposition. Compare this with the PR quadtree of Figure 13.11.","chapter-13","Advanced Tree Structures"
"and alternates discriminators at each level in a manner similar to the k-d tree. The","chapter-13","Advanced Tree Structures"
"bintree for the points of Figure 13.11 is shown in Figure 13.18. Alternatively, we","chapter-13","Advanced Tree Structures"
"can use a four-way decomposition of space centered on the data points. The tree","chapter-13","Advanced Tree Structures"
"resulting from such a decomposition is called a point quadtree. The point quadtree","chapter-13","Advanced Tree Structures"
"for the data points of Figure 13.11 is shown in Figure 13.19.","chapter-13","Advanced Tree Structures"
"Sec. 13.4 Further Reading 453","chapter-13","Advanced Tree Structures"
"13.3.4 Other Spatial Data Structures","chapter-13","Advanced Tree Structures"
"This section has barely scratched the surface of the field of spatial data structures.","chapter-13","Advanced Tree Structures"
"Dozens of distinct spatial data structures have been invented, many with variations","chapter-13","Advanced Tree Structures"
"and alternate implementations. Spatial data structures exist for storing many forms","chapter-13","Advanced Tree Structures"
"of spatial data other than points. The most important distinctions between are the","chapter-13","Advanced Tree Structures"
"tree structure (binary or not, regular decompositions or not) and the decomposition","chapter-13","Advanced Tree Structures"
"rule used to decide when the data contained within a region is so complex that the","chapter-13","Advanced Tree Structures"
"region must be subdivided.","chapter-13","Advanced Tree Structures"
"One such spatial data structure is the Region Quadtree for storing images where","chapter-13","Advanced Tree Structures"
"the pixel values tend to be blocky, such as a map of the countries of the world.","chapter-13","Advanced Tree Structures"
"The region quadtree uses a four-way regular decomposition scheme similar to the","chapter-13","Advanced Tree Structures"
"PR quadtree. The decomposition rule is simply to divide any node containing pixels","chapter-13","Advanced Tree Structures"
"of more than one color or value.","chapter-13","Advanced Tree Structures"
"Spatial data structures can also be used to store line object, rectangle object,","chapter-13","Advanced Tree Structures"
"or objects of arbitrary shape (such as polygons in two dimensions or polyhedra in","chapter-13","Advanced Tree Structures"
"three dimensions). A simple, yet effective, data structure for storing rectangles or","chapter-13","Advanced Tree Structures"
"arbitrary polygonal shapes can be derived from the PR quadtree. Pick a threshold","chapter-13","Advanced Tree Structures"
"value c, and subdivide any region into four quadrants if it contains more than c","chapter-13","Advanced Tree Structures"
"objects. A special case must be dealt with when more than c object intersect.","chapter-13","Advanced Tree Structures"
"Some of the most interesting developments in spatial data structures have to","chapter-13","Advanced Tree Structures"
"do with adapting them for disk-based applications. However, all such disk-based","chapter-13","Advanced Tree Structures"
"implementations boil down to storing the spatial data structure within some variant","chapter-13","Advanced Tree Structures"
"on either B-trees or hashing.","chapter-13","Advanced Tree Structures"
"13.4 Further Reading","chapter-13","Advanced Tree Structures"
"PATRICIA tries and other trie implementations are discussed in Information Re-","chapter-13","Advanced Tree Structures"
"trieval: Data Structures & Algorithms, Frakes and Baeza-Yates, eds. [FBY92].","chapter-13","Advanced Tree Structures"
"See Knuth [Knu97] for a discussion of the AVL tree. For further reading on","chapter-13","Advanced Tree Structures"
"splay trees, see “Self-adjusting Binary Search” by Sleator and Tarjan [ST85].","chapter-13","Advanced Tree Structures"
"The world of spatial data structures is rich and rapidly evolving. For a good","chapter-13","Advanced Tree Structures"
"introduction, see Foundations of Multidimensional and Metric Data Structures by","chapter-13","Advanced Tree Structures"
"Hanan Samet [Sam06]. This is also the best reference for more information on","chapter-13","Advanced Tree Structures"
"the PR quadtree. The k-d tree was invented by John Louis Bentley. For further","chapter-13","Advanced Tree Structures"
"information on the k-d tree, in addition to [Sam06], see [Ben75]. For information","chapter-13","Advanced Tree Structures"
"on using a quadtree to store arbitrary polygonal objects, see [SH92].","chapter-13","Advanced Tree Structures"
"For a discussion on the relative space requirements for two-way versus multi-","chapter-13","Advanced Tree Structures"
"way branching, see “A Generalized Comparison of Quadtree and Bintree Storage","chapter-13","Advanced Tree Structures"
"Requirements” by Shaffer, Juvvadi, and Heath [SJH93].","chapter-13","Advanced Tree Structures"
"Closely related to spatial data structures are data structures for storing multi-","chapter-13","Advanced Tree Structures"
"dimensional data (which might not necessarily be spatial in nature). A popular","chapter-13","Advanced Tree Structures"
"454 Chap. 13 Advanced Tree Structures","chapter-13","Advanced Tree Structures"
"data structure for storing such data is the R-tree, which was originally proposed by","chapter-13","Advanced Tree Structures"
"Guttman [Gut84].","chapter-13","Advanced Tree Structures"
"13.5 Exercises","chapter-13","Advanced Tree Structures"
"13.1 Show the binary trie (as illustrated by Figure 13.1) for the following collec-","chapter-13","Advanced Tree Structures"
"tion of values: 42, 12, 100, 10, 50, 31, 7, 11, 99.","chapter-13","Advanced Tree Structures"
"13.2 Show the PAT trie (as illustrated by Figure 13.3) for the following collection","chapter-13","Advanced Tree Structures"
"of values: 42, 12, 100, 10, 50, 31, 7, 11, 99.","chapter-13","Advanced Tree Structures"
"13.3 Write the insertion routine for a binary trie as shown in Figure 13.1.","chapter-13","Advanced Tree Structures"
"13.4 Write the deletion routine for a binary trie as shown in Figure 13.1.","chapter-13","Advanced Tree Structures"
"13.5 (a) Show the result (including appropriate rotations) of inserting the value","chapter-13","Advanced Tree Structures"
"39 into the AVL tree on the left in Figure 13.4.","chapter-13","Advanced Tree Structures"
"(b) Show the result (including appropriate rotations) of inserting the value","chapter-13","Advanced Tree Structures"
"300 into the AVL tree on the left in Figure 13.4.","chapter-13","Advanced Tree Structures"
"(c) Show the result (including appropriate rotations) of inserting the value","chapter-13","Advanced Tree Structures"
"50 into the AVL tree on the left in Figure 13.4.","chapter-13","Advanced Tree Structures"
"(d) Show the result (including appropriate rotations) of inserting the value","chapter-13","Advanced Tree Structures"
"1 into the AVL tree on the left in Figure 13.4.","chapter-13","Advanced Tree Structures"
"13.6 Show the splay tree that results from searching for value 75 in the splay tree","chapter-13","Advanced Tree Structures"
"of Figure 13.10(d).","chapter-13","Advanced Tree Structures"
"13.7 Show the splay tree that results from searching for value 18 in the splay tree","chapter-13","Advanced Tree Structures"
"of Figure 13.10(d).","chapter-13","Advanced Tree Structures"
"13.8 Some applications do not permit storing two records with duplicate key val-","chapter-13","Advanced Tree Structures"
"ues. In such a case, an attempt to insert a duplicate-keyed record into a tree","chapter-13","Advanced Tree Structures"
"structure such as a splay tree should result in a failure on insert. What is","chapter-13","Advanced Tree Structures"
"the appropriate action to take in a splay tree implementation when the insert","chapter-13","Advanced Tree Structures"
"routine is called with a duplicate-keyed record?","chapter-13","Advanced Tree Structures"
"13.9 Show the result of deleting point A from the k-d tree of Figure 13.11.","chapter-13","Advanced Tree Structures"
"13.10 (a) Show the result of building a k-d tree from the following points (in-","chapter-13","Advanced Tree Structures"
"serted in the order given). A (20, 20), B (10, 30), C (25, 50), D (35,","chapter-13","Advanced Tree Structures"
"25), E (30, 45), F (30, 35), G (55, 40), H (45, 35), I (50, 30).","chapter-13","Advanced Tree Structures"
"(b) Show the result of deleting point A from the tree you built in part (a).","chapter-13","Advanced Tree Structures"
"13.11 (a) Show the result of deleting F from the PR quadtree of Figure 13.16.","chapter-13","Advanced Tree Structures"
"(b) Show the result of deleting records E and F from the PR quadtree of","chapter-13","Advanced Tree Structures"
"Figure 13.16.","chapter-13","Advanced Tree Structures"
"13.12 (a) Show the result of building a PR quadtree from the following points","chapter-13","Advanced Tree Structures"
"(inserted in the order given). Assume the tree is representing a space of","chapter-13","Advanced Tree Structures"
"64 by 64 units. A (20, 20), B (10, 30), C (25, 50), D (35, 25), E (30,","chapter-13","Advanced Tree Structures"
"45), F (30, 35), G (45, 25), H (45, 30), I (50, 30).","chapter-13","Advanced Tree Structures"
"(b) Show the result of deleting point C from the tree you built in part (a).","chapter-13","Advanced Tree Structures"
"Sec. 13.6 Projects 455","chapter-13","Advanced Tree Structures"
"(c) Show the result of deleting point F from the resulting tree in part (b).","chapter-13","Advanced Tree Structures"
"13.13 On average, how many leaf nodes of a PR quadtree will typically be empty?","chapter-13","Advanced Tree Structures"
"Explain why.","chapter-13","Advanced Tree Structures"
"13.14 When performing a region search on a PR quadtree, we need only search","chapter-13","Advanced Tree Structures"
"those subtrees of an internal node whose corresponding square falls within","chapter-13","Advanced Tree Structures"
"the query circle. This is most easily computed by comparing the x and y","chapter-13","Advanced Tree Structures"
"ranges of the query circle against the x and y ranges of the square corre-","chapter-13","Advanced Tree Structures"
"sponding to the subtree. However, as illustrated by Figure 13.13, the x and","chapter-13","Advanced Tree Structures"
"y ranges might overlap without the circle actually intersecting the square.","chapter-13","Advanced Tree Structures"
"Write a function that accurately determines if a circle and a square intersect.","chapter-13","Advanced Tree Structures"
"13.15 (a) Show the result of building a bintree from the following points (inserted","chapter-13","Advanced Tree Structures"
"in the order given). Assume the tree is representing a space of 64 by 64","chapter-13","Advanced Tree Structures"
"units. A (20, 20), B (10, 30), C (25, 50), D (35, 25), E (30, 45), F (30,","chapter-13","Advanced Tree Structures"
"35), G (45, 25), H (45, 30), I (50, 30).","chapter-13","Advanced Tree Structures"
"(b) Show the result of deleting point C from the tree you built in part (a).","chapter-13","Advanced Tree Structures"
"(c) Show the result of deleting point F from the resulting tree in part (b).","chapter-13","Advanced Tree Structures"
"13.16 Compare the trees constructed for Exercises 12 and 15 in terms of the number","chapter-13","Advanced Tree Structures"
"of internal nodes, full leaf nodes, empty leaf nodes, and total depths of the","chapter-13","Advanced Tree Structures"
"two trees.","chapter-13","Advanced Tree Structures"
"13.17 Show the result of building a point quadtree from the following points (in-","chapter-13","Advanced Tree Structures"
"serted in the order given). Assume the tree is representing a space of 64 by","chapter-13","Advanced Tree Structures"
"64 units. A (20, 20), B (10, 30), C (25, 50), D (35, 25), E (30, 45), F (31,","chapter-13","Advanced Tree Structures"
"35), G (45, 26), H (44, 30), I (50, 30).","chapter-13","Advanced Tree Structures"
"13.6 Projects","chapter-13","Advanced Tree Structures"
"13.1 Use the trie data structure to devise a program to sort variable-length strings.","chapter-13","Advanced Tree Structures"
"The program’s running time should be proportional to the total number of","chapter-13","Advanced Tree Structures"
"letters in all of the strings. Note that some strings might be very long while","chapter-13","Advanced Tree Structures"
"most are short.","chapter-13","Advanced Tree Structures"
"13.2 Define the set of suffix strings for a string S to be S, S without its first char-","chapter-13","Advanced Tree Structures"
"acter, S without its first two characters, and so on. For example, the complete","chapter-13","Advanced Tree Structures"
"set of suffix strings for “HELLO” would be","chapter-13","Advanced Tree Structures"
"{HELLO, ELLO, LLO, LO, O}.","chapter-13","Advanced Tree Structures"
"A suffix tree is a PAT trie that contains all of the suffix strings for a given","chapter-13","Advanced Tree Structures"
"string, and associates each suffix with the complete string. The advantage","chapter-13","Advanced Tree Structures"
"of a suffix tree is that it allows a search for strings using “wildcards.” For","chapter-13","Advanced Tree Structures"
"example, the search key “TH*” means to find all strings with “TH” as the","chapter-13","Advanced Tree Structures"
"first two characters. This can easily be done with a regular trie. Searching","chapter-13","Advanced Tree Structures"
"for “*TH” is not efficient in a regular trie, but it is efficient in a suffix tree.","chapter-13","Advanced Tree Structures"
"456 Chap. 13 Advanced Tree Structures","chapter-13","Advanced Tree Structures"
"Implement the suffix tree for a dictionary of words or phrases, with support","chapter-13","Advanced Tree Structures"
"for wildcard search.","chapter-13","Advanced Tree Structures"
"13.3 Revise the BST class of Section 5.4 to use the AVL tree rotations. Your new","chapter-13","Advanced Tree Structures"
"implementation should not modify the original BST class ADT. Compare","chapter-13","Advanced Tree Structures"
"your AVL tree against an implementation of the standard BST over a wide","chapter-13","Advanced Tree Structures"
"variety of input data. Under what conditions does the splay tree actually save","chapter-13","Advanced Tree Structures"
"time?","chapter-13","Advanced Tree Structures"
"13.4 Revise the BST class of Section 5.4 to use the splay tree rotations. Your new","chapter-13","Advanced Tree Structures"
"implementation should not modify the original BST class ADT. Compare","chapter-13","Advanced Tree Structures"
"your splay tree against an implementation of the standard BST over a wide","chapter-13","Advanced Tree Structures"
"variety of input data. Under what conditions does the splay tree actually save","chapter-13","Advanced Tree Structures"
"time?","chapter-13","Advanced Tree Structures"
"13.5 Implement a city database using the k-d tree. Each database record contains","chapter-13","Advanced Tree Structures"
"the name of the city (a string of arbitrary length) and the coordinates of the","chapter-13","Advanced Tree Structures"
"city expressed as integer x- and y-coordinates. Your database should allow","chapter-13","Advanced Tree Structures"
"records to be inserted, deleted by name or coordinate, and searched by name","chapter-13","Advanced Tree Structures"
"or coordinate. You should also support region queries, that is, a request to","chapter-13","Advanced Tree Structures"
"print all records within a given distance of a specified point.","chapter-13","Advanced Tree Structures"
"13.6 Implement a city database using the PR quadtree. Each database record con-","chapter-13","Advanced Tree Structures"
"tains the name of the city (a string of arbitrary length) and the coordinates","chapter-13","Advanced Tree Structures"
"of the city expressed as integer x- and y-coordinates. Your database should","chapter-13","Advanced Tree Structures"
"allow records to be inserted, deleted by name or coordinate, and searched by","chapter-13","Advanced Tree Structures"
"name or coordinate. You should also support region queries, that is, a request","chapter-13","Advanced Tree Structures"
"to print all records within a given distance of a specified point.","chapter-13","Advanced Tree Structures"
"13.7 Implement and test the PR quadtree, using the composite design to imple-","chapter-13","Advanced Tree Structures"
"ment the insert, search, and delete operations.","chapter-13","Advanced Tree Structures"
"13.8 Implement a city database using the bintree. Each database record contains","chapter-13","Advanced Tree Structures"
"the name of the city (a string of arbitrary length) and the coordinates of the","chapter-13","Advanced Tree Structures"
"city expressed as integer x- and y-coordinates. Your database should allow","chapter-13","Advanced Tree Structures"
"records to be inserted, deleted by name or coordinate, and searched by name","chapter-13","Advanced Tree Structures"
"or coordinate. You should also support region queries, that is, a request to","chapter-13","Advanced Tree Structures"
"print all records within a given distance of a specified point.","chapter-13","Advanced Tree Structures"
"13.9 Implement a city database using the point quadtree. Each database record","chapter-13","Advanced Tree Structures"
"contains the name of the city (a string of arbitrary length) and the coordinates","chapter-13","Advanced Tree Structures"
"of the city expressed as integer x- and y-coordinates. Your database should","chapter-13","Advanced Tree Structures"
"allow records to be inserted, deleted by name or coordinate, and searched by","chapter-13","Advanced Tree Structures"
"name or coordinate. You should also support region queries, that is, a request","chapter-13","Advanced Tree Structures"
"to print all records within a given distance of a specified point.","chapter-13","Advanced Tree Structures"
"13.10 Use the PR quadtree to implement an efficient solution to Problem 6.5. That","chapter-13","Advanced Tree Structures"
"is, store the set of points in a PR quadtree. For each point, the PR quadtree","chapter-13","Advanced Tree Structures"
"is used to find those points within distance D that should be equivalenced.","chapter-13","Advanced Tree Structures"
"What is the asymptotic complexity of this solution?","chapter-13","Advanced Tree Structures"
"Sec. 13.6 Projects 457","chapter-13","Advanced Tree Structures"
"13.11 Select any two of the point representations described in this chapter (i.e., the","chapter-13","Advanced Tree Structures"
"k-d tree, the PR quadtree, the bintree, and the point quadtree). Implement","chapter-13","Advanced Tree Structures"
"your two choices and compare them over a wide range of data sets. Describe","chapter-13","Advanced Tree Structures"
"which is easier to implement, which appears to be more space efficient, and","chapter-13","Advanced Tree Structures"
"which appears to be more time efficient.","chapter-13","Advanced Tree Structures"
"13.12 Implement a representation for a collection of (two dimensional) rectangles","chapter-13","Advanced Tree Structures"
"using a quadtree based on regular decomposition. Assume that the space","chapter-13","Advanced Tree Structures"
"being represented is a square whose width and height are some power of","chapter-13","Advanced Tree Structures"
"two. Rectangles are assumed to have integer coordinates and integer width","chapter-13","Advanced Tree Structures"
"and height. Pick some value c, and use as a decomposition rule that a region","chapter-13","Advanced Tree Structures"
"is subdivided into four equal-sized regions whenever it contains more that c","chapter-13","Advanced Tree Structures"
"rectangles. A special case occurs if all of these rectangles intersect at some","chapter-13","Advanced Tree Structures"
"point within the current region (because decomposing such a node would","chapter-13","Advanced Tree Structures"
"never reach termination). In this situation, the node simply stores pointers","chapter-13","Advanced Tree Structures"
"to more than c rectangles. Try your representation on data sets of rectangles","chapter-13","Advanced Tree Structures"
"with varying values of c.","chapter-13","Advanced Tree Structures"
"Often it is easy to invent an equation to model the behavior of an algorithm or","chapter-14","Analysis Techniques"
"data structure. Often it is easy to derive a closed-form solution for the equation","chapter-14","Analysis Techniques"
"should it contain a recurrence or summation. But sometimes analysis proves more","chapter-14","Analysis Techniques"
"difficult. It may take a clever insight to derive the right model, such as the snow-","chapter-14","Analysis Techniques"
"plow argument for analyzing the average run length resulting from Replacement","chapter-14","Analysis Techniques"
"Selection (Section 8.5.2). In this example, once the snowplow argument is under-","chapter-14","Analysis Techniques"
"stood, the resulting equations follow naturally. Sometimes, developing the model","chapter-14","Analysis Techniques"
"is straightforward but analyzing the resulting equations is not. An example is the","chapter-14","Analysis Techniques"
"average-case analysis for Quicksort. The equation given in Section 7.5 simply enu-","chapter-14","Analysis Techniques"
"merates all possible cases for the pivot position, summing corresponding costs for","chapter-14","Analysis Techniques"
"the recursive calls to Quicksort. However, deriving a closed-form solution for the","chapter-14","Analysis Techniques"
"resulting recurrence relation is not as easy.","chapter-14","Analysis Techniques"
"Many analyses of iterative algorithms use a summation to model the cost of a","chapter-14","Analysis Techniques"
"loop. Techniques for finding closed-form solutions to summations are presented in","chapter-14","Analysis Techniques"
"Section 14.1. The cost for many algorithms based on recursion are best modeled","chapter-14","Analysis Techniques"
"by recurrence relations. A discussion of techniques for solving recurrences is pro-","chapter-14","Analysis Techniques"
"vided in Section 14.2. These sections build on the introduction to summations and","chapter-14","Analysis Techniques"
"recurrences provided in Section 2.4, so the reader should already be familiar with","chapter-14","Analysis Techniques"
"that material.","chapter-14","Analysis Techniques"
"Section 14.3 provides an introduction to the topic of amortized analysis. Am-","chapter-14","Analysis Techniques"
"ortized analysis deals with the cost of a series of operations. Perhaps a single","chapter-14","Analysis Techniques"
"operation in the series has high cost, but as a result the cost of the remaining oper-","chapter-14","Analysis Techniques"
"ations is limited. Amortized analysis has been used successfully to analyze several","chapter-14","Analysis Techniques"
"of the algorithms presented in previous sections, including the cost of a series of","chapter-14","Analysis Techniques"
"UNION/FIND operations (Section 6.2), the cost of partition in Quicksort (Sec-","chapter-14","Analysis Techniques"
"tion 7.5), the cost of a series of splay tree operations (Section 13.2), and the cost of","chapter-14","Analysis Techniques"
"a series of operations on self-organizing lists (Section 9.2). Section 14.3 discusses","chapter-14","Analysis Techniques"
"the topic in more detail.","chapter-14","Analysis Techniques"
"461","chapter-14","Analysis Techniques"
"462 Chap. 14 Analysis Techniques","chapter-14","Analysis Techniques"
"14.1 Summation Techniques","chapter-14","Analysis Techniques"
"Consider the following simple summation.","chapter-14","Analysis Techniques"
"Xn","chapter-14","Analysis Techniques"
"i=1","chapter-14","Analysis Techniques"
"i.","chapter-14","Analysis Techniques"
"In Section 2.6.3 it was proved by induction that this summation has the well-known","chapter-14","Analysis Techniques"
"closed form n(n + 1)/2. But while induction is a good technique for proving that","chapter-14","Analysis Techniques"
"a proposed closed-form expression is correct, how do we find a candidate closed-","chapter-14","Analysis Techniques"
"form expression to test in the first place? Let us try to think through this problem","chapter-14","Analysis Techniques"
"from first principles, as though we had never seen it before.","chapter-14","Analysis Techniques"
"A good place to begin analyzing a summation it is to give an estimate of its","chapter-14","Analysis Techniques"
"value for a given n. Observe that the biggest term for this summation is n, and","chapter-14","Analysis Techniques"
"there are n terms being summed up. So the total must be less than n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
". Actually,","chapter-14","Analysis Techniques"
"most terms are much less than n, and the sizes of the terms grows linearly. If we","chapter-14","Analysis Techniques"
"were to draw a picture with bars for the size of the terms, their heights would form a","chapter-14","Analysis Techniques"
"line, and we could enclose them in a box n units wide and n units high. It is easy to","chapter-14","Analysis Techniques"
"see from this that a closer estimate for the summation is about (n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
")/2. Having this","chapter-14","Analysis Techniques"
"estimate in hand helps us when trying to determine an exact closed-form solution,","chapter-14","Analysis Techniques"
"because we will hopefully recognize if our proposed solution is badly wrong.","chapter-14","Analysis Techniques"
"Let us now consider some ways that we might hit upon an exact equation for","chapter-14","Analysis Techniques"
"the closed form solution to this summation. One particularly clever approach we","chapter-14","Analysis Techniques"
"can take is to observe that we can “pair up” the first and last terms, the second and","chapter-14","Analysis Techniques"
"(n − 1)th terms, and so on. Each pair sums to n + 1. The number of pairs is n/2.","chapter-14","Analysis Techniques"
"Thus, the solution is n(n+ 1)/2. This is pretty, and there is no doubt about it being","chapter-14","Analysis Techniques"
"correct. The problem is that it is not a useful technique for solving many other","chapter-14","Analysis Techniques"
"summations.","chapter-14","Analysis Techniques"
"Now let us try to do something a bit more general. We already recognized","chapter-14","Analysis Techniques"
"that, because the largest term is n and there are n terms, the summation is less","chapter-14","Analysis Techniques"
"than n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
". If we are lucky, the closed form solution is a polynomial. Using that as","chapter-14","Analysis Techniques"
"a working assumption, we can invoke a technique called guess-and-test. We will","chapter-14","Analysis Techniques"
"guess that the closed-form solution for this summation is a polynomial of the form","chapter-14","Analysis Techniques"
"c1n","chapter-14","Analysis Techniques"
"2 + c2n + c3 for some constants c1, c2, and c3. If this is true, then we can plug","chapter-14","Analysis Techniques"
"in the answers to small cases of the summation to solve for the coefficients. For","chapter-14","Analysis Techniques"
"this example, substituting 0, 1, and 2 for n leads to three simultaneous equations.","chapter-14","Analysis Techniques"
"Because the summation when n = 0 is just 0, c3 must be 0. For n = 1 and n = 2","chapter-14","Analysis Techniques"
"we get the two equations","chapter-14","Analysis Techniques"
"c1 + c2 = 1","chapter-14","Analysis Techniques"
"4c1 + 2c2 = 3,","chapter-14","Analysis Techniques"
"Sec. 14.1 Summation Techniques 463","chapter-14","Analysis Techniques"
"which in turn yield c1 = 1/2 and c2 = 1/2. Thus, if the closed-form solution for","chapter-14","Analysis Techniques"
"the summation is a polynomial, it can only be","chapter-14","Analysis Techniques"
"1/2n","chapter-14","Analysis Techniques"
"2 + 1/2n + 0","chapter-14","Analysis Techniques"
"which is more commonly written","chapter-14","Analysis Techniques"
"n(n + 1)","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"At this point, we still must do the “test” part of the guess-and-test approach. We","chapter-14","Analysis Techniques"
"can use an induction proof to verify whether our candidate closed-form solution is","chapter-14","Analysis Techniques"
"correct. In this case it is indeed correct, as shown by Example 2.11. The induc-","chapter-14","Analysis Techniques"
"tion proof is necessary because our initial assumption that the solution is a simple","chapter-14","Analysis Techniques"
"polynomial could be wrong. For example, it might have been that the true solution","chapter-14","Analysis Techniques"
"includes a logarithmic term, such as c1n","chapter-14","Analysis Techniques"
"2 + c2n log n. The process shown here is","chapter-14","Analysis Techniques"
"essentially fitting a curve to a fixed number of points. Because there is always an","chapter-14","Analysis Techniques"
"n-degree polynomial that fits n + 1 points, we have not done enough work to be","chapter-14","Analysis Techniques"
"sure that we to know the true equation without the induction proof.","chapter-14","Analysis Techniques"
"Guess-and-test is useful whenever the solution is a polynomial expression. In","chapter-14","Analysis Techniques"
"particular, similar reasoning can be used to solve for Pn","chapter-14","Analysis Techniques"
"i=1 i","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"P","chapter-14","Analysis Techniques"
", or more generally","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"i=1 i","chapter-14","Analysis Techniques"
"c","chapter-14","Analysis Techniques"
"for c any positive integer. Why is this not a universal approach to solving","chapter-14","Analysis Techniques"
"summations? Because many summations do not have a polynomial as their closed","chapter-14","Analysis Techniques"
"form solution.","chapter-14","Analysis Techniques"
"A more general approach is based on the subtract-and-guess or divide-and-","chapter-14","Analysis Techniques"
"guess strategies. One form of subtract-and-guess is known as the shifting method.","chapter-14","Analysis Techniques"
"The shifting method subtracts the summation from a variation on the summation.","chapter-14","Analysis Techniques"
"The variation selected for the subtraction should be one that makes most of the","chapter-14","Analysis Techniques"
"terms cancel out. To solve sum f, we pick a known function g and find a pattern in","chapter-14","Analysis Techniques"
"terms of f(n) − g(n) or f(n)/g(n).","chapter-14","Analysis Techniques"
"Example 14.1 Find the closed form solution for Pn","chapter-14","Analysis Techniques"
"i=1 i using the divide-","chapter-14","Analysis Techniques"
"and-guess approach. We will try two example functions to illustrate the","chapter-14","Analysis Techniques"
"divide-and-guess method: dividing by n and dividing by f(n − 1). Our","chapter-14","Analysis Techniques"
"goal is to find patterns that we can use to guess a closed-form expression as","chapter-14","Analysis Techniques"
"our candidate for testing with an induction proof. To aid us in finding such","chapter-14","Analysis Techniques"
"patterns, we can construct a table showing the first few numbers of each","chapter-14","Analysis Techniques"
"function, and the result of dividing one by the other, as follows.","chapter-14","Analysis Techniques"
"n 1 2 3 4 5 6 7 8 9 10","chapter-14","Analysis Techniques"
"f(n) 1 3 6 10 15 21 28 36 46 57","chapter-14","Analysis Techniques"
"n 1 2 3 4 5 6 7 8 9 10","chapter-14","Analysis Techniques"
"f(n)/n 2/2 3/2 4/2 5/2 6/2 7/2 8/2 9/2 10/2 11/2","chapter-14","Analysis Techniques"
"f(n−1) 0 1 3 6 10 15 21 28 36 46","chapter-14","Analysis Techniques"
"f(n)/f(n−1) 3/1 4/2 5/3 6/4 7/5 8/6 9/7 10/8 11/9","chapter-14","Analysis Techniques"
"464 Chap. 14 Analysis Techniques","chapter-14","Analysis Techniques"
"Dividing by both n and f(n − 1) happen to give us useful patterns to","chapter-14","Analysis Techniques"
"work with. f(n)","chapter-14","Analysis Techniques"
"n =","chapter-14","Analysis Techniques"
"n+1","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
", and f(n)","chapter-14","Analysis Techniques"
"f(n−1) =","chapter-14","Analysis Techniques"
"n+1","chapter-14","Analysis Techniques"
"n−1","chapter-14","Analysis Techniques"
". Of course, lots of other","chapter-14","Analysis Techniques"
"guesses for function g do not work. For example, f(n) − n = f(n −","chapter-14","Analysis Techniques"
"1). Knowing that f(n) = f(n − 1) + n is not useful for determining the","chapter-14","Analysis Techniques"
"closed form solution to this summation. Or consider f(n) − f(n − 1) = n.","chapter-14","Analysis Techniques"
"Again, knowing that f(n) = f(n − 1) + n is not useful. Finding the right","chapter-14","Analysis Techniques"
"combination of equations can be like finding a needle in a haystack.","chapter-14","Analysis Techniques"
"In our first example, we can see directly what the closed-form solution","chapter-14","Analysis Techniques"
"should be. Since f(n)","chapter-14","Analysis Techniques"
"n =","chapter-14","Analysis Techniques"
"n+1","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
", obviously f(n) = n(n + 1)/2.","chapter-14","Analysis Techniques"
"Dividing f(n) by f(n − 1) does not give so obvious a result, but it","chapter-14","Analysis Techniques"
"provides another useful illustration.","chapter-14","Analysis Techniques"
"f(n)","chapter-14","Analysis Techniques"
"f(n − 1) =","chapter-14","Analysis Techniques"
"n + 1","chapter-14","Analysis Techniques"
"n − 1","chapter-14","Analysis Techniques"
"f(n)(n − 1) = (n + 1)f(n − 1)","chapter-14","Analysis Techniques"
"f(n)(n − 1) = (n + 1)(f(n) − n)","chapter-14","Analysis Techniques"
"nf(n) − f(n) = nf(n) + f(n) − n","chapter-14","Analysis Techniques"
"2 − n","chapter-14","Analysis Techniques"
"2f(n) = n","chapter-14","Analysis Techniques"
"2 + n = n(n + 1)","chapter-14","Analysis Techniques"
"f(n) = n(n + 1)","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"Once again, we still do not have a proof that f(n) = n(n+ 1)/2. Why?","chapter-14","Analysis Techniques"
"Because we did not prove that f(n)/n = (n + 1)/2 nor that f(n)/f(n −","chapter-14","Analysis Techniques"
"1) = (n + 1)(n − 1). We merely hypothesized patterns from looking at a","chapter-14","Analysis Techniques"
"few terms. Fortunately, it is easy to check our hypothesis with induction.","chapter-14","Analysis Techniques"
"Example 14.2 Solve the summation","chapter-14","Analysis Techniques"
"Xn","chapter-14","Analysis Techniques"
"i=1","chapter-14","Analysis Techniques"
"1/2","chapter-14","Analysis Techniques"
"i","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"We will begin by writing out a table listing the first few values of the sum-","chapter-14","Analysis Techniques"
"mation, to see if we can detect a pattern.","chapter-14","Analysis Techniques"
"n 1 2 3 4 5 6","chapter-14","Analysis Techniques"
"f(n)","chapter-14","Analysis Techniques"
"1","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"3","chapter-14","Analysis Techniques"
"4","chapter-14","Analysis Techniques"
"7","chapter-14","Analysis Techniques"
"8","chapter-14","Analysis Techniques"
"15","chapter-14","Analysis Techniques"
"16","chapter-14","Analysis Techniques"
"31","chapter-14","Analysis Techniques"
"32","chapter-14","Analysis Techniques"
"63","chapter-14","Analysis Techniques"
"64","chapter-14","Analysis Techniques"
"1 − f(n)","chapter-14","Analysis Techniques"
"1","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"1","chapter-14","Analysis Techniques"
"4","chapter-14","Analysis Techniques"
"1","chapter-14","Analysis Techniques"
"8","chapter-14","Analysis Techniques"
"1","chapter-14","Analysis Techniques"
"16","chapter-14","Analysis Techniques"
"1","chapter-14","Analysis Techniques"
"32","chapter-14","Analysis Techniques"
"1","chapter-14","Analysis Techniques"
"64","chapter-14","Analysis Techniques"
"Sec. 14.1 Summation Techniques 465","chapter-14","Analysis Techniques"
"By direct inspection of the second line of the table, we might recognize the","chapter-14","Analysis Techniques"
"pattern f(n) = 2","chapter-14","Analysis Techniques"
"n−1","chapter-14","Analysis Techniques"
"2n . A simple induction proof can then prove that this","chapter-14","Analysis Techniques"
"always holds true. Alternatively, consider if we hadn’t noticed the pattern","chapter-14","Analysis Techniques"
"for the form of f(n). We might observe that f(n) appears to be reaching","chapter-14","Analysis Techniques"
"an asymptote at one. In which case, we might consider looking at the dif-","chapter-14","Analysis Techniques"
"ference between f(n) and the expected asymptote. This result is shown in","chapter-14","Analysis Techniques"
"the last line of the table, which has a clear pattern since the ith entry is of","chapter-14","Analysis Techniques"
"1/2","chapter-14","Analysis Techniques"
"i","chapter-14","Analysis Techniques"
". From this we can easily deduce a guess that f(n) = 1 −","chapter-14","Analysis Techniques"
"1","chapter-14","Analysis Techniques"
"2n . Again,","chapter-14","Analysis Techniques"
"a simple induction proof will verify the guess.","chapter-14","Analysis Techniques"
"Example 14.3 Solve the summation","chapter-14","Analysis Techniques"
"f(n) = Xn","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"ari = a + ar + ar2 + · · · + arn","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"This is called a geometric series. Our goal is to find some function g(n)","chapter-14","Analysis Techniques"
"such that the difference between f(n) and g(n) one from the other leaves","chapter-14","Analysis Techniques"
"us with an easily manipulated equation. Because the difference between","chapter-14","Analysis Techniques"
"consecutive terms of the summation is a factor of r, we can shift terms if","chapter-14","Analysis Techniques"
"we multiply the entire expression by r:","chapter-14","Analysis Techniques"
"rf(n) = r","chapter-14","Analysis Techniques"
"Xn","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"ari = ar + ar2 + ar3 + · · · + arn+1","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"We can now subtract the one equation from the other, as follows:","chapter-14","Analysis Techniques"
"f(n) − rf(n) = a + ar + ar2 + ar3 + · · · + arn","chapter-14","Analysis Techniques"
"− (ar + ar2 + ar3 + · · · + arn","chapter-14","Analysis Techniques"
") − arn+1","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"The result leaves only the end terms:","chapter-14","Analysis Techniques"
"f(n) − rf(n) = Xn","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"ari − r","chapter-14","Analysis Techniques"
"Xn","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"ari","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"(1 − r)f(n) = a − arn+1","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"Thus, we get the result","chapter-14","Analysis Techniques"
"f(n) = a − arn+1","chapter-14","Analysis Techniques"
"1 − r","chapter-14","Analysis Techniques"
"where r 6= 1.","chapter-14","Analysis Techniques"
"466 Chap. 14 Analysis Techniques","chapter-14","Analysis Techniques"
"Example 14.4 For our second example of the shifting method, we solve","chapter-14","Analysis Techniques"
"f(n) = Xn","chapter-14","Analysis Techniques"
"i=1","chapter-14","Analysis Techniques"
"i2","chapter-14","Analysis Techniques"
"i = 1 · 2","chapter-14","Analysis Techniques"
"1 + 2 · 2","chapter-14","Analysis Techniques"
"2 + 3 · 2","chapter-14","Analysis Techniques"
"3 + · · · + n · 2","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"We can achieve our goal if we multiply by two:","chapter-14","Analysis Techniques"
"2f(n) = 2Xn","chapter-14","Analysis Techniques"
"i=1","chapter-14","Analysis Techniques"
"i2","chapter-14","Analysis Techniques"
"i = 1 · 2","chapter-14","Analysis Techniques"
"2 + 2 · 2","chapter-14","Analysis Techniques"
"3 + 3 · 2","chapter-14","Analysis Techniques"
"4 + · · · + (n − 1)· 2","chapter-14","Analysis Techniques"
"n + n · 2","chapter-14","Analysis Techniques"
"n+1","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"The ith term of 2f(n) is i · 2","chapter-14","Analysis Techniques"
"i+1, while the (i + 1)th term of f(n) is","chapter-14","Analysis Techniques"
"(i + 1) · 2","chapter-14","Analysis Techniques"
"i+1. Subtracting one expression from the other yields the sum-","chapter-14","Analysis Techniques"
"mation of 2","chapter-14","Analysis Techniques"
"i","chapter-14","Analysis Techniques"
"and a few non-canceled terms:","chapter-14","Analysis Techniques"
"2f(n) − f(n) = 2Xn","chapter-14","Analysis Techniques"
"i=1","chapter-14","Analysis Techniques"
"i2","chapter-14","Analysis Techniques"
"i −","chapter-14","Analysis Techniques"
"Xn","chapter-14","Analysis Techniques"
"i=1","chapter-14","Analysis Techniques"
"i2","chapter-14","Analysis Techniques"
"i","chapter-14","Analysis Techniques"
"=","chapter-14","Analysis Techniques"
"Xn","chapter-14","Analysis Techniques"
"i=1","chapter-14","Analysis Techniques"
"i2","chapter-14","Analysis Techniques"
"i+1 −","chapter-14","Analysis Techniques"
"Xn","chapter-14","Analysis Techniques"
"i=1","chapter-14","Analysis Techniques"
"i2","chapter-14","Analysis Techniques"
"i","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"Shift i’s value in the second summation, substituting (i + 1) for i:","chapter-14","Analysis Techniques"
"= n2","chapter-14","Analysis Techniques"
"n+1 +","chapter-14","Analysis Techniques"
"nX−1","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"i2","chapter-14","Analysis Techniques"
"i+1 −","chapter-14","Analysis Techniques"
"nX−1","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"(i + 1)2i+1","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"Break the second summation into two parts:","chapter-14","Analysis Techniques"
"= n2","chapter-14","Analysis Techniques"
"n+1 +","chapter-14","Analysis Techniques"
"nX−1","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"i2","chapter-14","Analysis Techniques"
"i+1 −","chapter-14","Analysis Techniques"
"nX−1","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"i2","chapter-14","Analysis Techniques"
"i+1 −","chapter-14","Analysis Techniques"
"nX−1","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"i+1","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"Cancel like terms:","chapter-14","Analysis Techniques"
"= n2","chapter-14","Analysis Techniques"
"n+1 −","chapter-14","Analysis Techniques"
"nX−1","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"i+1","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"Again shift i’s value in the summation, substituting i for (i + 1):","chapter-14","Analysis Techniques"
"= n2","chapter-14","Analysis Techniques"
"n+1 −","chapter-14","Analysis Techniques"
"Xn","chapter-14","Analysis Techniques"
"i=1","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"i","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"Replace the new summation with a solution that we already know:","chapter-14","Analysis Techniques"
"= n2","chapter-14","Analysis Techniques"
"n+1 −","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"n+1 − 2","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"Finally, reorganize the equation:","chapter-14","Analysis Techniques"
"= (n − 1)2n+1 + 2.","chapter-14","Analysis Techniques"
"Sec. 14.2 Recurrence Relations 467","chapter-14","Analysis Techniques"
"14.2 Recurrence Relations","chapter-14","Analysis Techniques"
"Recurrence relations are often used to model the cost of recursive functions. For","chapter-14","Analysis Techniques"
"example, the standard Mergesort (Section 7.4) takes a list of size n, splits it in half,","chapter-14","Analysis Techniques"
"performs Mergesort on each half, and finally merges the two sublists in n steps.","chapter-14","Analysis Techniques"
"The cost for this can be modeled as","chapter-14","Analysis Techniques"
"T(n) = 2T(n/2) + n.","chapter-14","Analysis Techniques"
"In other words, the cost of the algorithm on input of size n is two times the cost for","chapter-14","Analysis Techniques"
"input of size n/2 (due to the two recursive calls to Mergesort) plus n (the time to","chapter-14","Analysis Techniques"
"merge the sublists together again).","chapter-14","Analysis Techniques"
"There are many approaches to solving recurrence relations, and we briefly con-","chapter-14","Analysis Techniques"
"sider three here. The first is an estimation technique: Guess the upper and lower","chapter-14","Analysis Techniques"
"bounds for the recurrence, use induction to prove the bounds, and tighten as re-","chapter-14","Analysis Techniques"
"quired. The second approach is to expand the recurrence to convert it to a summa-","chapter-14","Analysis Techniques"
"tion and then use summation techniques. The third approach is to take advantage","chapter-14","Analysis Techniques"
"of already proven theorems when the recurrence is of a suitable form. In particu-","chapter-14","Analysis Techniques"
"lar, typical divide and conquer algorithms such as Mergesort yield recurrences of a","chapter-14","Analysis Techniques"
"form that fits a pattern for which we have a ready solution.","chapter-14","Analysis Techniques"
"14.2.1 Estimating Upper and Lower Bounds","chapter-14","Analysis Techniques"
"The first approach to solving recurrences is to guess the answer and then attempt","chapter-14","Analysis Techniques"
"to prove it correct. If a correct upper or lower bound estimate is given, an easy","chapter-14","Analysis Techniques"
"induction proof will verify this fact. If the proof is successful, then try to tighten","chapter-14","Analysis Techniques"
"the bound. If the induction proof fails, then loosen the bound and try again. Once","chapter-14","Analysis Techniques"
"the upper and lower bounds match, you are finished. This is a useful technique","chapter-14","Analysis Techniques"
"when you are only looking for asymptotic complexities. When seeking a precise","chapter-14","Analysis Techniques"
"closed-form solution (i.e., you seek the constants for the expression), this method","chapter-14","Analysis Techniques"
"will probably be too much work.","chapter-14","Analysis Techniques"
"Example 14.5 Use the guessing technique to find the asymptotic bounds","chapter-14","Analysis Techniques"
"for Mergesort, whose running time is described by the equation","chapter-14","Analysis Techniques"
"T(n) = 2T(n/2) + n; T(2) = 1.","chapter-14","Analysis Techniques"
"We begin by guessing that this recurrence has an upper bound in O(n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"). To","chapter-14","Analysis Techniques"
"be more precise, assume that","chapter-14","Analysis Techniques"
"T(n) ≤ n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"We prove this guess is correct by induction. In this proof, we assume that","chapter-14","Analysis Techniques"
"n is a power of two, to make the calculations easy. For the base case,","chapter-14","Analysis Techniques"
"468 Chap. 14 Analysis Techniques","chapter-14","Analysis Techniques"
"T(2) = 1 ≤ 2","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
". For the induction step, we need to show that T(n) ≤ n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"implies that T(2n) ≤ (2n)","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"for n = 2N , N ≥ 1. The induction hypothesis","chapter-14","Analysis Techniques"
"is","chapter-14","Analysis Techniques"
"T(i) ≤ i","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
", for all i ≤ n.","chapter-14","Analysis Techniques"
"It follows that","chapter-14","Analysis Techniques"
"T(2n) = 2T(n) + 2n ≤ 2n","chapter-14","Analysis Techniques"
"2 + 2n ≤ 4n","chapter-14","Analysis Techniques"
"2 ≤ (2n)","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"which is what we wanted to prove. Thus, T(n) is in O(n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
").","chapter-14","Analysis Techniques"
"Is O(n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
") a good estimate? In the next-to-last step we went from n","chapter-14","Analysis Techniques"
"2+2n","chapter-14","Analysis Techniques"
"to the much larger 4n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
". This suggests that O(n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
") is a high estimate. If we","chapter-14","Analysis Techniques"
"guess something smaller, such as T(n) ≤ cn for some constant c, it should","chapter-14","Analysis Techniques"
"be clear that this cannot work because c2n = 2cn and there is no room for","chapter-14","Analysis Techniques"
"the extra n cost to join the two pieces together. Thus, the true cost must be","chapter-14","Analysis Techniques"
"somewhere between cn and n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"Let us now try T(n) ≤ n log n. For the base case, the definition of the","chapter-14","Analysis Techniques"
"recurrence sets T(2) = 1 ≤ (2·log 2) = 2. Assume (induction hypothesis)","chapter-14","Analysis Techniques"
"that T(n) ≤ n log n. Then,","chapter-14","Analysis Techniques"
"T(2n) = 2T(n) + 2n ≤ 2n log n + 2n ≤ 2n(log n + 1) ≤ 2n log 2n","chapter-14","Analysis Techniques"
"which is what we seek to prove. In similar fashion, we can prove that T(n)","chapter-14","Analysis Techniques"
"is in Ω(n log n). Thus, T(n) is also Θ(n log n).","chapter-14","Analysis Techniques"
"Example 14.6 We know that the factorial function grows exponentially.","chapter-14","Analysis Techniques"
"How does it compare to 2","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"? To n","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"? Do they all grow “equally fast” (in an","chapter-14","Analysis Techniques"
"asymptotic sense)? We can begin by looking at a few initial terms.","chapter-14","Analysis Techniques"
"n 1 2 3 4 5 6 7 8 9","chapter-14","Analysis Techniques"
"n! 1 2 6 24 120 720 5040 40320 362880","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"n 2 4 8 16 32 64 128 256 512","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"n 1 4 9 256 3125 46656 823543 16777216 387420489","chapter-14","Analysis Techniques"
"We can also look at these functions in terms of their recurrences.","chapter-14","Analysis Techniques"
"n! = ","chapter-14","Analysis Techniques"
"1 n = 1","chapter-14","Analysis Techniques"
"n(n − 1)! n > 1","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"n =","chapter-14","Analysis Techniques"
"2 n = 1","chapter-14","Analysis Techniques"
"2(2n−1","chapter-14","Analysis Techniques"
") n > 1","chapter-14","Analysis Techniques"
"Sec. 14.2 Recurrence Relations 469","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"n =","chapter-14","Analysis Techniques"
"n n = 1","chapter-14","Analysis Techniques"
"n(n","chapter-14","Analysis Techniques"
"n−1","chapter-14","Analysis Techniques"
") n > 1","chapter-14","Analysis Techniques"
"At this point, our intuition should be telling us pretty clearly the relative","chapter-14","Analysis Techniques"
"growth rates of these three functions. But how do we prove formally which","chapter-14","Analysis Techniques"
"grows the fastest? And how do we decide if the differences are significant","chapter-14","Analysis Techniques"
"in an asymptotic sense, or just constant factor differences?","chapter-14","Analysis Techniques"
"We can use logarithms to help us get an idea about the relative growth","chapter-14","Analysis Techniques"
"rates of these functions. Clearly, log 2n = n. Equally clearly, log n","chapter-14","Analysis Techniques"
"n =","chapter-14","Analysis Techniques"
"n log n. We can easily see from this that 2","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"is o(n","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"), that is, n","chapter-14","Analysis Techniques"
"n grows","chapter-14","Analysis Techniques"
"asymptotically faster than 2","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"How does n! fit into this? We can again take advantage of logarithms.","chapter-14","Analysis Techniques"
"Obviously n! ≤ n","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
", so we know that log n! is O(n log n). But what about","chapter-14","Analysis Techniques"
"a lower bound for the factorial function? Consider the following.","chapter-14","Analysis Techniques"
"n! = n × (n − 1) × · · · ×","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"× (","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"− 1) × · · · × 2 × 1","chapter-14","Analysis Techniques"
"≥","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"×","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"× · · · ×","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"× 1 × · · · × 1 × 1","chapter-14","Analysis Techniques"
"= (n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
")","chapter-14","Analysis Techniques"
"n/2","chapter-14","Analysis Techniques"
"Therefore","chapter-14","Analysis Techniques"
"log n! ≥ log(n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
")","chapter-14","Analysis Techniques"
"n/2 = (n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
") log(n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
").","chapter-14","Analysis Techniques"
"In other words, log n! is in Ω(n log n). Thus, log n! = Θ(n log n).","chapter-14","Analysis Techniques"
"Note that this does not mean that n! = Θ(n","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"). Because log n","chapter-14","Analysis Techniques"
"2 =","chapter-14","Analysis Techniques"
"2 log n, it follows that log n = Θ(log n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
") but n 6= Θ(n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"). The log func-","chapter-14","Analysis Techniques"
"tion often works as a “flattener” when dealing with asymptotics. That is,","chapter-14","Analysis Techniques"
"whenever log f(n) is in O(log g(n)) we know that f(n) is in O(g(n)).","chapter-14","Analysis Techniques"
"But knowing that log f(n) = Θ(log g(n)) does not necessarily mean that","chapter-14","Analysis Techniques"
"f(n) = Θ(g(n)).","chapter-14","Analysis Techniques"
"Example 14.7 What is the growth rate of the Fibonacci sequence? We","chapter-14","Analysis Techniques"
"define the Fibonacci sequence as f(n) = f(n − 1) + f(n − 2) for n ≥ 2;","chapter-14","Analysis Techniques"
"f(0) = f(1) = 1.","chapter-14","Analysis Techniques"
"In this case it is useful to compare the ratio of f(n) to f(n − 1). The","chapter-14","Analysis Techniques"
"following table shows the first few values.","chapter-14","Analysis Techniques"
"n 1 2 3 4 5 6 7","chapter-14","Analysis Techniques"
"f(n) 1 2 3 5 8 13 21","chapter-14","Analysis Techniques"
"f(n)/f(n − 1) 1 2 1.5 1.666 1.625 1.615 1.619","chapter-14","Analysis Techniques"
"470 Chap. 14 Analysis Techniques","chapter-14","Analysis Techniques"
"If we continue for more terms, the ratio appears to converge on a value","chapter-14","Analysis Techniques"
"slightly greater then 1.618. Assuming f(n)/f(n − 1) really does converge","chapter-14","Analysis Techniques"
"to a fixed value as n grows, we can determine what that value must be.","chapter-14","Analysis Techniques"
"f(n)","chapter-14","Analysis Techniques"
"f(n − 2) =","chapter-14","Analysis Techniques"
"f(n − 1)","chapter-14","Analysis Techniques"
"f(n − 2) +","chapter-14","Analysis Techniques"
"f(n − 2)","chapter-14","Analysis Techniques"
"f(n − 2) → x + 1","chapter-14","Analysis Techniques"
"For some value x. This follows from the fact that f(n) = f(n − 1) +","chapter-14","Analysis Techniques"
"f(n − 2). We divide by f(n − 2) to make the second term go away, and","chapter-14","Analysis Techniques"
"we also get something useful in the first term. Remember that the goal of","chapter-14","Analysis Techniques"
"such manipulations is to give us an equation that relates f(n) to something","chapter-14","Analysis Techniques"
"without recursive calls.","chapter-14","Analysis Techniques"
"For large n, we also observe that:","chapter-14","Analysis Techniques"
"f(n)","chapter-14","Analysis Techniques"
"f(n − 2) =","chapter-14","Analysis Techniques"
"f(n)","chapter-14","Analysis Techniques"
"f(n − 1)","chapter-14","Analysis Techniques"
"f(n − 1)","chapter-14","Analysis Techniques"
"f(n − 2) → x","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"as n gets big. This comes from multiplying f(n)/f(n − 2) by f(n −","chapter-14","Analysis Techniques"
"1)/f(n − 1) and rearranging.","chapter-14","Analysis Techniques"
"If x exists, then x","chapter-14","Analysis Techniques"
"2 −x−1 → 0. Using the quadratic equation, the only","chapter-14","Analysis Techniques"
"solution greater than one is","chapter-14","Analysis Techniques"
"x =","chapter-14","Analysis Techniques"
"1 + √","chapter-14","Analysis Techniques"
"5","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"≈ 1.618.","chapter-14","Analysis Techniques"
"This expression also has the name φ. What does this say about the growth","chapter-14","Analysis Techniques"
"rate of the Fibonacci sequence? It is exponential, with f(n) = Θ(φ","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
").","chapter-14","Analysis Techniques"
"More precisely, f(n) converges to","chapter-14","Analysis Techniques"
"φ","chapter-14","Analysis Techniques"
"n − (1 − φ)","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"√","chapter-14","Analysis Techniques"
"5","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"14.2.2 Expanding Recurrences","chapter-14","Analysis Techniques"
"Estimating bounds is effective if you only need an approximation to the answer.","chapter-14","Analysis Techniques"
"More precise techniques are required to find an exact solution. One approach is","chapter-14","Analysis Techniques"
"called expanding the recurrence. In this method, the smaller terms on the right","chapter-14","Analysis Techniques"
"side of the equation are in turn replaced by their definition. This is the expanding","chapter-14","Analysis Techniques"
"step. These terms are again expanded, and so on, until a full series with no recur-","chapter-14","Analysis Techniques"
"rence results. This yields a summation, and techniques for solving summations can","chapter-14","Analysis Techniques"
"then be used. A couple of simple expansions were shown in Section 2.4. A more","chapter-14","Analysis Techniques"
"complex example is given below.","chapter-14","Analysis Techniques"
"Sec. 14.2 Recurrence Relations 471","chapter-14","Analysis Techniques"
"Example 14.8 Find the solution for","chapter-14","Analysis Techniques"
"T(n) = 2T(n/2) + 5n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"; T(1) = 7.","chapter-14","Analysis Techniques"
"For simplicity we assume that n is a power of two, so we will rewrite it as","chapter-14","Analysis Techniques"
"n = 2k","chapter-14","Analysis Techniques"
". This recurrence can be expanded as follows:","chapter-14","Analysis Techniques"
"T(n) = 2T(n/2) + 5n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"= 2(2T(n/4) + 5(n/2)2","chapter-14","Analysis Techniques"
") + 5n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"= 2(2(2T(n/8) + 5(n/4)2","chapter-14","Analysis Techniques"
") + 5(n/2)2","chapter-14","Analysis Techniques"
") + 5n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"= 2kT(1) + 2k−1","chapter-14","Analysis Techniques"
"· 5","chapter-14","Analysis Techniques"
 "n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"k−1","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"+ · · · + 2 · 5","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"+ 5n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"This last expression can best be represented by a summation as follows:","chapter-14","Analysis Techniques"
"7n + 5X","chapter-14","Analysis Techniques"
"k−1","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"/2","chapter-14","Analysis Techniques"
"i","chapter-14","Analysis Techniques"
"= 7n + 5n","chapter-14","Analysis Techniques"
"2X","chapter-14","Analysis Techniques"
"k−1","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"1/2","chapter-14","Analysis Techniques"
"i","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"From Equation 2.6, we have:","chapter-14","Analysis Techniques"
"= 7n + 5n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"2 − 1/2","chapter-14","Analysis Techniques"
"k−1","chapter-14","Analysis Techniques"
"= 7n + 5n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"(2 − 2/n)","chapter-14","Analysis Techniques"
"= 7n + 10n","chapter-14","Analysis Techniques"
"2 − 10n","chapter-14","Analysis Techniques"
"= 10n","chapter-14","Analysis Techniques"
"2 − 3n.","chapter-14","Analysis Techniques"
"This is the exact solution to the recurrence for n a power of two. At this","chapter-14","Analysis Techniques"
"point, we should use a simple induction proof to verify that our solution is","chapter-14","Analysis Techniques"
"indeed correct.","chapter-14","Analysis Techniques"
"Example 14.9 Our next example models the cost of the algorithm to build","chapter-14","Analysis Techniques"
"a heap. Recall from Section 5.5 that to build a heap, we first heapify the","chapter-14","Analysis Techniques"
"two subheaps, then push down the root to its proper position. The cost is:","chapter-14","Analysis Techniques"
"f(n) ≤ 2f(n/2) + 2 log n.","chapter-14","Analysis Techniques"
"Let us find a closed form solution for this recurrence. We can expand","chapter-14","Analysis Techniques"
"the recurrence a few times to see that","chapter-14","Analysis Techniques"
"472 Chap. 14 Analysis Techniques","chapter-14","Analysis Techniques"
"f(n) ≤ 2f(n/2) + 2 log n","chapter-14","Analysis Techniques"
"≤ 2[2f(n/4) + 2 log n/2] + 2 log n","chapter-14","Analysis Techniques"
"≤ 2[2(2f(n/8) + 2 log n/4) + 2 log n/2] + 2 log n","chapter-14","Analysis Techniques"
"We can deduce from this expansion that this recurrence is equivalent to","chapter-14","Analysis Techniques"
"following summation and its derivation:","chapter-14","Analysis Techniques"
"f(n) ≤","chapter-14","Analysis Techniques"
"log","chapter-14","Analysis Techniques"
"Xn−1","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"i+1 log(n/2","chapter-14","Analysis Techniques"
"i","chapter-14","Analysis Techniques"
")","chapter-14","Analysis Techniques"
"= 2","chapter-14","Analysis Techniques"
"log","chapter-14","Analysis Techniques"
"Xn−1","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"i","chapter-14","Analysis Techniques"
"(log n − i)","chapter-14","Analysis Techniques"
"= 2 log n","chapter-14","Analysis Techniques"
"log","chapter-14","Analysis Techniques"
"Xn−1","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"i − 4","chapter-14","Analysis Techniques"
"log","chapter-14","Analysis Techniques"
"Xn−1","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"i2","chapter-14","Analysis Techniques"
"i−1","chapter-14","Analysis Techniques"
"= 2n log n − 2 log n − 2n log n + 4n − 4","chapter-14","Analysis Techniques"
"= 4n − 2 log n − 4.","chapter-14","Analysis Techniques"
"14.2.3 Divide and Conquer Recurrences","chapter-14","Analysis Techniques"
"The third approach to solving recurrences is to take advantage of known theorems","chapter-14","Analysis Techniques"
"that provide the solution for classes of recurrences. Of particular practical use is","chapter-14","Analysis Techniques"
"a theorem that gives the answer for a class known as divide and conquer recur-","chapter-14","Analysis Techniques"
"rences. These have the form","chapter-14","Analysis Techniques"
"T(n) = aT(n/b) + cnk","chapter-14","Analysis Techniques"
"; T(1) = c","chapter-14","Analysis Techniques"
"where a, b, c, and k are constants. In general, this recurrence describes a problem","chapter-14","Analysis Techniques"
"of size n divided into a subproblems of size n/b, while cnk","chapter-14","Analysis Techniques"
"is the amount of work","chapter-14","Analysis Techniques"
"necessary to combine the partial solutions. Mergesort is an example of a divide and","chapter-14","Analysis Techniques"
"conquer algorithm, and its recurrence fits this form. So does binary search. We use","chapter-14","Analysis Techniques"
"the method of expanding recurrences to derive the general solution for any divide","chapter-14","Analysis Techniques"
"and conquer recurrence, assuming that n = b","chapter-14","Analysis Techniques"
"m.","chapter-14","Analysis Techniques"
"T(n) = aT(n/b) + cnk","chapter-14","Analysis Techniques"
"= a(aT(n/b2","chapter-14","Analysis Techniques"
") + c(n/b)","chapter-14","Analysis Techniques"
"k","chapter-14","Analysis Techniques"
") + cnk","chapter-14","Analysis Techniques"
"= a(a[aT(n/b3","chapter-14","Analysis Techniques"
") + c(n/b2","chapter-14","Analysis Techniques"
")","chapter-14","Analysis Techniques"
"k","chapter-14","Analysis Techniques"
"] + c(n/b)","chapter-14","Analysis Techniques"
"k","chapter-14","Analysis Techniques"
") + cnk","chapter-14","Analysis Techniques"
"Sec. 14.2 Recurrence Relations 473","chapter-14","Analysis Techniques"
"= a","chapter-14","Analysis Techniques"
"mT(1) + a","chapter-14","Analysis Techniques"
"m−1","chapter-14","Analysis Techniques"
"c(n/bm−1","chapter-14","Analysis Techniques"
")","chapter-14","Analysis Techniques"
"k + · · · + ac(n/b)","chapter-14","Analysis Techniques"
"k + cnk","chapter-14","Analysis Techniques"
"= a","chapter-14","Analysis Techniques"
"mc + a","chapter-14","Analysis Techniques"
"m−1","chapter-14","Analysis Techniques"
"c(n/bm−1","chapter-14","Analysis Techniques"
")","chapter-14","Analysis Techniques"
"k + · · · + ac(n/b)","chapter-14","Analysis Techniques"
"k + cnk","chapter-14","Analysis Techniques"
"= c","chapter-14","Analysis Techniques"
"Xm","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"a","chapter-14","Analysis Techniques"
"m−i","chapter-14","Analysis Techniques"
"b","chapter-14","Analysis Techniques"
"ik","chapter-14","Analysis Techniques"
"= camXm","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"(b","chapter-14","Analysis Techniques"
"k","chapter-14","Analysis Techniques"
"/a)","chapter-14","Analysis Techniques"
"i","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"Note that","chapter-14","Analysis Techniques"
"a","chapter-14","Analysis Techniques"
"m = a","chapter-14","Analysis Techniques"
"logb n = n","chapter-14","Analysis Techniques"
"logb a","chapter-14","Analysis Techniques"
". (14.1)","chapter-14","Analysis Techniques"
"The summation is a geometric series whose sum depends on the ratio r = b","chapter-14","Analysis Techniques"
"k/a.","chapter-14","Analysis Techniques"
"There are three cases.","chapter-14","Analysis Techniques"
"1. r < 1. From Equation 2.4,","chapter-14","Analysis Techniques"
"Xm","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"r","chapter-14","Analysis Techniques"
"i < 1/(1 − r), a constant.","chapter-14","Analysis Techniques"
"Thus,","chapter-14","Analysis Techniques"
"T(n) = Θ(a","chapter-14","Analysis Techniques"
"m) = Θ(n","chapter-14","Analysis Techniques"
"logba","chapter-14","Analysis Techniques"
").","chapter-14","Analysis Techniques"
"2. r = 1. Because r = b","chapter-14","Analysis Techniques"
"k/a, we know that a = b","chapter-14","Analysis Techniques"
"k","chapter-14","Analysis Techniques"
". From the definition","chapter-14","Analysis Techniques"
"of logarithms it follows immediately that k = logb a. We also note from","chapter-14","Analysis Techniques"
"Equation 14.1 that m = logb n. Thus,","chapter-14","Analysis Techniques"
"Xm","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"r = m + 1 = logb n + 1.","chapter-14","Analysis Techniques"
"Because a","chapter-14","Analysis Techniques"
"m = n logb a = n","chapter-14","Analysis Techniques"
"k","chapter-14","Analysis Techniques"
", we have","chapter-14","Analysis Techniques"
"T(n) = Θ(n","chapter-14","Analysis Techniques"
"logb a","chapter-14","Analysis Techniques"
"log n) = Θ(n","chapter-14","Analysis Techniques"
"k","chapter-14","Analysis Techniques"
"log n).","chapter-14","Analysis Techniques"
"3. r > 1. From Equation 2.5,","chapter-14","Analysis Techniques"
"Xm","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"r =","chapter-14","Analysis Techniques"
"r","chapter-14","Analysis Techniques"
"m+1 − 1","chapter-14","Analysis Techniques"
"r − 1","chapter-14","Analysis Techniques"
"= Θ(r","chapter-14","Analysis Techniques"
"m).","chapter-14","Analysis Techniques"
"Thus,","chapter-14","Analysis Techniques"
"T(n) = Θ(a","chapter-14","Analysis Techniques"
"mr","chapter-14","Analysis Techniques"
"m) = Θ(a","chapter-14","Analysis Techniques"
"m(b","chapter-14","Analysis Techniques"
"k","chapter-14","Analysis Techniques"
"/a)","chapter-14","Analysis Techniques"
"m) = Θ(b","chapter-14","Analysis Techniques"
"km) = Θ(n","chapter-14","Analysis Techniques"
"k","chapter-14","Analysis Techniques"
").","chapter-14","Analysis Techniques"
"We can summarize the above derivation as the following theorem, sometimes","chapter-14","Analysis Techniques"
"referred to as the Master Theorem.","chapter-14","Analysis Techniques"
"474 Chap. 14 Analysis Techniques","chapter-14","Analysis Techniques"
"Theorem 14.1 (The Master Theorem) For any recurrence relation of the form","chapter-14","Analysis Techniques"
"T(n) = aT(n/b) + cnk","chapter-14","Analysis Techniques"
", T(1) = c, the following relationships hold.","chapter-14","Analysis Techniques"
"T(n) =","chapter-14","Analysis Techniques"
"","chapter-14","Analysis Techniques"
"","chapter-14","Analysis Techniques"
"","chapter-14","Analysis Techniques"
"Θ(n","chapter-14","Analysis Techniques"
"logb a","chapter-14","Analysis Techniques"
") if a > bk","chapter-14","Analysis Techniques"
"Θ(n","chapter-14","Analysis Techniques"
"k","chapter-14","Analysis Techniques"
"log n) if a = b","chapter-14","Analysis Techniques"
"k","chapter-14","Analysis Techniques"
"Θ(n","chapter-14","Analysis Techniques"
"k","chapter-14","Analysis Techniques"
") if a < bk","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"This theorem may be applied whenever appropriate, rather than re-deriving the","chapter-14","Analysis Techniques"
"solution for the recurrence.","chapter-14","Analysis Techniques"
"Example 14.10 Apply the Master Theorem to solve","chapter-14","Analysis Techniques"
"T(n) = 3T(n/5) + 8n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"Because a = 3, b = 5, c = 8, and k = 2, we find that 3 < 5","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
". Applying","chapter-14","Analysis Techniques"
"case (3) of the theorem, T(n) = Θ(n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
").","chapter-14","Analysis Techniques"
"Example 14.11 Use the Master Theorem to solve the recurrence relation","chapter-14","Analysis Techniques"
"for Mergesort:","chapter-14","Analysis Techniques"
"T(n) = 2T(n/2) + n; T(1) = 1.","chapter-14","Analysis Techniques"
"Because a = 2, b = 2, c = 1, and k = 1, we find that 2 = 21","chapter-14","Analysis Techniques"
". Applying","chapter-14","Analysis Techniques"
"case (2) of the theorem, T(n) = Θ(n log n).","chapter-14","Analysis Techniques"
"14.2.4 Average-Case Analysis of Quicksort","chapter-14","Analysis Techniques"
"In Section 7.5, we determined that the average-case analysis of Quicksort had the","chapter-14","Analysis Techniques"
"following recurrence:","chapter-14","Analysis Techniques"
"T(n) = cn +","chapter-14","Analysis Techniques"
"1","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"nX−1","chapter-14","Analysis Techniques"
"k=0","chapter-14","Analysis Techniques"
"[T(k) + T(n − 1 − k)], T(0) = T(1) = c.","chapter-14","Analysis Techniques"
"The cn term is an upper bound on the findpivot and partition steps. This","chapter-14","Analysis Techniques"
"equation comes from assuming that the partitioning element is equally likely to","chapter-14","Analysis Techniques"
"occur in any position k. It can be simplified by observing that the two recurrence","chapter-14","Analysis Techniques"
"terms T(k) and T(n − 1 − k) are equivalent, because one simply counts up from","chapter-14","Analysis Techniques"
"T(0) to T(n − 1) while the other counts down from T(n − 1) to T(0). This yields","chapter-14","Analysis Techniques"
"T(n) = cn +","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"nX−1","chapter-14","Analysis Techniques"
"k=0","chapter-14","Analysis Techniques"
"T(k).","chapter-14","Analysis Techniques"
"Sec. 14.2 Recurrence Relations 475","chapter-14","Analysis Techniques"
"This form is known as a recurrence with full history. The key to solving such a","chapter-14","Analysis Techniques"
"recurrence is to cancel out the summation terms. The shifting method for summa-","chapter-14","Analysis Techniques"
"tions provides a way to do this. Multiply both sides by n and subtract the result","chapter-14","Analysis Techniques"
"from the formula for nT(n + 1):","chapter-14","Analysis Techniques"
"nT(n) = cn2 + 2","chapter-14","Analysis Techniques"
"nX−1","chapter-14","Analysis Techniques"
"k=1","chapter-14","Analysis Techniques"
"T(k)","chapter-14","Analysis Techniques"
"(n + 1)T(n + 1) = c(n + 1)2 + 2Xn","chapter-14","Analysis Techniques"
"k=1","chapter-14","Analysis Techniques"
"T(k).","chapter-14","Analysis Techniques"
"Subtracting nT(n) from both sides yields:","chapter-14","Analysis Techniques"
"(n + 1)T(n + 1) − nT(n) = c(n + 1)2 − cn2 + 2T(n)","chapter-14","Analysis Techniques"
"(n + 1)T(n + 1) − nT(n) = c(2n + 1) + 2T(n)","chapter-14","Analysis Techniques"
"(n + 1)T(n + 1) = c(2n + 1) + (n + 2)T(n)","chapter-14","Analysis Techniques"
"T(n + 1) = c(2n + 1)","chapter-14","Analysis Techniques"
"n + 1","chapter-14","Analysis Techniques"
"+","chapter-14","Analysis Techniques"
"n + 2","chapter-14","Analysis Techniques"
"n + 1","chapter-14","Analysis Techniques"
"T(n).","chapter-14","Analysis Techniques"
"At this point, we have eliminated the summation and can now use our normal meth-","chapter-14","Analysis Techniques"
"ods for solving recurrences to get a closed-form solution. Note that c(2n+1)","chapter-14","Analysis Techniques"
"n+1 < 2c,","chapter-14","Analysis Techniques"
"so we can simplify the result. Expanding the recurrence, we get","chapter-14","Analysis Techniques"
"T(n + 1) ≤ 2c +","chapter-14","Analysis Techniques"
"n + 2","chapter-14","Analysis Techniques"
"n + 1","chapter-14","Analysis Techniques"
"T(n)","chapter-14","Analysis Techniques"
"= 2c +","chapter-14","Analysis Techniques"
"n + 2","chapter-14","Analysis Techniques"
"n + 1 ","chapter-14","Analysis Techniques"
"2c +","chapter-14","Analysis Techniques"
"n + 1","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"T(n − 1)","chapter-14","Analysis Techniques"
"= 2c +","chapter-14","Analysis Techniques"
"n + 2","chapter-14","Analysis Techniques"
"n + 1 ","chapter-14","Analysis Techniques"
"2c +","chapter-14","Analysis Techniques"
"n + 1","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"2c +","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"n − 1","chapter-14","Analysis Techniques"
"T(n − 2)","chapter-14","Analysis Techniques"
"= 2c +","chapter-14","Analysis Techniques"
"n + 2","chapter-14","Analysis Techniques"
"n + 1 ","chapter-14","Analysis Techniques"
"2c + · · · +","chapter-14","Analysis Techniques"
"4","chapter-14","Analysis Techniques"
"3","chapter-14","Analysis Techniques"
"(2c +","chapter-14","Analysis Techniques"
"3","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"T(1))","chapter-14","Analysis Techniques"
"= 2c","chapter-14","Analysis Techniques"
"1 +","chapter-14","Analysis Techniques"
"n + 2","chapter-14","Analysis Techniques"
"n + 1","chapter-14","Analysis Techniques"
"+","chapter-14","Analysis Techniques"
"n + 2","chapter-14","Analysis Techniques"
"n + 1","chapter-14","Analysis Techniques"
"n + 1","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"+ · · · +","chapter-14","Analysis Techniques"
"n + 2","chapter-14","Analysis Techniques"
"n + 1","chapter-14","Analysis Techniques"
"n + 1","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"· · ·","chapter-14","Analysis Techniques"
"3","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"= 2c","chapter-14","Analysis Techniques"
"1 + (n + 2) ","chapter-14","Analysis Techniques"
"1","chapter-14","Analysis Techniques"
"n + 1","chapter-14","Analysis Techniques"
"+","chapter-14","Analysis Techniques"
"1","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"+ · · · +","chapter-14","Analysis Techniques"
"1","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"= 2c + 2c(n + 2) (Hn+1 − 1)","chapter-14","Analysis Techniques"
"for Hn+1, the Harmonic Series. From Equation 2.10, Hn+1 = Θ(log n), so the","chapter-14","Analysis Techniques"
"final solution is Θ(n log n).","chapter-14","Analysis Techniques"
"476 Chap. 14 Analysis Techniques","chapter-14","Analysis Techniques"
"14.3 Amortized Analysis","chapter-14","Analysis Techniques"
"This section presents the concept of amortized analysis, which is the analysis for","chapter-14","Analysis Techniques"
"a series of operations taken as a whole. In particular, amortized analysis allows us","chapter-14","Analysis Techniques"
"to deal with the situation where the worst-case cost for n operations is less than","chapter-14","Analysis Techniques"
"n times the worst-case cost of any one operation. Rather than focusing on the indi-","chapter-14","Analysis Techniques"
"vidual cost of each operation independently and summing them, amortized analysis","chapter-14","Analysis Techniques"
"looks at the cost of the entire series and “charges” each individual operation with a","chapter-14","Analysis Techniques"
"share of the total cost.","chapter-14","Analysis Techniques"
"We can apply the technique of amortized analysis in the case of a series of se-","chapter-14","Analysis Techniques"
"quential searches in an unsorted array. For n random searches, the average-case","chapter-14","Analysis Techniques"
"cost for each search is n/2, and so the expected total cost for the series is n","chapter-14","Analysis Techniques"
"2/2.","chapter-14","Analysis Techniques"
"Unfortunately, in the worst case all of the searches would be to the last item in the","chapter-14","Analysis Techniques"
"array. In this case, each search costs n for a total worst-case cost of n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
". Compare","chapter-14","Analysis Techniques"
"this to the cost for a series of n searches such that each item in the array is searched","chapter-14","Analysis Techniques"
"for precisely once. In this situation, some of the searches must be expensive, but","chapter-14","Analysis Techniques"
"also some searches must be cheap. The total number of searches, in the best, av-","chapter-14","Analysis Techniques"
"erage, and worst case, for this problem must be Pn","chapter-14","Analysis Techniques"
"i=i","chapter-14","Analysis Techniques"
"i ≈ n","chapter-14","Analysis Techniques"
"2/2. This is a factor","chapter-14","Analysis Techniques"
"of two better than the more pessimistic analysis that charges each operation in the","chapter-14","Analysis Techniques"
"series with its worst-case cost.","chapter-14","Analysis Techniques"
"As another example of amortized analysis, consider the process of increment-","chapter-14","Analysis Techniques"
"ing a binary counter. The algorithm is to move from the lower-order (rightmost)","chapter-14","Analysis Techniques"
"bit toward the high-order (leftmost) bit, changing 1s to 0s until the first 0 is en-","chapter-14","Analysis Techniques"
"countered. This 0 is changed to a 1, and the increment operation is done. Below is","chapter-14","Analysis Techniques"
"Java code to implement the increment operation, assuming that a binary number of","chapter-14","Analysis Techniques"
"length n is stored in array A of length n.","chapter-14","Analysis Techniques"
"for (i=0; ((i<A.length) && (A[i] == 1)); i++)","chapter-14","Analysis Techniques"
"A[i] = 0;","chapter-14","Analysis Techniques"
"if (i < A.length)","chapter-14","Analysis Techniques"
"A[i] = 1;","chapter-14","Analysis Techniques"
"If we count from 0 through 2","chapter-14","Analysis Techniques"
"n − 1, (requiring a counter with at least n bits),","chapter-14","Analysis Techniques"
"what is the average cost for an increment operation in terms of the number of bits","chapter-14","Analysis Techniques"
"processed? Naive worst-case analysis says that if all n bits are 1 (except for the","chapter-14","Analysis Techniques"
"high-order bit), then n bits need to be processed. Thus, if there are 2","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"increments,","chapter-14","Analysis Techniques"
"then the cost is n2","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
". However, this is much too high, because it is rare for so many","chapter-14","Analysis Techniques"
"bits to be processed. In fact, half of the time the low-order bit is 0, and so only","chapter-14","Analysis Techniques"
"that bit is processed. One quarter of the time, the low-order two bits are 01, and","chapter-14","Analysis Techniques"
"so only the low-order two bits are processed. Another way to view this is that the","chapter-14","Analysis Techniques"
"low-order bit is always flipped, the bit to its left is flipped half the time, the next","chapter-14","Analysis Techniques"
"bit one quarter of the time, and so on. We can capture this with the summation","chapter-14","Analysis Techniques"
"Sec. 14.3 Amortized Analysis 477","chapter-14","Analysis Techniques"
"(charging costs to bits going from right to left)","chapter-14","Analysis Techniques"
"nX−1","chapter-14","Analysis Techniques"
"i=0","chapter-14","Analysis Techniques"
"1","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"i","chapter-14","Analysis Techniques"
"< 2.","chapter-14","Analysis Techniques"
"In other words, the average number of bits flipped on each increment is 2, leading","chapter-14","Analysis Techniques"
"to a total cost of only 2 · 2","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"for a series of 2","chapter-14","Analysis Techniques"
"n","chapter-14","Analysis Techniques"
"increments.","chapter-14","Analysis Techniques"
"A useful concept for amortized analysis is illustrated by a simple variation on","chapter-14","Analysis Techniques"
"the stack data structure, where the pop function is slightly modified to take a sec-","chapter-14","Analysis Techniques"
"ond parameter k indicating that k pop operations are to be performed. This revised","chapter-14","Analysis Techniques"
"pop function, called multipop, might look as follows:","chapter-14","Analysis Techniques"
"/** pop k elements from stack */","chapter-14","Analysis Techniques"
"void multipop(int k);","chapter-14","Analysis Techniques"
"The “local” worst-case analysis for multipop is Θ(n) for n elements in the","chapter-14","Analysis Techniques"
"stack. Thus, if there are m1 calls to push and m2 calls to multipop, then the","chapter-14","Analysis Techniques"
"naive worst-case cost for the series of operation is m1 + m2 · n = m1 + m2 · m1.","chapter-14","Analysis Techniques"
"This analysis is unreasonably pessimistic. Clearly it is not really possible to pop","chapter-14","Analysis Techniques"
"m1 elements each time multipop is called. Analysis that focuses on single op-","chapter-14","Analysis Techniques"
"erations cannot deal with this global limit, and so we turn to amortized analysis to","chapter-14","Analysis Techniques"
"model the entire series of operations.","chapter-14","Analysis Techniques"
"The key to an amortized analysis of this problem lies in the concept of poten-","chapter-14","Analysis Techniques"
"tial. At any given time, a certain number of items may be on the stack. The cost for","chapter-14","Analysis Techniques"
"multipop can be no more than this number of items. Each call to push places","chapter-14","Analysis Techniques"
"another item on the stack, which can be removed by only a single multipop op-","chapter-14","Analysis Techniques"
"eration. Thus, each call to push raises the potential of the stack by one item. The","chapter-14","Analysis Techniques"
"sum of costs for all calls to multipop can never be more than the total potential of","chapter-14","Analysis Techniques"
"the stack (aside from a constant time cost associated with each call to multipop","chapter-14","Analysis Techniques"
"itself).","chapter-14","Analysis Techniques"
"The amortized cost for any series of push and multipop operations is the","chapter-14","Analysis Techniques"
"sum of three costs. First, each of the push operations takes constant time. Second,","chapter-14","Analysis Techniques"
"each multipop operation takes a constant time in overhead, regardless of the","chapter-14","Analysis Techniques"
"number of items popped on that call. Finally, we count the sum of the potentials","chapter-14","Analysis Techniques"
"expended by all multipop operations, which is at most m1, the number of push","chapter-14","Analysis Techniques"
"operations. This total cost can therefore be expressed as","chapter-14","Analysis Techniques"
"m1 + (m2 + m1) = Θ(m1 + m2).","chapter-14","Analysis Techniques"
"A similar argument was used in our analysis for the partition function in the","chapter-14","Analysis Techniques"
"Quicksort algorithm (Section 7.5). While on any given pass through the while","chapter-14","Analysis Techniques"
"loop the left or right pointers might move all the way through the remainder of the","chapter-14","Analysis Techniques"
"478 Chap. 14 Analysis Techniques","chapter-14","Analysis Techniques"
"partition, doing so would reduce the number of times that the while loop can be","chapter-14","Analysis Techniques"
"further executed.","chapter-14","Analysis Techniques"
"Our final example uses amortized analysis to prove a relationship between the","chapter-14","Analysis Techniques"
"cost of the move-to-front self-organizing list heuristic from Section 9.2 and the cost","chapter-14","Analysis Techniques"
"for the optimal static ordering of the list.","chapter-14","Analysis Techniques"
"Recall that, for a series of search operations, the minimum cost for a static","chapter-14","Analysis Techniques"
"list results when the list is sorted by frequency of access to its records. This is","chapter-14","Analysis Techniques"
"the optimal ordering for the records if we never allow the positions of records to","chapter-14","Analysis Techniques"
"change, because the most-frequently accessed record is first (and thus has least","chapter-14","Analysis Techniques"
"cost), followed by the next most frequently accessed record, and so on.","chapter-14","Analysis Techniques"
"Theorem 14.2 The total number of comparisons required by any series S of n or","chapter-14","Analysis Techniques"
"more searches on a self-organizing list of length n using the move-to-front heuristic","chapter-14","Analysis Techniques"
"is never more than twice the total number of comparisons required when series S is","chapter-14","Analysis Techniques"
"applied to the list stored in its optimal static order.","chapter-14","Analysis Techniques"
"Proof: Each comparison of the search key with a record in the list is either suc-","chapter-14","Analysis Techniques"
"cessful or unsuccessful. For m searches, there must be exactly m successful com-","chapter-14","Analysis Techniques"
"parisons for both the self-organizing list and the static list. The total number of","chapter-14","Analysis Techniques"
"unsuccessful comparisons in the self-organizing list is the sum, over all pairs of","chapter-14","Analysis Techniques"
"distinct keys, of the number of unsuccessful comparisons made between that pair.","chapter-14","Analysis Techniques"
"Consider a particular pair of keys A and B. For any sequence of searches S,","chapter-14","Analysis Techniques"
"the total number of (unsuccessful) comparisons between A and B is identical to the","chapter-14","Analysis Techniques"
"number of comparisons between A and B required for the subsequence of S made up","chapter-14","Analysis Techniques"
"only of searches for A or B. Call this subsequence SAB. In other words, including","chapter-14","Analysis Techniques"
"searches for other keys does not affect the relative position of A and B and so does","chapter-14","Analysis Techniques"
"not affect the relative contribution to the total cost of the unsuccessful comparisons","chapter-14","Analysis Techniques"
"between A and B.","chapter-14","Analysis Techniques"
"The number of unsuccessful comparisons between A and B made by the move-","chapter-14","Analysis Techniques"
"to-front heuristic on subsequence SAB is at most twice the number of unsuccessful","chapter-14","Analysis Techniques"
"comparisons between A and B required when SAB is applied to the optimal static","chapter-14","Analysis Techniques"
"ordering for the list. To see this, assume that SAB containsi As and j Bs, with i ≤ j.","chapter-14","Analysis Techniques"
"Under the optimal static ordering, i unsuccessful comparisons are required because","chapter-14","Analysis Techniques"
"B must appear before A in the list (because its access frequency is higher). Move-to-","chapter-14","Analysis Techniques"
"front will yield an unsuccessful comparison whenever the request sequence changes","chapter-14","Analysis Techniques"
"from A to B or from B to A. The total number of such changes possible is 2i because","chapter-14","Analysis Techniques"
"each change involves an A and each A can be part of at most two changes.","chapter-14","Analysis Techniques"
"Because the total number of unsuccessful comparisons required by move-to-","chapter-14","Analysis Techniques"
"front for any given pair of keys is at most twice that required by the optimal static","chapter-14","Analysis Techniques"
"ordering, the total number of unsuccessful comparisons required by move-to-front","chapter-14","Analysis Techniques"
"for all pairs of keys is also at most twice as high. Because the number of successful","chapter-14","Analysis Techniques"
"Sec. 14.4 Further Reading 479","chapter-14","Analysis Techniques"
"comparisons is the same for both methods, the total number of comparisons re-","chapter-14","Analysis Techniques"
"quired by move-to-front is less than twice the number of comparisons required by","chapter-14","Analysis Techniques"
"the optimal static ordering. ✷","chapter-14","Analysis Techniques"
"14.4 Further Reading","chapter-14","Analysis Techniques"
"A good introduction to solving recurrence relations appears in Applied Combina-","chapter-14","Analysis Techniques"
"torics by Fred S. Roberts [Rob84]. For a more advanced treatment, see Concrete","chapter-14","Analysis Techniques"
"Mathematics by Graham, Knuth, and Patashnik [GKP94].","chapter-14","Analysis Techniques"
"Cormen, Leiserson, and Rivest provide a good discussion on various methods","chapter-14","Analysis Techniques"
"for performing amortized analysis in Introduction to Algorithms [CLRS09]. For","chapter-14","Analysis Techniques"
"an amortized analysis that the splay tree requires m log n time to perform a series","chapter-14","Analysis Techniques"
"of m operations on n nodes when m > n, see “Self-Adjusting Binary Search","chapter-14","Analysis Techniques"
"Trees” by Sleator and Tarjan [ST85]. The proof for Theorem 14.2 comes from","chapter-14","Analysis Techniques"
"“Amortized Analysis of Self-Organizing Sequential Search Heuristics” by Bentley","chapter-14","Analysis Techniques"
"and McGeoch [BM85].","chapter-14","Analysis Techniques"
"14.5 Exercises","chapter-14","Analysis Techniques"
"14.1 Use the technique of guessing a polynomial and deriving the coefficients to","chapter-14","Analysis Techniques"
"solve the summation","chapter-14","Analysis Techniques"
"Xn","chapter-14","Analysis Techniques"
"i=1","chapter-14","Analysis Techniques"
"i","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"14.2 Use the technique of guessing a polynomial and deriving the coefficients to","chapter-14","Analysis Techniques"
"solve the summation","chapter-14","Analysis Techniques"
"Xn","chapter-14","Analysis Techniques"
"i=1","chapter-14","Analysis Techniques"
"i","chapter-14","Analysis Techniques"
"3","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"14.3 Find, and prove correct, a closed-form solution for","chapter-14","Analysis Techniques"
"X","chapter-14","Analysis Techniques"
"b","chapter-14","Analysis Techniques"
"i=a","chapter-14","Analysis Techniques"
"i","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"14.4 Use subtract-and-guess or divide-and-guess to find the closed form solution","chapter-14","Analysis Techniques"
"for the following summation. You must first find a pattern from which to","chapter-14","Analysis Techniques"
"deduce a potential closed form solution, and then prove that the proposed","chapter-14","Analysis Techniques"
"solution is correct.","chapter-14","Analysis Techniques"
"Xn","chapter-14","Analysis Techniques"
"i=1","chapter-14","Analysis Techniques"
"i/2","chapter-14","Analysis Techniques"
"i","chapter-14","Analysis Techniques"
"480 Chap. 14 Analysis Techniques","chapter-14","Analysis Techniques"
"14.5 Use the shifting method to solve the summation","chapter-14","Analysis Techniques"
"Xn","chapter-14","Analysis Techniques"
"i=1","chapter-14","Analysis Techniques"
"i","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"14.6 Use the shifting method to solve the summation","chapter-14","Analysis Techniques"
"Xn","chapter-14","Analysis Techniques"
"i=1","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
"i","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"14.7 Use the shifting method to solve the summation","chapter-14","Analysis Techniques"
"Xn","chapter-14","Analysis Techniques"
"i=1","chapter-14","Analysis Techniques"
"i2","chapter-14","Analysis Techniques"
"n−i","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"14.8 Consider the following code fragment.","chapter-14","Analysis Techniques"
"sum = 0; inc = 0;","chapter-14","Analysis Techniques"
"for (i=1; i<=n; i++)","chapter-14","Analysis Techniques"
"for (j=1; j<=i; j++) {","chapter-14","Analysis Techniques"
"sum = sum + inc;","chapter-14","Analysis Techniques"
"inc++;","chapter-14","Analysis Techniques"
"}","chapter-14","Analysis Techniques"
"(a) Determine a summation that defines the final value for variable sum as","chapter-14","Analysis Techniques"
"a function of n.","chapter-14","Analysis Techniques"
"(b) Determine a closed-form solution for your summation.","chapter-14","Analysis Techniques"
"14.9 A chocolate company decides to promote its chocolate bars by including a","chapter-14","Analysis Techniques"
"coupon with each bar. A bar costs a dollar, and with c coupons you get a free","chapter-14","Analysis Techniques"
"bar. So depending on the value of c, you get more than one bar of chocolate","chapter-14","Analysis Techniques"
"for a dollar when considering the value of the coupons. How much chocolate","chapter-14","Analysis Techniques"
"is a dollar worth (as a function of c)?","chapter-14","Analysis Techniques"
"14.10 Write and solve a recurrence relation to compute the number of times Fibr is","chapter-14","Analysis Techniques"
"called in the Fibr function of Exercise 2.11.","chapter-14","Analysis Techniques"
"14.11 Give and prove the closed-form solution for the recurrence relation T(n) =","chapter-14","Analysis Techniques"
"T(n − 1) + 1, T(1) = 1.","chapter-14","Analysis Techniques"
"14.12 Give and prove the closed-form solution for the recurrence relation T(n) =","chapter-14","Analysis Techniques"
"T(n − 1) + c, T(1) = c.","chapter-14","Analysis Techniques"
"14.13 Prove by induction that the closed-form solution for the recurrence relation","chapter-14","Analysis Techniques"
"T(n) = 2T(n/2) + n; T(2) = 1","chapter-14","Analysis Techniques"
"is in Ω(n log n).","chapter-14","Analysis Techniques"
"Sec. 14.5 Exercises 481","chapter-14","Analysis Techniques"
"14.14 For the following recurrence, give a closed-form solution. You should not","chapter-14","Analysis Techniques"
"give an exact solution, but only an asymptotic solution (i.e., using Θ nota-","chapter-14","Analysis Techniques"
"tion). You may assume that n is a power of 2. Prove that your answer is","chapter-14","Analysis Techniques"
"correct.","chapter-14","Analysis Techniques"
"T(n) = T(n/2) + √","chapter-14","Analysis Techniques"
"n for n > 1; T(1) = 1.","chapter-14","Analysis Techniques"
"14.15 Using the technique of expanding the recurrence, find the exact closed-form","chapter-14","Analysis Techniques"
"solution for the recurrence relation","chapter-14","Analysis Techniques"
"T(n) = 2T(n/2) + n; T(2) = 2.","chapter-14","Analysis Techniques"
"You may assume that n is a power of 2.","chapter-14","Analysis Techniques"
"14.16 Section 5.5 provides an asymptotic analysis for the worst-case cost of func-","chapter-14","Analysis Techniques"
"tion buildHeap. Give an exact worst-case analysis for buildHeap.","chapter-14","Analysis Techniques"
"14.17 For each of the following recurrences, find and then prove (using induction)","chapter-14","Analysis Techniques"
"an exact closed-form solution. When convenient, you may assume that n is","chapter-14","Analysis Techniques"
"a power of 2.","chapter-14","Analysis Techniques"
"(a) T(n) = T(n − 1) + n/2 for n > 1; T(1) = 1.","chapter-14","Analysis Techniques"
"(b) T(n) = 2T(n/2) + n for n > 2; T(2) = 2.","chapter-14","Analysis Techniques"
"14.18 Use Theorem 14.1 to prove that binary search requires Θ(log n) time.","chapter-14","Analysis Techniques"
"14.19 Recall that when a hash table gets to be more than about one half full, its","chapter-14","Analysis Techniques"
"performance quickly degrades. One solution to this problem is to reinsert","chapter-14","Analysis Techniques"
"all elements of the hash table into a new hash table that is twice as large.","chapter-14","Analysis Techniques"
"Assuming that the (expected) average case cost to insert into a hash table is","chapter-14","Analysis Techniques"
"Θ(1), prove that the average cost to insert is still Θ(1) when this re-insertion","chapter-14","Analysis Techniques"
"policy is used.","chapter-14","Analysis Techniques"
"14.20 Given a 2-3 tree with N nodes, prove that inserting M additional nodes re-","chapter-14","Analysis Techniques"
"quires O(M + N) node splits.","chapter-14","Analysis Techniques"
"14.21 One approach to implementing an array-based list where the list size is un-","chapter-14","Analysis Techniques"
"known is to let the array grow and shrink. This is known as a dynamic array.","chapter-14","Analysis Techniques"
"When necessary, we can grow or shrink the array by copying the array’s con-","chapter-14","Analysis Techniques"
"tents to a new array. If we are careful about the size of the new array, this","chapter-14","Analysis Techniques"
"copy operation can be done rarely enough so as not to affect the amortized","chapter-14","Analysis Techniques"
"cost of the operations.","chapter-14","Analysis Techniques"
"(a) What is the amortized cost of inserting elements into the list if the array","chapter-14","Analysis Techniques"
"is initially of size 1 and we double the array size whenever the number","chapter-14","Analysis Techniques"
"of elements that we wish to store exceeds the size of the array? Assume","chapter-14","Analysis Techniques"
"that the insert itself cost O(1) time per operation and so we are just","chapter-14","Analysis Techniques"
"concerned with minimizing the copy time to the new array.","chapter-14","Analysis Techniques"
"482 Chap. 14 Analysis Techniques","chapter-14","Analysis Techniques"
"(b) Consider an underflow strategy that cuts the array size in half whenever","chapter-14","Analysis Techniques"
"the array falls below half full. Give an example where this strategy leads","chapter-14","Analysis Techniques"
"to a bad amortized cost. Again, we are only interested in measuring the","chapter-14","Analysis Techniques"
"time of the array copy operations.","chapter-14","Analysis Techniques"
"(c) Give a better underflow strategy than that suggested in part (b). Your","chapter-14","Analysis Techniques"
"goal is to find a strategy whose amortized analysis shows that array","chapter-14","Analysis Techniques"
"copy requires O(n) time for a series of n operations.","chapter-14","Analysis Techniques"
"14.22 Recall that two vertices in an undirected graph are in the same connected","chapter-14","Analysis Techniques"
"component if there is a path connecting them. A good algorithm to find the","chapter-14","Analysis Techniques"
"connected components of an undirected graph begins by calling a DFS on","chapter-14","Analysis Techniques"
"the first vertex. All vertices reached by the DFS are in the same connected","chapter-14","Analysis Techniques"
"component and are so marked. We then look through the vertex mark array","chapter-14","Analysis Techniques"
"until an unmarked vertex i is found. Again calling the DFS on i, all vertices","chapter-14","Analysis Techniques"
"reachable from i are in a second connected component. We continue work-","chapter-14","Analysis Techniques"
"ing through the mark array until all vertices have been assigned to some","chapter-14","Analysis Techniques"
"connected component. A sketch of the algorithm is as follows:","chapter-14","Analysis Techniques"
"static void concom(Graph G) {","chapter-14","Analysis Techniques"
"int i;","chapter-14","Analysis Techniques"
"for (i=0; i<G.n(); i++) // For n vertices in graph","chapter-14","Analysis Techniques"
"G.setMark(i, 0); // Vertex i in no component","chapter-14","Analysis Techniques"
"int comp = 1; // Current component","chapter-14","Analysis Techniques"
"for (i=0; i<G.n(); i++)","chapter-14","Analysis Techniques"
"if (G.getMark(i) == 0) // Start a new component","chapter-14","Analysis Techniques"
"DFS component(G, i, comp++);","chapter-14","Analysis Techniques"
"for (i=0; i<G.n(); i++)","chapter-14","Analysis Techniques"
"out.append(i + " " + G.getMark(i) + " ");","chapter-14","Analysis Techniques"
"}","chapter-14","Analysis Techniques"
"static void DFS component(Graph G, int v, int comp) {","chapter-14","Analysis Techniques"
"G.setMark(v, comp);","chapter-14","Analysis Techniques"
"for (int w = G.first(v); w < G.n(); w = G.next(v, w))","chapter-14","Analysis Techniques"
"if (G.getMark(w) == 0)","chapter-14","Analysis Techniques"
"DFS component(G, w, comp);","chapter-14","Analysis Techniques"
"}","chapter-14","Analysis Techniques"
"Use the concept of potential from amortized analysis to explain why the total","chapter-14","Analysis Techniques"
"cost of this algorithm is Θ(|V| + |E|). (Note that this will not be a true","chapter-14","Analysis Techniques"
"amortized analysis because this algorithm does not allow an arbitrary series","chapter-14","Analysis Techniques"
"of DFS operations but rather is fixed to do a single call to DFS from each","chapter-14","Analysis Techniques"
"vertex.)","chapter-14","Analysis Techniques"
"14.23 Give a proof similar to that used for Theorem 14.2 to show that the total","chapter-14","Analysis Techniques"
"number of comparisons required by any series of n or more searches S on a","chapter-14","Analysis Techniques"
"self-organizing list of length n using the count heuristic is never more than","chapter-14","Analysis Techniques"
"twice the total number of comparisons required when series S is applied to","chapter-14","Analysis Techniques"
"the list stored in its optimal static order.","chapter-14","Analysis Techniques"
"Sec. 14.6 Projects 483","chapter-14","Analysis Techniques"
"14.24 Use mathematical induction to prove that","chapter-14","Analysis Techniques"
"Xn","chapter-14","Analysis Techniques"
"i=1","chapter-14","Analysis Techniques"
"F ib(i) = F ib(n − 2) − 1, for n ≥ 1.","chapter-14","Analysis Techniques"
"14.25 Use mathematical induction to prove that Fib(i) is even if and only if n is","chapter-14","Analysis Techniques"
"divisible by 3.","chapter-14","Analysis Techniques"
"14.26 Use mathematical induction to prove that for n ≥ 6, f ib(n) > (3/2)n−1","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"14.27 Find closed forms for each of the following recurrences.","chapter-14","Analysis Techniques"
"(a) F(n) = F(n − 1) + 3; F(1) = 2.","chapter-14","Analysis Techniques"
"(b) F(n) = 2F(n − 1); F(0) = 1.","chapter-14","Analysis Techniques"
"(c) F(n) = 2F(n − 1) + 1; F(1) = 1.","chapter-14","Analysis Techniques"
"(d) F(n) = 2nF(n − 1); F(0) = 1.","chapter-14","Analysis Techniques"
"(e) F(n) = 2nF(n − 1); F(0) = 1.","chapter-14","Analysis Techniques"
"(f) F(n) = 2 + Pn−1","chapter-14","Analysis Techniques"
"i=1 F(i); F(1) = 1.","chapter-14","Analysis Techniques"
"14.28 Find Θ for each of the following recurrence relations.","chapter-14","Analysis Techniques"
"(a) T(n) = 2T(n/2) + n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"(b) T(n) = 2T(n/2) + 5.","chapter-14","Analysis Techniques"
"(c) T(n) = 4T(n/2) + n.","chapter-14","Analysis Techniques"
"(d) T(n) = 2T(n/2) + n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"(e) T(n) = 4T(n/2) + n","chapter-14","Analysis Techniques"
"3","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"(f) T(n) = 4T(n/3) + n.","chapter-14","Analysis Techniques"
"(g) T(n) = 4T(n/3) + n","chapter-14","Analysis Techniques"
"2","chapter-14","Analysis Techniques"
".","chapter-14","Analysis Techniques"
"(h) T(n) = 2T(n/2) + log n.","chapter-14","Analysis Techniques"
"(i) T(n) = 2T(n/2) + n log n.","chapter-14","Analysis Techniques"
"14.6 Projects","chapter-14","Analysis Techniques"
"14.1 Implement the UNION/FIND algorithm of Section 6.2 using both path com-","chapter-14","Analysis Techniques"
"pression and the weighted union rule. Count the total number of node ac-","chapter-14","Analysis Techniques"
"cesses required for various series of equivalences to determine if the actual","chapter-14","Analysis Techniques"
"performance of the algorithm matches the expected cost of Θ(n log∗ n).","chapter-14","Analysis Techniques"
"How do I know if I have a good algorithm to solve a problem? If my algorithm runs","chapter-15","Lower Bounds"
"in Θ(n log n) time, is that good? It would be if I were sorting the records stored","chapter-15","Lower Bounds"
"in an array. But it would be terrible if I were searching the array for the largest","chapter-15","Lower Bounds"
"element. The value of an algorithm must be determined in relation to the inherent","chapter-15","Lower Bounds"
"complexity of the problem at hand.","chapter-15","Lower Bounds"
"In Section 3.6 we defined the upper bound for a problem to be the upper bound","chapter-15","Lower Bounds"
"of the best algorithm we know for that problem, and the lower bound to be the","chapter-15","Lower Bounds"
"tightest lower bound that we can prove over all algorithms for that problem. While","chapter-15","Lower Bounds"
"we usually can recognize the upper bound for a given algorithm, finding the tightest","chapter-15","Lower Bounds"
"lower bound for all possible algorithms is often difficult, especially if that lower","chapter-15","Lower Bounds"
"bound is more than the “trivial” lower bound determined by measuring the amount","chapter-15","Lower Bounds"
"of input that must be processed.","chapter-15","Lower Bounds"
"The benefits of being able to discover a strong lower bound are significant. In","chapter-15","Lower Bounds"
"particular, when we can make the upper and lower bounds for a problem meet, this","chapter-15","Lower Bounds"
"means that we truly understand our problem in a theoretical sense. It also saves","chapter-15","Lower Bounds"
"us the effort of attempting to discover more (asymptotically) efficient algorithms","chapter-15","Lower Bounds"
"when no such algorithm can exist.","chapter-15","Lower Bounds"
"Often the most effective way to determine the lower bound for a problem is","chapter-15","Lower Bounds"
"to find a reduction to another problem whose lower bound is already known. This","chapter-15","Lower Bounds"
"is the subject of Chapter 17. However, this approach does not help us when we","chapter-15","Lower Bounds"
"cannot find a suitable “similar problem.” Our focus in this chapter is discovering","chapter-15","Lower Bounds"
"and proving lower bounds from first principles. Our most significant example of","chapter-15","Lower Bounds"
"a lower bounds argument so far is the proof from Section 7.9 that the problem of","chapter-15","Lower Bounds"
"sorting is O(n log n) in the worst case.","chapter-15","Lower Bounds"
"Section 15.1 reviews the concept of a lower bound for a problem and presents","chapter-15","Lower Bounds"
"the basic “algorithm” for finding a good algorithm. Section 15.2 discusses lower","chapter-15","Lower Bounds"
"bounds on searching in lists, both those that are unordered and those that are or-","chapter-15","Lower Bounds"
"dered. Section 15.3 deals with finding the maximum value in a list, and presents a","chapter-15","Lower Bounds"
"model for selection based on building a partially ordered set. Section 15.4 presents","chapter-15","Lower Bounds"
"485","chapter-15","Lower Bounds"
"486 Chap. 15 Lower Bounds","chapter-15","Lower Bounds"
"the concept of an adversarial lower bounds proof. Section 15.5 illustrates the con-","chapter-15","Lower Bounds"
"cept of a state space lower bound. Section 15.6 presents a linear time worst-case","chapter-15","Lower Bounds"
"algorithm for finding the ith biggest element on a list. Section 15.7 continues our","chapter-15","Lower Bounds"
"discussion of sorting with a quest for the algorithm that requires the absolute fewest","chapter-15","Lower Bounds"
"number of comparisons needed to sort a list.","chapter-15","Lower Bounds"
"15.1 Introduction to Lower Bounds Proofs","chapter-15","Lower Bounds"
"The lower bound for the problem is the tightest (highest) lower bound that we can","chapter-15","Lower Bounds"
"prove for all possible algorithms that solve the problem.1 This can be a difficult bar,","chapter-15","Lower Bounds"
"given that we cannot possibly know all algorithms for any problem, because there","chapter-15","Lower Bounds"
"are theoretically an infinite number. However, we can often recognize a simple","chapter-15","Lower Bounds"
"lower bound based on the amount of input that must be examined. For example,","chapter-15","Lower Bounds"
"we can argue that the lower bound for any algorithm to find the maximum-valued","chapter-15","Lower Bounds"
"element in an unsorted list must be Ω(n) because any algorithm must examine all","chapter-15","Lower Bounds"
"of the inputs to be sure that it actually finds the maximum value.","chapter-15","Lower Bounds"
"In the case of maximum finding, the fact that we know of a simple algorithm","chapter-15","Lower Bounds"
"that runs in O(n) time, combined with the fact that any algorithm needs Ω(n) time,","chapter-15","Lower Bounds"
"is significant. Because our upper and lower bounds meet (within a constant factor),","chapter-15","Lower Bounds"
"we know that we do have a “good” algorithm for solving the problem. It is possible","chapter-15","Lower Bounds"
"that someone can develop an implementation that is a “little” faster than an existing","chapter-15","Lower Bounds"
"one, by a constant factor. But we know that its not possible to develop one that is","chapter-15","Lower Bounds"
"asymptotically better.","chapter-15","Lower Bounds"
"We must be careful about how we interpret this last statement, however. The","chapter-15","Lower Bounds"
"world is certainly better off for the invention of Quicksort, even though Mergesort","chapter-15","Lower Bounds"
"was available at the time. Quicksort is not asymptotically faster than Mergesort, yet","chapter-15","Lower Bounds"
"is not merely a “tuning” of Mergesort either. Quicksort is a substantially different","chapter-15","Lower Bounds"
"approach to sorting. So even when our upper and lower bounds for a problem meet,","chapter-15","Lower Bounds"
"there are still benefits to be gained from a new, clever algorithm.","chapter-15","Lower Bounds"
"So now we have an answer to the question “How do I know if I have a good","chapter-15","Lower Bounds"
"algorithm to solve a problem?” An algorithm is good (asymptotically speaking) if","chapter-15","Lower Bounds"
"its upper bound matches the problem’s lower bound. If they match, we know to","chapter-15","Lower Bounds"
"stop trying to find an (asymptotically) faster algorithm. What if the (known) upper","chapter-15","Lower Bounds"
"bound for our algorithm does not match the (known) lower bound for the problem?","chapter-15","Lower Bounds"
"In this case, we might not know what to do. Is our upper bound flawed, and the","chapter-15","Lower Bounds"
"algorithm is really faster than we can prove? Is our lower bound weak, and the true","chapter-15","Lower Bounds"
"lower bound for the problem is greater? Or is our algorithm simply not the best?","chapter-15","Lower Bounds"
"1Throughout this discussion, it should be understood that any mention of bounds must specify","chapter-15","Lower Bounds"
"what class of inputs are being considered. Do we mean the bound for the worst case input? The","chapter-15","Lower Bounds"
"average cost over all inputs? Regardless of which class of inputs we consider, all of the issues raised","chapter-15","Lower Bounds"
"apply equally.","chapter-15","Lower Bounds"
"Sec. 15.1 Introduction to Lower Bounds Proofs 487","chapter-15","Lower Bounds"
"Now we know precisely what we are aiming for when designing an algorithm:","chapter-15","Lower Bounds"
"We want to find an algorithm who’s upper bound matches the lower bound of the","chapter-15","Lower Bounds"
"problem. Putting together all that we know so far about algorithms, we can organize","chapter-15","Lower Bounds"
"our thinking into the following “algorithm for designing algorithms.”2","chapter-15","Lower Bounds"
"If the upper and lower bounds match,","chapter-15","Lower Bounds"
"then stop,","chapter-15","Lower Bounds"
"else if the bounds are close or the problem isn’t important,","chapter-15","Lower Bounds"
"then stop,","chapter-15","Lower Bounds"
"else if the problem definition focuses on the wrong thing,","chapter-15","Lower Bounds"
"then restate it,","chapter-15","Lower Bounds"
"else if the algorithm is too slow,","chapter-15","Lower Bounds"
"then find a faster algorithm,","chapter-15","Lower Bounds"
"else if lower bound is too weak,","chapter-15","Lower Bounds"
"then generate a stronger bound.","chapter-15","Lower Bounds"
"We can repeat this process until we are satisfied or exhausted.","chapter-15","Lower Bounds"
"This brings us smack up against one of the toughest tasks in analysis. Lower","chapter-15","Lower Bounds"
"bounds proofs are notoriously difficult to construct. The problem is coming up with","chapter-15","Lower Bounds"
"arguments that truly cover all of the things that any algorithm possibly could do.","chapter-15","Lower Bounds"
"The most common fallacy is to argue from the point of view of what some good","chapter-15","Lower Bounds"
"algorithm actually does do, and claim that any algorithm must do the same. This","chapter-15","Lower Bounds"
"simply is not true, and any lower bounds proof that refers to specific behavior that","chapter-15","Lower Bounds"
"must take place should be viewed with some suspicion.","chapter-15","Lower Bounds"
"Let us consider the Towers of Hanoi problem again. Recall from Section 2.5","chapter-15","Lower Bounds"
"that our basic algorithm is to move n − 1 disks (recursively) to the middle pole,","chapter-15","Lower Bounds"
"move the bottom disk to the third pole, and then move n−1 disks (again recursively)","chapter-15","Lower Bounds"
"from the middle to the third pole. This algorithm generates the recurrence T(n) =","chapter-15","Lower Bounds"
"2T(n − 1) + 1 = 2n − 1. So, the upper bound for our algorithm is 2","chapter-15","Lower Bounds"
"n − 1. But is","chapter-15","Lower Bounds"
"this the best algorithm for the problem? What is the lower bound for the problem?","chapter-15","Lower Bounds"
"For our first try at a lower bounds proof, the “trivial” lower bound is that we","chapter-15","Lower Bounds"
"must move every disk at least once, for a minimum cost of n. Slightly better is to","chapter-15","Lower Bounds"
"observe that to get the bottom disk to the third pole, we must move every other disk","chapter-15","Lower Bounds"
"at least twice (once to get them off the bottom disk, and once to get them over to","chapter-15","Lower Bounds"
"the third pole). This yields a cost of 2n − 1, which still is not a good match for our","chapter-15","Lower Bounds"
"algorithm. Is the problem in the algorithm or in the lower bound?","chapter-15","Lower Bounds"
"We can get to the correct lower bound by the following reasoning: To move the","chapter-15","Lower Bounds"
"biggest disk from first to the last pole, we must first have all of the other n−1 disks","chapter-15","Lower Bounds"
"out of the way, and the only way to do that is to move them all to the middle pole","chapter-15","Lower Bounds"
"(for a cost of at least T(n − 1)). We then must move the bottom disk (for a cost of","chapter-15","Lower Bounds"
"2This is a minor reformulation of the “algorithm” given by Gregory J.E. Rawlins in his book","chapter-15","Lower Bounds"
"“Compared to What?”","chapter-15","Lower Bounds"
"488 Chap. 15 Lower Bounds","chapter-15","Lower Bounds"
"at least one). After that, we must move the n − 1 remaining disks from the middle","chapter-15","Lower Bounds"
"pole to the third pole (for a cost of at least T(n − 1)). Thus, no possible algorithm","chapter-15","Lower Bounds"
"can solve the problem in less than 2","chapter-15","Lower Bounds"
"n − 1 steps. Thus, our algorithm is optimal.3","chapter-15","Lower Bounds"
"Of course, there are variations to a given problem. Changes in the problem","chapter-15","Lower Bounds"
"definition might or might not lead to changes in the lower bound. Two possible","chapter-15","Lower Bounds"
"changes to the standard Towers of Hanoi problem are:","chapter-15","Lower Bounds"
"• Not all disks need to start on the first pole.","chapter-15","Lower Bounds"
"• Multiple disks can be moved at one time.","chapter-15","Lower Bounds"
"The first variation does not change the lower bound (at least not asymptotically).","chapter-15","Lower Bounds"
"The second one does.","chapter-15","Lower Bounds"
"15.2 Lower Bounds on Searching Lists","chapter-15","Lower Bounds"
"In Section 7.9 we presented an important lower bounds proof to show that the","chapter-15","Lower Bounds"
"problem of sorting is Θ(n log n) in the worst case. In Chapter 9 we discussed a","chapter-15","Lower Bounds"
"number of algorithms to search in sorted and unsorted lists, but we did not provide","chapter-15","Lower Bounds"
"any lower bounds proofs to this important problem. We will extend our pool of","chapter-15","Lower Bounds"
"techniques for lower bounds proofs in this section by studying lower bounds for","chapter-15","Lower Bounds"
"searching unsorted and sorted lists.","chapter-15","Lower Bounds"
"15.2.1 Searching in Unsorted Lists","chapter-15","Lower Bounds"
"Given an (unsorted) list L of n elements and a search key K, we seek to identify one","chapter-15","Lower Bounds"
"element in L which has key value K, if any exists. For the rest of this discussion,","chapter-15","Lower Bounds"
"we will assume that the key values for the elements in L are unique, that the set of","chapter-15","Lower Bounds"
"all possible keys is totally ordered (that is, the operations <, =, and > are defined","chapter-15","Lower Bounds"
"for all pairs of key values), and that comparison is our only way to find the relative","chapter-15","Lower Bounds"
"ordering of two keys. Our goal is to solve the problem using the minimum number","chapter-15","Lower Bounds"
"of comparisons.","chapter-15","Lower Bounds"
"Given this definition for searching, we can easily come up with the standard","chapter-15","Lower Bounds"
"sequential search algorithm, and we can also see that the lower bound for this prob-","chapter-15","Lower Bounds"
"lem is “obviously” n comparisons. (Keep in mind that the key K might not actually","chapter-15","Lower Bounds"
"appear in the list.) However, lower bounds proofs are a bit slippery, and it is in-","chapter-15","Lower Bounds"
"structive to see how they can go wrong.","chapter-15","Lower Bounds"
"Theorem 15.1 The lower bound for the problem of searching in an unsorted list","chapter-15","Lower Bounds"
"is n comparisons.","chapter-15","Lower Bounds"
"3Recalling the advice to be suspicious of any lower bounds proof that argues a given behavior","chapter-15","Lower Bounds"
"“must” happen, this proof should be raising red flags. However, in this particular case the problem is","chapter-15","Lower Bounds"
"so constrained that there really is no (better) alternative to this particular sequence of events.","chapter-15","Lower Bounds"
"Sec. 15.2 Lower Bounds on Searching Lists 489","chapter-15","Lower Bounds"
"Here is our first attempt at proving the theorem.","chapter-15","Lower Bounds"
"Proof 1: We will try a proof by contradiction. Assume an algorithm A exists that","chapter-15","Lower Bounds"
"requires only n − 1 (or less) comparisons of K with elements of L. Because there","chapter-15","Lower Bounds"
"are n elements of L, A must have avoided comparing K with L[i] for some value","chapter-15","Lower Bounds"
"i. We can feed the algorithm an input with K in position i. Such an input is legal in","chapter-15","Lower Bounds"
"our model, so the algorithm is incorrect. ✷","chapter-15","Lower Bounds"
"Is this proof correct? Unfortunately no. First of all, any given algorithm need","chapter-15","Lower Bounds"
"not necessarily consistently skip any given position i in its n − 1 searches. For","chapter-15","Lower Bounds"
"example, it is not necessary that all algorithms search the list from left to right. It","chapter-15","Lower Bounds"
"is not even necessary that all algorithms search the same n − 1 positions first each","chapter-15","Lower Bounds"
"time through the list.","chapter-15","Lower Bounds"
"We can try to dress up the proof as follows: Proof 2: On any given run of the","chapter-15","Lower Bounds"
"algorithm, if n − 1 elements are compared against K, then some element position","chapter-15","Lower Bounds"
"(call it position i) gets skipped. It is possible that K is in position i at that time, and","chapter-15","Lower Bounds"
"will not be found. Therefore, n comparisons are required. ✷","chapter-15","Lower Bounds"
"Unfortunately, there is another error that needs to be fixed. It is not true that","chapter-15","Lower Bounds"
"all algorithms for solving the problem must work by comparing elements of L","chapter-15","Lower Bounds"
"against K. An algorithm might make useful progress by comparing elements of L","chapter-15","Lower Bounds"
"against each other. For example, if we compare two elements of L, then compare","chapter-15","Lower Bounds"
"the greater against K and find that this element is less than K, we know that the","chapter-15","Lower Bounds"
"other element is also less than K. It seems intuitively obvious that such compar-","chapter-15","Lower Bounds"
"isons won’t actually lead to a faster algorithm, but how do we know for sure? We","chapter-15","Lower Bounds"
"somehow need to generalize the proof to account for this approach.","chapter-15","Lower Bounds"
"We will now present a useful abstraction for expressing the state of knowledge","chapter-15","Lower Bounds"
"for the value relationships among a set of objects. A total order defines relation-","chapter-15","Lower Bounds"
"ships within a collection of objects such that for every pair of objects, one is greater","chapter-15","Lower Bounds"
"than the other. A partially ordered set or poset is a set on which only a partial","chapter-15","Lower Bounds"
"order is defined. That is, there can be pairs of elements for which we cannot de-","chapter-15","Lower Bounds"
"cide which is “greater”. For our purpose here, the partial order is the state of our","chapter-15","Lower Bounds"
"current knowledge about the objects, such that zero or more of the order relations","chapter-15","Lower Bounds"
"between pairs of elements are known. We can represent this knowledge by drawing","chapter-15","Lower Bounds"
"directed acyclic graphs (DAGs) showing the known relationships, as illustrated by","chapter-15","Lower Bounds"
"Figure 15.1.","chapter-15","Lower Bounds"
"Proof 3: Initially, we know nothing about the relative order of the elements in L,","chapter-15","Lower Bounds"
"or their relationship to K. So initially, we can view the n elements in L as being in","chapter-15","Lower Bounds"
"n separate partial orders. Any comparison between two elements in L can affect","chapter-15","Lower Bounds"
"the structure of the partial orders. This is somewhat similar to the UNION/FIND","chapter-15","Lower Bounds"
"algorithm implemented using parent pointer trees, described in Section 6.2.","chapter-15","Lower Bounds"
"Now, every comparison between elements in L can at best combine two of the","chapter-15","Lower Bounds"
"partial orders together. Any comparison between K and an element, say A, in L can","chapter-15","Lower Bounds"
"at best eliminate the partial order that contains A. Thus, if we spend m comparisons","chapter-15","Lower Bounds"
"490 Chap. 15 Lower Bounds","chapter-15","Lower Bounds"
"A","chapter-15","Lower Bounds"
"F","chapter-15","Lower Bounds"
"B","chapter-15","Lower Bounds"
"D","chapter-15","Lower Bounds"
"C","chapter-15","Lower Bounds"
"E","chapter-15","Lower Bounds"
"G","chapter-15","Lower Bounds"
"Figure 15.1 Illustration of using a poset to model our current knowledge of the","chapter-15","Lower Bounds"
"relationships among a collection of objects. A directed acyclic graph (DAG) is","chapter-15","Lower Bounds"
"used to draw the poset (assume all edges are directed downward). In this example,","chapter-15","Lower Bounds"
"our knowledge is such that we don’t know how A or B relate to any of the other","chapter-15","Lower Bounds"
"objects. However, we know that both C and G are greater than E and F. Further,","chapter-15","Lower Bounds"
"we know that C is greater than D, and that E is greater than F.","chapter-15","Lower Bounds"
"comparing elements in L we have at least n − m partial orders. Every such partial","chapter-15","Lower Bounds"
"order needs at least one comparison against K to make sure that K is not somewhere","chapter-15","Lower Bounds"
"in that partial order. Thus, any algorithm must make at least n comparisons in the","chapter-15","Lower Bounds"
"worst case. ✷","chapter-15","Lower Bounds"
"15.2.2 Searching in Sorted Lists","chapter-15","Lower Bounds"
"We will now assume that list L is sorted. In this case, is linear search still optimal?","chapter-15","Lower Bounds"
"Clearly no, but why not? Because we have additional information to work with that","chapter-15","Lower Bounds"
"we do not have when the list is unsorted. We know that the standard binary search","chapter-15","Lower Bounds"
"algorithm has a worst case cost of O(log n). Can we do better than this? We can","chapter-15","Lower Bounds"
"prove that this is the best possible in the worst case with a proof similar to that used","chapter-15","Lower Bounds"
"to show the lower bound on sorting.","chapter-15","Lower Bounds"
"Again we use the decision tree to model our algorithm. Unlike when searching","chapter-15","Lower Bounds"
"an unsorted list, comparisons between elements of L tell us nothing new about their","chapter-15","Lower Bounds"
"relative order, so we consider only comparisons between K and an element in L. At","chapter-15","Lower Bounds"
"the root of the decision tree, our knowledge rules out no positions in L, so all are","chapter-15","Lower Bounds"
"potential candidates. As we take branches in the decision tree based on the result","chapter-15","Lower Bounds"
"of comparing K to an element in L, we gradually rule out potential candidates.","chapter-15","Lower Bounds"
"Eventually we reach a leaf node in the tree representing the single position in L","chapter-15","Lower Bounds"
"that can contain K. There must be at least n + 1 nodes in the tree because we have","chapter-15","Lower Bounds"
"n + 1 distinct positions that K can be in (any position in L, plus not in L at all).","chapter-15","Lower Bounds"
"Some path in the tree must be at least log n levels deep, and the deepest node in the","chapter-15","Lower Bounds"
"tree represents the worst case for that algorithm. Thus, any algorithm on a sorted","chapter-15","Lower Bounds"
"array requires at least Ω(log n) comparisons in the worst case.","chapter-15","Lower Bounds"
"We can modify this proof to find the average cost lower bound. Again, we","chapter-15","Lower Bounds"
"model algorithms using decision trees. Except now we are interested not in the","chapter-15","Lower Bounds"
"depth of the deepest node (the worst case) and therefore the tree with the least-","chapter-15","Lower Bounds"
"deepest node. Instead, we are interested in knowing what the minimum possible is","chapter-15","Lower Bounds"
"Sec. 15.3 Finding the Maximum Value 491","chapter-15","Lower Bounds"
"for the “average depth” of the leaf nodes. Define the total path length as the sum","chapter-15","Lower Bounds"
"of the levels for each node. The cost of an outcome is the level of the corresponding","chapter-15","Lower Bounds"
"node plus 1. The average cost of the algorithm is the average cost of the outcomes","chapter-15","Lower Bounds"
"(total path length/n). What is the tree with the least average depth? This is equiva-","chapter-15","Lower Bounds"
"lent to the tree that corresponds to binary search. Thus, binary search is optimal in","chapter-15","Lower Bounds"
"the average case.","chapter-15","Lower Bounds"
"While binary search is indeed an optimal algorithm for a sorted list in the worst","chapter-15","Lower Bounds"
"and average cases when searching a sorted array, there are a number of circum-","chapter-15","Lower Bounds"
"stances that might lead us to select another algorithm instead. One possibility is","chapter-15","Lower Bounds"
"that we know something about the distribution of the data in the array. We saw in","chapter-15","Lower Bounds"
"Section 9.1 that if each position in L is equally likely to hold X (equivalently, the","chapter-15","Lower Bounds"
"data are well distributed along the full key range), then an interpolation search is","chapter-15","Lower Bounds"
"Θ(log log n) in the average case. If the data are not sorted, then using binary search","chapter-15","Lower Bounds"
"requires us to pay the cost of sorting the list in advance, which is only worthwhile if","chapter-15","Lower Bounds"
"many (at least O(log n)) searches will be performed on the list. Binary search also","chapter-15","Lower Bounds"
"requires that the list (even if sorted) be implemented using an array or some other","chapter-15","Lower Bounds"
"structure that supports random access to all elements with equal cost. Finally, if we","chapter-15","Lower Bounds"
"know all search requests in advance, we might prefer to sort the list by frequency","chapter-15","Lower Bounds"
"and do linear search in extreme search distributions, as discussed in Section 9.2.","chapter-15","Lower Bounds"
"15.3 Finding the Maximum Value","chapter-15","Lower Bounds"
"How can we find the ith largest value in a sorted list? Obviously we just go to the","chapter-15","Lower Bounds"
"ith position. But what if we have an unsorted list? Can we do better than to sort","chapter-15","Lower Bounds"
"it? If we are looking for the minimum or maximum value, certainly we can do","chapter-15","Lower Bounds"
"better than sorting the list. Is this true for the second biggest value? For the median","chapter-15","Lower Bounds"
"value? In later sections we will examine those questions. For this section, we","chapter-15","Lower Bounds"
"will continue our examination of lower bounds proofs by reconsidering the simple","chapter-15","Lower Bounds"
"problem of finding the maximum value in an unsorted list.","chapter-15","Lower Bounds"
"Here is a simple algorithm for finding the largest value.","chapter-15","Lower Bounds"
"/** @return Position of largest value in array A */","chapter-15","Lower Bounds"
"static int largest(int[] A) {","chapter-15","Lower Bounds"
"int currlarge = 0; // Holds largest element position","chapter-15","Lower Bounds"
"for (int i=1; i<A.length; i++) // For each element","chapter-15","Lower Bounds"
"if (A[currlarge] < A[i]) // if A[i] is larger","chapter-15","Lower Bounds"
"currlarge = i; // remember its position","chapter-15","Lower Bounds"
"return currlarge; // Return largest position","chapter-15","Lower Bounds"
"}","chapter-15","Lower Bounds"
"Obviously this algorithm requires n comparisons. Is this optimal? It should be","chapter-15","Lower Bounds"
"intuitively obvious that it is, but let us try to prove it. (Before reading further you","chapter-15","Lower Bounds"
"might try writing down your own proof.)","chapter-15","Lower Bounds"
"492 Chap. 15 Lower Bounds","chapter-15","Lower Bounds"
"Proof 1: The winner must compare against all other elements, so there must be","chapter-15","Lower Bounds"
"n − 1 comparisons. ✷","chapter-15","Lower Bounds"
"This proof is clearly wrong, because the winner does not need to explicitly com-","chapter-15","Lower Bounds"
"pare against all other elements to be recognized. For example, a standard single-","chapter-15","Lower Bounds"
"elimination playoff sports tournament requires only n − 1 comparisons, and the","chapter-15","Lower Bounds"
"winner does not play every opponent. So let’s try again.","chapter-15","Lower Bounds"
"Proof 2: Only the winner does not lose. There are n − 1 losers. A single compar-","chapter-15","Lower Bounds"
"ison generates (at most) one (new) loser. Therefore, there must be n − 1 compar-","chapter-15","Lower Bounds"
"isons. ✷","chapter-15","Lower Bounds"
"This proof is sound. However, it will be useful later to abstract this by introduc-","chapter-15","Lower Bounds"
"ing the concept of posets as we did in Section 15.2.1. We can view the maximum-","chapter-15","Lower Bounds"
"finding problem as starting with a poset where there are no known relationships, so","chapter-15","Lower Bounds"
"every member of the collection is in its own separate DAG of one element.","chapter-15","Lower Bounds"
"Proof 2a: To find the largest value, we start with a poset of n DAGs each with","chapter-15","Lower Bounds"
"a single element, and we must build a poset having all elements in one DAG such","chapter-15","Lower Bounds"
"that there is one maximum value (and by implication, n − 1 losers). We wish to","chapter-15","Lower Bounds"
"connect the elements of the poset into a single DAG with the minimum number of","chapter-15","Lower Bounds"
"links. This requires at least n − 1 links. A comparison provides at most one new","chapter-15","Lower Bounds"
"link. Thus, a minimum of n − 1 comparisons must be made. ✷","chapter-15","Lower Bounds"
"What is the average cost of largest? Because it always does the same num-","chapter-15","Lower Bounds"
"ber of comparisons, clearly it must cost n − 1 comparisons. We can also consider","chapter-15","Lower Bounds"
"the number of assignments that largest must do. Function largest might do","chapter-15","Lower Bounds"
"an assignment on any iteration of the for loop.","chapter-15","Lower Bounds"
"Because this event does happen, or does not happen, if we are given no informa-","chapter-15","Lower Bounds"
"tion about distribution we could guess that an assignment is made after each com-","chapter-15","Lower Bounds"
"parison with a probability of one half. But this is clearly wrong. In fact, largest","chapter-15","Lower Bounds"
"does an assignment on the ith iteration if and only if A[i] is the biggest of the the","chapter-15","Lower Bounds"
"first i elements. Assuming all permutations are equally likely, the probability of","chapter-15","Lower Bounds"
"this being true is 1/i. Thus, the average number of assignments done is","chapter-15","Lower Bounds"
"1 +Xn","chapter-15","Lower Bounds"
"i=2","chapter-15","Lower Bounds"
"1","chapter-15","Lower Bounds"
"i","chapter-15","Lower Bounds"
"=","chapter-15","Lower Bounds"
"Xn","chapter-15","Lower Bounds"
"i=1","chapter-15","Lower Bounds"
"1","chapter-15","Lower Bounds"
"i","chapter-15","Lower Bounds"
"which is the Harmonic Series Hn. Hn = Θ(log n). More exactly, Hn is close to","chapter-15","Lower Bounds"
"loge n.","chapter-15","Lower Bounds"
"How “reliable” is this average? That is, how much will a given run of the","chapter-15","Lower Bounds"
"program deviate from the mean cost? According to Ceby ˇ sev’s Inequality, an obser- ˇ","chapter-15","Lower Bounds"
"vation will fall within two standard deviations of the mean at least 75% of the time.","chapter-15","Lower Bounds"
"For Largest, the variance is","chapter-15","Lower Bounds"
"Hn −","chapter-15","Lower Bounds"
"π","chapter-15","Lower Bounds"
"2","chapter-15","Lower Bounds"
"6","chapter-15","Lower Bounds"
"= loge n −","chapter-15","Lower Bounds"
"π","chapter-15","Lower Bounds"
"2","chapter-15","Lower Bounds"
"6","chapter-15","Lower Bounds"
".","chapter-15","Lower Bounds"
"Sec. 15.4 Adversarial Lower Bounds Proofs 493","chapter-15","Lower Bounds"
"The standard deviation is thus about p","chapter-15","Lower Bounds"
"loge n. So, 75% of the observations are","chapter-15","Lower Bounds"
"between loge n − 2","chapter-15","Lower Bounds"
"p","chapter-15","Lower Bounds"
"loge n and loge n + 2p","chapter-15","Lower Bounds"
"loge n. Is this a narrow spread or a","chapter-15","Lower Bounds"
"wide spread? Compared to the mean value, this spread is pretty wide, meaning that","chapter-15","Lower Bounds"
"the number of assignments varies widely from run to run of the program.","chapter-15","Lower Bounds"
"15.4 Adversarial Lower Bounds Proofs","chapter-15","Lower Bounds"
"Our next problem will be finding the second largest in a collection of objects. Con-","chapter-15","Lower Bounds"
"sider what happens in a standard single-elimination tournament. Even if we assume","chapter-15","Lower Bounds"
"that the “best” team wins in every game, is the second best the one that loses in the","chapter-15","Lower Bounds"
"finals? Not necessarily. We might expect that the second best must lose to the best,","chapter-15","Lower Bounds"
"but they might meet at any time.","chapter-15","Lower Bounds"
"Let us go through our standard “algorithm for finding algorithms” by first","chapter-15","Lower Bounds"
"proposing an algorithm, then a lower bound, and seeing if they match. Unlike","chapter-15","Lower Bounds"
"our analysis for most problems, this time we are going to count the exact number","chapter-15","Lower Bounds"
"of comparisons involved and attempt to minimize this count. A simple algorithm","chapter-15","Lower Bounds"
"for finding the second largest is to first find the maximum (in n − 1 comparisons),","chapter-15","Lower Bounds"
"discard it, and then find the maximum of the remaining elements (in n − 2 compar-","chapter-15","Lower Bounds"
"isons) for a total cost of 2n − 3 comparisons. Is this optimal? That seems doubtful,","chapter-15","Lower Bounds"
"but let us now proceed to the step of attempting to prove a lower bound.","chapter-15","Lower Bounds"
"Theorem 15.2 The lower bound for finding the second largest value is 2n − 3.","chapter-15","Lower Bounds"
"Proof: Any element that loses to anything other than the maximum cannot be","chapter-15","Lower Bounds"
"second. So, the only candidates for second place are those that lost to the maximum.","chapter-15","Lower Bounds"
"Function largest might compare the maximum element to n − 1 others. Thus,","chapter-15","Lower Bounds"
"we might need n − 2 additional comparisons to find the second largest. ✷","chapter-15","Lower Bounds"
"This proof is wrong. It exhibits the necessity fallacy: “Our algorithm does","chapter-15","Lower Bounds"
"something, therefore all algorithms solving the problem must do the same.”","chapter-15","Lower Bounds"
"This leaves us with our best lower bounds argument at the moment being that","chapter-15","Lower Bounds"
"finding the second largest must cost at least as much as finding the largest, or n−1.","chapter-15","Lower Bounds"
"Let us take another try at finding a better algorithm by adopting a strategy of divide","chapter-15","Lower Bounds"
"and conquer. What if we break the list into halves, and run largest on each","chapter-15","Lower Bounds"
"half? We can then compare the two winners (we have now used a total of n − 1","chapter-15","Lower Bounds"
"comparisons), and remove the winner from its half. Another call to largest on","chapter-15","Lower Bounds"
"the winner’s half yields its second best. A final comparison against the winner of","chapter-15","Lower Bounds"
"the other half gives us the true second place winner. The total cost is d3n/2e−2. Is","chapter-15","Lower Bounds"
"this optimal? What if we break the list into four pieces? The best would be d5n/4e.","chapter-15","Lower Bounds"
"What if we break the list into eight pieces? Then the cost would be about d9n/8e.","chapter-15","Lower Bounds"
"Notice that as we break the list into more parts, comparisons among the winners of","chapter-15","Lower Bounds"
"the parts becomes a larger concern.","chapter-15","Lower Bounds"
"494 Chap. 15 Lower Bounds","chapter-15","Lower Bounds"
"Figure 15.2 An example of building a binomial tree. Pairs of elements are","chapter-15","Lower Bounds"
"combined by choosing one of the parents to be the root of the entire tree. Given","chapter-15","Lower Bounds"
"two trees of size four, one of the roots is chosen to be the root for the combined","chapter-15","Lower Bounds"
"tree of eight nodes.","chapter-15","Lower Bounds"
"Looking at this another way, the only candidates for second place are losers to","chapter-15","Lower Bounds"
"the eventual winner, and our goal is to have as few of these as possible. So we need","chapter-15","Lower Bounds"
"to keep track of the set of elements that have lost in direct comparison to the (even-","chapter-15","Lower Bounds"
"tual) winner. We also observe that we learn the most from a comparison when both","chapter-15","Lower Bounds"
"competitors are known to be larger than the same number of other values. So we","chapter-15","Lower Bounds"
"would like to arrange our comparisons to be against “equally strong” competitors.","chapter-15","Lower Bounds"
"We can do all of this with a binomial tree. A binomial tree of height m has 2","chapter-15","Lower Bounds"
"m","chapter-15","Lower Bounds"
"nodes. Either it is a single node (if m = 0), or else it is two height m − 1 binomial","chapter-15","Lower Bounds"
"trees with one tree’s root becoming a child of the other. Figure 15.2 illustrates how","chapter-15","Lower Bounds"
"a binomial tree with eight nodes would be constructed.","chapter-15","Lower Bounds"
"The resulting algorithm is simple in principle: Build the binomial tree for all n","chapter-15","Lower Bounds"
"elements, and then compare the dlog ne children of the root to find second place.","chapter-15","Lower Bounds"
"We could store the binomial tree as an explicit tree structure, and easily build it in","chapter-15","Lower Bounds"
"time linear on the number of comparisons as each comparison requires one link be","chapter-15","Lower Bounds"
"added. Because the shape of a binomial tree is heavily constrained, we can also","chapter-15","Lower Bounds"
"store the binomial tree implicitly in an array, much as we do for a heap. Assume","chapter-15","Lower Bounds"
"that two trees, each with 2","chapter-15","Lower Bounds"
"k nodes, are in the array. The first tree is in positions 1","chapter-15","Lower Bounds"
"to 2","chapter-15","Lower Bounds"
"k","chapter-15","Lower Bounds"
". The second tree is in positions 2","chapter-15","Lower Bounds"
"k + 1 to 2","chapter-15","Lower Bounds"
"k+1. The root of each subtree is in","chapter-15","Lower Bounds"
"the final array position for that subtree.","chapter-15","Lower Bounds"
"To join two trees, we simply compare the roots of the subtrees. If necessary,","chapter-15","Lower Bounds"
"swap the subtrees so that tree with the the larger root element becomes the second","chapter-15","Lower Bounds"
"subtree. This trades space (we only need space for the data values, no node point-","chapter-15","Lower Bounds"
"ers) for time (in the worst case, all of the data swapping might cost O(n log n),","chapter-15","Lower Bounds"
"though this does not affect the number of comparisons required). Note that for","chapter-15","Lower Bounds"
"some applications, this is an important observation that the array’s data swapping","chapter-15","Lower Bounds"
"requires no comparisons. If a comparison is simply a check between two integers,","chapter-15","Lower Bounds"
"then of course moving half the values within the array is too expensive. But if a","chapter-15","Lower Bounds"
"comparison requires that a competition be held between two sports teams, then the","chapter-15","Lower Bounds"
"cost of a little bit (or even a lot) of book keeping becomes irrelevent.","chapter-15","Lower Bounds"
"Because the binomial tree’s root has log n children, and building the tree re-","chapter-15","Lower Bounds"
"quires n − 1 comparisons, the number of comparisons required by this algorithm is","chapter-15","Lower Bounds"
"n + dlog ne − 2. This is clearly better than our previous algorithm. Is it optimal?","chapter-15","Lower Bounds"
"Sec. 15.4 Adversarial Lower Bounds Proofs 495","chapter-15","Lower Bounds"
"We now go back to trying to improve the lower bounds proof. To do this,","chapter-15","Lower Bounds"
"we introduce the concept of an adversary. The adversary’s job is to make an","chapter-15","Lower Bounds"
"algorithm’s cost as high as possible. Imagine that the adversary keeps a list of all","chapter-15","Lower Bounds"
"possible inputs. We view the algorithm as asking the adversary for information","chapter-15","Lower Bounds"
"about the algorithm’s input. The adversary may never lie, in that its answer must","chapter-15","Lower Bounds"
"be consistent with the previous answers. But it is permitted to “rearrange” the input","chapter-15","Lower Bounds"
"as it sees fit in order to drive the total cost for the algorithm as high as possible. In","chapter-15","Lower Bounds"
"particular, when the algorithm asks a question, the adversary must answer in a way","chapter-15","Lower Bounds"
"that is consistent with at least one remaining input. The adversary then crosses out","chapter-15","Lower Bounds"
"all remaining inputs inconsistent with that answer. Keep in mind that there is not","chapter-15","Lower Bounds"
"really an entity within the computer program that is the adversary, and we don’t","chapter-15","Lower Bounds"
"actually modify the program. The adversary operates merely as an analysis device,","chapter-15","Lower Bounds"
"to help us reason about the program.","chapter-15","Lower Bounds"
"As an example of the adversary concept, consider the standard game of Hang-","chapter-15","Lower Bounds"
"man. Player A picks a word and tells player B how many letters the word has.","chapter-15","Lower Bounds"
"Player B guesses various letters. If B guesses a letter in the word, then A will in-","chapter-15","Lower Bounds"
"dicate which position(s) in the word have the letter. Player B is permitted to make","chapter-15","Lower Bounds"
"only so many guesses of letters not in the word before losing.","chapter-15","Lower Bounds"
"In the Hangman game example, the adversary is imagined to hold a dictionary","chapter-15","Lower Bounds"
"of words of some selected length. Each time the player guesses a letter, the ad-","chapter-15","Lower Bounds"
"versary consults the dictionary and decides if more words will be eliminated by","chapter-15","Lower Bounds"
"accepting the letter (and indicating which positions it holds) or saying that its not","chapter-15","Lower Bounds"
"in the word. The adversary can make any decision it chooses, so long as at least","chapter-15","Lower Bounds"
"one word in the dictionary is consistent with all of the decisions. In this way, the","chapter-15","Lower Bounds"
"adversary can hope to make the player guess as many letters as possible.","chapter-15","Lower Bounds"
"Before explaining how the adversary plays a role in our lower bounds proof,","chapter-15","Lower Bounds"
"first observe that at least n − 1 values must lose at least once. This requires at least","chapter-15","Lower Bounds"
"n − 1 compares. In addition, at least k − 1 values must lose to the second largest","chapter-15","Lower Bounds"
"value. That is, k direct losers to the winner must be compared. There must be at","chapter-15","Lower Bounds"
"least n + k − 2 comparisons. The question is: How low can we make k?","chapter-15","Lower Bounds"
"Call the strength of element A[i] the number of elements that A[i] is (known","chapter-15","Lower Bounds"
"to be) bigger than. If A[i] has strength a, and A[j] has strength b, then the winner","chapter-15","Lower Bounds"
"has strength a + b + 1. The algorithm gets to know the (current) strengths for each","chapter-15","Lower Bounds"
"element, and it gets to pick which two elements are compared next. The adversary","chapter-15","Lower Bounds"
"gets to decide who wins any given comparison. What strategy by the adversary","chapter-15","Lower Bounds"
"would cause the algorithm to learn the least from any given comparison? It should","chapter-15","Lower Bounds"
"minimize the rate at which any element improves it strength. It can do this by","chapter-15","Lower Bounds"
"making the element with the greater strength win at every comparison. This is a","chapter-15","Lower Bounds"
"“fair” use of an adversary in that it represents the results of providing a worst-case","chapter-15","Lower Bounds"
"input for that given algorithm.","chapter-15","Lower Bounds"
"496 Chap. 15 Lower Bounds","chapter-15","Lower Bounds"
"To minimize the effects of worst-case behavior, the algorithm’s best strategy is","chapter-15","Lower Bounds"
"to maximize the minimum improvement in strength by balancing the strengths of","chapter-15","Lower Bounds"
"any two competitors. From the algorithm’s point of view, the best outcome is that","chapter-15","Lower Bounds"
"an element doubles in strength. This happens whenever a = b, where a and b are","chapter-15","Lower Bounds"
"the strengths of the two elements being compared. All strengths begin at zero, so","chapter-15","Lower Bounds"
"the winner must make at least k comparisons when 2","chapter-15","Lower Bounds"
"k−1 < n ≤ 2","chapter-15","Lower Bounds"
"k","chapter-15","Lower Bounds"
". Thus, there","chapter-15","Lower Bounds"
"must be at least n + dlog ne − 2 comparisons. So our algorithm is optimal.","chapter-15","Lower Bounds"
"15.5 State Space Lower Bounds Proofs","chapter-15","Lower Bounds"
"We now consider the problem of finding both the minimum and the maximum from","chapter-15","Lower Bounds"
"an (unsorted) list of values. This might be useful if we want to know the range of","chapter-15","Lower Bounds"
"a collection of values to be plotted, for the purpose of drawing the plot’s scales.","chapter-15","Lower Bounds"
"Of course we could find them independently in 2n − 2 comparisons. A slight","chapter-15","Lower Bounds"
"modification is to find the maximum in n − 1 comparisons, remove it from the","chapter-15","Lower Bounds"
"list, and then find the minimum in n − 2 further comparisons for a total of 2n − 3","chapter-15","Lower Bounds"
"comparisons. Can we do better than this?","chapter-15","Lower Bounds"
"Before continuing, think a moment about how this problem of finding the mini-","chapter-15","Lower Bounds"
"mum and the maximum compares to the problem of the last section, that of finding","chapter-15","Lower Bounds"
"the second biggest value (and by implication, the maximum). Which of these two","chapter-15","Lower Bounds"
"problems do you think is harder? It is probably not at all obvious to you that one","chapter-15","Lower Bounds"
"problem is harder or easier than the other. There is intuition that argues for ei-","chapter-15","Lower Bounds"
"ther case. On the one hand intuition might argue that the process of finding the","chapter-15","Lower Bounds"
"maximum should tell you something about the second biggest value, more than","chapter-15","Lower Bounds"
"that process should tell you about the minimum value. On the other hand, any","chapter-15","Lower Bounds"
"given comparison tells you something about which of two can be a candidate for","chapter-15","Lower Bounds"
"maximum value, and which can be a candidate for minimum value, thus making","chapter-15","Lower Bounds"
"progress in both directions.","chapter-15","Lower Bounds"
"We will start by considering a simple divide-and-conquer approach to finding","chapter-15","Lower Bounds"
"the minimum and maximum. Split the list into two parts and find the minimum and","chapter-15","Lower Bounds"
"maximum elements in each part. Then compare the two minimums and maximums","chapter-15","Lower Bounds"
"to each other with a further two comparisons to get the final result. The algorithm","chapter-15","Lower Bounds"
"is shown in Figure 15.3.","chapter-15","Lower Bounds"
"The cost of this algorithm can be modeled by the following recurrence.","chapter-15","Lower Bounds"
"T(n) =","chapter-15","Lower Bounds"
"","chapter-15","Lower Bounds"
"","chapter-15","Lower Bounds"
"","chapter-15","Lower Bounds"
"0 n = 1","chapter-15","Lower Bounds"
"1 n = 2","chapter-15","Lower Bounds"
"T(bn/2c) + T(dn/2e) + 2 n > 2","chapter-15","Lower Bounds"
"This is a rather interesting recurrence, and its solution ranges between 3n/2−2","chapter-15","Lower Bounds"
"(when n = 2i or n = 21 ± 1) and 5n/3 − 2 (when n = 3 × 2","chapter-15","Lower Bounds"
"i","chapter-15","Lower Bounds"
"). We can infer from","chapter-15","Lower Bounds"
"this behavior that how we divide the list affects the performance of the algorithm.","chapter-15","Lower Bounds"
"Sec. 15.5 State Space Lower Bounds Proofs 497","chapter-15","Lower Bounds"
"/** @return The minimum and maximum values in A","chapter-15","Lower Bounds"
"between positions l and r */","chapter-15","Lower Bounds"
"static void MinMax(int A[], int l, int r, int Out[]) {","chapter-15","Lower Bounds"
"if (l == r) { // n=1","chapter-15","Lower Bounds"
"Out[0] = A[r];","chapter-15","Lower Bounds"
"Out[1] = A[r];","chapter-15","Lower Bounds"
"}","chapter-15","Lower Bounds"
"else if (l+1 == r) { // n=2","chapter-15","Lower Bounds"
"Out[0] = Math.min(A[l], A[r]);","chapter-15","Lower Bounds"
"Out[1] = Math.max(A[l], A[r]);","chapter-15","Lower Bounds"
"}","chapter-15","Lower Bounds"
"else { // n>2","chapter-15","Lower Bounds"
"int[] Out1 = new int[2];","chapter-15","Lower Bounds"
"int[] Out2 = new int[2];","chapter-15","Lower Bounds"
"int mid = (l + r)/2;","chapter-15","Lower Bounds"
"MinMax(A, l, mid, Out1);","chapter-15","Lower Bounds"
"MinMax(A, mid+1, r, Out2);","chapter-15","Lower Bounds"
"Out[0] = Math.min(Out1[0], Out2[0]);","chapter-15","Lower Bounds"
"Out[1] = Math.max(Out1[1], Out2[1]);","chapter-15","Lower Bounds"
"}","chapter-15","Lower Bounds"
"}","chapter-15","Lower Bounds"
"Figure 15.3 Recursive algorithm for finding the minimum and maximum values","chapter-15","Lower Bounds"
"in an array.","chapter-15","Lower Bounds"
"For example, what if we have six items in the list? If we break the list into two","chapter-15","Lower Bounds"
"sublists of three elements, the cost would be 8. If we break the list into a sublist of","chapter-15","Lower Bounds"
"size two and another of size four, then the cost would only be 7.","chapter-15","Lower Bounds"
"With divide and conquer, the best algorithm is the one that minimizes the work,","chapter-15","Lower Bounds"
"not necessarily the one that balances the input sizes. One lesson to learn from this","chapter-15","Lower Bounds"
"example is that it can be important to pay attention to what happens for small sizes","chapter-15","Lower Bounds"
"of n, because any division of the list will eventually produce many small lists.","chapter-15","Lower Bounds"
"We can model all possible divide-and-conquer strategies for this problem with","chapter-15","Lower Bounds"
"the following recurrence.","chapter-15","Lower Bounds"
"T(n) =","chapter-15","Lower Bounds"
"","chapter-15","Lower Bounds"
"","chapter-15","Lower Bounds"
"","chapter-15","Lower Bounds"
"0 n = 1","chapter-15","Lower Bounds"
"1 n = 2","chapter-15","Lower Bounds"
"min1≤k≤n−1{T(k) + T(n − k)} + 2 n > 2","chapter-15","Lower Bounds"
"That is, we want to find a way to break up the list that will minimize the total","chapter-15","Lower Bounds"
"work. If we examine various ways of breaking up small lists, we will eventually","chapter-15","Lower Bounds"
"recognize that breaking the list into a sublist of size 2 and a sublist of size n − 2","chapter-15","Lower Bounds"
"will always produce results as good as any other division. This strategy yields the","chapter-15","Lower Bounds"
"following recurrence.","chapter-15","Lower Bounds"
"T(n) =","chapter-15","Lower Bounds"
"","chapter-15","Lower Bounds"
"","chapter-15","Lower Bounds"
"","chapter-15","Lower Bounds"
"0 n = 1","chapter-15","Lower Bounds"
"1 n = 2","chapter-15","Lower Bounds"
"T(n − 2) + 3 n > 2","chapter-15","Lower Bounds"
"498 Chap. 15 Lower Bounds","chapter-15","Lower Bounds"
"This recurrence (and the corresponding algorithm) yields T(n) = d3n/2e − 2","chapter-15","Lower Bounds"
"comparisons. Is this optimal? We now introduce yet another tool to our collection","chapter-15","Lower Bounds"
"of lower bounds proof techniques: The state space proof.","chapter-15","Lower Bounds"
"We will model our algorithm by defining a state that the algorithm must be in at","chapter-15","Lower Bounds"
"any given instant. We can then define the start state, the end state, and the transitions","chapter-15","Lower Bounds"
"between states that any algorithm can support. From this, we will reason about the","chapter-15","Lower Bounds"
"minimum number of states that the algorithm must go through to get from the start","chapter-15","Lower Bounds"
"to the end, to reach a state space lower bound.","chapter-15","Lower Bounds"
"At any given instant, we can track the following four categories of elements:","chapter-15","Lower Bounds"
"• Untested: Elements that have not been tested.","chapter-15","Lower Bounds"
"• Winners: Elements that have won at least once, and never lost.","chapter-15","Lower Bounds"
"• Losers: Elements that have lost at least once, and never won.","chapter-15","Lower Bounds"
"• Middle: Elements that have both won and lost at least once.","chapter-15","Lower Bounds"
"We define the current state to be a vector of four values, (U, W, L, M) for","chapter-15","Lower Bounds"
"untested, winners, losers, and middles, respectively. For a set of n elements, the","chapter-15","Lower Bounds"
"initial state of the algorithm is (n, 0, 0, 0) and the end state is (0, 1, 1, n − 2). Thus,","chapter-15","Lower Bounds"
"every run for any algorithm must go from state (n, 0, 0, 0) to state (0, 1, 1, n − 2).","chapter-15","Lower Bounds"
"We also observe that once an element is identified to be a middle, it can then be","chapter-15","Lower Bounds"
"ignored because it can neither be the minimum nor the maximum.","chapter-15","Lower Bounds"
"Given that there are four types of elements, there are 10 types of comparison.","chapter-15","Lower Bounds"
"Comparing with a middle cannot be more efficient than other comparisons, so we","chapter-15","Lower Bounds"
"should ignore those, leaving six comparisons of interest. We can enumerate the","chapter-15","Lower Bounds"
"effects of each comparison type as follows. If we are in state (i, j, k, l) and we have","chapter-15","Lower Bounds"
"a comparison, then the state changes are as follows.","chapter-15","Lower Bounds"
"U : U (i − 2, j + 1, k + 1, l)","chapter-15","Lower Bounds"
"W : W (i, j − 1, k, l + 1)","chapter-15","Lower Bounds"
"L : L (i, j, k − 1, l + 1)","chapter-15","Lower Bounds"
"L : U (i − 1, j + 1, k, l)","chapter-15","Lower Bounds"
"or (i − 1, j, k, l + 1)","chapter-15","Lower Bounds"
"W : U (i − 1, j, k + 1, l)","chapter-15","Lower Bounds"
"or (i − 1, j, k, l + 1)","chapter-15","Lower Bounds"
"W : L (i, j, k, l)","chapter-15","Lower Bounds"
"or (i, j − 1, k − 1, l + 2)","chapter-15","Lower Bounds"
"Now, let us consider what an adversary will do for the various comparisons.","chapter-15","Lower Bounds"
"The adversary will make sure that each comparison does the least possible amount","chapter-15","Lower Bounds"
"of work in taking the algorithm toward the goal state. For example, comparing a","chapter-15","Lower Bounds"
"winner to a loser is of no value because the worst case result is always to learn","chapter-15","Lower Bounds"
"nothing new (the winner remains a winner and the loser remains a loser). Thus,","chapter-15","Lower Bounds"
"only the following five transitions are of interest:","chapter-15","Lower Bounds"
"Sec. 15.6 Finding the ith Best Element 499","chapter-15","Lower Bounds"
"...","chapter-15","Lower Bounds"
"... i−1","chapter-15","Lower Bounds"
"n−i","chapter-15","Lower Bounds"
"Figure 15.4 The poset that represents the minimum information necessary to","chapter-15","Lower Bounds"
"determine the ith element in a list. We need to know which element has i − 1","chapter-15","Lower Bounds"
"values less and n − i values more, but we do not need to know the relationships","chapter-15","Lower Bounds"
"among the elements with values less or greater than the ith element.","chapter-15","Lower Bounds"
"U : U (i − 2, j + 1, k + 1, l)","chapter-15","Lower Bounds"
"L : U (i − 1, j + 1, k, l)","chapter-15","Lower Bounds"
"W : U (i − 1, j, k + 1, l)","chapter-15","Lower Bounds"
"W : W (i, j − 1, k, l + 1)","chapter-15","Lower Bounds"
"L : L (i, j, k − 1, l + 1)","chapter-15","Lower Bounds"
"Only the last two transition types increase the number of middles, so there","chapter-15","Lower Bounds"
"must be n−2 of these. The number of untested elements must go to 0, and the first","chapter-15","Lower Bounds"
"transition is the most efficient way to do this. Thus, dn/2e of these are required.","chapter-15","Lower Bounds"
"Our conclusion is that the minimum possible number of transitions (comparisons)","chapter-15","Lower Bounds"
"is n + dn/2e − 2. Thus, our algorithm is optimal.","chapter-15","Lower Bounds"
"15.6 Finding the ith Best Element","chapter-15","Lower Bounds"
"We now tackle the problem of finding the ith best element in a list. As observed","chapter-15","Lower Bounds"
"earlier, one solution is to sort the list and simply look in the ith position. However,","chapter-15","Lower Bounds"
"this process provides considerably more information than we need to solve the","chapter-15","Lower Bounds"
"problem. The minimum amount of information that we actually need to know can","chapter-15","Lower Bounds"
"be visualized as shown in Figure 15.4. That is, all we need to know is the i − 1","chapter-15","Lower Bounds"
"items less than our desired value, and the n − i items greater. We do not care about","chapter-15","Lower Bounds"
"the relative order within the upper and lower groups. So can we find the required","chapter-15","Lower Bounds"
"information faster than by first sorting? Looking at the lower bound, can we tighten","chapter-15","Lower Bounds"
"that beyond the trivial lower bound of n comparisons? We will focus on the specific","chapter-15","Lower Bounds"
"question of finding the median element (i.e., the element with rank n/2), because","chapter-15","Lower Bounds"
"the resulting algorithm can easily be modified to find the ith largest value for any i.","chapter-15","Lower Bounds"
"Looking at the Quicksort algorithm might give us some insight into solving the","chapter-15","Lower Bounds"
"median problem. Recall that Quicksort works by selecting a pivot value, partition-","chapter-15","Lower Bounds"
"ing the array into those elements less than the pivot and those greater than the pivot,","chapter-15","Lower Bounds"
"and moving the pivot to its proper location in the array. If the pivot is in position i,","chapter-15","Lower Bounds"
"then we are done. If not, we can solve the subproblem recursively by only consid-","chapter-15","Lower Bounds"
"ering one of the sublists. That is, if the pivot ends up in position k > i, then we","chapter-15","Lower Bounds"
"500 Chap. 15 Lower Bounds","chapter-15","Lower Bounds"
"Figure 15.5 A method for finding a pivot for partitioning a list that guarantees","chapter-15","Lower Bounds"
"at least a fixed fraction of the list will be in each partition. We divide the list into","chapter-15","Lower Bounds"
"groups of five elements, and find the median for each group. We then recursively","chapter-15","Lower Bounds"
"find the median of these n/5 medians. The median of five elements is guaran-","chapter-15","Lower Bounds"
"teed to have at least two in each partition. The median of three medians from","chapter-15","Lower Bounds"
"a collection of 15 elements is guaranteed to have at least five elements in each","chapter-15","Lower Bounds"
"partition.","chapter-15","Lower Bounds"
"simply solve by finding the ith best element in the left partition. If the pivot is at","chapter-15","Lower Bounds"
"position k < i, then we wish to find the i − kth element in the right partition.","chapter-15","Lower Bounds"
"What is the worst case cost of this algorithm? As with Quicksort, we get bad","chapter-15","Lower Bounds"
"performance if the pivot is the first or last element in the array. This would lead to","chapter-15","Lower Bounds"
"possibly O(n","chapter-15","Lower Bounds"
"2","chapter-15","Lower Bounds"
") performance. However, if the pivot were to always cut the array in","chapter-15","Lower Bounds"
"half, then our cost would be modeled by the recurrence T(n) = T(n/2) +n = 2n","chapter-15","Lower Bounds"
"or O(n) cost.","chapter-15","Lower Bounds"
"Finding the average cost requires us to use a recurrence with full history, similar","chapter-15","Lower Bounds"
"to the one we used to model the cost of Quicksort. If we do this, we will find that","chapter-15","Lower Bounds"
"T(n) is in O(n) in the average case.","chapter-15","Lower Bounds"
"Is it possible to modify our algorithm to get worst-case linear time? To do","chapter-15","Lower Bounds"
"this, we need to pick a pivot that is guaranteed to discard a fixed fraction of the","chapter-15","Lower Bounds"
"elements. We cannot just choose a pivot at random, because doing so will not meet","chapter-15","Lower Bounds"
"this guarantee. The ideal situation would be if we could pick the median value for","chapter-15","Lower Bounds"
"the pivot each time. But that is essentially the same problem that we are trying to","chapter-15","Lower Bounds"
"solve to begin with.","chapter-15","Lower Bounds"
"Notice, however, that if we choose any constant c, and then if we pick the","chapter-15","Lower Bounds"
"median from a sample of size n/c, then we can guarantee that we will discard","chapter-15","Lower Bounds"
"at least n/2c elements. Actually, we can do better than this by selecting small","chapter-15","Lower Bounds"
"subsets of a constant size (so we can find the median of each in constant time), and","chapter-15","Lower Bounds"
"then taking the median of these medians. Figure 15.5 illustrates this idea. This","chapter-15","Lower Bounds"
"observation leads directly to the following algorithm.","chapter-15","Lower Bounds"
"• Choose the n/5 medians for groups of five elements from the list. Choosing","chapter-15","Lower Bounds"
"the median of five items can be done in constant time.","chapter-15","Lower Bounds"
"• Recursively, select M, the median of the n/5 medians-of-fives.","chapter-15","Lower Bounds"
"• Partition the list into those elements larger and smaller than M.","chapter-15","Lower Bounds"
"Sec. 15.7 Optimal Sorting 501","chapter-15","Lower Bounds"
"While selecting the median in this way is guaranteed to eliminate a fraction of","chapter-15","Lower Bounds"
"the elements (leaving at most d(7n − 5)/10e elements left), we still need to be sure","chapter-15","Lower Bounds"
"that our recursion yields a linear-time algorithm. We model the algorithm by the","chapter-15","Lower Bounds"
"following recurrence.","chapter-15","Lower Bounds"
"T(n) ≤ T(dn/5e) + T(d(7n − 5)/10e) + 6dn/5e + n − 1.","chapter-15","Lower Bounds"
"The T(dn/5e) term comes from computing the median of the medians-of-fives,","chapter-15","Lower Bounds"
"the 6dn/5e term comes from the cost to calculate the median-of-fives (exactly six","chapter-15","Lower Bounds"
"comparisons for each group of five element), and the T(d(7n−5)/10e) term comes","chapter-15","Lower Bounds"
"from the recursive call of the remaining (up to) 70% of the elements that might be","chapter-15","Lower Bounds"
"left.","chapter-15","Lower Bounds"
"We will prove that this recurrence is linear by assuming that it is true for some","chapter-15","Lower Bounds"
"constant r, and then show that T(n) ≤ rn for all n greater than some bound.","chapter-15","Lower Bounds"
"T(n) ≤ T(d","chapter-15","Lower Bounds"
"n","chapter-15","Lower Bounds"
"5","chapter-15","Lower Bounds"
"e) + T(d","chapter-15","Lower Bounds"
"7n − 5","chapter-15","Lower Bounds"
"10","chapter-15","Lower Bounds"
"e) + 6d","chapter-15","Lower Bounds"
"n","chapter-15","Lower Bounds"
"5","chapter-15","Lower Bounds"
"e + n − 1","chapter-15","Lower Bounds"
"≤ r(","chapter-15","Lower Bounds"
"n","chapter-15","Lower Bounds"
"5","chapter-15","Lower Bounds"
"+ 1) + r(","chapter-15","Lower Bounds"
"7n − 5","chapter-15","Lower Bounds"
"10","chapter-15","Lower Bounds"
"+ 1) + 6(n","chapter-15","Lower Bounds"
"5","chapter-15","Lower Bounds"
"+ 1) + n − 1","chapter-15","Lower Bounds"
"≤ (","chapter-15","Lower Bounds"
"r","chapter-15","Lower Bounds"
"5","chapter-15","Lower Bounds"
"+","chapter-15","Lower Bounds"
"7r","chapter-15","Lower Bounds"
"10","chapter-15","Lower Bounds"
"+","chapter-15","Lower Bounds"
"11","chapter-15","Lower Bounds"
"5","chapter-15","Lower Bounds"
")n +","chapter-15","Lower Bounds"
"3r","chapter-15","Lower Bounds"
"2","chapter-15","Lower Bounds"
"+ 5","chapter-15","Lower Bounds"
"≤","chapter-15","Lower Bounds"
"9r + 22","chapter-15","Lower Bounds"
"10","chapter-15","Lower Bounds"
"n +","chapter-15","Lower Bounds"
"3r + 10","chapter-15","Lower Bounds"
"2","chapter-15","Lower Bounds"
".","chapter-15","Lower Bounds"
"This is true for r ≥ 23 and n ≥ 380. This provides a base case that allows us to","chapter-15","Lower Bounds"
"use induction to prove that ∀n ≥ 380, T(n) ≤ 23n.","chapter-15","Lower Bounds"
"In reality, this algorithm is not practical because its constant factor costs are so","chapter-15","Lower Bounds"
"high. So much work is being done to guarantee linear time performance that it is","chapter-15","Lower Bounds"
"more efficient on average to rely on chance to select the pivot, perhaps by picking","chapter-15","Lower Bounds"
"it at random or picking the middle value out of the current subarray.","chapter-15","Lower Bounds"
"15.7 Optimal Sorting","chapter-15","Lower Bounds"
"We conclude this section with an effort to find the sorting algorithm with the ab-","chapter-15","Lower Bounds"
"solute fewest possible comparisons. It might well be that the result will not be","chapter-15","Lower Bounds"
"practical for a general-purpose sorting algorithm. But recall our analogy earlier to","chapter-15","Lower Bounds"
"sports tournaments. In sports, a “comparison” between two teams or individuals","chapter-15","Lower Bounds"
"means doing a competition between the two. This is fairly expensive (at least com-","chapter-15","Lower Bounds"
"pared to some minor book keeping in a computer), and it might be worth trading a","chapter-15","Lower Bounds"
"fair amount of book keeping to cut down on the number of games that need to be","chapter-15","Lower Bounds"
"played. What if we want to figure out how to hold a tournament that will give us","chapter-15","Lower Bounds"
"the exact ordering for all teams in the fewest number of total games? Of course,","chapter-15","Lower Bounds"
"we are assuming that the results of each game will be “accurate” in that we assume","chapter-15","Lower Bounds"
"502 Chap. 15 Lower Bounds","chapter-15","Lower Bounds"
"not only that the outcome of A playing B would always be the same (at least over","chapter-15","Lower Bounds"
"the time period of the tournament), but that transitivity in the results also holds. In","chapter-15","Lower Bounds"
"practice these are unrealistic assumptions, but such assumptions are implicitly part","chapter-15","Lower Bounds"
"of many tournament organizations. Like most tournament organizers, we can sim-","chapter-15","Lower Bounds"
"ply accept these assumptions and come up with an algorithm for playing the games","chapter-15","Lower Bounds"
"that gives us some rank ordering based on the results we obtain.","chapter-15","Lower Bounds"
"Recall Insertion Sort, where we put element i into a sorted sublist of the first i−","chapter-15","Lower Bounds"
"1 elements. What if we modify the standard Insertion Sort algorithm to use binary","chapter-15","Lower Bounds"
"search to locate where the ith element goes in the sorted sublist? This algorithm","chapter-15","Lower Bounds"
"is called binary insert sort. As a general-purpose sorting algorithm, this is not","chapter-15","Lower Bounds"
"practical because we then have to (on average) move about i/2 elements to make","chapter-15","Lower Bounds"
"room for the newly inserted element in the sorted sublist. But if we count only","chapter-15","Lower Bounds"
"comparisons, binary insert sort is pretty good. And we can use some ideas from","chapter-15","Lower Bounds"
"binary insert sort to get closer to an algorithm that uses the absolute minimum","chapter-15","Lower Bounds"
"number of comparisons needed to sort.","chapter-15","Lower Bounds"
"Consider what happens when we run binary insert sort on five elements. How","chapter-15","Lower Bounds"
"many comparisons do we need to do? We can insert the second element with one","chapter-15","Lower Bounds"
"comparison, the third with two comparisons, and the fourth with 2 comparisons.","chapter-15","Lower Bounds"
"When we insert the fifth element into the sorted list of four elements, we need to","chapter-15","Lower Bounds"
"do three comparisons in the worst case. Notice exactly what happens when we","chapter-15","Lower Bounds"
"attempt to do this insertion. We compare the fifth element against the second. If the","chapter-15","Lower Bounds"
"fifth is bigger, we have to compare it against the third, and if it is bigger we have","chapter-15","Lower Bounds"
"to compare it against the fourth. In general, when is binary search most efficient?","chapter-15","Lower Bounds"
"When we have 2","chapter-15","Lower Bounds"
"i − 1 elements in the list. It is least efficient when we have 2","chapter-15","Lower Bounds"
"i","chapter-15","Lower Bounds"
"elements in the list. So, we can do a bit better if we arrange our insertions to avoid","chapter-15","Lower Bounds"
"inserting an element into a list of size 2","chapter-15","Lower Bounds"
"i","chapter-15","Lower Bounds"
"if possible.","chapter-15","Lower Bounds"
"Figure 15.6 illustrates a different organization for the comparisons that we","chapter-15","Lower Bounds"
"might do. First we compare the first and second element, and the third and fourth","chapter-15","Lower Bounds"
"elements. The two winners are then compared, yielding a binomial tree. We can","chapter-15","Lower Bounds"
"view this as a (sorted) chain of three elements, with element A hanging off from the","chapter-15","Lower Bounds"
"root. If we then insert element B into the sorted chain of three elements, we will","chapter-15","Lower Bounds"
"end up with one of the two posets shown on the right side of Figure 15.6, at a cost of","chapter-15","Lower Bounds"
"2 comparisons. We can then merge A into the chain, for a cost of two comparisons","chapter-15","Lower Bounds"
"(because we already know that it is smaller then either one or two elements, we are","chapter-15","Lower Bounds"
"actually merging it into a list of two or three elements). Thus, the total number of","chapter-15","Lower Bounds"
"comparisons needed to sort the five elements is at most seven instead of eight.","chapter-15","Lower Bounds"
"If we have ten elements to sort, we can first make five pairs of elements (using","chapter-15","Lower Bounds"
"five compares) and then sort the five winners using the algorithm just described","chapter-15","Lower Bounds"
"(using seven more compares). Now all we need to do is to deal with the original","chapter-15","Lower Bounds"
"losers. We can generalize this process for any number of elements as:","chapter-15","Lower Bounds"
"• Pair up all the nodes with b","chapter-15","Lower Bounds"
"n","chapter-15","Lower Bounds"
"2","chapter-15","Lower Bounds"
"c comparisons.","chapter-15","Lower Bounds"
"Sec. 15.7 Optimal Sorting 503","chapter-15","Lower Bounds"
"A","chapter-15","Lower Bounds"
"B","chapter-15","Lower Bounds"
"or","chapter-15","Lower Bounds"
"A","chapter-15","Lower Bounds"
"A","chapter-15","Lower Bounds"
"Figure 15.6 Organizing comparisons for sorting five elements. First we order","chapter-15","Lower Bounds"
"two pairs of elements, and then compare the two winners to form a binomial tree","chapter-15","Lower Bounds"
"of four elements. The original loser to the root is labeled A, and the remaining","chapter-15","Lower Bounds"
"three elements form a sorted chain. We then insert element B into the sorted","chapter-15","Lower Bounds"
"chain. Finally, we put A into the resulting chain to yield a final sorted list.","chapter-15","Lower Bounds"
"• Recursively sort the winners.","chapter-15","Lower Bounds"
"• Fold in the losers.","chapter-15","Lower Bounds"
"We use binary insert to place the losers. However, we are free to choose the","chapter-15","Lower Bounds"
"best ordering for inserting, keeping in mind the fact that binary search has the","chapter-15","Lower Bounds"
"same cost for 2","chapter-15","Lower Bounds"
"i","chapter-15","Lower Bounds"
"through 2","chapter-15","Lower Bounds"
"i+1 − 1 items. For example, binary search requires three","chapter-15","Lower Bounds"
"comparisons in the worst case for lists of size 4, 5, 6, or 7. So we pick the order of","chapter-15","Lower Bounds"
"inserts to optimize the binary searches, which means picking an order that avoids","chapter-15","Lower Bounds"
"growing a sublist size such that it crosses the boundary on list size to require an","chapter-15","Lower Bounds"
"additional comparison. This sort is called merge insert sort, and also known as","chapter-15","Lower Bounds"
"the Ford and Johnson sort.","chapter-15","Lower Bounds"
"For ten elements, given the poset shown in Figure 15.7 we fold in the last","chapter-15","Lower Bounds"
"four elements (labeled 1 to 4) in the order Element 3, Element 4, Element 1, and","chapter-15","Lower Bounds"
"finally Element 2. Element 3 will be inserted into a list of size three, costing two","chapter-15","Lower Bounds"
"comparisons. Depending on where Element 3 then ends up in the list, Element 4","chapter-15","Lower Bounds"
"will now be inserted into a list of size 2 or 3, costing two comparisons in either","chapter-15","Lower Bounds"
"case. Depending on where Elements 3 and 4 are in the list, Element 1 will now be","chapter-15","Lower Bounds"
"inserted into a list of size 5, 6, or 7, all of which requires three comparisons to place","chapter-15","Lower Bounds"
"in sort order. Finally, Element 2 will be inserted into a list of size 5, 6, or 7.","chapter-15","Lower Bounds"
"Merge insert sort is pretty good, but is it optimal? Recall from Section 7.9 that","chapter-15","Lower Bounds"
"no sorting algorithm can be faster than Ω(n log n). To be precise, the information","chapter-15","Lower Bounds"
"theoretic lower bound for sorting can be proved to be dlog n!e. That is, we can","chapter-15","Lower Bounds"
"prove a lower bound of exactly dlog n!e comparisons. Merge insert sort gives us","chapter-15","Lower Bounds"
"a number of comparisons equal to this information theoretic lower bound for all","chapter-15","Lower Bounds"
"values up to n = 12. At n = 12, merge insert sort requires 30 comparisons","chapter-15","Lower Bounds"
"while the information theoretic lower bound is only 29 comparisons. However, for","chapter-15","Lower Bounds"
"such a small number of elements, it is possible to do an exhaustive study of every","chapter-15","Lower Bounds"
"possible arrangement of comparisons. It turns out that there is in fact no possible","chapter-15","Lower Bounds"
"arrangement of comparisons that makes the lower bound less than 30 comparisons","chapter-15","Lower Bounds"
"when n = 12. Thus, the information theoretic lower bound is an underestimate in","chapter-15","Lower Bounds"
"this case, because 30 really is the best that can be done.","chapter-15","Lower Bounds"
"504 Chap. 15 Lower Bounds","chapter-15","Lower Bounds"
"1","chapter-15","Lower Bounds"
"2","chapter-15","Lower Bounds"
"4","chapter-15","Lower Bounds"
"3","chapter-15","Lower Bounds"
"Figure 15.7 Merge insert sort for ten elements. First five pairs of elements are","chapter-15","Lower Bounds"
"compared. The five winners are then sorted. This leaves the elements labeled 1-4","chapter-15","Lower Bounds"
"to be sorted into the chain made by the remaining six elements.","chapter-15","Lower Bounds"
"Call the optimal worst cost for n elements S(n). We know that S(n + 1) ≤","chapter-15","Lower Bounds"
"S(n)+dlog(n+ 1)e because we could sort n elements and use binary insert for the","chapter-15","Lower Bounds"
"last one. For all n and m, S(n + m) ≤ S(n) + S(m) + M(m, n) where M(m, n)","chapter-15","Lower Bounds"
"is the best time to merge two sorted lists. For n = 47, it turns out that we can do","chapter-15","Lower Bounds"
"better by splitting the list into pieces of size 5 and 42, and then merging. Thus,","chapter-15","Lower Bounds"
"merge sort is not quite optimal. But it is extremely good, and nearly optimal for","chapter-15","Lower Bounds"
"smallish numbers of elements.","chapter-15","Lower Bounds"
"15.8 Further Reading","chapter-15","Lower Bounds"
"Much of the material in this book is also covered in many other textbooks on data","chapter-15","Lower Bounds"
"structures and algorithms. The biggest exception is that not many other textbooks","chapter-15","Lower Bounds"
"cover lower bounds proofs in any significant detail, as is done in this chapter. Those","chapter-15","Lower Bounds"
"that do focus on the same example problems (search and selection) because it tells","chapter-15","Lower Bounds"
"such a tight and compelling story regarding related topics, while showing off the","chapter-15","Lower Bounds"
"major techniques for lower bounds proofs. Two examples of such textbooks are","chapter-15","Lower Bounds"
"“Computer Algorithms” by Baase and Van Gelder [BG00], and “Compared to","chapter-15","Lower Bounds"
"What?” by Gregory J.E. Rawlins [Raw92]. “Fundamentals of Algorithmics” by","chapter-15","Lower Bounds"
"Brassard and Bratley [BB96] also covers lower bounds proofs.","chapter-15","Lower Bounds"
"15.9 Exercises","chapter-15","Lower Bounds"
"15.1 Consider the so-called “algorithm for algorithms” in Section 15.1. Is this","chapter-15","Lower Bounds"
"really an algorithm? Review the definition of an algorithm from Section 1.4.","chapter-15","Lower Bounds"
"Which parts of the definition apply, and which do not? Is the “algorithm for","chapter-15","Lower Bounds"
"algorithms” a heuristic for finding a good algorithm? Why or why not?","chapter-15","Lower Bounds"
"15.2 Single-elimination tournaments are notorious for their scheduling difficul-","chapter-15","Lower Bounds"
"ties. Imagine that you are organizing a tournament for n basketball teams","chapter-15","Lower Bounds"
"(you may assume that n = 2i","chapter-15","Lower Bounds"
"for some integer i). We will further simplify","chapter-15","Lower Bounds"
"Sec. 15.9 Exercises 505","chapter-15","Lower Bounds"
"things by assuming that each game takes less than an hour, and that each team","chapter-15","Lower Bounds"
"can be scheduled for a game every hour if necessary. (Note that everything","chapter-15","Lower Bounds"
"said here about basketball courts is also true about processors in a parallel","chapter-15","Lower Bounds"
"algorithm to solve the maximum-finding problem).","chapter-15","Lower Bounds"
"(a) How many basketball courts do we need to insure that every team can","chapter-15","Lower Bounds"
"play whenever we want to minimize the total tournament time?","chapter-15","Lower Bounds"
"(b) How long will the tournament be in this case?","chapter-15","Lower Bounds"
"(c) What is the total number of “court-hours” available? How many total","chapter-15","Lower Bounds"
"hours are courts being used? How many total court-hours are unused?","chapter-15","Lower Bounds"
"(d) Modify the algorithm in such a way as to reduce the total number of","chapter-15","Lower Bounds"
"courts needed, by perhaps not letting every team play whenever possi-","chapter-15","Lower Bounds"
"ble. This will increase the total hours of the tournament, but try to keep","chapter-15","Lower Bounds"
"the increase as low as possible. For your new algorithm, how long is the","chapter-15","Lower Bounds"
"tournament, how many courts are needed, how many total court-hours","chapter-15","Lower Bounds"
"are available, how many court-hours are used, and how many unused?","chapter-15","Lower Bounds"
"15.3 Explain why the cost of splitting a list of six into two lists of three to find the","chapter-15","Lower Bounds"
"minimum and maximum elements requires eight comparisons, while split-","chapter-15","Lower Bounds"
"ting the list into a list of two and a list of four costs only seven comparisons.","chapter-15","Lower Bounds"
"15.4 Write out a table showing the number of comparisons required to find the","chapter-15","Lower Bounds"
"minimum and maximum for all divisions for all values of n ≤ 13.","chapter-15","Lower Bounds"
"15.5 Present an adversary argument as a lower bounds proof to show that n − 1","chapter-15","Lower Bounds"
"comparisons are necessary to find the maximum of n values in the worst case.","chapter-15","Lower Bounds"
"15.6 Present an adversary argument as a lower bounds proof to show that n com-","chapter-15","Lower Bounds"
"parisons are necessary in the worst case when searching for an element with","chapter-15","Lower Bounds"
"value X (if one exists) from among n elements.","chapter-15","Lower Bounds"
"15.7 Section 15.6 claims that by picking a pivot that always discards at least a","chapter-15","Lower Bounds"
"fixed fraction c of the remaining array, the resulting algorithm will be linear.","chapter-15","Lower Bounds"
"Explain why this is true. Hint: The Master Theorem (Theorem 14.1) might","chapter-15","Lower Bounds"
"help you.","chapter-15","Lower Bounds"
"15.8 Show that any comparison-based algorithm for finding the median must use","chapter-15","Lower Bounds"
"at least n − 1 comparisons.","chapter-15","Lower Bounds"
"15.9 Show that any comparison-based algorithm for finding the second-smallest","chapter-15","Lower Bounds"
"of n values can be extended to find the smallest value also, without requiring","chapter-15","Lower Bounds"
"any more comparisons to be performed.","chapter-15","Lower Bounds"
"15.10 Show that any comparison-based algorithm for sorting can be modified to","chapter-15","Lower Bounds"
"remove all duplicates without requiring any more comparisons to be per-","chapter-15","Lower Bounds"
"formed.","chapter-15","Lower Bounds"
"15.11 Show that any comparison-based algorithm for removing duplicates from a","chapter-15","Lower Bounds"
"list of values must use Ω(n log n) comparisons.","chapter-15","Lower Bounds"
"15.12 Given a list of n elements, an element of the list is a majority if it appears","chapter-15","Lower Bounds"
"more than n/2 times.","chapter-15","Lower Bounds"
"506 Chap. 15 Lower Bounds","chapter-15","Lower Bounds"
"(a) Assume that the input is a list of integers. Design an algorithm that is","chapter-15","Lower Bounds"
"linear in the number of integer-integer comparisons in the worst case","chapter-15","Lower Bounds"
"that will find and report the majority if one exists, and report that there","chapter-15","Lower Bounds"
"is no majority if no such integer exists in the list.","chapter-15","Lower Bounds"
"(b) Assume that the input is a list of elements that have no relative ordering,","chapter-15","Lower Bounds"
"such as colors or fruit. So all that you can do when you compare two","chapter-15","Lower Bounds"
"elements is ask if they are the same or not. Design an algorithm that is","chapter-15","Lower Bounds"
"linear in the number of element-element comparisons in the worst case","chapter-15","Lower Bounds"
"that will find a majority if one exists, and report that there is no majority","chapter-15","Lower Bounds"
"if no such element exists in the list.","chapter-15","Lower Bounds"
"15.13 Given an undirected graph G, the problem is to determine whether or not G","chapter-15","Lower Bounds"
"is connected. Use an adversary argument to prove that it is necessary to look","chapter-15","Lower Bounds"
"at all (n","chapter-15","Lower Bounds"
"2 − n)/2 potential edges in the worst case.","chapter-15","Lower Bounds"
"15.14 (a) Write an equation that describes the average cost for finding the median.","chapter-15","Lower Bounds"
"(b) Solve your equation from part (a).","chapter-15","Lower Bounds"
"15.15 (a) Write an equation that describes the average cost for finding the ith-","chapter-15","Lower Bounds"
"smallest value in an array. This will be a function of both n and i,","chapter-15","Lower Bounds"
"T(n, i).","chapter-15","Lower Bounds"
"(b) Solve your equation from part (a).","chapter-15","Lower Bounds"
"15.16 Suppose that you have n objects that have identical weight, except for one","chapter-15","Lower Bounds"
"that is a bit heavier than the others. You have a balance scale. You can place","chapter-15","Lower Bounds"
"objects on each side of the scale and see which collection is heavier. Your","chapter-15","Lower Bounds"
"goal is to find the heavier object, with the minimum number of weighings.","chapter-15","Lower Bounds"
"Find and prove matching upper and lower bounds for this problem.","chapter-15","Lower Bounds"
"15.17 Imagine that you are organizing a basketball tournament for 10 teams. You","chapter-15","Lower Bounds"
"know that the merge insert sort will give you a full ranking of the 10 teams","chapter-15","Lower Bounds"
"with the minimum number of games played. Assume that each game can be","chapter-15","Lower Bounds"
"played in less than an hour, and that any team can play as many games in","chapter-15","Lower Bounds"
"a row as necessary. Show a schedule for this tournament that also attempts","chapter-15","Lower Bounds"
"to minimize the number of total hours for the tournament and the number of","chapter-15","Lower Bounds"
"courts used. If you have to make a tradeoff between the two, then attempt to","chapter-15","Lower Bounds"
"minimize the total number of hours that basketball courts are idle.","chapter-15","Lower Bounds"
"15.18 Write the complete algorithm for the merge insert sort sketched out in Sec-","chapter-15","Lower Bounds"
"tion 15.7.","chapter-15","Lower Bounds"
"15.19 Here is a suggestion for what might be a truly optimal sorting algorithm. Pick","chapter-15","Lower Bounds"
"the best set of comparisons for input lists of size 2. Then pick the best set of","chapter-15","Lower Bounds"
"comparisons for size 3, size 4, size 5, and so on. Combine them together into","chapter-15","Lower Bounds"
"one program with a big case statement. Is this an algorithm?","chapter-15","Lower Bounds"
"Sec. 15.10 Projects 507","chapter-15","Lower Bounds"
"15.10 Projects","chapter-15","Lower Bounds"
"15.1 Implement the median-finding algorithm of Section 15.6. Then, modify this","chapter-15","Lower Bounds"
"algorithm to allow finding the ith element for any value i < n.","chapter-15","Lower Bounds"
"This chapter presents several fundamental topics related to the theory of algorithms.","chapter-16","Patterns of Algorithms"
"Included are dynamic programming (Section 16.1), randomized algorithms (Sec-","chapter-16","Patterns of Algorithms"
"tion 16.2), and the concept of a transform (Section 16.3.5). Each of these can be","chapter-16","Patterns of Algorithms"
"viewed as an example of an “algorithmic pattern” that is commonly used for a","chapter-16","Patterns of Algorithms"
"wide variety of applications. In addition, Section 16.3 presents a number of nu-","chapter-16","Patterns of Algorithms"
"merical algorithms. Section 16.2 on randomized algorithms includes the Skip List","chapter-16","Patterns of Algorithms"
"(Section 16.2.2). The Skip List is a probabilistic data structure that can be used","chapter-16","Patterns of Algorithms"
"to implement the dictionary ADT. The Skip List is no more complicated than the","chapter-16","Patterns of Algorithms"
"BST. Yet it often outperforms the BST because the Skip List’s efficiency is not tied","chapter-16","Patterns of Algorithms"
"to the values or insertion order of the dataset being stored.","chapter-16","Patterns of Algorithms"
"16.1 Dynamic Programming","chapter-16","Patterns of Algorithms"
"Consider again the recursive function for computing the nth Fibonacci number.","chapter-16","Patterns of Algorithms"
"/** Recursively generate and return the n’th Fibonacci","chapter-16","Patterns of Algorithms"
"number */","chapter-16","Patterns of Algorithms"
"static long fibr(int n) {","chapter-16","Patterns of Algorithms"
"// fibr(91) is the largest value that fits in a long","chapter-16","Patterns of Algorithms"
"assert (n > 0) && (n <= 91) : "n out of range";","chapter-16","Patterns of Algorithms"
"if ((n == 1) || (n == 2)) return 1; // Base case","chapter-16","Patterns of Algorithms"
"return fibr(n-1) + fibr(n-2); // Recursive call","chapter-16","Patterns of Algorithms"
"}","chapter-16","Patterns of Algorithms"
"The cost of this algorithm (in terms of function calls) is the size of the nth Fi-","chapter-16","Patterns of Algorithms"
"bonacci number itself, which our analysis of Section 14.2 showed to be exponential","chapter-16","Patterns of Algorithms"
"(approximately n","chapter-16","Patterns of Algorithms"
"1.62). Why is this so expensive? Primarily because two recursive","chapter-16","Patterns of Algorithms"
"calls are made by the function, and the work that they do is largely redundant. That","chapter-16","Patterns of Algorithms"
"is, each of the two calls is recomputing most of the series, as is each sub-call, and so","chapter-16","Patterns of Algorithms"
"on. Thus, the smaller values of the function are being recomputed a huge number","chapter-16","Patterns of Algorithms"
"of times. If we could eliminate this redundancy, the cost would be greatly reduced.","chapter-16","Patterns of Algorithms"
"509","chapter-16","Patterns of Algorithms"
"510 Chap. 16 Patterns of Algorithms","chapter-16","Patterns of Algorithms"
"The approach that we will use can also improve any algorithm that spends most of","chapter-16","Patterns of Algorithms"
"its time recomputing common subproblems.","chapter-16","Patterns of Algorithms"
"One way to accomplish this goal is to keep a table of values, and first check the","chapter-16","Patterns of Algorithms"
"table to see if the computation can be avoided. Here is a straightforward example","chapter-16","Patterns of Algorithms"
"of doing so.","chapter-16","Patterns of Algorithms"
"int fibrt(int n) {","chapter-16","Patterns of Algorithms"
"// Assume Values has at least n slots, and all","chapter-16","Patterns of Algorithms"
"// slots are initialized to 0","chapter-16","Patterns of Algorithms"
"if (n <= 2) return 1; // Base case","chapter-16","Patterns of Algorithms"
"if (Values[n] == 0)","chapter-16","Patterns of Algorithms"
"Values[n] = fibrt(n-1) + fibrt(n-2);","chapter-16","Patterns of Algorithms"
"return Values[n];","chapter-16","Patterns of Algorithms"
"}","chapter-16","Patterns of Algorithms"
"This version of the algorithm will not compute a value more than once, so its","chapter-16","Patterns of Algorithms"
"cost should be linear. Of course, we didn’t actually need to use a table storing all of","chapter-16","Patterns of Algorithms"
"the values, since future computations do not need access to all prior subproblems.","chapter-16","Patterns of Algorithms"
"Instead, we could build the value by working from 0 and 1 up to n rather than","chapter-16","Patterns of Algorithms"
"backwards from n down to 0 and 1. Going up from the bottom we only need to","chapter-16","Patterns of Algorithms"
"store the previous two values of the function, as is done by our iterative version.","chapter-16","Patterns of Algorithms"
"/** Iteratively generate and return the n’th Fibonacci","chapter-16","Patterns of Algorithms"
"number */","chapter-16","Patterns of Algorithms"
"static long fibi(int n) {","chapter-16","Patterns of Algorithms"
"// fibr(91) is the largest value that fits in a long","chapter-16","Patterns of Algorithms"
"assert (n > 0) && (n <= 91) : "n out of range";","chapter-16","Patterns of Algorithms"
"long curr, prev, past;","chapter-16","Patterns of Algorithms"
"if ((n == 1) || (n == 2)) return 1;","chapter-16","Patterns of Algorithms"
"curr = prev = 1; // curr holds current Fib value","chapter-16","Patterns of Algorithms"
"for (int i=3; i<=n; i++) { // Compute next value","chapter-16","Patterns of Algorithms"
"past = prev; // past holds fibi(i-2)","chapter-16","Patterns of Algorithms"
"prev = curr; // prev holds fibi(i-1)","chapter-16","Patterns of Algorithms"
"curr = past + prev; // curr now holds fibi(i)","chapter-16","Patterns of Algorithms"
"}","chapter-16","Patterns of Algorithms"
"return curr;","chapter-16","Patterns of Algorithms"
"}","chapter-16","Patterns of Algorithms"
"Recomputing of subproblems comes up in many algorithms. It is not so com-","chapter-16","Patterns of Algorithms"
"mon that we can store only a few prior results as we did for fibi. Thus, there are","chapter-16","Patterns of Algorithms"
"many times where storing a complete table of subresults will be useful.","chapter-16","Patterns of Algorithms"
"This approach to designing an algorithm that works by storing a table of results","chapter-16","Patterns of Algorithms"
"for subproblems is called dynamic programming. The name is somewhat arcane,","chapter-16","Patterns of Algorithms"
"because it doesn’t bear much obvious similarity to the process that is taking place","chapter-16","Patterns of Algorithms"
"when storing subproblems in a table. However, it comes originally from the field of","chapter-16","Patterns of Algorithms"
"dynamic control systems, which got its start before what we think of as computer","chapter-16","Patterns of Algorithms"
"programming. The act of storing precomputed values in a table for later reuse is","chapter-16","Patterns of Algorithms"
"referred to as “programming” in that field.","chapter-16","Patterns of Algorithms"
"Sec. 16.1 Dynamic Programming 511","chapter-16","Patterns of Algorithms"
"Dynamic programming is a powerful alternative to the standard principle of","chapter-16","Patterns of Algorithms"
"divide and conquer. In divide and conquer, a problem is split into subproblems,","chapter-16","Patterns of Algorithms"
"the subproblems are solved (independently), and then recombined into a solution","chapter-16","Patterns of Algorithms"
"for the problem being solved. Dynamic programming is appropriate whenever (1)","chapter-16","Patterns of Algorithms"
"subproblems are solved repeatedly, and (2) we can find a suitable way of doing the","chapter-16","Patterns of Algorithms"
"necessary bookkeeping. Dynamic programming algorithms are usually not imple-","chapter-16","Patterns of Algorithms"
"mented by simply using a table to store subproblems for recursive calls (i.e., going","chapter-16","Patterns of Algorithms"
"backwards as is done by fibrt). Instead, such algorithms are typically imple-","chapter-16","Patterns of Algorithms"
"mented by building the table of subproblems from the bottom up. Thus, fibi bet-","chapter-16","Patterns of Algorithms"
"ter represents the most common form of dynamic programming than does fibrt,","chapter-16","Patterns of Algorithms"
"even though it doesn’t use the complete table.","chapter-16","Patterns of Algorithms"
"16.1.1 The Knapsack Problem","chapter-16","Patterns of Algorithms"
"We will next consider a problem that appears with many variations in a variety","chapter-16","Patterns of Algorithms"
"of commercial settings. Many businesses need to package items with the greatest","chapter-16","Patterns of Algorithms"
"efficiency. One way to describe this basic idea is in terms of packing items into","chapter-16","Patterns of Algorithms"
"a knapsack, and so we will refer to this as the Knapsack Problem. We will first","chapter-16","Patterns of Algorithms"
"define a particular formulation of the knapsack problem, and then we will discuss","chapter-16","Patterns of Algorithms"
"an algorithm to solve it based on dynamic programming. We will see other versions","chapter-16","Patterns of Algorithms"
"of the knapsack problem in the exercises and in Chapter 17.","chapter-16","Patterns of Algorithms"
"Assume that we have a knapsack with a certain amount of space that we will","chapter-16","Patterns of Algorithms"
"define using integer value K. We also have n items each with a certain size such","chapter-16","Patterns of Algorithms"
"that that item i has integer size ki","chapter-16","Patterns of Algorithms"
". The problem is to find a subset of the n items","chapter-16","Patterns of Algorithms"
"whose sizes exactly sum to K, if one exists. For example, if our knapsack has","chapter-16","Patterns of Algorithms"
"capacity K = 5 and the two items are of size k1 = 2 and k2 = 4, then no such","chapter-16","Patterns of Algorithms"
"subset exists. But if we add a third item of size k3 = 1, then we can fill the knapsack","chapter-16","Patterns of Algorithms"
"exactly with the second and third items. We can define the problem more formally","chapter-16","Patterns of Algorithms"
"as: Find S ⊂ {1, 2, ..., n} such that","chapter-16","Patterns of Algorithms"
"X","chapter-16","Patterns of Algorithms"
"i∈S","chapter-16","Patterns of Algorithms"
"ki = K.","chapter-16","Patterns of Algorithms"
"Example 16.1 Assume that we are given a knapsack of size K = 163","chapter-16","Patterns of Algorithms"
"and 10 items of sizes 4, 9, 15, 19, 27, 44, 54, 68, 73, 101. Can we find a","chapter-16","Patterns of Algorithms"
"subset of the items that exactly fills the knapsack? You should take a few","chapter-16","Patterns of Algorithms"
"minutes and try to do this before reading on and looking at the answer.","chapter-16","Patterns of Algorithms"
"One solution to the problem is: 19, 27, 44, 73.","chapter-16","Patterns of Algorithms"
"Example 16.2 Having solved the previous example for knapsack of size","chapter-16","Patterns of Algorithms"
"163, how hard is it now to solve for a knapsack of size 164?","chapter-16","Patterns of Algorithms"
"512 Chap. 16 Patterns of Algorithms","chapter-16","Patterns of Algorithms"
"Unfortunately, knowing the answer for 163 is of almost no use at all","chapter-16","Patterns of Algorithms"
"when solving for 164. One solution is: 9, 54, 101.","chapter-16","Patterns of Algorithms"
"If you tried solving these examples, you probably found yourself doing a lot of","chapter-16","Patterns of Algorithms"
"trial-and-error and a lot of backtracking. To come up with an algorithm, we want","chapter-16","Patterns of Algorithms"
"an organized way to go through the possible subsets. Is there a way to make the","chapter-16","Patterns of Algorithms"
"problem smaller, so that we can apply divide and conquer? We essentially have two","chapter-16","Patterns of Algorithms"
"parts to the input: The knapsack size K and the n items. It probably will not do us","chapter-16","Patterns of Algorithms"
"much good to try and break the knapsack into pieces and solve the sub-pieces (since","chapter-16","Patterns of Algorithms"
"we already saw that knowing the answer for a knapsack of size 163 did nothing to","chapter-16","Patterns of Algorithms"
"help us solve the problem for a knapsack of size 164).","chapter-16","Patterns of Algorithms"
"So, what can we say about solving the problem with or without the nth item?","chapter-16","Patterns of Algorithms"
"This seems to lead to a way to break down the problem. If the nth item is not","chapter-16","Patterns of Algorithms"
"needed for a solution (that is, if we can solve the problem with the first n−1 items)","chapter-16","Patterns of Algorithms"
"then we can also solve the problem when the nth item is available (we just ignore","chapter-16","Patterns of Algorithms"
"it). On the other hand, if we do include the nth item as a member of the solution","chapter-16","Patterns of Algorithms"
"subset, then we now would need to solve the problem with the first n − 1 items","chapter-16","Patterns of Algorithms"
"and a knapsack of size K − kn (since the nth item is taking up kn space in the","chapter-16","Patterns of Algorithms"
"knapsack).","chapter-16","Patterns of Algorithms"
"To organize this process, we can define the problem in terms of two parameters:","chapter-16","Patterns of Algorithms"
"the knapsack size K and the number of items n. Denote a given instance of the","chapter-16","Patterns of Algorithms"
"problem as P(n, K). Now we can say that P(n, K) has a solution if and only if","chapter-16","Patterns of Algorithms"
"there exists a solution for either P(n − 1, K) or P(n − 1, K − kn). That is, we can","chapter-16","Patterns of Algorithms"
"solve P(n, K) only if we can solve one of the sub problems where we use or do","chapter-16","Patterns of Algorithms"
"not use the nth item. Of course, the ordering of the items is arbitrary. We just need","chapter-16","Patterns of Algorithms"
"to give them some order to keep things straight.","chapter-16","Patterns of Algorithms"
"Continuing this idea, to solve any subproblem of size n − 1, we need only to","chapter-16","Patterns of Algorithms"
"solve two subproblems of size n − 2. And so on, until we are down to only one","chapter-16","Patterns of Algorithms"
"item that either fills the knapsack or not. This naturally leads to a cost expressed","chapter-16","Patterns of Algorithms"
"by the recurrence relation T(n) = 2T(n − 1) + c = Θ(2n","chapter-16","Patterns of Algorithms"
"). That can be pretty","chapter-16","Patterns of Algorithms"
"expensive!","chapter-16","Patterns of Algorithms"
"But... we should quickly realize that there are only n(K + 1) subproblems","chapter-16","Patterns of Algorithms"
"to solve! Clearly, there is the possibility that many subproblems are being solved","chapter-16","Patterns of Algorithms"
"repeatedly. This is a natural opportunity to apply dynamic programming. We sim-","chapter-16","Patterns of Algorithms"
"ply build an array of size n × K + 1 to contain the solutions for all subproblems","chapter-16","Patterns of Algorithms"
"P(i, k), 1 ≤ i ≤ n, 0 ≤ k ≤ K.","chapter-16","Patterns of Algorithms"
"There are two approaches to actually solving the problem. One is to start with","chapter-16","Patterns of Algorithms"
"our problem of size P(n, K) and make recursive calls to solve the subproblems,","chapter-16","Patterns of Algorithms"
"each time checking the array to see if a subproblem has been solved, and filling","chapter-16","Patterns of Algorithms"
"in the corresponding cell in the array whenever we get a new subproblem solution.","chapter-16","Patterns of Algorithms"
"The other is to start filling the array for row 1 (which indicates a successful solution","chapter-16","Patterns of Algorithms"
"Sec. 16.1 Dynamic Programming 513","chapter-16","Patterns of Algorithms"
"only for a knapsack of size k1). We then fill in the succeeding rows from i = 2 to","chapter-16","Patterns of Algorithms"
"n, left to right, as follows.","chapter-16","Patterns of Algorithms"
"if P(n − 1, K) has a solution,","chapter-16","Patterns of Algorithms"
"then P(n, K) has a solution","chapter-16","Patterns of Algorithms"
"else if P(n − 1, K − kn) has a solution","chapter-16","Patterns of Algorithms"
"then P(n, K) has a solution","chapter-16","Patterns of Algorithms"
"else P(n, K) has no solution.","chapter-16","Patterns of Algorithms"
"In other words, a new slot in the array gets its solution by looking at two slots in","chapter-16","Patterns of Algorithms"
"the preceding row. Since filling each slot in the array takes constant time, the total","chapter-16","Patterns of Algorithms"
"cost of the algorithm is Θ(nK).","chapter-16","Patterns of Algorithms"
"Example 16.3 Solve the Knapsack Problem for K = 10 and five items","chapter-16","Patterns of Algorithms"
"with sizes 9, 2, 7, 4, 1. We do this by building the following array.","chapter-16","Patterns of Algorithms"
"0 1 2 3 4 5 6 7 8 9 10","chapter-16","Patterns of Algorithms"
"k1 = 9 O − − − − − − − − I −","chapter-16","Patterns of Algorithms"
"k2 = 2 O − I − − − − − − O −","chapter-16","Patterns of Algorithms"
"k3 = 7 O − O − − − − I − I/O −","chapter-16","Patterns of Algorithms"
"k4 = 4 O − O − I − I O − O −","chapter-16","Patterns of Algorithms"
"k5 = 1 O I O I O I O I/O I O I","chapter-16","Patterns of Algorithms"
"Key:","chapter-16","Patterns of Algorithms"
"-: No solution for P(i, k).","chapter-16","Patterns of Algorithms"
"O: Solution(s) for P(i, k) with i omitted.","chapter-16","Patterns of Algorithms"
"I: Solution(s) for P(i, k) with i included.","chapter-16","Patterns of Algorithms"
"I/O: Solutions for P(i, k) with i included AND omitted.","chapter-16","Patterns of Algorithms"
"For example, P(3, 9) stores value I/O. It contains O because P(2, 9)","chapter-16","Patterns of Algorithms"
"has a solution. It contains I because P(2, 2) = P(2, 9 − 7) has a solution.","chapter-16","Patterns of Algorithms"
"Since P(5, 10) is marked with an I, it has a solution. We can determine","chapter-16","Patterns of Algorithms"
"what that solution actually is by recognizing that it includes the 5th item","chapter-16","Patterns of Algorithms"
"(of size 1), which then leads us to look at the solution for P(4, 9). This","chapter-16","Patterns of Algorithms"
"in turn has a solution that omits the 4th item, leading us to P(3, 9). At","chapter-16","Patterns of Algorithms"
"this point, we can either use the third item or not. We can find a solution","chapter-16","Patterns of Algorithms"
"by taking one branch. We can find all solutions by following all branches","chapter-16","Patterns of Algorithms"
"when there is a choice.","chapter-16","Patterns of Algorithms"
"16.1.2 All-Pairs Shortest Paths","chapter-16","Patterns of Algorithms"
"We next consider the problem of finding the shortest distance between all pairs of","chapter-16","Patterns of Algorithms"
"vertices in the graph, called the all-pairs shortest-paths problem. To be precise,","chapter-16","Patterns of Algorithms"
"for every u, v ∈ V, calculate d(u, v).","chapter-16","Patterns of Algorithms"
"514 Chap. 16 Patterns of Algorithms","chapter-16","Patterns of Algorithms"
"∞","chapter-16","Patterns of Algorithms"
"∞","chapter-16","Patterns of Algorithms"
"∞","chapter-16","Patterns of Algorithms"
"∞","chapter-16","Patterns of Algorithms"
"1 7","chapter-16","Patterns of Algorithms"
"4","chapter-16","Patterns of Algorithms"
"5","chapter-16","Patterns of Algorithms"
"3","chapter-16","Patterns of Algorithms"
"2 11","chapter-16","Patterns of Algorithms"
"12","chapter-16","Patterns of Algorithms"
"1","chapter-16","Patterns of Algorithms"
"0","chapter-16","Patterns of Algorithms"
"2","chapter-16","Patterns of Algorithms"
"3","chapter-16","Patterns of Algorithms"
"Figure 16.1 An example of k-paths in Floyd’s algorithm. Path 1, 3 is a 0-path","chapter-16","Patterns of Algorithms"
"by definition. Path 3, 0, 2 is not a 0-path, but it is a 1-path (as well as a 2-path, a","chapter-16","Patterns of Algorithms"
"3-path, and a 4-path) because the largest intermediate vertex is 0. Path 1, 3, 2 is","chapter-16","Patterns of Algorithms"
"a 4-path, but not a 3-path because the intermediate vertex is 3. All paths in this","chapter-16","Patterns of Algorithms"
"graph are 4-paths.","chapter-16","Patterns of Algorithms"
"One solution is to run Dijkstra’s algorithm for finding the single-source shortest","chapter-16","Patterns of Algorithms"
"path (see Section 11.4.1) |V| times, each time computing the shortest path from a","chapter-16","Patterns of Algorithms"
"different start vertex. If G is sparse (that is, |E| = Θ(|V|)) then this is a good","chapter-16","Patterns of Algorithms"
"solution, because the total cost will be Θ(|V|","chapter-16","Patterns of Algorithms"
"2 + |V||E| log |V|) = Θ(|V|","chapter-16","Patterns of Algorithms"
"2","chapter-16","Patterns of Algorithms"
"log |V|)","chapter-16","Patterns of Algorithms"
"for the version of Dijkstra’s algorithm based on priority queues. For a dense graph,","chapter-16","Patterns of Algorithms"
"the priority queue version of Dijkstra’s algorithm yields a cost of Θ(|V|","chapter-16","Patterns of Algorithms"
"3","chapter-16","Patterns of Algorithms"
"log |V|),","chapter-16","Patterns of Algorithms"
"but the version using MinVertex yields a cost of Θ(|V|","chapter-16","Patterns of Algorithms"
"3","chapter-16","Patterns of Algorithms"
").","chapter-16","Patterns of Algorithms"
"Another solution that limits processing time to Θ(|V|","chapter-16","Patterns of Algorithms"
"3","chapter-16","Patterns of Algorithms"
") regardless of the num-","chapter-16","Patterns of Algorithms"
"ber of edges is known as Floyd’s algorithm. It is an example of dynamic program-","chapter-16","Patterns of Algorithms"
"ming. The chief problem with solving this problem is organizing the search process","chapter-16","Patterns of Algorithms"
"so that we do not repeatedly solve the same subproblems. We will do this organi-","chapter-16","Patterns of Algorithms"
"zation through the use of the k-path. Define a k-path from vertex v to vertex u to","chapter-16","Patterns of Algorithms"
"be any path whose intermediate vertices (aside from v and u) all have indices less","chapter-16","Patterns of Algorithms"
"than k. A 0-path is defined to be a direct edge from v to u. Figure 16.1 illustrates","chapter-16","Patterns of Algorithms"
"the concept of k-paths.","chapter-16","Patterns of Algorithms"
"Define Dk(v, u) to be the length of the shortest k-path from vertex v to vertex u.","chapter-16","Patterns of Algorithms"
"Assume that we already know the shortest k-path from v to u. The shortest (k + 1)-","chapter-16","Patterns of Algorithms"
"path either goes through vertex k or it does not. If it does go through k, then","chapter-16","Patterns of Algorithms"
"the best path is the best k-path from v to k followed by the best k-path from k","chapter-16","Patterns of Algorithms"
"to u. Otherwise, we should keep the best k-path seen before. Floyd’s algorithm","chapter-16","Patterns of Algorithms"
"simply checks all of the possibilities in a triple loop. Here is the implementation","chapter-16","Patterns of Algorithms"
"for Floyd’s algorithm. At the end of the algorithm, array D stores the all-pairs","chapter-16","Patterns of Algorithms"
"shortest distances.","chapter-16","Patterns of Algorithms"
"Sec. 16.2 Randomized Algorithms 515","chapter-16","Patterns of Algorithms"
"/** Compute all-pairs shortest paths */","chapter-16","Patterns of Algorithms"
"static void Floyd(Graph G, int[][] D) {","chapter-16","Patterns of Algorithms"
"for (int i=0; i<G.n(); i++) // Initialize D with weights","chapter-16","Patterns of Algorithms"
"for (int j=0; j<G.n(); j++)","chapter-16","Patterns of Algorithms"
"if (G.weight(i, j) != 0) D[i][j] = G.weight(i, j);","chapter-16","Patterns of Algorithms"
"for (int k=0; k<G.n(); k++) // Compute all k paths","chapter-16","Patterns of Algorithms"
"for (int i=0; i<G.n(); i++)","chapter-16","Patterns of Algorithms"
"for (int j=0; j<G.n(); j++)","chapter-16","Patterns of Algorithms"
"if ((D[i][k] != Integer.MAX VALUE) &&","chapter-16","Patterns of Algorithms"
"(D[k][j] != Integer.MAX VALUE) &&","chapter-16","Patterns of Algorithms"
"(D[i][j] > (D[i][k] + D[k][j])))","chapter-16","Patterns of Algorithms"
"D[i][j] = D[i][k] + D[k][j];","chapter-16","Patterns of Algorithms"
"}","chapter-16","Patterns of Algorithms"
"Clearly this algorithm requires Θ(|V|","chapter-16","Patterns of Algorithms"
"3","chapter-16","Patterns of Algorithms"
") running time, and it is the best choice","chapter-16","Patterns of Algorithms"
"for dense graphs because it is (relatively) fast and easy to implement.","chapter-16","Patterns of Algorithms"
"16.2 Randomized Algorithms","chapter-16","Patterns of Algorithms"
"In this section, we will consider how introducing randomness into our algorithms","chapter-16","Patterns of Algorithms"
"might speed things up, although perhaps at the expense of accuracy. But often we","chapter-16","Patterns of Algorithms"
"can reduce the possibility for error to be as low as we like, while still speeding up","chapter-16","Patterns of Algorithms"
"the algorithm.","chapter-16","Patterns of Algorithms"
"16.2.1 Randomized algorithms for finding large values","chapter-16","Patterns of Algorithms"
"In Section 15.1 we determined that the lower bound cost of finding the maximum","chapter-16","Patterns of Algorithms"
"value in an unsorted list is Ω(n). This is the least time needed to be certain that we","chapter-16","Patterns of Algorithms"
"have found the maximum value. But what if we are willing to relax our requirement","chapter-16","Patterns of Algorithms"
"for certainty? The first question is: What do we mean by this? There are many","chapter-16","Patterns of Algorithms"
"aspects to “certainty” and we might relax the requirement in various ways.","chapter-16","Patterns of Algorithms"
"There are several possible guarantees that we might require from an algorithm","chapter-16","Patterns of Algorithms"
"that produces X as the maximum value, when the true maximum is Y . So far","chapter-16","Patterns of Algorithms"
"we have assumed that we require X to equal Y . This is known as an exact or","chapter-16","Patterns of Algorithms"
"deterministic algorithm to solve the problem. We could relax this and require only","chapter-16","Patterns of Algorithms"
"that X’s rank is “close to” Y ’s rank (perhaps within a fixed distance or percentage).","chapter-16","Patterns of Algorithms"
"This is known as an approximation algorithm. We could require that X is “usually”","chapter-16","Patterns of Algorithms"
"Y . This is known as a probabilistic algorithm. Finally, we could require only that","chapter-16","Patterns of Algorithms"
"X’s rank is “usually” “close” to Y ’s rank. This is known as a heuristic algorithm.","chapter-16","Patterns of Algorithms"
"There are also different ways that we might choose to sacrifice reliability for","chapter-16","Patterns of Algorithms"
"speed. These types of algorithms also have names.","chapter-16","Patterns of Algorithms"
"1. Las Vegas Algorithms: We always find the maximum value, and “usually”","chapter-16","Patterns of Algorithms"
"we find it fast. Such algorithms have a guaranteed result, but do not guarantee","chapter-16","Patterns of Algorithms"
"fast running time.","chapter-16","Patterns of Algorithms"
"516 Chap. 16 Patterns of Algorithms","chapter-16","Patterns of Algorithms"
"2. Monte Carlo Algorithms: We find the maximum value fast, or we don’t get","chapter-16","Patterns of Algorithms"
"an answer at all (but fast). While such algorithms have good running time,","chapter-16","Patterns of Algorithms"
"their result is not guaranteed.","chapter-16","Patterns of Algorithms"
"Here is an example of an algorithm for finding a large value that gives up its","chapter-16","Patterns of Algorithms"
"guarantee of getting the best value in exchange for an improved running time. This","chapter-16","Patterns of Algorithms"
"is an example of a probabilistic algorithm, since it includes steps that are affected","chapter-16","Patterns of Algorithms"
"by random events. Choose m elements at random, and pick the best one of those","chapter-16","Patterns of Algorithms"
"as the answer. For large n, if m ≈ log n, the answer is pretty good. The cost is","chapter-16","Patterns of Algorithms"
"m − 1 compares (since we must find the maximum of m values). But we don’t","chapter-16","Patterns of Algorithms"
"know for sure what we will get. However, we can estimate that the rank will be","chapter-16","Patterns of Algorithms"
"about mn","chapter-16","Patterns of Algorithms"
"m+1 . For example, if n = 1, 000, 000 and m = log n = 20, then we expect","chapter-16","Patterns of Algorithms"
"that the largest of the 20 randomly selected values be among the top 5% of the n","chapter-16","Patterns of Algorithms"
"values.","chapter-16","Patterns of Algorithms"
"Next, consider a slightly different problem where the goal is to pick a number","chapter-16","Patterns of Algorithms"
"in the upper half of n values. We would pick the maximum from among the first","chapter-16","Patterns of Algorithms"
"n+1","chapter-16","Patterns of Algorithms"
"2","chapter-16","Patterns of Algorithms"
"values for a cost of n/2 comparisons. Can we do better than this? Not if we","chapter-16","Patterns of Algorithms"
"want to guarantee getting the correct answer. But if we are willing to accept near","chapter-16","Patterns of Algorithms"
"certainty instead of absolute certainty, we can gain a lot in terms of speed.","chapter-16","Patterns of Algorithms"
"As an alternative, consider this probabilistic algorithm. Pick 2 numbers and","chapter-16","Patterns of Algorithms"
"choose the greater. This will be in the upper half with probability 3/4 (since it is","chapter-16","Patterns of Algorithms"
"not in the upper half only when both numbers we choose happen to be in the lower","chapter-16","Patterns of Algorithms"
"half). Is a probability of 3/4 not good enough? Then we simply pick more numbers!","chapter-16","Patterns of Algorithms"
"For k numbers, the greatest is in upper half with probability 1 −","chapter-16","Patterns of Algorithms"
"1","chapter-16","Patterns of Algorithms"
"2","chapter-16","Patterns of Algorithms"
"k","chapter-16","Patterns of Algorithms"
", regardless of","chapter-16","Patterns of Algorithms"
"the number n that we pick from, so long as n is much larger than k (otherwise","chapter-16","Patterns of Algorithms"
"the chances might become even better). If we pick ten numbers, then the chance","chapter-16","Patterns of Algorithms"
"of failure is only one in 2","chapter-16","Patterns of Algorithms"
"10 = 1024. What if we really want to be sure, because","chapter-16","Patterns of Algorithms"
"lives depend on drawing a number from the upper half? If we pick 30 numbers,","chapter-16","Patterns of Algorithms"
"we can fail only one time in a billion. If we pick enough numbers, then the chance","chapter-16","Patterns of Algorithms"
"of picking a small number is less than the chance of the power failing during the","chapter-16","Patterns of Algorithms"
"computation. Picking 100 numbers means that we can fail only one time in 10100","chapter-16","Patterns of Algorithms"
"which is less chance than any disaster that you can imagine disrupting the process.","chapter-16","Patterns of Algorithms"
"16.2.2 Skip Lists","chapter-16","Patterns of Algorithms"
"This section presents a probabilistic search structure called the Skip List. Like","chapter-16","Patterns of Algorithms"
"BSTs, Skip Lists are designed to overcome a basic limitation of array-based and","chapter-16","Patterns of Algorithms"
"linked lists: Either search or update operations require linear time. The Skip List","chapter-16","Patterns of Algorithms"
"is an example of a probabilistic data structure, because it makes some of its","chapter-16","Patterns of Algorithms"
"decisions at random.","chapter-16","Patterns of Algorithms"
"Skip Lists provide an alternative to the BST and related tree structures. The pri-","chapter-16","Patterns of Algorithms"
"mary problem with the BST is that it may easily become unbalanced. The 2-3 tree","chapter-16","Patterns of Algorithms"
"of Chapter 10 is guaranteed to remain balanced regardless of the order in which data","chapter-16","Patterns of Algorithms"
"Sec. 16.2 Randomized Algorithms 517","chapter-16","Patterns of Algorithms"
"values are inserted, but it is rather complicated to implement. Chapter 13 presents","chapter-16","Patterns of Algorithms"
"the AVL tree and the splay tree, which are also guaranteed to provide good per-","chapter-16","Patterns of Algorithms"
"formance, but at the cost of added complexity as compared to the BST. The Skip","chapter-16","Patterns of Algorithms"
"List is easier to implement than known balanced tree structures. The Skip List is","chapter-16","Patterns of Algorithms"
"not guaranteed to provide good performance (where good performance is defined","chapter-16","Patterns of Algorithms"
"as Θ(log n) search, insertion, and deletion time), but it will provide good perfor-","chapter-16","Patterns of Algorithms"
"mance with extremely high probability (unlike the BST which has a good chance","chapter-16","Patterns of Algorithms"
"of performing poorly). As such it represents a good compromise between difficulty","chapter-16","Patterns of Algorithms"
"of implementation and performance.","chapter-16","Patterns of Algorithms"
"Figure 16.2 illustrates the concept behind the Skip List. Figure 16.2(a) shows a","chapter-16","Patterns of Algorithms"
"simple linked list whose nodes are ordered by key value. To search a sorted linked","chapter-16","Patterns of Algorithms"
"list requires that we move down the list one node at a time, visiting Θ(n) nodes","chapter-16","Patterns of Algorithms"
"in the average case. What if we add a pointer to every other node that lets us skip","chapter-16","Patterns of Algorithms"
"alternating nodes, as shown in Figure 16.2(b)? Define nodes with a single pointer","chapter-16","Patterns of Algorithms"
"as level 0 Skip List nodes, and nodes with two pointers as level 1 Skip List nodes.","chapter-16","Patterns of Algorithms"
"To search, follow the level 1 pointers until a value greater than the search key","chapter-16","Patterns of Algorithms"
"has been found, go back to the previous level 1 node, then revert to a level 0 pointer","chapter-16","Patterns of Algorithms"
"to travel one more node if necessary. This effectively cuts the work in half. We","chapter-16","Patterns of Algorithms"
"can continue adding pointers to selected nodes in this way — give a third pointer","chapter-16","Patterns of Algorithms"
"to every fourth node, give a fourth pointer to every eighth node, and so on — until","chapter-16","Patterns of Algorithms"
"we reach the ultimate of log n pointers in the first and middle nodes for a list of","chapter-16","Patterns of Algorithms"
"n nodes as illustrated in Figure 16.2(c). To search, start with the bottom row of","chapter-16","Patterns of Algorithms"
"pointers, going as far as possible and skipping many nodes at a time. Then, shift","chapter-16","Patterns of Algorithms"
"up to shorter and shorter steps as required. With this arrangement, the worst-case","chapter-16","Patterns of Algorithms"
"number of accesses is Θ(log n).","chapter-16","Patterns of Algorithms"
"We will store with each Skip List node an array named forward that stores","chapter-16","Patterns of Algorithms"
"the pointers as shown in Figure 16.2(c). Position forward[0] stores a level 0","chapter-16","Patterns of Algorithms"
"pointer, forward[1] stores a level 1 pointer, and so on. The Skip List object","chapter-16","Patterns of Algorithms"
"includes data member level that stores the highest level for any node currently","chapter-16","Patterns of Algorithms"
"in the Skip List. The Skip List stores a header node named head with level","chapter-16","Patterns of Algorithms"
"pointers. The find function is shown in Figure 16.3.","chapter-16","Patterns of Algorithms"
"Searching for a node with value 62 in the Skip List of Figure 16.2(c) begins at","chapter-16","Patterns of Algorithms"
"the header node. Follow the header node’s pointer at level, which in this example","chapter-16","Patterns of Algorithms"
"is level 2. This points to the node with value 31. Because 31 is less than 62, we","chapter-16","Patterns of Algorithms"
"next try the pointer from forward[2] of 31’s node to reach 69. Because 69 is","chapter-16","Patterns of Algorithms"
"greater than 62, we cannot go forward but must instead decrement the current level","chapter-16","Patterns of Algorithms"
"counter to 1. We next try to follow forward[1] of 31 to reach the node with","chapter-16","Patterns of Algorithms"
"value 58. Because 58 is smaller than 62, we follow 58’s forward[1] pointer","chapter-16","Patterns of Algorithms"
"to 69. Because 69 is too big, follow 58’s level 0 pointer to 62. Because 62 is not","chapter-16","Patterns of Algorithms"
"less than 62, we fall out of the while loop and move one step forward to the node","chapter-16","Patterns of Algorithms"
"with value 62.","chapter-16","Patterns of Algorithms"
"518 Chap. 16 Patterns of Algorithms","chapter-16","Patterns of Algorithms"
"head","chapter-16","Patterns of Algorithms"
"(a)","chapter-16","Patterns of Algorithms"
"1","chapter-16","Patterns of Algorithms"
"head","chapter-16","Patterns of Algorithms"
"(b)","chapter-16","Patterns of Algorithms"
"0","chapter-16","Patterns of Algorithms"
"1","chapter-16","Patterns of Algorithms"
"2","chapter-16","Patterns of Algorithms"
"head","chapter-16","Patterns of Algorithms"
"(c)","chapter-16","Patterns of Algorithms"
"0","chapter-16","Patterns of Algorithms"
"0","chapter-16","Patterns of Algorithms"
"25 30 58 31 42 62","chapter-16","Patterns of Algorithms"
"5 25 30 58 69 42 62","chapter-16","Patterns of Algorithms"
"5","chapter-16","Patterns of Algorithms"
"5 25 58 30 31 62","chapter-16","Patterns of Algorithms"
"31","chapter-16","Patterns of Algorithms"
"42","chapter-16","Patterns of Algorithms"
"69","chapter-16","Patterns of Algorithms"
"69","chapter-16","Patterns of Algorithms"
"Figure 16.2 Illustration of the Skip List concept. (a) A simple linked list.","chapter-16","Patterns of Algorithms"
"(b) Augmenting the linked list with additional pointers at every other node. To","chapter-16","Patterns of Algorithms"
"find the node with key value 62, we visit the nodes with values 25, 31, 58, and 69,","chapter-16","Patterns of Algorithms"
"then we move from the node with key value 58 to the one with value 62. (c) The","chapter-16","Patterns of Algorithms"
"ideal Skip List, guaranteeing O(log n) search time. To find the node with key","chapter-16","Patterns of Algorithms"
"value 62, we visit nodes in the order 31, 69, 58, then 69 again, and finally, 62.","chapter-16","Patterns of Algorithms"
"/** Skiplist Search */","chapter-16","Patterns of Algorithms"
"public E find(Key searchKey) {","chapter-16","Patterns of Algorithms"
"SkipNode<Key,E> x = head; // Dummy header node","chapter-16","Patterns of Algorithms"
"for (int i=level; i>=0; i--) // For each level...","chapter-16","Patterns of Algorithms"
"while ((x.forward[i] != null) && // go forward","chapter-16","Patterns of Algorithms"
"(searchKey.compareTo(x.forward[i].key()) > 0))","chapter-16","Patterns of Algorithms"
"x = x.forward[i]; // Go one last step","chapter-16","Patterns of Algorithms"
"x = x.forward[0]; // Move to actual record, if it exists","chapter-16","Patterns of Algorithms"
"if ((x != null) && (searchKey.compareTo(x.key()) == 0))","chapter-16","Patterns of Algorithms"
"return x.element(); // Got it","chapter-16","Patterns of Algorithms"
"else return null; // Its not there","chapter-16","Patterns of Algorithms"
"}","chapter-16","Patterns of Algorithms"
"Figure 16.3 Implementation for the Skip List find function.","chapter-16","Patterns of Algorithms"
"Sec. 16.2 Randomized Algorithms 519","chapter-16","Patterns of Algorithms"
"/** Insert a record into the skiplist */","chapter-16","Patterns of Algorithms"
"public void insert(Key k, E newValue) {","chapter-16","Patterns of Algorithms"
"int newLevel = randomLevel(); // New node’s level","chapter-16","Patterns of Algorithms"
"if (newLevel > level) // If new node is deeper","chapter-16","Patterns of Algorithms"
"AdjustHead(newLevel); // adjust the header","chapter-16","Patterns of Algorithms"
"// Track end of level","chapter-16","Patterns of Algorithms"
"SkipNode<Key,E>[] update =","chapter-16","Patterns of Algorithms"
"(SkipNode<Key,E>[])new SkipNode[level+1];","chapter-16","Patterns of Algorithms"
"SkipNode<Key,E> x = head; // Start at header node","chapter-16","Patterns of Algorithms"
"for (int i=level; i>=0; i--) { // Find insert position","chapter-16","Patterns of Algorithms"
"while((x.forward[i] != null) &&","chapter-16","Patterns of Algorithms"
"(k.compareTo(x.forward[i].key()) > 0))","chapter-16","Patterns of Algorithms"
"x = x.forward[i];","chapter-16","Patterns of Algorithms"
"update[i] = x; // Track end at level i","chapter-16","Patterns of Algorithms"
"}","chapter-16","Patterns of Algorithms"
"x = new SkipNode<Key,E>(k, newValue, newLevel);","chapter-16","Patterns of Algorithms"
"for (int i=0; i<=newLevel; i++) { // Splice into list","chapter-16","Patterns of Algorithms"
"x.forward[i] = update[i].forward[i]; // Who x points to","chapter-16","Patterns of Algorithms"
"update[i].forward[i] = x; // Who y points to","chapter-16","Patterns of Algorithms"
"}","chapter-16","Patterns of Algorithms"
"size++; // Increment dictionary size","chapter-16","Patterns of Algorithms"
"}","chapter-16","Patterns of Algorithms"
"Figure 16.4 Implementation for the Skip List Insert function.","chapter-16","Patterns of Algorithms"
"The ideal Skip List of Figure 16.2(c) has been organized so that (if the first and","chapter-16","Patterns of Algorithms"
"last nodes are not counted) half of the nodes have only one pointer, one quarter","chapter-16","Patterns of Algorithms"
"have two, one eighth have three, and so on. The distances are equally spaced; in","chapter-16","Patterns of Algorithms"
"effect this is a “perfectly balanced” Skip List. Maintaining such balance would be","chapter-16","Patterns of Algorithms"
"expensive during the normal process of insertions and deletions. The key to Skip","chapter-16","Patterns of Algorithms"
"Lists is that we do not worry about any of this. Whenever inserting a node, we","chapter-16","Patterns of Algorithms"
"assign it a level (i.e., some number of pointers). The assignment is random, using","chapter-16","Patterns of Algorithms"
"a geometric distribution yielding a 50% probability that the node will have one","chapter-16","Patterns of Algorithms"
"pointer, a 25% probability that it will have two, and so on. The following function","chapter-16","Patterns of Algorithms"
"determines the level based on such a distribution:","chapter-16","Patterns of Algorithms"
"/** Pick a level using a geometric distribution */","chapter-16","Patterns of Algorithms"
"int randomLevel() {","chapter-16","Patterns of Algorithms"
"int lev;","chapter-16","Patterns of Algorithms"
"for (lev=0; DSutil.random(2) == 0; lev++); // Do nothing","chapter-16","Patterns of Algorithms"
"return lev;","chapter-16","Patterns of Algorithms"
"}","chapter-16","Patterns of Algorithms"
"Once the proper level for the node has been determined, the next step is to find","chapter-16","Patterns of Algorithms"
"where the node should be inserted and link it in as appropriate at all of its levels.","chapter-16","Patterns of Algorithms"
"Figure 16.4 shows an implementation for inserting a new value into the Skip List.","chapter-16","Patterns of Algorithms"
"Figure 16.5 illustrates the Skip List insertion process. In this example, we","chapter-16","Patterns of Algorithms"
"begin by inserting a node with value 10 into an empty Skip List. Assume that","chapter-16","Patterns of Algorithms"
"randomLevel returns a value of 1 (i.e., the node is at level 1, with 2 pointers).","chapter-16","Patterns of Algorithms"
"Because the empty Skip List has no nodes, the level of the list (and thus the level","chapter-16","Patterns of Algorithms"
"520 Chap. 16 Patterns of Algorithms","chapter-16","Patterns of Algorithms"
"(a) (b)","chapter-16","Patterns of Algorithms"
"(c) (d)","chapter-16","Patterns of Algorithms"
"(e)","chapter-16","Patterns of Algorithms"
"head head","chapter-16","Patterns of Algorithms"
"head head","chapter-16","Patterns of Algorithms"
"head","chapter-16","Patterns of Algorithms"
"5 20 2 20 5","chapter-16","Patterns of Algorithms"
"2 5 10 20 30","chapter-16","Patterns of Algorithms"
"10","chapter-16","Patterns of Algorithms"
"10 20","chapter-16","Patterns of Algorithms"
"10","chapter-16","Patterns of Algorithms"
"10","chapter-16","Patterns of Algorithms"
"Figure 16.5 Illustration of Skip List insertion. (a) The Skip List after inserting","chapter-16","Patterns of Algorithms"
"initial value 10 at level 1. (b) The Skip List after inserting value 20 at level 0.","chapter-16","Patterns of Algorithms"
"(c) The Skip List after inserting value 5 at level 0. (d) The Skip List after inserting","chapter-16","Patterns of Algorithms"
"value 2 at level 3. (e) The final Skip List after inserting value 30 at level 2.","chapter-16","Patterns of Algorithms"
"Sec. 16.2 Randomized Algorithms 521","chapter-16","Patterns of Algorithms"
"of the header node) must be set to 1. The new node is inserted, yielding the Skip","chapter-16","Patterns of Algorithms"
"List of Figure 16.5(a).","chapter-16","Patterns of Algorithms"
"Next, insert the value 20. Assume this time that randomLevel returns 0. The","chapter-16","Patterns of Algorithms"
"search process goes to the node with value 10, and the new node is inserted after,","chapter-16","Patterns of Algorithms"
"as shown in Figure 16.5(b). The third node inserted has value 5, and again assume","chapter-16","Patterns of Algorithms"
"that randomLevel returns 0. This yields the Skip List of Figure 16.5.c.","chapter-16","Patterns of Algorithms"
"The fourth node inserted has value 2, and assume that randomLevel re-","chapter-16","Patterns of Algorithms"
"turns 3. This means that the level of the Skip List must rise, causing the header","chapter-16","Patterns of Algorithms"
"node to gain an additional two (null) pointers. At this point, the new node is","chapter-16","Patterns of Algorithms"
"added to the front of the list, as shown in Figure 16.5(d).","chapter-16","Patterns of Algorithms"
"Finally, insert a node with value 30 at level 2. This time, let us take a close","chapter-16","Patterns of Algorithms"
"look at what array update is used for. It stores the farthest node reached at each","chapter-16","Patterns of Algorithms"
"level during the search for the proper location of the new node. The search pro-","chapter-16","Patterns of Algorithms"
"cess begins in the header node at level 3 and proceeds to the node storing value 2.","chapter-16","Patterns of Algorithms"
"Because forward[3] for this node is null, we cannot go further at this level.","chapter-16","Patterns of Algorithms"
"Thus, update[3] stores a pointer to the node with value 2. Likewise, we cannot","chapter-16","Patterns of Algorithms"
"proceed at level 2, so update[2] also stores a pointer to the node with value 2.","chapter-16","Patterns of Algorithms"
"At level 1, we proceed to the node storing value 10. This is as far as we can go","chapter-16","Patterns of Algorithms"
"at level 1, so update[1] stores a pointer to the node with value 10. Finally, at","chapter-16","Patterns of Algorithms"
"level 0 we end up at the node with value 20. At this point, we can add in the new","chapter-16","Patterns of Algorithms"
"node with value 30. For each value i, the new node’s forward[i] pointer is","chapter-16","Patterns of Algorithms"
"set to be update[i]->forward[i], and the nodes stored in update[i] for","chapter-16","Patterns of Algorithms"
"indices 0 through 2 have their forward[i] pointers changed to point to the new","chapter-16","Patterns of Algorithms"
"node. This “splices” the new node into the Skip List at all levels.","chapter-16","Patterns of Algorithms"
"The remove function is left as an exercise. It is similar to insertion in that the","chapter-16","Patterns of Algorithms"
"update array is built as part of searching for the record to be deleted. Then those","chapter-16","Patterns of Algorithms"
"nodes specified by the update array have their forward pointers adjusted to point","chapter-16","Patterns of Algorithms"
"around the node being deleted.","chapter-16","Patterns of Algorithms"
"A newly inserted node could have a high level generated by randomLevel,","chapter-16","Patterns of Algorithms"
"or a low level. It is possible that many nodes in the Skip List could have many","chapter-16","Patterns of Algorithms"
"pointers, leading to unnecessary insert cost and yielding poor (i.e., Θ(n)) perfor-","chapter-16","Patterns of Algorithms"
"mance during search, because not many nodes will be skipped. Conversely, too","chapter-16","Patterns of Algorithms"
"many nodes could have a low level. In the worst case, all nodes could be at level 0,","chapter-16","Patterns of Algorithms"
"equivalent to a regular linked list. If so, search will again require Θ(n) time. How-","chapter-16","Patterns of Algorithms"
"ever, the probability that performance will be poor is quite low. There is only one","chapter-16","Patterns of Algorithms"
"chance in 1024 that ten nodes in a row will be at level 0. The motto of probabilistic","chapter-16","Patterns of Algorithms"
"data structures such as the Skip List is “Don’t worry, be happy.” We simply accept","chapter-16","Patterns of Algorithms"
"the results of randomLevel and expect that probability will eventually work in","chapter-16","Patterns of Algorithms"
"our favor. The advantage of this approach is that the algorithms are simple, while","chapter-16","Patterns of Algorithms"
"requiring only Θ(log n) time for all operations in the average case.","chapter-16","Patterns of Algorithms"
"522 Chap. 16 Patterns of Algorithms","chapter-16","Patterns of Algorithms"
"In practice, the Skip List will probably have better performance than a BST. The","chapter-16","Patterns of Algorithms"
"BST can have bad performance caused by the order in which data are inserted. For","chapter-16","Patterns of Algorithms"
"example, if n nodes are inserted into a BST in ascending order of their key value,","chapter-16","Patterns of Algorithms"
"then the BST will look like a linked list with the deepest node at depth n − 1. The","chapter-16","Patterns of Algorithms"
"Skip List’s performance does not depend on the order in which values are inserted","chapter-16","Patterns of Algorithms"
"into the list. As the number of nodes in the Skip List increases, the probability of","chapter-16","Patterns of Algorithms"
"encountering the worst case decreases geometrically. Thus, the Skip List illustrates","chapter-16","Patterns of Algorithms"
"a tension between the theoretical worst case (in this case, Θ(n) for a Skip List","chapter-16","Patterns of Algorithms"
"operation), and a rapidly increasing probability of average-case performance of","chapter-16","Patterns of Algorithms"
"Θ(log n), that characterizes probabilistic data structures.","chapter-16","Patterns of Algorithms"
"16.3 Numerical Algorithms","chapter-16","Patterns of Algorithms"
"This section presents a variety of algorithms related to mathematical computations","chapter-16","Patterns of Algorithms"
"on numbers. Examples are activities like multiplying two numbers or raising a","chapter-16","Patterns of Algorithms"
"number to a given power. In particular, we are concerned with situations where","chapter-16","Patterns of Algorithms"
"built-in integer or floating-point operations cannot be used because the values being","chapter-16","Patterns of Algorithms"
"operated on are too large. Similar concerns arise for operations on polynomials or","chapter-16","Patterns of Algorithms"
"matrices.","chapter-16","Patterns of Algorithms"
"Since we cannot rely on the hardware to process the inputs in a single constant-","chapter-16","Patterns of Algorithms"
"time operation, we are concerned with how to most effectively implement the op-","chapter-16","Patterns of Algorithms"
"eration to minimize the time cost. This begs a question as to how we should apply","chapter-16","Patterns of Algorithms"
"our normal measures of asymptotic cost in terms of growth rates on input size.","chapter-16","Patterns of Algorithms"
"First, what is an instance of addition or multiplication? Each value of the operands","chapter-16","Patterns of Algorithms"
"yields a different problem instance. And what is the input size when multiplying","chapter-16","Patterns of Algorithms"
"two numbers? If we view the input size as two (since two numbers are input), then","chapter-16","Patterns of Algorithms"
"any non-constant-time algorithm has a growth rate that is infinitely high compared","chapter-16","Patterns of Algorithms"
"to the growth of the input. This makes no sense, especially in light of the fact that","chapter-16","Patterns of Algorithms"
"we know from grade school arithmetic that adding or multiplying numbers does","chapter-16","Patterns of Algorithms"
"seem to get more difficult as the value of the numbers involved increases. In fact,","chapter-16","Patterns of Algorithms"
"we know from standard grade school algorithms that the cost of standard addition","chapter-16","Patterns of Algorithms"
"is linear on the number of digits being added, and multiplication has cost n × m","chapter-16","Patterns of Algorithms"
"when multiplying an m-digit number by an n-digit number.","chapter-16","Patterns of Algorithms"
"The number of digits for the operands does appear to be a key consideration","chapter-16","Patterns of Algorithms"
"when we are performing a numeric algorithm that is sensitive to input size. The","chapter-16","Patterns of Algorithms"
"number of digits is simply the log of the value, for a suitable base of the log. Thus,","chapter-16","Patterns of Algorithms"
"for the purpose of calculating asymptotic growth rates of algorithms, we will con-","chapter-16","Patterns of Algorithms"
"sider the “size” of an input value to be the log of that value. Given this view, there","chapter-16","Patterns of Algorithms"
"are a number of features that seem to relate such operations.","chapter-16","Patterns of Algorithms"
"• Arithmetic operations on large values are not cheap.","chapter-16","Patterns of Algorithms"
"• There is only one instance of value n.","chapter-16","Patterns of Algorithms"
"Sec. 16.3 Numerical Algorithms 523","chapter-16","Patterns of Algorithms"
"• There are 2","chapter-16","Patterns of Algorithms"
"k","chapter-16","Patterns of Algorithms"
"instances of length k or less.","chapter-16","Patterns of Algorithms"
"• The size (length) of value n is log n.","chapter-16","Patterns of Algorithms"
"• The cost of a particular algorithm can decrease when n increases in value","chapter-16","Patterns of Algorithms"
"(say when going from a value of 2","chapter-16","Patterns of Algorithms"
"k − 1 to 2","chapter-16","Patterns of Algorithms"
"k","chapter-16","Patterns of Algorithms"
"to 2","chapter-16","Patterns of Algorithms"
"k + 1), but generally","chapter-16","Patterns of Algorithms"
"increases when n increases in length.","chapter-16","Patterns of Algorithms"
"16.3.1 Exponentiation","chapter-16","Patterns of Algorithms"
"We will start our examination of standard numerical algorithms by considering how","chapter-16","Patterns of Algorithms"
"to perform exponentiation. That is, how do we compute mn","chapter-16","Patterns of Algorithms"
"? We could multiply","chapter-16","Patterns of Algorithms"
"by m a total of n − 1 times. Can we do better? Yes, there is a simple divide","chapter-16","Patterns of Algorithms"
"and conquer approach that we can use. We can recognize that, when n is even,","chapter-16","Patterns of Algorithms"
"mn = mn/2mn/2","chapter-16","Patterns of Algorithms"
". If n is odd, then mn = mbn/2cmbn/2cm. This leads to the","chapter-16","Patterns of Algorithms"
"following recursive algorithm","chapter-16","Patterns of Algorithms"
"int Power(base, exp) {","chapter-16","Patterns of Algorithms"
"if exp = 0 return 1;","chapter-16","Patterns of Algorithms"
"int half = Power(base, exp/2); // integer division of exp","chapter-16","Patterns of Algorithms"
"half = half * half;","chapter-16","Patterns of Algorithms"
"if (odd(exp)) then half = half * base;","chapter-16","Patterns of Algorithms"
"return half;","chapter-16","Patterns of Algorithms"
"}","chapter-16","Patterns of Algorithms"
"Function Power has recurrence relation","chapter-16","Patterns of Algorithms"
"f(n) = ","chapter-16","Patterns of Algorithms"
"0 n = 1","chapter-16","Patterns of Algorithms"
"f(bn/2c) + 1 + n mod 2 n > 1","chapter-16","Patterns of Algorithms"
"whose solution is","chapter-16","Patterns of Algorithms"
"f(n) = blog nc + β(n) − 1","chapter-16","Patterns of Algorithms"
"where β is the number of 1’s in the binary representation of n.","chapter-16","Patterns of Algorithms"
"How does this cost compare with the problem size? The original problem size","chapter-16","Patterns of Algorithms"
"is log m + log n, and the number of multiplications required is log n. This is far","chapter-16","Patterns of Algorithms"
"better (in fact, exponentially better) than performing n − 1 multiplications.","chapter-16","Patterns of Algorithms"
"16.3.2 Largest Common Factor","chapter-16","Patterns of Algorithms"
"We will next present Euclid’s algorithm for finding the largest common factor","chapter-16","Patterns of Algorithms"
"(LCF) for two integers. The LCF is the largest integer that divides both inputs","chapter-16","Patterns of Algorithms"
"evenly.","chapter-16","Patterns of Algorithms"
"First we make this observation: If k divides n and m, then k divides n−m. We","chapter-16","Patterns of Algorithms"
"know this is true because if k divides n then n = ak for some integer a, and if k","chapter-16","Patterns of Algorithms"
"divides m then m = bk for some integer b. So, LCF(n, m) = LCF(n − m, n) =","chapter-16","Patterns of Algorithms"
"LCF(m, n − m) = LCF(m, n).","chapter-16","Patterns of Algorithms"
"524 Chap. 16 Patterns of Algorithms","chapter-16","Patterns of Algorithms"
"Now, for any value n there exists k and l such that","chapter-16","Patterns of Algorithms"
"n = km + l where m > l ≥ 0.","chapter-16","Patterns of Algorithms"
"From the definition of the mod function, we can derive the fact that","chapter-16","Patterns of Algorithms"
"n = bn/mcm + n mod m.","chapter-16","Patterns of Algorithms"
"Since the LCF is a factor of both n and m, and since n = km + l, the LCF must","chapter-16","Patterns of Algorithms"
"therefore be a factor of both km and l, and also the largest common factor of each","chapter-16","Patterns of Algorithms"
"of these terms. As a consequence, LCF(n, m) = LCF(m, l) = LCF(m, n mod","chapter-16","Patterns of Algorithms"
"m).","chapter-16","Patterns of Algorithms"
"This observation leads to a simple algorithm. We will assume that n ≥ m. At","chapter-16","Patterns of Algorithms"
"each iteration we replace n with m and m with n mod m until we have driven m","chapter-16","Patterns of Algorithms"
"to zero.","chapter-16","Patterns of Algorithms"
"int LCF(int n, int m) {","chapter-16","Patterns of Algorithms"
"if (m == 0) return n;","chapter-16","Patterns of Algorithms"
"return LCF(m, n % m);","chapter-16","Patterns of Algorithms"
"}","chapter-16","Patterns of Algorithms"
"To determine how expensive this algorithm is, we need to know how much","chapter-16","Patterns of Algorithms"
"progress we are making at each step. Note that after two iterations, we have re-","chapter-16","Patterns of Algorithms"
"placed n with n mod m. So the key question becomes: How big is n mod m","chapter-16","Patterns of Algorithms"
"relative to n?","chapter-16","Patterns of Algorithms"
"n ≥ m ⇒ n/m ≥ 1","chapter-16","Patterns of Algorithms"
"⇒ 2bn/mc > n/m","chapter-16","Patterns of Algorithms"
"⇒ mbn/mc > n/2","chapter-16","Patterns of Algorithms"
"⇒ n − n/2 > n − mbn/mc = n mod m","chapter-16","Patterns of Algorithms"
"⇒ n/2 > n mod m","chapter-16","Patterns of Algorithms"
"Thus, function LCF will halve its first parameter in no more than 2 iterations.","chapter-16","Patterns of Algorithms"
"The total cost is then O(log n).","chapter-16","Patterns of Algorithms"
"16.3.3 Matrix Multiplication","chapter-16","Patterns of Algorithms"
"The standard algorithm for multiplying two n × n matrices requires Θ(n","chapter-16","Patterns of Algorithms"
"3","chapter-16","Patterns of Algorithms"
") time.","chapter-16","Patterns of Algorithms"
"It is possible to do better than this by rearranging and grouping the multiplications","chapter-16","Patterns of Algorithms"
"in various ways. One example of this is known as Strassen’s matrix multiplication","chapter-16","Patterns of Algorithms"
"algorithm.","chapter-16","Patterns of Algorithms"
"For simplicity, we will assume that n is a power of two. In the following, A","chapter-16","Patterns of Algorithms"
"and B are n × n arrays, while Aij and Bij refer to arrays of size n/2 × n/2. Using","chapter-16","Patterns of Algorithms"
"Sec. 16.3 Numerical Algorithms 525","chapter-16","Patterns of Algorithms"
"this notation, we can think of matrix multiplication using divide and conquer in the","chapter-16","Patterns of Algorithms"
"following way:","chapter-16","Patterns of Algorithms"
"A11 A12","chapter-16","Patterns of Algorithms"
"A21 A22  B11 B12","chapter-16","Patterns of Algorithms"
"B21 B22 ","chapter-16","Patterns of Algorithms"
"=","chapter-16","Patterns of Algorithms"
"A11B11 + A12B21 A11B12 + A12B22","chapter-16","Patterns of Algorithms"
"A21B11 + A22B21 A21B12 + A22B22 ","chapter-16","Patterns of Algorithms"
".","chapter-16","Patterns of Algorithms"
"Of course, each of the multiplications and additions on the right side of this","chapter-16","Patterns of Algorithms"
"equation are recursive calls on arrays of half size, and additions of arrays of half","chapter-16","Patterns of Algorithms"
"size, respectively. The recurrence relation for this algorithm is","chapter-16","Patterns of Algorithms"
"T(n) = 8T(n/2) + 4(n/2)2 = Θ(n","chapter-16","Patterns of Algorithms"
"3","chapter-16","Patterns of Algorithms"
").","chapter-16","Patterns of Algorithms"
"This closed form solution can easily be obtained by applying the Master Theo-","chapter-16","Patterns of Algorithms"
"rem 14.1.","chapter-16","Patterns of Algorithms"
"Strassen’s algorithm carefully rearranges the way that the various terms are","chapter-16","Patterns of Algorithms"
"multiplied and added together. It does so in a particular order, as expressed by the","chapter-16","Patterns of Algorithms"
"following equation:","chapter-16","Patterns of Algorithms"
"A11 A12","chapter-16","Patterns of Algorithms"
"A21 A22  B11 B12","chapter-16","Patterns of Algorithms"
"B21 B22 ","chapter-16","Patterns of Algorithms"
"=","chapter-16","Patterns of Algorithms"
"s1 + s2 − s4 + s6 s4 + s5","chapter-16","Patterns of Algorithms"
"s6 + s7 s2 − s3 + s5 − s7","chapter-16","Patterns of Algorithms"
".","chapter-16","Patterns of Algorithms"
"In other words, the result of the multiplication for an n × n array is obtained by","chapter-16","Patterns of Algorithms"
"a different series of matrix multiplications and additions for n/2 × n/2 arrays.","chapter-16","Patterns of Algorithms"
"Multiplications between subarrays also use Strassen’s algorithm, and the addition","chapter-16","Patterns of Algorithms"
"of two subarrays requires Θ(n","chapter-16","Patterns of Algorithms"
"2","chapter-16","Patterns of Algorithms"
") time. The subfactors are defined as follows:","chapter-16","Patterns of Algorithms"
"s1 = (A12 − A22) · (B21 + B22)","chapter-16","Patterns of Algorithms"
"s2 = (A11 + A22) · (B11 + B22)","chapter-16","Patterns of Algorithms"
"s3 = (A11 − A21) · (B11 + B12)","chapter-16","Patterns of Algorithms"
"s4 = (A11 + A12) · B22","chapter-16","Patterns of Algorithms"
"s5 = A11 · (B12 − B22)","chapter-16","Patterns of Algorithms"
"s6 = A22 · (B21 − B11)","chapter-16","Patterns of Algorithms"
"s7 = (A21 + A22) · B11","chapter-16","Patterns of Algorithms"
"With a little effort, you should be able to verify that this peculiar combination of","chapter-16","Patterns of Algorithms"
"operations does in fact produce the correct answer!","chapter-16","Patterns of Algorithms"
"Now, looking at the list of operations to compute the s factors, and then count-","chapter-16","Patterns of Algorithms"
"ing the additions/subtractions needed to put them together to get the final answers,","chapter-16","Patterns of Algorithms"
"we see that we need a total of seven (array) multiplications and 18 (array) addi-","chapter-16","Patterns of Algorithms"
"tions/subtractions to do the job. This leads to the recurrence","chapter-16","Patterns of Algorithms"
"T(n) = 7T(n/2) + 18(n/2)2","chapter-16","Patterns of Algorithms"
"T(n) = Θ(n","chapter-16","Patterns of Algorithms"
"log2 7","chapter-16","Patterns of Algorithms"
") = Θ(n","chapter-16","Patterns of Algorithms"
"2.81).","chapter-16","Patterns of Algorithms"
"526 Chap. 16 Patterns of Algorithms","chapter-16","Patterns of Algorithms"
"We obtained this closed form solution again by applying the Master Theorem.","chapter-16","Patterns of Algorithms"
"Unfortunately, while Strassen’s algorithm does in fact reduce the asymptotic","chapter-16","Patterns of Algorithms"
"complexity over the standard algorithm, the cost of the large number of addition","chapter-16","Patterns of Algorithms"
"and subtraction operations raises the constant factor involved considerably. This","chapter-16","Patterns of Algorithms"
"means that an extremely large array size is required to make Strassen’s algorithm","chapter-16","Patterns of Algorithms"
"practical in real applications.","chapter-16","Patterns of Algorithms"
"16.3.4 Random Numbers","chapter-16","Patterns of Algorithms"
"The success of randomized algorithms such as were presented in Section 16.2 de-","chapter-16","Patterns of Algorithms"
"pend on having access to a good random number generator. While modern compil-","chapter-16","Patterns of Algorithms"
"ers are likely to include a random number generator that is good enough for most","chapter-16","Patterns of Algorithms"
"purposes, it is helpful to understand how they work, and to even be able to construct","chapter-16","Patterns of Algorithms"
"your own in case you don’t trust the one provided. This is easy to do.","chapter-16","Patterns of Algorithms"
"First, let us consider what a random sequence. From the following list, which","chapter-16","Patterns of Algorithms"
"appears to be a sequence of “random” numbers?","chapter-16","Patterns of Algorithms"
"• 1, 1, 1, 1, 1, 1, 1, 1, 1, ...","chapter-16","Patterns of Algorithms"
"• 1, 2, 3, 4, 5, 6, 7, 8, 9, ...","chapter-16","Patterns of Algorithms"
"• 2, 7, 1, 8, 2, 8, 1, 8, 2, ...","chapter-16","Patterns of Algorithms"
"In fact, all three happen to be the beginning of a some sequence in which one","chapter-16","Patterns of Algorithms"
"could continue the pattern to generate more values (in case you do not recognize","chapter-16","Patterns of Algorithms"
"it, the third one is the initial digits of the irrational constant e). Viewed as a series","chapter-16","Patterns of Algorithms"
"of digits, ideally every possible sequence has equal probability of being generated","chapter-16","Patterns of Algorithms"
"(even the three sequences above). In fact, definitions of randomness generally have","chapter-16","Patterns of Algorithms"
"features such as:","chapter-16","Patterns of Algorithms"
"• One cannot predict the next item. The series is unpredictable.","chapter-16","Patterns of Algorithms"
"• The series cannot be described more briefly than simply listing it out. This is","chapter-16","Patterns of Algorithms"
"the equidistribution property.","chapter-16","Patterns of Algorithms"
"There is no such thing as a random number sequence, only “random enough”","chapter-16","Patterns of Algorithms"
"sequences. A sequence is pseudorandom if no future term can be predicted in","chapter-16","Patterns of Algorithms"
"polynomial time, given all past terms.","chapter-16","Patterns of Algorithms"
"Most computer systems use a deterministic algorithm to select pseudorandom","chapter-16","Patterns of Algorithms"
"numbers.1 The most commonly used approach historically is known as the Linear","chapter-16","Patterns of Algorithms"
"Congruential Method (LCM). The LCM method is quite simple. We begin by","chapter-16","Patterns of Algorithms"
"picking a seed that we will call r(1). Then, we can compute successive terms as","chapter-16","Patterns of Algorithms"
"follows.","chapter-16","Patterns of Algorithms"
"r(i) = (r(i − 1) × b) mod t","chapter-16","Patterns of Algorithms"
"where b and t are constants.","chapter-16","Patterns of Algorithms"
"1Another approach is based on using a computer chip that generates random numbers resulting","chapter-16","Patterns of Algorithms"
"from “thermal noise” in the system. Time will tell if this approach replaces deterministic approaches.","chapter-16","Patterns of Algorithms"
"Sec. 16.3 Numerical Algorithms 527","chapter-16","Patterns of Algorithms"
"By definition of the mod function, all generated numbers must be in the range","chapter-16","Patterns of Algorithms"
"0 to t − 1. Now, consider what happens when r(i) = r(j) for values i and j. Of","chapter-16","Patterns of Algorithms"
"course then r(i + 1) = r(j + 1) which means that we have a repeating cycle.","chapter-16","Patterns of Algorithms"
"Since the values coming out of the random number generator are between 0 and","chapter-16","Patterns of Algorithms"
"t − 1, the longest cycle that we can hope for has length t. In fact, since r(0) = 0, it","chapter-16","Patterns of Algorithms"
"cannot even be quite this long. It turns out that to get a good result, it is crucial to","chapter-16","Patterns of Algorithms"
"pick good values for both b and t. To see why, consider the following example.","chapter-16","Patterns of Algorithms"
"Example 16.4 Given a t value of 13, we can get very different results","chapter-16","Patterns of Algorithms"
"depending on the b value that we pick, in ways that are hard to predict.","chapter-16","Patterns of Algorithms"
"r(i) = 6r(i − 1) mod 13 =","chapter-16","Patterns of Algorithms"
"..., 1, 6, 10, 8, 9, 2, 12, 7, 3, 5, 4, 11, 1, ...","chapter-16","Patterns of Algorithms"
"r(i) = 7r(i − 1) mod 13 =","chapter-16","Patterns of Algorithms"
"..., 1, 7, 10, 5, 9, 11, 12, 6, 3, 8, 4, 2, 1, ...","chapter-16","Patterns of Algorithms"
"r(i) = 5r(i − 1) mod 13 =","chapter-16","Patterns of Algorithms"
"..., 1, 5, 12, 8, 1, ...","chapter-16","Patterns of Algorithms"
"..., 2, 10, 11, 3, 2, ...","chapter-16","Patterns of Algorithms"
"..., 4, 7, 9, 6, 4, ...","chapter-16","Patterns of Algorithms"
"..., 0, 0, ...","chapter-16","Patterns of Algorithms"
"Clearly, a b value of 5 is far inferior to b values of 6 or 7 in this example.","chapter-16","Patterns of Algorithms"
"If you would like to write a simple LCM random number generator of your","chapter-16","Patterns of Algorithms"
"own, an effective one can be made with the following formula.","chapter-16","Patterns of Algorithms"
"r(i) = 16807r(i − 1) mod 231 − 1.","chapter-16","Patterns of Algorithms"
"16.3.5 The Fast Fourier Transform","chapter-16","Patterns of Algorithms"
"As noted at the beginning of this section, multiplication is considerably more diffi-","chapter-16","Patterns of Algorithms"
"cult than addition. The cost to multiply two n-bit numbers directly is O(n","chapter-16","Patterns of Algorithms"
"2","chapter-16","Patterns of Algorithms"
"), while","chapter-16","Patterns of Algorithms"
"addition of two n-bit numbers is O(n).","chapter-16","Patterns of Algorithms"
"Recall from Section 2.3 that one property of logarithms is","chapter-16","Patterns of Algorithms"
"log nm = log n + log m.","chapter-16","Patterns of Algorithms"
"Thus, if taking logarithms and anti-logarithms were cheap, then we could reduce","chapter-16","Patterns of Algorithms"
"multiplication to addition by taking the log of the two operands, adding, and then","chapter-16","Patterns of Algorithms"
"taking the anti-log of the sum.","chapter-16","Patterns of Algorithms"
"Under normal circumstances, taking logarithms and anti-logarithms is expen-","chapter-16","Patterns of Algorithms"
"sive, and so this reduction would not be considered practical. However, this re-","chapter-16","Patterns of Algorithms"
"duction is precisely the basis for the slide rule. The slide rule uses a logarithmic","chapter-16","Patterns of Algorithms"
"scale to measure the lengths of two numbers, in effect doing the conversion to log-","chapter-16","Patterns of Algorithms"
"arithms automatically. These two lengths are then added together, and the inverse","chapter-16","Patterns of Algorithms"
"528 Chap. 16 Patterns of Algorithms","chapter-16","Patterns of Algorithms"
"logarithm of the sum is read off another logarithmic scale. The part normally con-","chapter-16","Patterns of Algorithms"
"sidered expensive (taking logarithms and anti-logarithms) is cheap because it is a","chapter-16","Patterns of Algorithms"
"physical part of the slide rule. Thus, the entire multiplication process can be done","chapter-16","Patterns of Algorithms"
"cheaply via a reduction to addition. In the days before electronic calculators, slide","chapter-16","Patterns of Algorithms"
"rules were routinely used by scientists and engineers to do basic calculations of this","chapter-16","Patterns of Algorithms"
"nature.","chapter-16","Patterns of Algorithms"
"Now consider the problem of multiplying polynomials. A vector a of n values","chapter-16","Patterns of Algorithms"
"can uniquely represent a polynomial of degree n − 1, expressed as","chapter-16","Patterns of Algorithms"
"Pa(x) =","chapter-16","Patterns of Algorithms"
"nX−1","chapter-16","Patterns of Algorithms"
"i=0","chapter-16","Patterns of Algorithms"
"aix","chapter-16","Patterns of Algorithms"
"i","chapter-16","Patterns of Algorithms"
".","chapter-16","Patterns of Algorithms"
"Alternatively, a polynomial can be uniquely represented by a list of its values at","chapter-16","Patterns of Algorithms"
"n distinct points. Finding the value for a polynomial at a given point is called","chapter-16","Patterns of Algorithms"
"evaluation. Finding the coefficients for the polynomial given the values at n points","chapter-16","Patterns of Algorithms"
"is called interpolation.","chapter-16","Patterns of Algorithms"
"To multiply two n − 1-degree polynomials A and B normally takes Θ(n","chapter-16","Patterns of Algorithms"
"2","chapter-16","Patterns of Algorithms"
") co-","chapter-16","Patterns of Algorithms"
"efficient multiplications. However, if we evaluate both polynomials (at the same","chapter-16","Patterns of Algorithms"
"points), we can simply multiply the corresponding pairs of values to get the corre-","chapter-16","Patterns of Algorithms"
"sponding values for polynomial AB.","chapter-16","Patterns of Algorithms"
"Example 16.5 Polynomial A: x","chapter-16","Patterns of Algorithms"
"2 + 1.","chapter-16","Patterns of Algorithms"
"Polynomial B: 2x","chapter-16","Patterns of Algorithms"
"2 − x + 1.","chapter-16","Patterns of Algorithms"
"Polynomial AB: 2x","chapter-16","Patterns of Algorithms"
"4 − x","chapter-16","Patterns of Algorithms"
"3 + 3x","chapter-16","Patterns of Algorithms"
"2 − x + 1.","chapter-16","Patterns of Algorithms"
"When we multiply the evaluations of A and B at points 0, 1, and -1, we","chapter-16","Patterns of Algorithms"
"get the following results.","chapter-16","Patterns of Algorithms"
"AB(−1) = (2)(4) = 8","chapter-16","Patterns of Algorithms"
"AB(0) = (1)(1) = 1","chapter-16","Patterns of Algorithms"
"AB(1) = (2)(2) = 4","chapter-16","Patterns of Algorithms"
"These results are the same as when we evaluate polynomial AB at these","chapter-16","Patterns of Algorithms"
"points.","chapter-16","Patterns of Algorithms"
"Note that evaluating any polynomial at 0 is easy. If we evaluate at 1 and -","chapter-16","Patterns of Algorithms"
"1, we can share a lot of the work between the two evaluations. But we would","chapter-16","Patterns of Algorithms"
"need five points to nail down polynomial AB, since it is a degree-4 polynomial.","chapter-16","Patterns of Algorithms"
"Fortunately, we can speed processing for any pair of values c and −c. This seems","chapter-16","Patterns of Algorithms"
"to indicate some promising ways to speed up the process of evaluating polynomials.","chapter-16","Patterns of Algorithms"
"But, evaluating two points in roughly the same time as evaluating one point only","chapter-16","Patterns of Algorithms"
"speeds the process by a constant factor. Is there some way to generalized these","chapter-16","Patterns of Algorithms"
"Sec. 16.3 Numerical Algorithms 529","chapter-16","Patterns of Algorithms"
"observations to speed things up further? And even if we do find a way to evaluate","chapter-16","Patterns of Algorithms"
"many points quickly, we will also need to interpolate the five values to get the","chapter-16","Patterns of Algorithms"
"coefficients of AB back.","chapter-16","Patterns of Algorithms"
"So we see that we could multiply two polynomials in less than Θ(n","chapter-16","Patterns of Algorithms"
"2","chapter-16","Patterns of Algorithms"
") operations","chapter-16","Patterns of Algorithms"
"if a fast way could be found to do evaluation/interpolation of 2n−1 points. Before","chapter-16","Patterns of Algorithms"
"considering further how this might be done, first observe again the relationship","chapter-16","Patterns of Algorithms"
"between evaluating a polynomial at values c and −c. In general, we can write","chapter-16","Patterns of Algorithms"
"Pa(x) = Ea(x) + Oa(x) where Ea is the even powers and Oa is the odd powers.","chapter-16","Patterns of Algorithms"
"So,","chapter-16","Patterns of Algorithms"
"Pa(x) =","chapter-16","Patterns of Algorithms"
"n/","chapter-16","Patterns of Algorithms"
"X","chapter-16","Patterns of Algorithms"
"2−1","chapter-16","Patterns of Algorithms"
"i=0","chapter-16","Patterns of Algorithms"
"a2ix","chapter-16","Patterns of Algorithms"
"2i +","chapter-16","Patterns of Algorithms"
"n/","chapter-16","Patterns of Algorithms"
"X","chapter-16","Patterns of Algorithms"
"2−1","chapter-16","Patterns of Algorithms"
"i=0","chapter-16","Patterns of Algorithms"
"a2i+1x","chapter-16","Patterns of Algorithms"
"2i+1","chapter-16","Patterns of Algorithms"
"The significance is that when evaluating the pair of values c and −c, we get","chapter-16","Patterns of Algorithms"
"Ea(c) + Oa(c) = Ea(c) − Oa(−c)","chapter-16","Patterns of Algorithms"
"Oa(c) = −Oa(−c)","chapter-16","Patterns of Algorithms"
"Thus, we only need to compute the Es and Os once instead of twice to get both","chapter-16","Patterns of Algorithms"
"evaluations.","chapter-16","Patterns of Algorithms"
"The key to fast polynomial multiplication is finding the right points to use for","chapter-16","Patterns of Algorithms"
"evaluation/interpolation to make the process efficient. In particular, we want to take","chapter-16","Patterns of Algorithms"
"advantage of symmetries, such as the one we see for evaluating x and −x. But we","chapter-16","Patterns of Algorithms"
"need to find even more symmetries between points if we want to do more than cut","chapter-16","Patterns of Algorithms"
"the work in half. We have to find symmetries not just between pairs of values, but","chapter-16","Patterns of Algorithms"
"also further symmetries between pairs of pairs, and then pairs of pairs of pairs, and","chapter-16","Patterns of Algorithms"
"so on.","chapter-16","Patterns of Algorithms"
"Recall that a complex number z has a real component and an imaginary compo-","chapter-16","Patterns of Algorithms"
"nent. We can consider the position of z on a number line if we use the y dimension","chapter-16","Patterns of Algorithms"
"for the imaginary component. Now, we will define a primitive nth root of unity if","chapter-16","Patterns of Algorithms"
"1. z","chapter-16","Patterns of Algorithms"
"n = 1 and","chapter-16","Patterns of Algorithms"
"2. z","chapter-16","Patterns of Algorithms"
"k 6= 1 for 0 < k < n.","chapter-16","Patterns of Algorithms"
"z","chapter-16","Patterns of Algorithms"
"0","chapter-16","Patterns of Algorithms"
", z1","chapter-16","Patterns of Algorithms"
", ..., zn−1","chapter-16","Patterns of Algorithms"
"are called the nth roots of unity. For example, when n = 4, then","chapter-16","Patterns of Algorithms"
"z = i or z = −i. In general, we have the identities e","chapter-16","Patterns of Algorithms"
"iπ = −1, and z","chapter-16","Patterns of Algorithms"
"j = e","chapter-16","Patterns of Algorithms"
"2πij/n =","chapter-16","Patterns of Algorithms"
"−1","chapter-16","Patterns of Algorithms"
"2j/n. The significance is that we can find as many points on a unit circle as we","chapter-16","Patterns of Algorithms"
"would need (see Figure 16.6). But these points are special in that they will allow","chapter-16","Patterns of Algorithms"
"us to do just the right computation necessary to get the needed symmetries to speed","chapter-16","Patterns of Algorithms"
"up the overall process of evaluating many points at once.","chapter-16","Patterns of Algorithms"
"The next step is to define how the computation is done. Define an n × n matrix","chapter-16","Patterns of Algorithms"
"Az with row i and column j as","chapter-16","Patterns of Algorithms"
"Az = (z","chapter-16","Patterns of Algorithms"
"ij ).","chapter-16","Patterns of Algorithms"
"530 Chap. 16 Patterns of Algorithms","chapter-16","Patterns of Algorithms"
"−i","chapter-16","Patterns of Algorithms"
"1","chapter-16","Patterns of Algorithms"
"i","chapter-16","Patterns of Algorithms"
"−i","chapter-16","Patterns of Algorithms"
"1","chapter-16","Patterns of Algorithms"
"i","chapter-16","Patterns of Algorithms"
"−1 −1","chapter-16","Patterns of Algorithms"
"Figure 16.6 Examples of the 4th and 8th roots of unity.","chapter-16","Patterns of Algorithms"
"The idea is that there is a row for each root (row i for z","chapter-16","Patterns of Algorithms"
"i","chapter-16","Patterns of Algorithms"
") while the columns corre-","chapter-16","Patterns of Algorithms"
"spond to the power of the exponent of the x value in the polynomial. For example,","chapter-16","Patterns of Algorithms"
"when n = 4 we have z = i. Thus, the Az array appears as follows.","chapter-16","Patterns of Algorithms"
"Az =","chapter-16","Patterns of Algorithms"
"1 1 1 1","chapter-16","Patterns of Algorithms"
"1 i −1 −i","chapter-16","Patterns of Algorithms"
"1 −1 1 −1","chapter-16","Patterns of Algorithms"
"1 −i −1 i","chapter-16","Patterns of Algorithms"
"Let a = [a0, a1, ..., an−1]","chapter-16","Patterns of Algorithms"
"T be a vector that stores the coefficients for the polyno-","chapter-16","Patterns of Algorithms"
"mial being evaluated. We can then do the calculations to evaluate the polynomial","chapter-16","Patterns of Algorithms"
"at the nth roots of unity by multiplying the Az matrix by the coefficient vector. The","chapter-16","Patterns of Algorithms"
"resulting vector Fz is called the Discrete Fourier Transform for the polynomial.","chapter-16","Patterns of Algorithms"
"Fz = Aza = b.","chapter-16","Patterns of Algorithms"
"bi =","chapter-16","Patterns of Algorithms"
"nX−1","chapter-16","Patterns of Algorithms"
"k=0","chapter-16","Patterns of Algorithms"
"akz","chapter-16","Patterns of Algorithms"
"ik","chapter-16","Patterns of Algorithms"
".","chapter-16","Patterns of Algorithms"
"When n = 8, then z =","chapter-16","Patterns of Algorithms"
"√","chapter-16","Patterns of Algorithms"
"i, since √","chapter-16","Patterns of Algorithms"
"i","chapter-16","Patterns of Algorithms"
"8","chapter-16","Patterns of Algorithms"
"= 1. So, the corresponding matrix is as","chapter-16","Patterns of Algorithms"
"follows.","chapter-16","Patterns of Algorithms"
"Az =","chapter-16","Patterns of Algorithms"
"1 1 1 1 1 1 1 1","chapter-16","Patterns of Algorithms"
"1","chapter-16","Patterns of Algorithms"
"√","chapter-16","Patterns of Algorithms"
"i i i√","chapter-16","Patterns of Algorithms"
"i −1 −","chapter-16","Patterns of Algorithms"
"√","chapter-16","Patterns of Algorithms"
"i −i −i","chapter-16","Patterns of Algorithms"
"√","chapter-16","Patterns of Algorithms"
"i","chapter-16","Patterns of Algorithms"
"1 i −1 −i 1 i −1 −i","chapter-16","Patterns of Algorithms"
"1 i","chapter-16","Patterns of Algorithms"
"√","chapter-16","Patterns of Algorithms"
"i −i","chapter-16","Patterns of Algorithms"
"√","chapter-16","Patterns of Algorithms"
"i −1 −i","chapter-16","Patterns of Algorithms"
"√","chapter-16","Patterns of Algorithms"
"i i −","chapter-16","Patterns of Algorithms"
"√","chapter-16","Patterns of Algorithms"
"i","chapter-16","Patterns of Algorithms"
"1 −1 1 −1 1 −1 1 −1","chapter-16","Patterns of Algorithms"
"1 −","chapter-16","Patterns of Algorithms"
"√","chapter-16","Patterns of Algorithms"
"i i −i","chapter-16","Patterns of Algorithms"
"√","chapter-16","Patterns of Algorithms"
"i −1","chapter-16","Patterns of Algorithms"
"√","chapter-16","Patterns of Algorithms"
"i −i i√","chapter-16","Patterns of Algorithms"
"i","chapter-16","Patterns of Algorithms"
"1 −i −1 i 1 −i −1 i","chapter-16","Patterns of Algorithms"
"1 −i","chapter-16","Patterns of Algorithms"
"√","chapter-16","Patterns of Algorithms"
"i −i −","chapter-16","Patterns of Algorithms"
"√","chapter-16","Patterns of Algorithms"
"i −1 i","chapter-16","Patterns of Algorithms"
"√","chapter-16","Patterns of Algorithms"
"i i √","chapter-16","Patterns of Algorithms"
"i","chapter-16","Patterns of Algorithms"
"We still have two problems. We need to be able to multiply this matrix and","chapter-16","Patterns of Algorithms"
"the vector faster than just by performing a standard matrix-vector multiplication,","chapter-16","Patterns of Algorithms"
"Sec. 16.3 Numerical Algorithms 531","chapter-16","Patterns of Algorithms"
"otherwise the cost is still n","chapter-16","Patterns of Algorithms"
"2 multiplies to do the evaluation. Even if we can mul-","chapter-16","Patterns of Algorithms"
"tiply the matrix and vector cheaply, we still need to be able to reverse the process.","chapter-16","Patterns of Algorithms"
"That is, after transforming the two input polynomials by evaluating them, and then","chapter-16","Patterns of Algorithms"
"pair-wise multiplying the evaluated points, we must interpolate those points to get","chapter-16","Patterns of Algorithms"
"the resulting polynomial back that corresponds to multiplying the original input","chapter-16","Patterns of Algorithms"
"polynomials.","chapter-16","Patterns of Algorithms"
"The interpolation step is nearly identical to the evaluation step.","chapter-16","Patterns of Algorithms"
"F","chapter-16","Patterns of Algorithms"
"−1","chapter-16","Patterns of Algorithms"
"z = A","chapter-16","Patterns of Algorithms"
"−1","chapter-16","Patterns of Algorithms"
"z","chapter-16","Patterns of Algorithms"
"b","chapter-16","Patterns of Algorithms"
"0 = a","chapter-16","Patterns of Algorithms"
"0","chapter-16","Patterns of Algorithms"
".","chapter-16","Patterns of Algorithms"
"We need to find A−1","chapter-16","Patterns of Algorithms"
"z","chapter-16","Patterns of Algorithms"
". This turns out to be simple to compute, and is defined as","chapter-16","Patterns of Algorithms"
"follows.","chapter-16","Patterns of Algorithms"
"A","chapter-16","Patterns of Algorithms"
"−1","chapter-16","Patterns of Algorithms"
"z =","chapter-16","Patterns of Algorithms"
"1","chapter-16","Patterns of Algorithms"
"n","chapter-16","Patterns of Algorithms"
"A1/z.","chapter-16","Patterns of Algorithms"
"In other words, interpolation (the inverse transformation) requires the same com-","chapter-16","Patterns of Algorithms"
"putation as evaluation, except that we substitute 1/z for z (and multiply by 1/n at","chapter-16","Patterns of Algorithms"
"the end). So, if we can do one fast, we can do the other fast.","chapter-16","Patterns of Algorithms"
"If you examine the example Az matrix for n = 8, you should see that there","chapter-16","Patterns of Algorithms"
"are symmetries within the matrix. For example, the top half is identical to the","chapter-16","Patterns of Algorithms"
"bottom half with suitable sign changes on some rows and columns. Likewise for the","chapter-16","Patterns of Algorithms"
"left and right halves. An efficient divide and conquer algorithm exists to perform","chapter-16","Patterns of Algorithms"
"both the evaluation and the interpolation in Θ(n log n) time. This is called the","chapter-16","Patterns of Algorithms"
"Discrete Fourier Transform (DFT). It is a recursive function that decomposes","chapter-16","Patterns of Algorithms"
"the matrix multiplications, taking advantage of the symmetries made available by","chapter-16","Patterns of Algorithms"
"doing evaluation at the nth roots of unity. The algorithm is as follows.","chapter-16","Patterns of Algorithms"
"Fourier Transform(double *Polynomial, int n) {","chapter-16","Patterns of Algorithms"
"// Compute the Fourier transform of Polynomial","chapter-16","Patterns of Algorithms"
"// with degree n. Polynomial is a list of","chapter-16","Patterns of Algorithms"
"// coefficients indexed from 0 to n-1. n is","chapter-16","Patterns of Algorithms"
"// assumed to be a power of 2.","chapter-16","Patterns of Algorithms"
"double Even[n/2], Odd[n/2], List1[n/2], List2[n/2];","chapter-16","Patterns of Algorithms"
"if (n==1) return Polynomial[0];","chapter-16","Patterns of Algorithms"
"for (j=0; j<=n/2-1; j++) {","chapter-16","Patterns of Algorithms"
"Even[j] = Polynomial[2j];","chapter-16","Patterns of Algorithms"
"Odd[j] = Polynomial[2j+1];","chapter-16","Patterns of Algorithms"
"}","chapter-16","Patterns of Algorithms"
"List1 = Fourier Transform(Even, n/2);","chapter-16","Patterns of Algorithms"
"List2 = Fourier Transform(Odd, n/2);","chapter-16","Patterns of Algorithms"
"for (j=0; j<=n-1, J++) {","chapter-16","Patterns of Algorithms"
"Imaginary z = pow(E, 2*i*PI*j/n);","chapter-16","Patterns of Algorithms"
"k = j % (n/2);","chapter-16","Patterns of Algorithms"
"Polynomial[j] = List1[k] + z*List2[k];","chapter-16","Patterns of Algorithms"
"}","chapter-16","Patterns of Algorithms"
"return Polynomial;","chapter-16","Patterns of Algorithms"
"}","chapter-16","Patterns of Algorithms"
"532 Chap. 16 Patterns of Algorithms","chapter-16","Patterns of Algorithms"
"Thus, the full process for multiplying polynomials A and B using the Fourier","chapter-16","Patterns of Algorithms"
"transform is as follows.","chapter-16","Patterns of Algorithms"
"1. Represent an n − 1-degree polynomial as 2n − 1 coefficients:","chapter-16","Patterns of Algorithms"
"[a0, a1, ..., an−1, 0, ..., 0]","chapter-16","Patterns of Algorithms"
"2. Perform Fourier Transform on the representations for A and B","chapter-16","Patterns of Algorithms"
"3. Pairwise multiply the results to get 2n − 1 values.","chapter-16","Patterns of Algorithms"
"4. Perform the inverse Fourier Transform to get the 2n − 1 degree poly-","chapter-16","Patterns of Algorithms"
"nomial AB.","chapter-16","Patterns of Algorithms"
"16.4 Further Reading","chapter-16","Patterns of Algorithms"
"For further information on Skip Lists, see “Skip Lists: A Probabilistic Alternative","chapter-16","Patterns of Algorithms"
"to Balanced Trees” by William Pugh [Pug90].","chapter-16","Patterns of Algorithms"
"16.5 Exercises","chapter-16","Patterns of Algorithms"
"16.1 Solve Towers of Hanoi using a dynamic programming algorithm.","chapter-16","Patterns of Algorithms"
"16.2 There are six possible permutations of the lines","chapter-16","Patterns of Algorithms"
"for (int k=0; k<G.n(); k++)","chapter-16","Patterns of Algorithms"
"for (int i=0; i<G.n(); i++)","chapter-16","Patterns of Algorithms"
"for (int j=0; j<G.n(); j++)","chapter-16","Patterns of Algorithms"
"in Floyd’s algorithm. Which ones give a correct algorithm?","chapter-16","Patterns of Algorithms"
"16.3 Show the result of running Floyd’s all-pairs shortest-paths algorithm on the","chapter-16","Patterns of Algorithms"
"graph of Figure 11.26.","chapter-16","Patterns of Algorithms"
"16.4 The implementation for Floyd’s algorithm given in Section 16.1.2 is ineffi-","chapter-16","Patterns of Algorithms"
"cient for adjacency lists because the edges are visited in a bad order when","chapter-16","Patterns of Algorithms"
"initializing array D. What is the cost of of this initialization step for the adja-","chapter-16","Patterns of Algorithms"
"cency list? How can this initialization step be revised so that it costs Θ(|V|","chapter-16","Patterns of Algorithms"
"2","chapter-16","Patterns of Algorithms"
")","chapter-16","Patterns of Algorithms"
"in the worst case?","chapter-16","Patterns of Algorithms"
"16.5 State the greatest possible lower bound that you can prove for the all-pairs","chapter-16","Patterns of Algorithms"
"shortest-paths problem, and justify your answer.","chapter-16","Patterns of Algorithms"
"16.6 Show the Skip List that results from inserting the following values. Draw","chapter-16","Patterns of Algorithms"
"the Skip List after each insert. With each value, assume the depth of its","chapter-16","Patterns of Algorithms"
"corresponding node is as given in the list.","chapter-16","Patterns of Algorithms"
"Sec. 16.6 Projects 533","chapter-16","Patterns of Algorithms"
"value depth","chapter-16","Patterns of Algorithms"
"5 2","chapter-16","Patterns of Algorithms"
"20 0","chapter-16","Patterns of Algorithms"
"30 0","chapter-16","Patterns of Algorithms"
"2 0","chapter-16","Patterns of Algorithms"
"25 1","chapter-16","Patterns of Algorithms"
"26 3","chapter-16","Patterns of Algorithms"
"31 0","chapter-16","Patterns of Algorithms"
"16.7 If we had a linked list that would never be modified, we can use a simpler","chapter-16","Patterns of Algorithms"
"approach than the Skip List to speed access. The concept would remain the","chapter-16","Patterns of Algorithms"
"same in that we add additional pointers to list nodes for efficient access to the","chapter-16","Patterns of Algorithms"
"ith element. How can we add a second pointer to each element of a singly","chapter-16","Patterns of Algorithms"
"linked list to allow access to an arbitrary element in O(log n) time?","chapter-16","Patterns of Algorithms"
"16.8 What is the expected (average) number of pointers for a Skip List node?","chapter-16","Patterns of Algorithms"
"16.9 Write a function to remove a node with given value from a Skip List.","chapter-16","Patterns of Algorithms"
"16.10 Write a function to find the ith node on a Skip List.","chapter-16","Patterns of Algorithms"
"16.6 Projects","chapter-16","Patterns of Algorithms"
"16.1 Complete the implementation of the Skip List-based dictionary begun in Sec-","chapter-16","Patterns of Algorithms"
"tion 16.2.2.","chapter-16","Patterns of Algorithms"
"16.2 Implement both a standard Θ(n","chapter-16","Patterns of Algorithms"
"3","chapter-16","Patterns of Algorithms"
") matrix multiplication algorithm and Stras-","chapter-16","Patterns of Algorithms"
"sen’s matrix multiplication algorithm (see Exercise 14.16.3.3). Using empir-","chapter-16","Patterns of Algorithms"
"ical testing, try to estimate the constant factors for the runtime equations of","chapter-16","Patterns of Algorithms"
"the two algorithms. How big must n be before Strassen’s algorithm becomes","chapter-16","Patterns of Algorithms"
"more efficient than the standard algorithm?","chapter-16","Patterns of Algorithms"
"This book describes data structures that can be used in a wide variety of problems,","chapter-17","Limits to Computation"
"and many examples of efficient algorithms. In general, our search algorithms strive","chapter-17","Limits to Computation"
"to be at worst in O(log n) to find a record, and our sorting algorithms strive to be","chapter-17","Limits to Computation"
"in O(n log n). A few algorithms have higher asymptotic complexity. Both Floyd’s","chapter-17","Limits to Computation"
"all-pairs shortest-paths algorithm and standard matrix multiply have running times","chapter-17","Limits to Computation"
"of Θ(n","chapter-17","Limits to Computation"
"3","chapter-17","Limits to Computation"
") (though for both, the amount of data being processed is Θ(n","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
")).","chapter-17","Limits to Computation"
"We can solve many problems efficiently because we have available (and choose","chapter-17","Limits to Computation"
"to use) efficient algorithms. Given any problem for which you know some alg-","chapter-17","Limits to Computation"
"orithm, it is always possible to write an inefficient algorithm to “solve” the problem.","chapter-17","Limits to Computation"
"For example, consider a sorting algorithm that tests every possible permutation of","chapter-17","Limits to Computation"
"its input until it finds the correct permutation that provides a sorted list. The running","chapter-17","Limits to Computation"
"time for this algorithm would be unacceptably high, because it is proportional to","chapter-17","Limits to Computation"
"the number of permutations which is n! for n inputs. When solving the minimum-","chapter-17","Limits to Computation"
"cost spanning tree problem, if we were to test every possible subset of edges to","chapter-17","Limits to Computation"
"see which forms the shortest minimum spanning tree, the amount of work would","chapter-17","Limits to Computation"
"be proportional to 2","chapter-17","Limits to Computation"
"|E|","chapter-17","Limits to Computation"
"for a graph with |E| edges. Fortunately, for both of these","chapter-17","Limits to Computation"
"problems we have more clever algorithms that allow us to find answers (relatively)","chapter-17","Limits to Computation"
"quickly without explicitly testing every possible solution.","chapter-17","Limits to Computation"
"Unfortunately, there are many computing problems for which the best possible","chapter-17","Limits to Computation"
"algorithm takes a long time to run. A simple example is the Towers of Hanoi","chapter-17","Limits to Computation"
"problem, which requires 2","chapter-17","Limits to Computation"
"n moves to “solve” a tower with n disks. It is not possible","chapter-17","Limits to Computation"
"for any computer program that solves the Towers of Hanoi problem to run in less","chapter-17","Limits to Computation"
"than Ω(2n","chapter-17","Limits to Computation"
") time, because that many moves must be printed out.","chapter-17","Limits to Computation"
"Besides those problems whose solutions must take a long time to run, there are","chapter-17","Limits to Computation"
"also many problems for which we simply do not know if there are efficient algo-","chapter-17","Limits to Computation"
"rithms or not. The best algorithms that we know for such problems are very slow,","chapter-17","Limits to Computation"
"but perhaps there are better ones waiting to be discovered. Of course, while having","chapter-17","Limits to Computation"
"a problem with high running time is bad, it is even worse to have a problem that","chapter-17","Limits to Computation"
"cannot be solved at all! Such problems do exist, and are discussed in Section 17.3.","chapter-17","Limits to Computation"
"535","chapter-17","Limits to Computation"
"536 Chap. 17 Limits to Computation","chapter-17","Limits to Computation"
"This chapter presents a brief introduction to the theory of expensive and im-","chapter-17","Limits to Computation"
"possible problems. Section 17.1 presents the concept of a reduction, which is the","chapter-17","Limits to Computation"
"central tool used for analyzing the difficulty of a problem (as opposed to analyzing","chapter-17","Limits to Computation"
"the cost of an algorithm). Reductions allow us to relate the difficulty of various","chapter-17","Limits to Computation"
"problems, which is often much easier than doing the analysis for a problem from","chapter-17","Limits to Computation"
"first principles. Section 17.2 discusses “hard” problems, by which we mean prob-","chapter-17","Limits to Computation"
"lems that require, or at least appear to require, time exponential on the input size.","chapter-17","Limits to Computation"
"Finally, Section 17.3 considers various problems that, while often simple to define","chapter-17","Limits to Computation"
"and comprehend, are in fact impossible to solve using a computer program. The","chapter-17","Limits to Computation"
"classic example of such a problem is deciding whether an arbitrary computer pro-","chapter-17","Limits to Computation"
"gram will go into an infinite loop when processing a specified input. This is known","chapter-17","Limits to Computation"
"as the halting problem.","chapter-17","Limits to Computation"
"17.1 Reductions","chapter-17","Limits to Computation"
"We begin with an important concept for understanding the relationships between","chapter-17","Limits to Computation"
"problems, called reduction. Reduction allows us to solve one problem in terms","chapter-17","Limits to Computation"
"of another. Equally importantly, when we wish to understand the difficulty of a","chapter-17","Limits to Computation"
"problem, reduction allows us to make relative statements about upper and lower","chapter-17","Limits to Computation"
"bounds on the cost of a problem (as opposed to an algorithm or program).","chapter-17","Limits to Computation"
"Because the concept of a problem is discussed extensively in this chapter, we","chapter-17","Limits to Computation"
"want notation to simplify problem descriptions. Throughout this chapter, a problem","chapter-17","Limits to Computation"
"will be defined in terms of a mapping between inputs and outputs, and the name of","chapter-17","Limits to Computation"
"the problem will be given in all capital letters. Thus, a complete definition of the","chapter-17","Limits to Computation"
"sorting problem could appear as follows:","chapter-17","Limits to Computation"
"SORTING:","chapter-17","Limits to Computation"
"Input: A sequence of integers x0, x1, x2, ..., xn−1.","chapter-17","Limits to Computation"
"Output: A permutation y0, y1, y2, ..., yn−1 of the sequence such that yi ≤ yj","chapter-17","Limits to Computation"
"whenever i < j.","chapter-17","Limits to Computation"
"When you buy or write a program to solve one problem, such as sorting, you","chapter-17","Limits to Computation"
"might be able to use it to help solve a different problem. This is known in software","chapter-17","Limits to Computation"
"engineering as software reuse. To illustrate this, let us consider another problem.","chapter-17","Limits to Computation"
"PAIRING:","chapter-17","Limits to Computation"
"Input: Two sequences of integers X = (x0, x1, ..., xn−1) and Y =","chapter-17","Limits to Computation"
"(y0, y1, ..., yn−1).","chapter-17","Limits to Computation"
"Output: A pairing of the elements in the two sequences such that the least","chapter-17","Limits to Computation"
"value in X is paired with the least value in Y, the next least value in X is paired","chapter-17","Limits to Computation"
"with the next least value in Y, and so on.","chapter-17","Limits to Computation"
"Sec. 17.1 Reductions 537","chapter-17","Limits to Computation"
"23","chapter-17","Limits to Computation"
"42","chapter-17","Limits to Computation"
"17","chapter-17","Limits to Computation"
"93","chapter-17","Limits to Computation"
"88","chapter-17","Limits to Computation"
"12","chapter-17","Limits to Computation"
"57","chapter-17","Limits to Computation"
"90","chapter-17","Limits to Computation"
"48","chapter-17","Limits to Computation"
"59","chapter-17","Limits to Computation"
"11","chapter-17","Limits to Computation"
"89","chapter-17","Limits to Computation"
"12","chapter-17","Limits to Computation"
"91","chapter-17","Limits to Computation"
"64","chapter-17","Limits to Computation"
"34","chapter-17","Limits to Computation"
"Figure 17.1 An illustration of PAIRING. The two lists of numbers are paired up","chapter-17","Limits to Computation"
"so that the least values from each list make a pair, the next smallest values from","chapter-17","Limits to Computation"
"each list make a pair, and so on.","chapter-17","Limits to Computation"
"Figure 17.1 illustrates PAIRING. One way to solve PAIRING is to use an exist-","chapter-17","Limits to Computation"
"ing sorting program to sort each of the two sequences, and then pair off items based","chapter-17","Limits to Computation"
"on their position in sorted order. Technically we say that in this solution, PAIRING","chapter-17","Limits to Computation"
"is reduced to SORTING, because SORTING is used to solve PAIRING.","chapter-17","Limits to Computation"
"Notice that reduction is a three-step process. The first step is to convert an","chapter-17","Limits to Computation"
"instance of PAIRING into two instances of SORTING. The conversion step in this","chapter-17","Limits to Computation"
"example is not very interesting; it simply takes each sequence and assigns it to an","chapter-17","Limits to Computation"
"array to be passed to SORTING. The second step is to sort the two arrays (i.e., apply","chapter-17","Limits to Computation"
"SORTING to each array). The third step is to convert the output of SORTING to","chapter-17","Limits to Computation"
"the output for PAIRING. This is done by pairing the first elements in the sorted","chapter-17","Limits to Computation"
"arrays, the second elements, and so on.","chapter-17","Limits to Computation"
"A reduction of PAIRING to SORTING helps to establish an upper bound on the","chapter-17","Limits to Computation"
"cost of PAIRING. In terms of asymptotic notation, assuming that we can find one","chapter-17","Limits to Computation"
"method to convert the inputs to PAIRING into inputs to SORTING “fast enough,”","chapter-17","Limits to Computation"
"and a second method to convert the result of SORTING back to the correct result","chapter-17","Limits to Computation"
"for PAIRING “fast enough,” then the asymptotic cost of PAIRING cannot be more","chapter-17","Limits to Computation"
"than the cost of SORTING. In this case, there is little work to be done to convert","chapter-17","Limits to Computation"
"from PAIRING to SORTING, or to convert the answer from SORTING back to the","chapter-17","Limits to Computation"
"answer for PAIRING, so the dominant cost of this solution is performing the sort","chapter-17","Limits to Computation"
"operation. Thus, an upper bound for PAIRING is in O(n log n).","chapter-17","Limits to Computation"
"It is important to note that the pairing problem does not require that elements","chapter-17","Limits to Computation"
"of the two sequences be sorted. This is merely one possible way to solve the prob-","chapter-17","Limits to Computation"
"lem. PAIRING only requires that the elements of the sequences be paired correctly.","chapter-17","Limits to Computation"
"Perhaps there is another way to do it? Certainly if we use sorting to solve PAIR-","chapter-17","Limits to Computation"
"ING, the algorithms will require Ω(n log n) time. But, another approach might","chapter-17","Limits to Computation"
"conceivably be faster.","chapter-17","Limits to Computation"
"538 Chap. 17 Limits to Computation","chapter-17","Limits to Computation"
"There is another use of reductions aside from applying an old algorithm to solve","chapter-17","Limits to Computation"
"a new problem (and thereby establishing an upper bound for the new problem).","chapter-17","Limits to Computation"
"That is to prove a lower bound on the cost of a new problem by showing that it","chapter-17","Limits to Computation"
"could be used as a solution for an old problem with a known lower bound.","chapter-17","Limits to Computation"
"Assume we can go the other way and convert SORTING to PAIRING “fast","chapter-17","Limits to Computation"
"enough.” What does this say about the minimum cost of PAIRING? We know","chapter-17","Limits to Computation"
"from Section 7.9 that the cost of SORTING in the worst and average cases is in","chapter-17","Limits to Computation"
"Ω(n log n). In other words, the best possible algorithm for sorting requires at least","chapter-17","Limits to Computation"
"n log n time.","chapter-17","Limits to Computation"
"Assume that PAIRING could be done in O(n) time. Then, one way to create a","chapter-17","Limits to Computation"
"sorting algorithm would be to convert SORTING into PAIRING, run the algorithm","chapter-17","Limits to Computation"
"for PAIRING, and finally convert the answer back to the answer for SORTING.","chapter-17","Limits to Computation"
"Provided that we can convert SORTING to/from PAIRING “fast enough,” this pro-","chapter-17","Limits to Computation"
"cess would yield an O(n) algorithm for sorting! Because this contradicts what we","chapter-17","Limits to Computation"
"know about the lower bound for SORTING, and the only flaw in the reasoning is","chapter-17","Limits to Computation"
"the initial assumption that PAIRING can be done in O(n) time, we can conclude","chapter-17","Limits to Computation"
"that there is no O(n) time algorithm for PAIRING. This reduction process tells us","chapter-17","Limits to Computation"
"that PAIRING must be at least as expensive as SORTING and so must itself have a","chapter-17","Limits to Computation"
"lower bound in Ω(n log n).","chapter-17","Limits to Computation"
"To complete this proof regarding the lower bound for PAIRING, we need now","chapter-17","Limits to Computation"
"to find a way to reduce SORTING to PAIRING. This is easily done. Take an in-","chapter-17","Limits to Computation"
"stance of SORTING (i.e., an array A of n elements). A second array B is generated","chapter-17","Limits to Computation"
"that simply stores i in position i for 0 ≤ i < n. Pass the two arrays to PAIRING.","chapter-17","Limits to Computation"
"Take the resulting set of pairs, and use the value from the B half of the pair to tell","chapter-17","Limits to Computation"
"which position in the sorted array the A half should take; that is, we can now reorder","chapter-17","Limits to Computation"
"the records in the A array using the corresponding value in the B array as the sort","chapter-17","Limits to Computation"
"key and running a simple Θ(n) Binsort. The conversion of SORTING to PAIRING","chapter-17","Limits to Computation"
"can be done in O(n) time, and likewise the conversion of the output of PAIRING","chapter-17","Limits to Computation"
"can be converted to the correct output for SORTING in O(n) time. Thus, the cost","chapter-17","Limits to Computation"
"of this “sorting algorithm” is dominated by the cost for PAIRING.","chapter-17","Limits to Computation"
"Consider any two problems for which a suitable reduction from one to the other","chapter-17","Limits to Computation"
"can be found. The first problem takes an arbitrary instance of its input, which we","chapter-17","Limits to Computation"
"will call I, and transforms I to a solution, which we will call SLN. The second prob-","chapter-17","Limits to Computation"
"lem takes an arbitrary instance of its input, which we will call I","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
", and transforms I","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
"to a solution, which we will call SLN0","chapter-17","Limits to Computation"
". We can define reduction more formally as a","chapter-17","Limits to Computation"
"three-step process:","chapter-17","Limits to Computation"
"1. Transform an arbitrary instance of the first problem to an instance of the","chapter-17","Limits to Computation"
"second problem. In other words, there must be a transformation from any","chapter-17","Limits to Computation"
"instance I of the first problem to an instance I","chapter-17","Limits to Computation"
"0 of the second problem.","chapter-17","Limits to Computation"
"2. Apply an algorithm for the second problem to the instance I","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
", yielding a","chapter-17","Limits to Computation"
"solution SLN0","chapter-17","Limits to Computation"
".","chapter-17","Limits to Computation"
"Sec. 17.1 Reductions 539","chapter-17","Limits to Computation"
"I","chapter-17","Limits to Computation"
"I’","chapter-17","Limits to Computation"
"Problem A:","chapter-17","Limits to Computation"
"Problem B","chapter-17","Limits to Computation"
"SLN","chapter-17","Limits to Computation"
"Transform 2","chapter-17","Limits to Computation"
"Transform 1","chapter-17","Limits to Computation"
"SLN’","chapter-17","Limits to Computation"
"Figure 17.2 The general process for reduction shown as a “blackbox” diagram.","chapter-17","Limits to Computation"
"3. Transform SLN0","chapter-17","Limits to Computation"
"to the solution of I, known as SLN. Note that SLN must in","chapter-17","Limits to Computation"
"fact be the correct solution for I for the reduction to be acceptable.","chapter-17","Limits to Computation"
"Figure 17.2 shows a graphical representation of the general reduction process,","chapter-17","Limits to Computation"
"showing the role of the two problems, and the two transformations. Figure 17.3","chapter-17","Limits to Computation"
"shows a similar diagram for the reduction of SORTING to PAIRING.","chapter-17","Limits to Computation"
"It is important to note that the reduction process does not give us an algorithm","chapter-17","Limits to Computation"
"for solving either problem by itself. It merely gives us a method for solving the first","chapter-17","Limits to Computation"
"problem given that we already have a solution to the second. More importantly for","chapter-17","Limits to Computation"
"the topics to be discussed in the remainder of this chapter, reduction gives us a way","chapter-17","Limits to Computation"
"to understand the bounds of one problem in terms of another. Specifically, given","chapter-17","Limits to Computation"
"efficient transformations, the upper bound of the first problem is at most the upper","chapter-17","Limits to Computation"
"bound of the second. Conversely, the lower bound of the second problem is at least","chapter-17","Limits to Computation"
"the lower bound of the first.","chapter-17","Limits to Computation"
"As a second example of reduction, consider the simple problem of multiplying","chapter-17","Limits to Computation"
"two n-digit numbers. The standard long-hand method for multiplication is to mul-","chapter-17","Limits to Computation"
"tiply the last digit of the first number by the second number (taking Θ(n) time),","chapter-17","Limits to Computation"
"multiply the second digit of the first number by the second number (again taking","chapter-17","Limits to Computation"
"Θ(n) time), and so on for each of the n digits of the first number. Finally, the in-","chapter-17","Limits to Computation"
"termediate results are added together. Note that adding two numbers of length M","chapter-17","Limits to Computation"
"and N can easily be done in Θ(M+N) time. Because each digit of the first number","chapter-17","Limits to Computation"
"540 Chap. 17 Limits to Computation","chapter-17","Limits to Computation"
"Transform 2","chapter-17","Limits to Computation"
"Integer Array A","chapter-17","Limits to Computation"
"Transform 1","chapter-17","Limits to Computation"
"PAIRING","chapter-17","Limits to Computation"
"Array of Pairs","chapter-17","Limits to Computation"
"Sorted","chapter-17","Limits to Computation"
"Integer Array","chapter-17","Limits to Computation"
"Integers","chapter-17","Limits to Computation"
"Array A","chapter-17","Limits to Computation"
"SORTING:","chapter-17","Limits to Computation"
"Integer","chapter-17","Limits to Computation"
"0 to n−1","chapter-17","Limits to Computation"
"Figure 17.3 A reduction of SORTING to PAIRING shown as a “blackbox”","chapter-17","Limits to Computation"
"diagram.","chapter-17","Limits to Computation"
"is multiplied against each digit of the second, this algorithm requires Θ(n","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
") time.","chapter-17","Limits to Computation"
"Asymptotically faster (but more complicated) algorithms are known, but none is so","chapter-17","Limits to Computation"
"fast as to be in O(n).","chapter-17","Limits to Computation"
"Next we ask the question: Is squaring an n-digit number as difficult as multi-","chapter-17","Limits to Computation"
"plying two n-digit numbers? We might hope that something about this special case","chapter-17","Limits to Computation"
"will allow for a faster algorithm than is required by the more general multiplication","chapter-17","Limits to Computation"
"problem. However, a simple reduction proof serves to show that squaring is “as","chapter-17","Limits to Computation"
"hard” as multiplying.","chapter-17","Limits to Computation"
"The key to the reduction is the following formula:","chapter-17","Limits to Computation"
"X × Y =","chapter-17","Limits to Computation"
"(X + Y )","chapter-17","Limits to Computation"
"2 − (X − Y )","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
"4","chapter-17","Limits to Computation"
".","chapter-17","Limits to Computation"
"The significance of this formula is that it allows us to convert an arbitrary instance","chapter-17","Limits to Computation"
"of multiplication to a series of operations involving three addition/subtractions","chapter-17","Limits to Computation"
"(each of which can be done in linear time), two squarings, and a division by 4.","chapter-17","Limits to Computation"
"Note that the division by 4 can be done in linear time (simply convert to binary,","chapter-17","Limits to Computation"
"shift right by two digits, and convert back).","chapter-17","Limits to Computation"
"This reduction shows that if a linear time algorithm for squaring can be found,","chapter-17","Limits to Computation"
"it can be used to construct a linear time algorithm for multiplication.","chapter-17","Limits to Computation"
"Sec. 17.2 Hard Problems 541","chapter-17","Limits to Computation"
"Our next example of reduction concerns the multiplication of two n × n matri-","chapter-17","Limits to Computation"
"ces. For this problem, we will assume that the values stored in the matrices are sim-","chapter-17","Limits to Computation"
"ple integers and that multiplying two simple integers takes constant time (because","chapter-17","Limits to Computation"
"multiplication of two int variables takes a fixed number of machine instructions).","chapter-17","Limits to Computation"
"The standard algorithm for multiplying two matrices is to multiply each element","chapter-17","Limits to Computation"
"of the first matrix’s first row by the corresponding element of the second matrix’s","chapter-17","Limits to Computation"
"first column, then adding the numbers. This takes Θ(n) time. Each of the n","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
"el-","chapter-17","Limits to Computation"
"ements of the solution are computed in similar fashion, requiring a total of Θ(n","chapter-17","Limits to Computation"
"3","chapter-17","Limits to Computation"
")","chapter-17","Limits to Computation"
"time. Faster algorithms are known (see the discussion of Strassen’s Algorithm in","chapter-17","Limits to Computation"
"Section 16.3.3), but none are so fast as to be in O(n","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
").","chapter-17","Limits to Computation"
"Now, consider the case of multiplying two symmetric matrices. A symmetric","chapter-17","Limits to Computation"
"matrix is one in which entry ij is equal to entry ji; that is, the upper-right triangle","chapter-17","Limits to Computation"
"of the matrix is a mirror image of the lower-left triangle. Is there something about","chapter-17","Limits to Computation"
"this restricted case that allows us to multiply two symmetric matrices faster than","chapter-17","Limits to Computation"
"in the general case? The answer is no, as can be seen by the following reduction.","chapter-17","Limits to Computation"
"Assume that we have been given two n × n matrices A and B. We can construct a","chapter-17","Limits to Computation"
"2n × 2n symmetric matrix from an arbitrary matrix A as follows:","chapter-17","Limits to Computation"
"0 A","chapter-17","Limits to Computation"
"A","chapter-17","Limits to Computation"
"T 0","chapter-17","Limits to Computation"
".","chapter-17","Limits to Computation"
"Here 0 stands for an n×n matrix composed of zero values, A is the original matrix,","chapter-17","Limits to Computation"
"and A","chapter-17","Limits to Computation"
"T stands for the transpose of matrix A.","chapter-17","Limits to Computation"
"1 Note that the resulting matrix is now","chapter-17","Limits to Computation"
"symmetric. We can convert matrix B to a symmetric matrix in a similar manner.","chapter-17","Limits to Computation"
"If symmetric matrices could be multiplied “quickly” (in particular, if they could","chapter-17","Limits to Computation"
"be multiplied together in Θ(n","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
") time), then we could find the result of multiplying","chapter-17","Limits to Computation"
"two arbitrary n × n matrices in Θ(n","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
") time by taking advantage of the following","chapter-17","Limits to Computation"
"observation:","chapter-17","Limits to Computation"
"0 A","chapter-17","Limits to Computation"
"A","chapter-17","Limits to Computation"
"T 0","chapter-17","Limits to Computation"
  "0 B","chapter-17","Limits to Computation"
"T","chapter-17","Limits to Computation"
"B 0","chapter-17","Limits to Computation"
"=","chapter-17","Limits to Computation"
"AB 0","chapter-17","Limits to Computation"
"0 A","chapter-17","Limits to Computation"
"TB","chapter-17","Limits to Computation"
"T","chapter-17","Limits to Computation"
".","chapter-17","Limits to Computation"
"In the above formula, AB is the result of multiplying matrices A and B together.","chapter-17","Limits to Computation"
"17.2 Hard Problems","chapter-17","Limits to Computation"
"There are several ways that a problem could be considered hard. For example, we","chapter-17","Limits to Computation"
"might have trouble understanding the definition of the problem itself. At the be-","chapter-17","Limits to Computation"
"ginning of a large data collection and analysis project, developers and their clients","chapter-17","Limits to Computation"
"might have only a hazy notion of what their goals actually are, and need to work","chapter-17","Limits to Computation"
"that out over time. For other types of problems, we might have trouble finding or","chapter-17","Limits to Computation"
"understanding an algorithm to solve the problem. Understanding spoken English","chapter-17","Limits to Computation"
"1The transpose operation takes position ij of the original matrix and places it in position ji of the","chapter-17","Limits to Computation"
"transpose matrix. This can easily be done in n","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
"time for an n × n matrix.","chapter-17","Limits to Computation"
"542 Chap. 17 Limits to Computation","chapter-17","Limits to Computation"
"and translating it to written text is an example of a problem whose goals are easy","chapter-17","Limits to Computation"
"to define, but whose solution is not easy to discover. But even though a natural","chapter-17","Limits to Computation"
"language processing algorithm might be difficult to write, the program’s running","chapter-17","Limits to Computation"
"time might be fairly fast. There are many practical systems today that solve aspects","chapter-17","Limits to Computation"
"of this problem in reasonable time.","chapter-17","Limits to Computation"
"None of these is what is commonly meant when a computer theoretician uses","chapter-17","Limits to Computation"
"the word “hard.” Throughout this section, “hard” means that the best-known alg-","chapter-17","Limits to Computation"
"orithm for the problem is expensive in its running time. One example of a hard","chapter-17","Limits to Computation"
"problem is Towers of Hanoi. It is easy to understand this problem and its solution.","chapter-17","Limits to Computation"
"It is also easy to write a program to solve this problem. But, it takes an extremely","chapter-17","Limits to Computation"
"long time to run for any “reasonably” large value of n. Try running a program to","chapter-17","Limits to Computation"
"solve Towers of Hanoi for only 30 disks!","chapter-17","Limits to Computation"
"The Towers of Hanoi problem takes exponential time, that is, its running time","chapter-17","Limits to Computation"
"is Θ(2n","chapter-17","Limits to Computation"
"). This is radically different from an algorithm that takes Θ(n log n) time","chapter-17","Limits to Computation"
"or Θ(n","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
") time. It is even radically different from a problem that takes Θ(n","chapter-17","Limits to Computation"
"4","chapter-17","Limits to Computation"
") time.","chapter-17","Limits to Computation"
"These are all examples of polynomial running time, because the exponents for all","chapter-17","Limits to Computation"
"terms of these equations are constants. Recall from Chapter 3 that if we buy a new","chapter-17","Limits to Computation"
"computer that runs twice as fast, the size of problem with complexity Θ(n","chapter-17","Limits to Computation"
"4","chapter-17","Limits to Computation"
") that","chapter-17","Limits to Computation"
"we can solve in a certain amount of time is increased by the fourth root of two.","chapter-17","Limits to Computation"
"In other words, there is a multiplicative factor increase, even if it is a rather small","chapter-17","Limits to Computation"
"one. This is true for any algorithm whose running time can be represented by a","chapter-17","Limits to Computation"
"polynomial.","chapter-17","Limits to Computation"
"Consider what happens if you buy a computer that is twice as fast and try to","chapter-17","Limits to Computation"
"solve a bigger Towers of Hanoi problem in a given amount of time. Because its","chapter-17","Limits to Computation"
"complexity is Θ(2n","chapter-17","Limits to Computation"
"), we can solve a problem only one disk bigger! There is no","chapter-17","Limits to Computation"
"multiplicative factor, and this is true for any exponential algorithm: A constant","chapter-17","Limits to Computation"
"factor increase in processing power results in only a fixed addition in problem-","chapter-17","Limits to Computation"
"solving power.","chapter-17","Limits to Computation"
"There are a number of other fundamental differences between polynomial run-","chapter-17","Limits to Computation"
"ning times and exponential running times that argues for treating them as quali-","chapter-17","Limits to Computation"
"tatively different. Polynomials are closed under composition and addition. Thus,","chapter-17","Limits to Computation"
"running polynomial-time programs in sequence, or having one program with poly-","chapter-17","Limits to Computation"
"nomial running time call another a polynomial number of times yields polynomial","chapter-17","Limits to Computation"
"time. Also, all computers known are polynomially related. That is, any program","chapter-17","Limits to Computation"
"that runs in polynomial time on any computer today, when transferred to any other","chapter-17","Limits to Computation"
"computer, will still run in polynomial time.","chapter-17","Limits to Computation"
"There is a practical reason for recognizing a distinction. In practice, most poly-","chapter-17","Limits to Computation"
"nomial time algorithms are “feasible” in that they can run reasonably large inputs","chapter-17","Limits to Computation"
"in reasonable time. In contrast, most algorithms requiring exponential time are","chapter-17","Limits to Computation"
"not practical to run even for fairly modest sizes of input. One could argue that","chapter-17","Limits to Computation"
"a program with high polynomial degree (such as n","chapter-17","Limits to Computation"
"100) is not practical, while an","chapter-17","Limits to Computation"
"Sec. 17.2 Hard Problems 543","chapter-17","Limits to Computation"
"exponential-time program with cost 1.001n","chapter-17","Limits to Computation"
"is practical. But the reality is that we","chapter-17","Limits to Computation"
"know of almost no problems where the best polynomial-time algorithm has high","chapter-17","Limits to Computation"
"degree (they nearly all have degree four or less), while almost no exponential-time","chapter-17","Limits to Computation"
"algorithms (whose cost is (O(c","chapter-17","Limits to Computation"
"n","chapter-17","Limits to Computation"
")) have their constant c close to one. So there is not","chapter-17","Limits to Computation"
"much gray area between polynomial and exponential time algorithms in practice.","chapter-17","Limits to Computation"
"For the rest of this chapter, we define a hard algorithm to be one that runs in","chapter-17","Limits to Computation"
"exponential time, that is, in Ω(c","chapter-17","Limits to Computation"
"n","chapter-17","Limits to Computation"
") for some constant c > 1. A definition for a hard","chapter-17","Limits to Computation"
"problem will be presented in the next section.","chapter-17","Limits to Computation"
"17.2.1 The Theory of N P-Completeness","chapter-17","Limits to Computation"
"Imagine a magical computer that works by guessing the correct solution from","chapter-17","Limits to Computation"
"among all of the possible solutions to a problem. Another way to look at this is","chapter-17","Limits to Computation"
"to imagine a super parallel computer that could test all possible solutions simul-","chapter-17","Limits to Computation"
"taneously. Certainly this magical (or highly parallel) computer can do anything a","chapter-17","Limits to Computation"
"normal computer can do. It might also solve some problems more quickly than a","chapter-17","Limits to Computation"
"normal computer can. Consider some problem where, given a guess for a solution,","chapter-17","Limits to Computation"
"checking the solution to see if it is correct can be done in polynomial time. Even","chapter-17","Limits to Computation"
"if the number of possible solutions is exponential, any given guess can be checked","chapter-17","Limits to Computation"
"in polynomial time (equivalently, all possible solutions are checked simultaneously","chapter-17","Limits to Computation"
"in polynomial time), and thus the problem can be solved in polynomial time by our","chapter-17","Limits to Computation"
"hypothetical magical computer. Another view of this concept is this: If you cannot","chapter-17","Limits to Computation"
"get the answer to a problem in polynomial time by guessing the right answer and","chapter-17","Limits to Computation"
"then checking it, then you cannot do it in polynomial time in any other way.","chapter-17","Limits to Computation"
"The idea of “guessing” the right answer to a problem — or checking all possible","chapter-17","Limits to Computation"
"solutions in parallel to determine which is correct — is called non-determinism.","chapter-17","Limits to Computation"
"An algorithm that works in this manner is called a non-deterministic algorithm,","chapter-17","Limits to Computation"
"and any problem with an algorithm that runs on a non-deterministic machine in","chapter-17","Limits to Computation"
"polynomial time is given a special name: It is said to be a problem in N P. Thus,","chapter-17","Limits to Computation"
"problems in N P are those problems that can be solved in polynomial time on a","chapter-17","Limits to Computation"
"non-deterministic machine.","chapter-17","Limits to Computation"
"Not all problems requiring exponential time on a regular computer are in N P.","chapter-17","Limits to Computation"
"For example, Towers of Hanoi is not in N P, because it must print out O(2","chapter-17","Limits to Computation"
"n","chapter-17","Limits to Computation"
") moves","chapter-17","Limits to Computation"
"for n disks. A non-deterministic machine cannot “guess” and print the correct","chapter-17","Limits to Computation"
"answer in less time.","chapter-17","Limits to Computation"
"On the other hand, consider the TRAVELING SALESMAN problem.","chapter-17","Limits to Computation"
"TRAVELING SALESMAN (1)","chapter-17","Limits to Computation"
"Input: A complete, directed graph G with positive distances assigned to","chapter-17","Limits to Computation"
"each edge in the graph.","chapter-17","Limits to Computation"
"Output: The shortest simple cycle that includes every vertex.","chapter-17","Limits to Computation"
"544 Chap. 17 Limits to Computation","chapter-17","Limits to Computation"
"A","chapter-17","Limits to Computation"
"3","chapter-17","Limits to Computation"
"E","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
"3 6","chapter-17","Limits to Computation"
"8","chapter-17","Limits to Computation"
"4","chapter-17","Limits to Computation"
"1","chapter-17","Limits to Computation"
"B","chapter-17","Limits to Computation"
"C","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
"1 1","chapter-17","Limits to Computation"
"D","chapter-17","Limits to Computation"
"Figure 17.4 An illustration of the TRAVELING SALESMAN problem. Five","chapter-17","Limits to Computation"
"vertices are shown, with edges between each pair of cities. The problem is to visit","chapter-17","Limits to Computation"
"all of the cities exactly once, returning to the start city, with the least total cost.","chapter-17","Limits to Computation"
"Figure 17.4 illustrates this problem. Five vertices are shown, with edges and","chapter-17","Limits to Computation"
"associated costs between each pair of edges. (For simplicity Figure 17.4 shows an","chapter-17","Limits to Computation"
"undirected graph, assuming that the cost is the same in both directions, though this","chapter-17","Limits to Computation"
"need not be the case.) If the salesman visits the cities in the order ABCDEA, he","chapter-17","Limits to Computation"
"will travel a total distance of 13. A better route would be ABDCEA, with cost 11.","chapter-17","Limits to Computation"
"The best route for this particular graph would be ABEDCA, with cost 9.","chapter-17","Limits to Computation"
"We cannot solve this problem in polynomial time with a guess-and-test non-","chapter-17","Limits to Computation"
"deterministic computer. The problem is that, given a candidate cycle, while we can","chapter-17","Limits to Computation"
"quickly check that the answer is indeed a cycle of the appropriate form, and while","chapter-17","Limits to Computation"
"we can quickly calculate the length of the cycle, we have no easy way of knowing","chapter-17","Limits to Computation"
"if it is in fact the shortest such cycle. However, we can solve a variant of this","chapter-17","Limits to Computation"
"problem cast in the form of a decision problem. A decision problem is simply one","chapter-17","Limits to Computation"
"whose answer is either YES or NO. The decision problem form of TRAVELING","chapter-17","Limits to Computation"
"SALESMAN is as follows:","chapter-17","Limits to Computation"
"TRAVELING SALESMAN (2)","chapter-17","Limits to Computation"
"Input: A complete, directed graph G with positive distances assigned to","chapter-17","Limits to Computation"
"each edge in the graph, and an integer k.","chapter-17","Limits to Computation"
"Output: YES if there is a simple cycle with total distance ≤ k containing","chapter-17","Limits to Computation"
"every vertex in G, and NO otherwise.","chapter-17","Limits to Computation"
"We can solve this version of the problem in polynomial time with a non-deter-","chapter-17","Limits to Computation"
"ministic computer. The non-deterministic algorithm simply checks all of the pos-","chapter-17","Limits to Computation"
"sible subsets of edges in the graph, in parallel. If any subset of the edges is an","chapter-17","Limits to Computation"
"appropriate cycle of total length less than or equal to k, the answer is YES; oth-","chapter-17","Limits to Computation"
"erwise the answer is NO. Note that it is only necessary that some subset meet the","chapter-17","Limits to Computation"
"requirement; it does not matter how many subsets fail. Checking a particular sub-","chapter-17","Limits to Computation"
"set is done in polynomial time by adding the distances of the edges and verifying","chapter-17","Limits to Computation"
"that the edges form a cycle that visits each vertex exactly once. Thus, the checking","chapter-17","Limits to Computation"
"algorithm runs in polynomial time. Unfortunately, there are 2","chapter-17","Limits to Computation"
"|E|","chapter-17","Limits to Computation"
"subsets to check,","chapter-17","Limits to Computation"
"Sec. 17.2 Hard Problems 545","chapter-17","Limits to Computation"
"so this algorithm cannot be converted to a polynomial time algorithm on a regu-","chapter-17","Limits to Computation"
"lar computer. Nor does anybody in the world know of any other polynomial time","chapter-17","Limits to Computation"
"algorithm to solve TRAVELING SALESMAN on a regular computer, despite the","chapter-17","Limits to Computation"
"fact that the problem has been studied extensively by many computer scientists for","chapter-17","Limits to Computation"
"many years.","chapter-17","Limits to Computation"
"It turns out that there is a large collection of problems with this property: We","chapter-17","Limits to Computation"
"know efficient non-deterministic algorithms, but we do not know if there are effi-","chapter-17","Limits to Computation"
"cient deterministic algorithms. At the same time, we have not been able to prove","chapter-17","Limits to Computation"
"that any of these problems do not have efficient deterministic algorithms. This class","chapter-17","Limits to Computation"
"of problems is called N P-complete. What is truly strange and fascinating about","chapter-17","Limits to Computation"
"N P-complete problems is that if anybody ever finds the solution to any one of","chapter-17","Limits to Computation"
"them that runs in polynomial time on a regular computer, then by a series of reduc-","chapter-17","Limits to Computation"
"tions, every other problem that is in N P can also be solved in polynomial time on","chapter-17","Limits to Computation"
"a regular computer!","chapter-17","Limits to Computation"
"Define a problem to be N P-hard if any problem in N P can be reduced to X","chapter-17","Limits to Computation"
"in polynomial time. Thus, X is as hard as any problem in N P. A problem X is","chapter-17","Limits to Computation"
"defined to be N P-complete if","chapter-17","Limits to Computation"
"1. X is in N P, and","chapter-17","Limits to Computation"
"2. X is N P-hard.","chapter-17","Limits to Computation"
"The requirement that a problem be N P-hard might seem to be impossible, but","chapter-17","Limits to Computation"
"in fact there are hundreds of such problems, including TRAVELING SALESMAN.","chapter-17","Limits to Computation"
"Another such problem is called K-CLIQUE.","chapter-17","Limits to Computation"
"K-CLIQUE","chapter-17","Limits to Computation"
"Input: An arbitrary undirected graph G and an integer k.","chapter-17","Limits to Computation"
"Output: YES if there is a complete subgraph of at least k vertices, and NO","chapter-17","Limits to Computation"
"otherwise.","chapter-17","Limits to Computation"
"Nobody knows whether there is a polynomial time solution for K-CLIQUE, but if","chapter-17","Limits to Computation"
"such an algorithm is found for K-CLIQUE or for TRAVELING SALESMAN, then","chapter-17","Limits to Computation"
"that solution can be modified to solve the other, or any other problem in N P, in","chapter-17","Limits to Computation"
"polynomial time.","chapter-17","Limits to Computation"
"The primary theoretical advantage of knowing that a problem P1 is N P-comp-","chapter-17","Limits to Computation"
"lete is that it can be used to show that another problem P2 is N P-complete. This is","chapter-17","Limits to Computation"
"done by finding a polynomial time reduction of P1 to P2. Because we already know","chapter-17","Limits to Computation"
"that all problems in N P can be reduced to P1 in polynomial time (by the definition","chapter-17","Limits to Computation"
"of N P-complete), we now know that all problems can be reduced to P2 as well by","chapter-17","Limits to Computation"
"the simple algorithm of reducing to P1 and then from there reducing to P2.","chapter-17","Limits to Computation"
"There is a practical advantage to knowing that a problem is N P-complete. It","chapter-17","Limits to Computation"
"relates to knowing that if a polynomial time solution can be found for any prob-","chapter-17","Limits to Computation"
"546 Chap. 17 Limits to Computation","chapter-17","Limits to Computation"
"TOH","chapter-17","Limits to Computation"
"Exponential time problems","chapter-17","Limits to Computation"
"NP problems","chapter-17","Limits to Computation"
"NP−complete problems","chapter-17","Limits to Computation"
"TRAVELING SALESMAN","chapter-17","Limits to Computation"
"SORTING","chapter-17","Limits to Computation"
"P problems","chapter-17","Limits to Computation"
"Figure 17.5 Our knowledge regarding the world of problems requiring expo-","chapter-17","Limits to Computation"
"nential time or less. Some of these problems are solvable in polynomial time by a","chapter-17","Limits to Computation"
"non-deterministic computer. Of these, some are known to be N P-complete, and","chapter-17","Limits to Computation"
"some are known to be solvable in polynomial time on a regular computer.","chapter-17","Limits to Computation"
"lem that is N P-complete, then a polynomial solution can be found for all such","chapter-17","Limits to Computation"
"problems. The implication is that,","chapter-17","Limits to Computation"
"1. Because no one has yet found such a solution, it must be difficult or impos-","chapter-17","Limits to Computation"
"sible to do; and","chapter-17","Limits to Computation"
"2. Effort to find a polynomial time solution for one N P-complete problem can","chapter-17","Limits to Computation"
"be considered to have been expended for all N P-complete problems.","chapter-17","Limits to Computation"
"How is N P-completeness of practical significance for typical programmers?","chapter-17","Limits to Computation"
"Well, if your boss demands that you provide a fast algorithm to solve a problem,","chapter-17","Limits to Computation"
"she will not be happy if you come back saying that the best you could do was","chapter-17","Limits to Computation"
"an exponential time algorithm. But, if you can prove that the problem is N P-","chapter-17","Limits to Computation"
"complete, while she still won’t be happy, at least she should not be mad at you! By","chapter-17","Limits to Computation"
"showing that her problem is N P-complete, you are in effect saying that the most","chapter-17","Limits to Computation"
"brilliant computer scientists for the last 50 years have been trying and failing to find","chapter-17","Limits to Computation"
"a polynomial time algorithm for her problem.","chapter-17","Limits to Computation"
"Problems that are solvable in polynomial time on a regular computer are said","chapter-17","Limits to Computation"
"to be in class P. Clearly, all problems in P are solvable in polynomial time on","chapter-17","Limits to Computation"
"a non-deterministic computer simply by neglecting to use the non-deterministic","chapter-17","Limits to Computation"
"capability. Some problems in N P are N P-complete. We can consider all problems","chapter-17","Limits to Computation"
"solvable in exponential time or better as an even bigger class of problems because","chapter-17","Limits to Computation"
"all problems solvable in polynomial time are solvable in exponential time. Thus, we","chapter-17","Limits to Computation"
"can view the world of exponential-time-or-better problems in terms of Figure 17.5.","chapter-17","Limits to Computation"
"The most important unanswered question in theoretical computer science is","chapter-17","Limits to Computation"
"whether P = N P. If they are equal, then there is a polynomial time algorithm","chapter-17","Limits to Computation"
"Sec. 17.2 Hard Problems 547","chapter-17","Limits to Computation"
"for TRAVELING SALESMAN and all related problems. Because TRAVELING","chapter-17","Limits to Computation"
"SALESMAN is known to be N P-complete, if a polynomial time algorithm were to","chapter-17","Limits to Computation"
"be found for this problem, then all problems in N P would also be solvable in poly-","chapter-17","Limits to Computation"
"nomial time. Conversely, if we were able to prove that TRAVELING SALESMAN","chapter-17","Limits to Computation"
"has an exponential time lower bound, then we would know that P 6= N P.","chapter-17","Limits to Computation"
"17.2.2 N P-Completeness Proofs","chapter-17","Limits to Computation"
"To start the process of being able to prove problems are N P-complete, we need to","chapter-17","Limits to Computation"
"prove just one problem H is N P-complete. After that, to show that any problem","chapter-17","Limits to Computation"
"X is N P-hard, we just need to reduce H to X. When doing N P-completeness","chapter-17","Limits to Computation"
"proofs, it is very important not to get this reduction backwards! If we reduce can-","chapter-17","Limits to Computation"
"didate problem X to known hard problem H, this means that we use H as a step to","chapter-17","Limits to Computation"
"solving X. All that means is that we have found a (known) hard way to solve X.","chapter-17","Limits to Computation"
"However, when we reduce known hard problem H to candidate problem X, that","chapter-17","Limits to Computation"
"means we are using X as a step to solve H. And if we know that H is hard, that","chapter-17","Limits to Computation"
"means X must also be hard (because if X were not hard, then neither would H be","chapter-17","Limits to Computation"
"hard).","chapter-17","Limits to Computation"
"So a crucial first step to getting this whole theory off the ground is finding one","chapter-17","Limits to Computation"
"problem that is N P-hard. The first proof that a problem is N P-hard (and because","chapter-17","Limits to Computation"
"it is in N P, therefore N P-complete) was done by Stephen Cook. For this feat,","chapter-17","Limits to Computation"
"Cook won the first Turing award, which is the closest Computer Science equivalent","chapter-17","Limits to Computation"
"to the Nobel Prize. The “grand-daddy” N P-complete problem that Cook used is","chapter-17","Limits to Computation"
"call SATISFIABILITY (or SAT for short).","chapter-17","Limits to Computation"
"A Boolean expression includes Boolean variables combined using the opera-","chapter-17","Limits to Computation"
"tors AND (·), OR (+), and NOT (to negate Boolean variable x we write x). A","chapter-17","Limits to Computation"
"literal is a Boolean variable or its negation. A clause is one or more literals OR’ed","chapter-17","Limits to Computation"
"together. Let E be a Boolean expression over variables x1, x2, ..., xn. Then we","chapter-17","Limits to Computation"
"define Conjunctive Normal Form (CNF) to be a Boolean expression written as a","chapter-17","Limits to Computation"
"series of clauses that are AND’ed together. For example,","chapter-17","Limits to Computation"
"E = (x5 + x7 + x8 + x10) · (x2 + x3) · (x1 + x3 + x6)","chapter-17","Limits to Computation"
"is in CNF, and has three clauses. Now we can define the problem SAT.","chapter-17","Limits to Computation"
"SATISFIABILITY (SAT)","chapter-17","Limits to Computation"
"Input: A Boolean expression E over variables x1, x2, ... in Conjunctive Nor-","chapter-17","Limits to Computation"
"mal Form.","chapter-17","Limits to Computation"
"Output: YES if there is an assignment to the variables that makes E true,","chapter-17","Limits to Computation"
"NO otherwise.","chapter-17","Limits to Computation"
"Cook proved that SAT is N P-hard. Explaining Cook’s proof is beyond the","chapter-17","Limits to Computation"
"scope of this book. But we can briefly summarize it as follows. Any decision","chapter-17","Limits to Computation"
"548 Chap. 17 Limits to Computation","chapter-17","Limits to Computation"
"problem F can be recast as some language acceptance problem L:","chapter-17","Limits to Computation"
"F(I) = YES ⇔ L(I","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
") = ACCEPT.","chapter-17","Limits to Computation"
"That is, if a decision problem F yields YES on input I, then there is a language L","chapter-17","Limits to Computation"
"containing string I","chapter-17","Limits to Computation"
"0 where I","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
"is some suitable transformation of input I. Conversely,","chapter-17","Limits to Computation"
"if F would give answer NO for input I, then I’s transformed version I","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
"is not in the","chapter-17","Limits to Computation"
"language L.","chapter-17","Limits to Computation"
"Turing machines are a simple model of computation for writing programs that","chapter-17","Limits to Computation"
"are language acceptors. There is a “universal” Turing machine that can take as in-","chapter-17","Limits to Computation"
"put a description for a Turing machine, and an input string, and return the execution","chapter-17","Limits to Computation"
"of that machine on that string. This Turing machine in turn can be cast as a Boolean","chapter-17","Limits to Computation"
"expression such that the expression is satisfiable if and only if the Turing machine","chapter-17","Limits to Computation"
"yields ACCEPT for that string. Cook used Turing machines in his proof because","chapter-17","Limits to Computation"
"they are simple enough that he could develop this transformation of Turing ma-","chapter-17","Limits to Computation"
"chines to Boolean expressions, but rich enough to be able to compute any function","chapter-17","Limits to Computation"
"that a regular computer can compute. The significance of this transformation is that","chapter-17","Limits to Computation"
"any decision problem that is performable by the Turing machine is transformable","chapter-17","Limits to Computation"
"to SAT. Thus, SAT is N P-hard.","chapter-17","Limits to Computation"
"As explained above, to show that a decision problem X is N P-complete, we","chapter-17","Limits to Computation"
"prove that X is in N P (normally easy, and normally done by giving a suitable","chapter-17","Limits to Computation"
"polynomial-time, nondeterministic algorithm) and then prove that X is N P-hard.","chapter-17","Limits to Computation"
"To prove that X is N P-hard, we choose a known N P-complete problem, say A.","chapter-17","Limits to Computation"
"We describe a polynomial-time transformation that takes an arbitrary instance I of","chapter-17","Limits to Computation"
"A to an instance I","chapter-17","Limits to Computation"
"0 of X. We then describe a polynomial-time transformation from","chapter-17","Limits to Computation"
"SLN0","chapter-17","Limits to Computation"
"to SLN such that SLN is the solution for I. The following example provides a","chapter-17","Limits to Computation"
"model for how an N P-completeness proof is done.","chapter-17","Limits to Computation"
"3-SATISFIABILITY (3 SAT)","chapter-17","Limits to Computation"
"Input: A Boolean expression E in CNF such that each clause contains ex-","chapter-17","Limits to Computation"
"actly 3 literals.","chapter-17","Limits to Computation"
"Output: YES if the expression can be satisfied, NO otherwise.","chapter-17","Limits to Computation"
"Example 17.1 3 SAT is a special case of SAT. Is 3 SAT easier than SAT?","chapter-17","Limits to Computation"
"Not if we can prove it to be N P-complete.","chapter-17","Limits to Computation"
"Theorem 17.1 3 SAT is N P-complete.","chapter-17","Limits to Computation"
"Proof: Prove that 3 SAT is in N P: Guess (nondeterministically) truth","chapter-17","Limits to Computation"
"values for the variables. The correctness of the guess can be verified in","chapter-17","Limits to Computation"
"polynomial time.","chapter-17","Limits to Computation"
"Prove that 3 SAT is N P-hard: We need a polynomial-time reduction","chapter-17","Limits to Computation"
"from SAT to 3 SAT. Let E = C1 · C2 · ... · Ck be any instance of SAT. Our","chapter-17","Limits to Computation"
"Sec. 17.2 Hard Problems 549","chapter-17","Limits to Computation"
"strategy is to replace any clause Ci","chapter-17","Limits to Computation"
"that does not have exactly three literals","chapter-17","Limits to Computation"
"with a set of clauses each having exactly three literals. (Recall that a literal","chapter-17","Limits to Computation"
"can be a variable such as x, or the negation of a variable such as x.) Let","chapter-17","Limits to Computation"
"Ci = x1 + x2 + ... + xj where x1, ..., xj are literals.","chapter-17","Limits to Computation"
"1. j = 1, so Ci = x1. Replace Ci with C","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
"i","chapter-17","Limits to Computation"
":","chapter-17","Limits to Computation"
"(x1 + y + z) · (x1 + y + z) · (x1 + y + z) · (x1 + y + z)","chapter-17","Limits to Computation"
"where y and z are variables not appearing in E. Clearly, C","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
"i","chapter-17","Limits to Computation"
"is satisfi-","chapter-17","Limits to Computation"
"able if and only if (x1) is satisfiable, meaning that x1 is true.","chapter-17","Limits to Computation"
"2. J = 2, so Ci = (x1 + x2). Replace Ci with","chapter-17","Limits to Computation"
"(x1 + x2 + z) · (x1 + x2 + z)","chapter-17","Limits to Computation"
"where z is a new variable not appearing in E. This new pair of clauses","chapter-17","Limits to Computation"
"is satisfiable if and only if (x1 + x2) is satisfiable, that is, either x1 or","chapter-17","Limits to Computation"
"x2 must be true.","chapter-17","Limits to Computation"
"3. j > 3. Replace Ci = (x1 + x2 + · · · + xj ) with","chapter-17","Limits to Computation"
"(x1 + x2 + z1) · (x3 + z1 + z2) · (x4 + z2 + z3) · ...","chapter-17","Limits to Computation"
"·(xj−2 + zj−4 + zj−3) · (xj−1 + xj + zj−3)","chapter-17","Limits to Computation"
"where z1, ..., zj−3 are new variables.","chapter-17","Limits to Computation"
"After appropriate replacements have been made for each Ci","chapter-17","Limits to Computation"
", a Boolean","chapter-17","Limits to Computation"
"expression results that is an instance of 3 SAT. Each replacement is satisfi-","chapter-17","Limits to Computation"
"able if and only if the original clause is satisfiable. The reduction is clearly","chapter-17","Limits to Computation"
"polynomial time.","chapter-17","Limits to Computation"
"For the first two cases it is fairly easy to see that the original clause","chapter-17","Limits to Computation"
"is satisfiable if and only if the resulting clauses are satisfiable. For the","chapter-17","Limits to Computation"
"case were we replaced a clause with more than three literals, consider the","chapter-17","Limits to Computation"
"following.","chapter-17","Limits to Computation"
"1. If E is satisfiable, then E0","chapter-17","Limits to Computation"
"is satisfiable: Assume xm is assigned","chapter-17","Limits to Computation"
"true. Then assign zt","chapter-17","Limits to Computation"
", t ≤ m − 2 as true and zk, t ≥ m − 1 as","chapter-17","Limits to Computation"
"false. Then all clauses in Case (3) are satisfied.","chapter-17","Limits to Computation"
"2. If x1, x2, ..., xj are all false, then z1, z2, ..., zj−3 are all true. But","chapter-17","Limits to Computation"
"then (xj−1 + xj−2 + zj−3) is false.","chapter-17","Limits to Computation"
"✷","chapter-17","Limits to Computation"
"Next we define the problem VERTEX COVER for use in further examples.","chapter-17","Limits to Computation"
"VERTEX COVER:","chapter-17","Limits to Computation"
"Input: A graph G and an integer k.","chapter-17","Limits to Computation"
"Output: YES if there is a subset S of the vertices in G of size k or less such","chapter-17","Limits to Computation"
"that every edge of G has at least one of its endpoints in S, and NO otherwise.","chapter-17","Limits to Computation"
"550 Chap. 17 Limits to Computation","chapter-17","Limits to Computation"
"Example 17.2 In this example, we make use of a simple conversion be-","chapter-17","Limits to Computation"
"tween two graph problems.","chapter-17","Limits to Computation"
"Theorem 17.2 VERTEX COVER is N P-complete.","chapter-17","Limits to Computation"
"Proof: Prove that VERTEX COVER is in N P: Simply guess a subset","chapter-17","Limits to Computation"
"of the graph and determine in polynomial time whether that subset is in fact","chapter-17","Limits to Computation"
"a vertex cover of size k or less.","chapter-17","Limits to Computation"
"Prove that VERTEX COVER is N P-hard: We will assume that K-","chapter-17","Limits to Computation"
"CLIQUE is already known to be N P-complete. (We will see this proof in","chapter-17","Limits to Computation"
"the next example. For now, just accept that it is true.)","chapter-17","Limits to Computation"
"Given that K-CLIQUE is N P-complete, we need to find a polynomial-","chapter-17","Limits to Computation"
"time transformation from the input to K-CLIQUE to the input to VERTEX","chapter-17","Limits to Computation"
"COVER, and another polynomial-time transformation from the output for","chapter-17","Limits to Computation"
"VERTEX COVER to the output for K-CLIQUE. This turns out to be a","chapter-17","Limits to Computation"
"simple matter, given the following observation. Consider a graph G and","chapter-17","Limits to Computation"
"a vertex cover S on G. Denote by S","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
"the set of vertices in G but not in S.","chapter-17","Limits to Computation"
"There can be no edge connecting any two vertices in S","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
"because, if there","chapter-17","Limits to Computation"
"were, then S would not be a vertex cover. Denote by G","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
"the inverse graph","chapter-17","Limits to Computation"
"for G, that is, the graph formed from the edges not in G. If S is of size","chapter-17","Limits to Computation"
"k, then S","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
"forms a clique of size n − k in graph G","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
". Thus, we can reduce","chapter-17","Limits to Computation"
"K-CLIQUE to VERTEX COVER simply by converting graph G to G","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
", and","chapter-17","Limits to Computation"
"asking if G","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
"has a VERTEX COVER of size n − k or smaller. If YES, then","chapter-17","Limits to Computation"
"there is a clique in G of size k; if NO then there is not. ✷","chapter-17","Limits to Computation"
"Example 17.3 So far, our N P-completeness proofs have involved trans-","chapter-17","Limits to Computation"
"formations between inputs of the same “type,” such as from a Boolean ex-","chapter-17","Limits to Computation"
"pression to a Boolean expression or from a graph to a graph. Sometimes an","chapter-17","Limits to Computation"
"N P-completeness proof involves a transformation between types of inputs,","chapter-17","Limits to Computation"
"as shown next.","chapter-17","Limits to Computation"
"Theorem 17.3 K-CLIQUE is N P-complete.","chapter-17","Limits to Computation"
"Proof: K-CLIQUE is in N P, because we can just guess a collection of k","chapter-17","Limits to Computation"
"vertices and test in polynomial time if it is a clique. Now we show that K-","chapter-17","Limits to Computation"
"CLIQUE is N P-hard by using a reduction from SAT. An instance of SAT","chapter-17","Limits to Computation"
"is a Boolean expression","chapter-17","Limits to Computation"
"B = C1 · C2 · ... · Cm","chapter-17","Limits to Computation"
"whose clauses we will describe by the notation","chapter-17","Limits to Computation"
"Ci = y[i, 1] + y[i, 2] + ... + y[i, ki","chapter-17","Limits to Computation"
"]","chapter-17","Limits to Computation"
"Sec. 17.2 Hard Problems 551","chapter-17","Limits to Computation"
"x1","chapter-17","Limits to Computation"
"x","chapter-17","Limits to Computation"
"x 1","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
"x2","chapter-17","Limits to Computation"
"C1 C2 C3","chapter-17","Limits to Computation"
"x3","chapter-17","Limits to Computation"
"x3","chapter-17","Limits to Computation"
"x1","chapter-17","Limits to Computation"
"Figure 17.6 The graph generated from Boolean expression B = (x1+x2)·(x1+","chapter-17","Limits to Computation"
"x2 +x3)·(x1 +x3). Literals from the first clause are labeled C1, and literals from","chapter-17","Limits to Computation"
"the second clause are labeled C2. There is an edge between every pair of vertices","chapter-17","Limits to Computation"
"except when both vertices represent instances of literals from the same clause, or","chapter-17","Limits to Computation"
"a negation of the same variable. Thus, the vertex labeled C1:y1 does not connect","chapter-17","Limits to Computation"
"to the vertex labeled C1 : y2 (because they are literals in the same clause) or the","chapter-17","Limits to Computation"
"vertex labeled C2:y1 (because they are opposite values for the same variable).","chapter-17","Limits to Computation"
"where ki","chapter-17","Limits to Computation"
"is the number of literals in Clause ci","chapter-17","Limits to Computation"
". We will transform this to an","chapter-17","Limits to Computation"
"instance of K-CLIQUE as follows. We build a graph","chapter-17","Limits to Computation"
"G = {v[i, j]|1 ≤ i ≤ m, 1 ≤ j ≤ ki},","chapter-17","Limits to Computation"
"that is, there is a vertex in G corresponding to every literal in Boolean","chapter-17","Limits to Computation"
"expression B. We will draw an edge between each pair of vertices v[i1, j1]","chapter-17","Limits to Computation"
"and v[i2, j2] unless (1) they are two literals within the same clause (i1 = i2)","chapter-17","Limits to Computation"
"or (2) they are opposite values for the same variable (i.e., one is negated","chapter-17","Limits to Computation"
"and the other is not). Set k = m. Figure 17.6 shows an example of this","chapter-17","Limits to Computation"
"transformation.","chapter-17","Limits to Computation"
"B is satisfiable if and only if G has a clique of size k or greater. B being","chapter-17","Limits to Computation"
"satisfiable implies that there is a truth assignment such that at least one","chapter-17","Limits to Computation"
"literal y[i, ji","chapter-17","Limits to Computation"
"] is true for each i. If so, then these m literals must correspond","chapter-17","Limits to Computation"
"to m vertices in a clique of size k = m. Conversely, if G has a clique of","chapter-17","Limits to Computation"
"size k or greater, then the clique must have size exactly k (because no two","chapter-17","Limits to Computation"
"vertices corresponding to literals in the same clause can be in the clique)","chapter-17","Limits to Computation"
"and there is one vertex v[i, ji","chapter-17","Limits to Computation"
"] in the clique for each i. There is a truth","chapter-17","Limits to Computation"
"assignment making each y[i, ji","chapter-17","Limits to Computation"
"] true. That truth assignment satisfies B.","chapter-17","Limits to Computation"
"We conclude that K-CLIQUE is N P-hard, therefore N P-complete. ✷","chapter-17","Limits to Computation"
"552 Chap. 17 Limits to Computation","chapter-17","Limits to Computation"
"17.2.3 Coping with N P-Complete Problems","chapter-17","Limits to Computation"
"Finding that your problem is N P-complete might not mean that you can just forget","chapter-17","Limits to Computation"
"about it. Traveling salesmen need to find reasonable sales routes regardless of the","chapter-17","Limits to Computation"
"complexity of the problem. What do you do when faced with an N P-complete","chapter-17","Limits to Computation"
"problem that you must solve?","chapter-17","Limits to Computation"
"There are several techniques to try. One approach is to run only small instances","chapter-17","Limits to Computation"
"of the problem. For some problems, this is not acceptable. For example, TRAVEL-","chapter-17","Limits to Computation"
"ING SALESMAN grows so quickly that it cannot be run on modern computers for","chapter-17","Limits to Computation"
"problem sizes much over 30 cities, which is not an unreasonable problem size for","chapter-17","Limits to Computation"
"real-life situations. However, some other problems in N P, while requiring expo-","chapter-17","Limits to Computation"
"nential time, still grow slowly enough that they allow solutions for problems of a","chapter-17","Limits to Computation"
"useful size.","chapter-17","Limits to Computation"
"Consider the Knapsack problem from Section 16.1.1. We have a dynamic pro-","chapter-17","Limits to Computation"
"gramming algorithm whose cost is Θ(nK) for n objects being fit into a knapsack of","chapter-17","Limits to Computation"
"size K. But it turns out that Knapsack is N P-complete. Isn’t this a contradiction?","chapter-17","Limits to Computation"
"Not when we consider the relationship between n and K. How big is K? Input size","chapter-17","Limits to Computation"
"is typically O(n lg K) because the item sizes are smaller than K. Thus, Θ(nK) is","chapter-17","Limits to Computation"
"exponential on input size.","chapter-17","Limits to Computation"
"This dynamic programming algorithm is tractable if the numbers are “reason-","chapter-17","Limits to Computation"
"able.” That is, we can successfully find solutions to the problem when nK is in","chapter-17","Limits to Computation"
"the thousands. Such an algorithm is called a pseudo-polynomial time algorithm.","chapter-17","Limits to Computation"
"This is different from TRAVELING SALESMAN which cannot possibly be solved","chapter-17","Limits to Computation"
"when n = 100 given current algorithms.","chapter-17","Limits to Computation"
"A second approach to handling N P-complete problems is to solve a special","chapter-17","Limits to Computation"
"instance of the problem that is not so hard. For example, many problems on graphs","chapter-17","Limits to Computation"
"are N P-complete, but the same problem on certain restricted types of graphs is","chapter-17","Limits to Computation"
"not as difficult. For example, while the VERTEX COVER and K-CLIQUE prob-","chapter-17","Limits to Computation"
"lems are N P-complete in general, there are polynomial time solutions for bipar-","chapter-17","Limits to Computation"
"tite graphs (i.e., graphs whose vertices can be separated into two subsets such","chapter-17","Limits to Computation"
"that no pair of vertices within one of the subsets has an edge between them). 2-","chapter-17","Limits to Computation"
"SATISFIABILITY (where every clause in a Boolean expression has at most two","chapter-17","Limits to Computation"
"literals) has a polynomial time solution. Several geometric problems require only","chapter-17","Limits to Computation"
"polynomial time in two dimensions, but are N P-complete in three dimensions or","chapter-17","Limits to Computation"
"more. KNAPSACK is considered to run in polynomial time if the numbers (and","chapter-17","Limits to Computation"
"K) are “small.” Small here means that they are polynomial on n, the number of","chapter-17","Limits to Computation"
"items.","chapter-17","Limits to Computation"
"In general, if we want to guarantee that we get the correct answer for an N P-","chapter-17","Limits to Computation"
"complete problem, we potentially need to examine all of the (exponential number","chapter-17","Limits to Computation"
"of) possible solutions. However, with some organization, we might be able to either","chapter-17","Limits to Computation"
"examine them quickly, or avoid examining a great many of the possible answers","chapter-17","Limits to Computation"
"in some cases. For example, Dynamic Programming (Section 16.1) attempts to","chapter-17","Limits to Computation"
"Sec. 17.2 Hard Problems 553","chapter-17","Limits to Computation"
"organize the processing of all the subproblems to a problem so that the work is","chapter-17","Limits to Computation"
"done efficiently.","chapter-17","Limits to Computation"
"If we need to do a brute-force search of the entire solution space, we can use","chapter-17","Limits to Computation"
"backtracking to visit all of the possible solutions organized in a solution tree. For","chapter-17","Limits to Computation"
"example, SATISFIABILITY has 2","chapter-17","Limits to Computation"
"n possible ways to assign truth values to the n","chapter-17","Limits to Computation"
"variables contained in the Boolean expression being satisfied. We can view this as","chapter-17","Limits to Computation"
"a tree of solutions by considering that we have a choice of making the first variable","chapter-17","Limits to Computation"
"true or false. Thus, we can put all solutions where the first variable is true on","chapter-17","Limits to Computation"
"one side of the tree, and the remaining solutions on the other. We then examine the","chapter-17","Limits to Computation"
"solutions by moving down one branch of the tree, until we reach a point where we","chapter-17","Limits to Computation"
"know the solution cannot be correct (such as if the current partial collection of as-","chapter-17","Limits to Computation"
"signments yields an unsatisfiable expression). At this point we backtrack and move","chapter-17","Limits to Computation"
"back up a node in the tree, and then follow down the alternate branch. If this fails,","chapter-17","Limits to Computation"
"we know to back up further in the tree as necessary and follow alternate branches,","chapter-17","Limits to Computation"
"until finally we either find a solution that satisfies the expression or exhaust the","chapter-17","Limits to Computation"
"tree. In some cases we avoid processing many potential solutions, or find a solution","chapter-17","Limits to Computation"
"quickly. In others, we end up visiting a large portion of the 2","chapter-17","Limits to Computation"
"n possible solutions.","chapter-17","Limits to Computation"
"Banch-and-Bounds is an extension of backtracking that applies to optimiza-","chapter-17","Limits to Computation"
"tion problems such as TRAVELING SALESMAN where we are trying to find the","chapter-17","Limits to Computation"
"shortest tour through the cities. We traverse the solution tree as with backtrack-","chapter-17","Limits to Computation"
"ing. However, we remember the best value found so far. Proceeding down a given","chapter-17","Limits to Computation"
"branch is equivalent to deciding which order to visit cities. So any node in the so-","chapter-17","Limits to Computation"
"lution tree represents some collection of cities visited so far. If the sum of these","chapter-17","Limits to Computation"
"distances exceeds the best tour found so far, then we know to stop pursuing this","chapter-17","Limits to Computation"
"branch of the tree. At this point we can immediately back up and take another","chapter-17","Limits to Computation"
"branch. If we have a quick method for finding a good (but not necessarily best)","chapter-17","Limits to Computation"
"solution, we can use this as an initial bound value to effectively prune portions of","chapter-17","Limits to Computation"
"the tree.","chapter-17","Limits to Computation"
"Another coping strategy is to find an approximate solution to the problem.","chapter-17","Limits to Computation"
"There are many approaches to finding approximate solutions. One way is to use","chapter-17","Limits to Computation"
"a heuristic to solve the problem, that is, an algorithm based on a “rule of thumb”","chapter-17","Limits to Computation"
"that does not always give the best answer. For example, the TRAVELING SALES-","chapter-17","Limits to Computation"
"MAN problem can be solved approximately by using the heuristic that we start at","chapter-17","Limits to Computation"
"an arbitrary city and then always proceed to the next unvisited city that is closest.","chapter-17","Limits to Computation"
"This rarely gives the shortest path, but the solution might be good enough. There","chapter-17","Limits to Computation"
"are many other heuristics for TRAVELING SALESMAN that do a better job.","chapter-17","Limits to Computation"
"Some approximation algorithms have guaranteed performance, such that the","chapter-17","Limits to Computation"
"answer will be within a certain percentage of the best possible answer. For exam-","chapter-17","Limits to Computation"
"ple, consider this simple heuristic for the VERTEX COVER problem: Let M be","chapter-17","Limits to Computation"
"a maximal (not necessarily maximum) matching in G. A matching pairs vertices","chapter-17","Limits to Computation"
"(with connecting edges) so that no vertex is paired with more than one partner.","chapter-17","Limits to Computation"
"554 Chap. 17 Limits to Computation","chapter-17","Limits to Computation"
"Maximal means to pick as many pairs as possible, selecting them in some order un-","chapter-17","Limits to Computation"
"til there are no more available pairs to select. Maximum means the matching that","chapter-17","Limits to Computation"
"gives the most pairs possible for a given graph. If OPT is the size of a minimum","chapter-17","Limits to Computation"
"vertex cover, then |M| ≤ 2 · OPT because at least one endpoint of every matched","chapter-17","Limits to Computation"
"edge must be in any vertex cover.","chapter-17","Limits to Computation"
"A better example of a guaranteed bound on a solution comes from simple","chapter-17","Limits to Computation"
"heuristics to solve the BIN PACKING problem.","chapter-17","Limits to Computation"
"BIN PACKING:","chapter-17","Limits to Computation"
"Input: Numbers x1, x2, ..., xn between 0 and 1, and an unlimited supply of","chapter-17","Limits to Computation"
"bins of size 1 (no bin can hold numbers whose sum exceeds 1).","chapter-17","Limits to Computation"
"Output: An assignment of numbers to bins that requires the fewest possible","chapter-17","Limits to Computation"
"bins.","chapter-17","Limits to Computation"
"BIN PACKING in its decision form (i.e., asking if the items can be packed in","chapter-17","Limits to Computation"
"less than k bins) is known to be N P-complete. One simple heuristic for solving","chapter-17","Limits to Computation"
"this problem is to use a “first fit” approach. We put the first number in the first","chapter-17","Limits to Computation"
"bin. We then put the second number in the first bin if it fits, otherwise we put it in","chapter-17","Limits to Computation"
"the second bin. For each subsequent number, we simply go through the bins in the","chapter-17","Limits to Computation"
"order we generated them and place the number in the first bin that fits. The number","chapter-17","Limits to Computation"
"of bins used is no more than twice the sum of the numbers, because every bin","chapter-17","Limits to Computation"
"(except perhaps one) must be at least half full. However, this “first fit” heuristic can","chapter-17","Limits to Computation"
"give us a result that is much worse than optimal. Consider the following collection","chapter-17","Limits to Computation"
"of numbers: 6 of 1/7 +, 6 of 1/3 +, and 6 of 1/2 +, where  is a small, positive","chapter-17","Limits to Computation"
"number. Properly organized, this requires 6 bins. But if done wrongly, we might","chapter-17","Limits to Computation"
"end up putting the numbers into 10 bins.","chapter-17","Limits to Computation"
"A better heuristic is to use decreasing first fit. This is the same as first fit, except","chapter-17","Limits to Computation"
"that we keep the bins sorted from most full to least full. Then when deciding where","chapter-17","Limits to Computation"
"to put the next item, we place it in the fullest bin that can hold it. This is similar to","chapter-17","Limits to Computation"
"the “best fit” heuristic for memory management discussed in Section 12.3. The sig-","chapter-17","Limits to Computation"
"nificant thing about this heuristic is not just that it tends to give better performance","chapter-17","Limits to Computation"
"than simple first fit. This decreasing first fit heuristic can be proven to require no","chapter-17","Limits to Computation"
"more than 11/9 the optimal number of bins. Thus, we have a guarantee on how","chapter-17","Limits to Computation"
"much inefficiency can result when using the heuristic.","chapter-17","Limits to Computation"
"The theory of N P-completeness gives a technique for separating tractable from","chapter-17","Limits to Computation"
"(probably) intractable problems. Recalling the algorithm for generating algorithms","chapter-17","Limits to Computation"
"in Section 15.1, we can refine it for problems that we suspect are N P-complete.","chapter-17","Limits to Computation"
"When faced with a new problem, we might alternate between checking if it is","chapter-17","Limits to Computation"
"tractable (that is, we try to find a polynomial-time solution) and checking if it is","chapter-17","Limits to Computation"
"intractable (we try to prove the problem is N P-complete). While proving that","chapter-17","Limits to Computation"
"some problem is N P-complete does not actually make our upper bound for our","chapter-17","Limits to Computation"
"Sec. 17.3 Impossible Problems 555","chapter-17","Limits to Computation"
"algorithm match the lower bound for the problem with certainty, it is nearly as","chapter-17","Limits to Computation"
"good. Once we realize that a problem is N P-complete, then we know that our next","chapter-17","Limits to Computation"
"step must either be to redefine the problem to make it easier, or else use one of the","chapter-17","Limits to Computation"
"“coping” strategies discussed in this section.","chapter-17","Limits to Computation"
"17.3 Impossible Problems","chapter-17","Limits to Computation"
"Even the best programmer sometimes writes a program that goes into an infinite","chapter-17","Limits to Computation"
"loop. Of course, when you run a program that has not stopped, you do not know","chapter-17","Limits to Computation"
"for sure if it is just a slow program or a program in an infinite loop. After “enough","chapter-17","Limits to Computation"
"time,” you shut it down. Wouldn’t it be great if your compiler could look at your","chapter-17","Limits to Computation"
"program and tell you before you run it that it will get into an infinite loop? To be","chapter-17","Limits to Computation"
"more specific, given a program and a particular input, it would be useful to know if","chapter-17","Limits to Computation"
"executing the program on that input will result in an infinite loop without actually","chapter-17","Limits to Computation"
"running the program.","chapter-17","Limits to Computation"
"Unfortunately, the Halting Problem, as this is called, cannot be solved. There","chapter-17","Limits to Computation"
"will never be a computer program that can positively determine, for an arbitrary","chapter-17","Limits to Computation"
"program P, if P will halt for all input. Nor will there even be a computer program","chapter-17","Limits to Computation"
"that can positively determine if arbitrary program P will halt for a specified input I.","chapter-17","Limits to Computation"
"How can this be? Programmers look at programs regularly to determine if they will","chapter-17","Limits to Computation"
"halt. Surely this can be automated. As a warning to those who believe any program","chapter-17","Limits to Computation"
"can be analyzed in this way, carefully examine the following code fragment before","chapter-17","Limits to Computation"
"reading on.","chapter-17","Limits to Computation"
"while (n > 1)","chapter-17","Limits to Computation"
"if (ODD(n))","chapter-17","Limits to Computation"
"n = 3 * n + 1;","chapter-17","Limits to Computation"
"else","chapter-17","Limits to Computation"
"n = n / 2;","chapter-17","Limits to Computation"
"This is a famous piece of code. The sequence of values that is assigned to n","chapter-17","Limits to Computation"
"by this code is sometimes called the Collatz sequence for input value n. Does","chapter-17","Limits to Computation"
"this code fragment halt for all values of n? Nobody knows the answer. Every","chapter-17","Limits to Computation"
"input that has been tried halts. But does it always halt? Note that for this code","chapter-17","Limits to Computation"
"fragment, because we do not know if it halts, we also do not know an upper bound","chapter-17","Limits to Computation"
"for its running time. As for the lower bound, we can easily show Ω(log n) (see","chapter-17","Limits to Computation"
"Exercise 3.14).","chapter-17","Limits to Computation"
"Personally, I have faith that someday some smart person will completely ana-","chapter-17","Limits to Computation"
"lyze the Collatz function, proving once and for all that the code fragment halts for","chapter-17","Limits to Computation"
"all values of n. Doing so may well give us techniques that advance our ability to","chapter-17","Limits to Computation"
"do algorithm analysis in general. Unfortunately, proofs from computability — the","chapter-17","Limits to Computation"
"branch of computer science that studies what is impossible to do with a computer","chapter-17","Limits to Computation"
"— compel us to believe that there will always be another bit of program code that","chapter-17","Limits to Computation"
"556 Chap. 17 Limits to Computation","chapter-17","Limits to Computation"
"we cannot analyze. This comes as a result of the fact that the Halting Problem is","chapter-17","Limits to Computation"
"unsolvable.","chapter-17","Limits to Computation"
"17.3.1 Uncountability","chapter-17","Limits to Computation"
"Before proving that the Halting Problem is unsolvable, we first prove that not all","chapter-17","Limits to Computation"
"functions can be implemented as a computer program. This must be so because the","chapter-17","Limits to Computation"
"number of programs is much smaller than the number of possible functions.","chapter-17","Limits to Computation"
"A set is said to be countable (or countably infinite if it is a set with an infinite","chapter-17","Limits to Computation"
"number of members) if every member of the set can be uniquely assigned to a","chapter-17","Limits to Computation"
"positive integer. A set is said to be uncountable (or uncountably infinite) if it is","chapter-17","Limits to Computation"
"not possible to assign every member of the set to its own positive integer.","chapter-17","Limits to Computation"
"To understand what is meant when we say “assigned to a positive integer,”","chapter-17","Limits to Computation"
"imagine that there is an infinite row of bins, labeled 1, 2, 3, and so on. Take a set","chapter-17","Limits to Computation"
"and start placing members of the set into bins, with at most one member per bin. If","chapter-17","Limits to Computation"
"we can find a way to assign all of the set members to bins, then the set is countable.","chapter-17","Limits to Computation"
"For example, consider the set of positive even integers 2, 4, and so on. We can","chapter-17","Limits to Computation"
"assign an integer i to bin i/2 (or, if we don’t mind skipping some bins, then we can","chapter-17","Limits to Computation"
"assign even number i to bin i). Thus, the set of even integers is countable. This","chapter-17","Limits to Computation"
"should be no surprise, because intuitively there are “fewer” positive even integers","chapter-17","Limits to Computation"
"than there are positive integers, even though both are infinite sets. But there are not","chapter-17","Limits to Computation"
"really any more positive integers than there are positive even integers, because we","chapter-17","Limits to Computation"
"can uniquely assign every positive integer to some positive even integer by simply","chapter-17","Limits to Computation"
"assigning positive integer i to positive even integer 2i.","chapter-17","Limits to Computation"
"On the other hand, the set of all integers is also countable, even though this set","chapter-17","Limits to Computation"
"appears to be “bigger” than the set of positive integers. This is true because we can","chapter-17","Limits to Computation"
"assign 0 to positive integer 1, 1 to positive integer 2, -1 to positive integer 3, 2 to","chapter-17","Limits to Computation"
"positive integer 4, -2 to positive integer 5, and so on. In general, assign positive","chapter-17","Limits to Computation"
"integer value i to positive integer value 2i, and assign negative integer value −i to","chapter-17","Limits to Computation"
"positive integer value 2i + 1. We will never run out of positive integers to assign,","chapter-17","Limits to Computation"
"and we know exactly which positive integer every integer is assigned to. Because","chapter-17","Limits to Computation"
"every integer gets an assignment, the set of integers is countably infinite.","chapter-17","Limits to Computation"
"Are the number of programs countable or uncountable? A program can be","chapter-17","Limits to Computation"
"viewed as simply a string of characters (including special punctuation, spaces, and","chapter-17","Limits to Computation"
"line breaks). Let us assume that the number of different characters that can appear","chapter-17","Limits to Computation"
"in a program is P. (Using the ASCII character set, P must be less than 128, but","chapter-17","Limits to Computation"
"the actual number does not matter). If the number of strings is countable, then","chapter-17","Limits to Computation"
"surely the number of programs is also countable. We can assign strings to the","chapter-17","Limits to Computation"
"bins as follows. Assign the null string to the first bin. Now, take all strings of","chapter-17","Limits to Computation"
"one character, and assign them to the next P bins in “alphabetic” or ASCII code","chapter-17","Limits to Computation"
"order. Next, take all strings of two characters, and assign them to the next P","chapter-17","Limits to Computation"
"2 bins,","chapter-17","Limits to Computation"
"again in ASCII code order working from left to right. Strings of three characters","chapter-17","Limits to Computation"
"Sec. 17.3 Impossible Problems 557","chapter-17","Limits to Computation"
"are likewise assigned to bins, then strings of length four, and so on. In this way, a","chapter-17","Limits to Computation"
"string of any given length can be assigned to some bin.","chapter-17","Limits to Computation"
"By this process, any string of finite length is assigned to some bin. So any pro-","chapter-17","Limits to Computation"
"gram, which is merely a string of finite length, is assigned to some bin. Because all","chapter-17","Limits to Computation"
"programs are assigned to some bin, the set of all programs is countable. Naturally","chapter-17","Limits to Computation"
"most of the strings in the bins are not legal programs, but this is irrelevant. All that","chapter-17","Limits to Computation"
"matters is that the strings that do correspond to programs are also in the bins.","chapter-17","Limits to Computation"
"Now we consider the number of possible functions. To keep things simple,","chapter-17","Limits to Computation"
"assume that all functions take a single positive integer as input and yield a sin-","chapter-17","Limits to Computation"
"gle positive integer as output. We will call such functions integer functions. A","chapter-17","Limits to Computation"
"function is simply a mapping from input values to output values. Of course, not","chapter-17","Limits to Computation"
"all computer programs literally take integers as input and yield integers as output.","chapter-17","Limits to Computation"
"However, everything that computers read and write is essentially a series of num-","chapter-17","Limits to Computation"
"bers, which may be interpreted as letters or something else. Any useful computer","chapter-17","Limits to Computation"
"program’s input and output can be coded as integer values, so our simple model","chapter-17","Limits to Computation"
"of computer input and output is sufficiently general to cover all possible computer","chapter-17","Limits to Computation"
"programs.","chapter-17","Limits to Computation"
"We now wish to see if it is possible to assign all of the integer functions to the","chapter-17","Limits to Computation"
"infinite set of bins. If so, then the number of functions is countable, and it might","chapter-17","Limits to Computation"
"then be possible to assign every integer function to a program. If the set of integer","chapter-17","Limits to Computation"
"functions cannot be assigned to bins, then there will be integer functions that must","chapter-17","Limits to Computation"
"have no corresponding program.","chapter-17","Limits to Computation"
"Imagine each integer function as a table with two columns and an infinite num-","chapter-17","Limits to Computation"
"ber of rows. The first column lists the positive integers starting at 1. The second","chapter-17","Limits to Computation"
"column lists the output of the function when given the value in the first column","chapter-17","Limits to Computation"
"as input. Thus, the table explicitly describes the mapping from input to output for","chapter-17","Limits to Computation"
"each function. Call this a function table.","chapter-17","Limits to Computation"
"Next we will try to assign function tables to bins. To do so we must order the","chapter-17","Limits to Computation"
"functions, but it does not matter what order we choose. For example, Bin 1 could","chapter-17","Limits to Computation"
"store the function that always returns 1 regardless of the input value. Bin 2 could","chapter-17","Limits to Computation"
"store the function that returns its input. Bin 3 could store the function that doubles","chapter-17","Limits to Computation"
"its input and adds 5. Bin 4 could store a function for which we can see no simple","chapter-17","Limits to Computation"
"relationship between input and output.2 These four functions as assigned to the first","chapter-17","Limits to Computation"
"four bins are shown in Figure 17.7.","chapter-17","Limits to Computation"
"Can we assign every function to a bin? The answer is no, because there is","chapter-17","Limits to Computation"
"always a way to create a new function that is not in any of the bins. Suppose that","chapter-17","Limits to Computation"
"somebody presents a way of assigning functions to bins that they claim includes","chapter-17","Limits to Computation"
"all of the functions. We can build a new function that has not been assigned to","chapter-17","Limits to Computation"
"2There is no requirement for a function to have any discernible relationship between input and","chapter-17","Limits to Computation"
"output. A function is simply a mapping of inputs to outputs, with no constraint on how the mapping","chapter-17","Limits to Computation"
"is determined.","chapter-17","Limits to Computation"
"558 Chap. 17 Limits to Computation","chapter-17","Limits to Computation"
"f1","chapter-17","Limits to Computation"
"(x) f2","chapter-17","Limits to Computation"
"(x) f3","chapter-17","Limits to Computation"
"(x) f4","chapter-17","Limits to Computation"
"(x)","chapter-17","Limits to Computation"
"1","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
"3","chapter-17","Limits to Computation"
"4","chapter-17","Limits to Computation"
"5","chapter-17","Limits to Computation"
"6","chapter-17","Limits to Computation"
"1","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
"3","chapter-17","Limits to Computation"
"4","chapter-17","Limits to Computation"
"5","chapter-17","Limits to Computation"
"6","chapter-17","Limits to Computation"
"1","chapter-17","Limits to Computation"
"1","chapter-17","Limits to Computation"
"1","chapter-17","Limits to Computation"
"1","chapter-17","Limits to Computation"
"1","chapter-17","Limits to Computation"
"1 6","chapter-17","Limits to Computation"
"5","chapter-17","Limits to Computation"
"4","chapter-17","Limits to Computation"
"3","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
"1","chapter-17","Limits to Computation"
"1 2 3 4 5","chapter-17","Limits to Computation"
"7","chapter-17","Limits to Computation"
"9","chapter-17","Limits to Computation"
"11","chapter-17","Limits to Computation"
"13","chapter-17","Limits to Computation"
"15","chapter-17","Limits to Computation"
"17","chapter-17","Limits to Computation"
"15","chapter-17","Limits to Computation"
"1","chapter-17","Limits to Computation"
"7","chapter-17","Limits to Computation"
"13","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
"7","chapter-17","Limits to Computation"
"x","chapter-17","Limits to Computation"
"6","chapter-17","Limits to Computation"
"5","chapter-17","Limits to Computation"
"4","chapter-17","Limits to Computation"
"3","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
"1 1","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
"3","chapter-17","Limits to Computation"
"4","chapter-17","Limits to Computation"
"5","chapter-17","Limits to Computation"
"6","chapter-17","Limits to Computation"
"x x x","chapter-17","Limits to Computation"
"Figure 17.7 An illustration of assigning functions to bins.","chapter-17","Limits to Computation"
"fnew f (x) 1","chapter-17","Limits to Computation"
"(x) f2","chapter-17","Limits to Computation"
"(x) f3","chapter-17","Limits to Computation"
"(x) f4","chapter-17","Limits to Computation"
"(x)","chapter-17","Limits to Computation"
"1 2 3 4 5","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
"3","chapter-17","Limits to Computation"
"12","chapter-17","Limits to Computation"
"14","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
"3","chapter-17","Limits to Computation"
"4","chapter-17","Limits to Computation"
"5","chapter-17","Limits to Computation"
"6","chapter-17","Limits to Computation"
"1","chapter-17","Limits to Computation"
"1","chapter-17","Limits to Computation"
"1","chapter-17","Limits to Computation"
"1","chapter-17","Limits to Computation"
"1","chapter-17","Limits to Computation"
"1 1","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
"3","chapter-17","Limits to Computation"
"4","chapter-17","Limits to Computation"
"5","chapter-17","Limits to Computation"
"6","chapter-17","Limits to Computation"
"1","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
"3","chapter-17","Limits to Computation"
"4","chapter-17","Limits to Computation"
"5","chapter-17","Limits to Computation"
"6","chapter-17","Limits to Computation"
"1","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
"3","chapter-17","Limits to Computation"
"4","chapter-17","Limits to Computation"
"5","chapter-17","Limits to Computation"
"6","chapter-17","Limits to Computation"
"7","chapter-17","Limits to Computation"
"9","chapter-17","Limits to Computation"
"11","chapter-17","Limits to Computation"
"13","chapter-17","Limits to Computation"
"15","chapter-17","Limits to Computation"
"17","chapter-17","Limits to Computation"
"15","chapter-17","Limits to Computation"
"1","chapter-17","Limits to Computation"
"7","chapter-17","Limits to Computation"
"13","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
"7","chapter-17","Limits to Computation"
"x x x","chapter-17","Limits to Computation"
"1 1","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
"3","chapter-17","Limits to Computation"
"4","chapter-17","Limits to Computation"
"5","chapter-17","Limits to Computation"
"6","chapter-17","Limits to Computation"
"x x","chapter-17","Limits to Computation"
"1","chapter-17","Limits to Computation"
"2","chapter-17","Limits to Computation"
"3","chapter-17","Limits to Computation"
"4","chapter-17","Limits to Computation"
"5","chapter-17","Limits to Computation"
"6","chapter-17","Limits to Computation"
"Figure 17.8 Illustration for the argument that the number of integer functions is","chapter-17","Limits to Computation"
"uncountable.","chapter-17","Limits to Computation"
"any bin, as follows. Take the output value for input 1 from the function in the first","chapter-17","Limits to Computation"
"bin. Call this value F1(1). Add 1 to it, and assign the result as the output of a new","chapter-17","Limits to Computation"
"function for input value 1. Regardless of the remaining values assigned to our new","chapter-17","Limits to Computation"
"function, it must be different from the first function in the table, because the two","chapter-17","Limits to Computation"
"give different outputs for input 1. Now take the output value for 2 from the second","chapter-17","Limits to Computation"
"function in the table (known as F2(2)). Add 1 to this value and assign it as the","chapter-17","Limits to Computation"
"output for 2 in our new function. Thus, our new function must be different from","chapter-17","Limits to Computation"
"the function of Bin 2, because they will differ at least at the second value. Continue","chapter-17","Limits to Computation"
"in this manner, assigning Fnew(i) = Fi(i) + 1 for all values i. Thus, the new","chapter-17","Limits to Computation"
"function must be different from any function Fi at least at position i. This procedure","chapter-17","Limits to Computation"
"for constructing a new function not already in the table is called diagonalization.","chapter-17","Limits to Computation"
"Because the new function is different from every other function, it must not be in","chapter-17","Limits to Computation"
"the table. This is true no matter how we try to assign functions to bins, and so the","chapter-17","Limits to Computation"
"number of integer functions is uncountable. The significance of this is that not all","chapter-17","Limits to Computation"
"functions can possibly be assigned to programs, so there must be functions with no","chapter-17","Limits to Computation"
"corresponding program. Figure 17.8 illustrates this argument.","chapter-17","Limits to Computation"
"Sec. 17.3 Impossible Problems 559","chapter-17","Limits to Computation"
"17.3.2 The Halting Problem Is Unsolvable","chapter-17","Limits to Computation"
"While there might be intellectual appeal to knowing that there exists some function","chapter-17","Limits to Computation"
"that cannot be computed by a computer program, does this mean that there is any","chapter-17","Limits to Computation"
"such useful function? After all, does it really matter if no program can compute a","chapter-17","Limits to Computation"
"“nonsense” function such as shown in Bin 4 of Figure 17.7? Now we will prove","chapter-17","Limits to Computation"
"that the Halting Problem cannot be computed by any computer program. The proof","chapter-17","Limits to Computation"
"is by contradiction.","chapter-17","Limits to Computation"
"We begin by assuming that there is a function named halt that can solve the","chapter-17","Limits to Computation"
"Halting Problem. Obviously, it is not possible to write out something that does not","chapter-17","Limits to Computation"
"exist, but here is a plausible sketch of what a function to solve the Halting Problem","chapter-17","Limits to Computation"
"might look like if it did exist. Function halt takes two inputs: a string representing","chapter-17","Limits to Computation"
"the source code for a program or function, and another string representing the input","chapter-17","Limits to Computation"
"that we wish to determine if the input program or function halts on. Function halt","chapter-17","Limits to Computation"
"does some work to make a decision (which is encapsulated into some fictitious","chapter-17","Limits to Computation"
"function named PROGRAM HALTS). Function halt then returns true if the input","chapter-17","Limits to Computation"
"program or function does halt on the given input, and false otherwise.","chapter-17","Limits to Computation"
"bool halt(String prog, String input) {","chapter-17","Limits to Computation"
"if (PROGRAM HALTS(prog, input))","chapter-17","Limits to Computation"
"return true;","chapter-17","Limits to Computation"
"else","chapter-17","Limits to Computation"
"return false;","chapter-17","Limits to Computation"
"}","chapter-17","Limits to Computation"
"We now will examine two simple functions that clearly can exist because the","chapter-17","Limits to Computation"
"complete code for them is presented here:","chapter-17","Limits to Computation"
"// Return true if "prog" halts when given itself as input","chapter-17","Limits to Computation"
"bool selfhalt(String prog) {","chapter-17","Limits to Computation"
"if (halt(prog, prog))","chapter-17","Limits to Computation"
"return true;","chapter-17","Limits to Computation"
"else","chapter-17","Limits to Computation"
"return false;","chapter-17","Limits to Computation"
"}","chapter-17","Limits to Computation"
"// Return the reverse of what selfhalt returns on "prog"","chapter-17","Limits to Computation"
"void contrary(String prog) {","chapter-17","Limits to Computation"
"if (selfhalt(prog))","chapter-17","Limits to Computation"
"while (true); // Go into an infinite loop","chapter-17","Limits to Computation"
"}","chapter-17","Limits to Computation"
"What happens if we make a program whose sole purpose is to execute the func-","chapter-17","Limits to Computation"
"tion contrary and run that program with itself as input? One possibility is that","chapter-17","Limits to Computation"
"the call to selfhalt returns true; that is, selfhalt claims that contrary","chapter-17","Limits to Computation"
"will halt when run on itself. In that case, contrary goes into an infinite loop (and","chapter-17","Limits to Computation"
"thus does not halt). On the other hand, if selfhalt returns false, then halt is","chapter-17","Limits to Computation"
"proclaiming that contrary does not halt on itself, and contrary then returns,","chapter-17","Limits to Computation"
"560 Chap. 17 Limits to Computation","chapter-17","Limits to Computation"
"that is, it halts. Thus, contrary does the contrary of what halt says that it will","chapter-17","Limits to Computation"
"do.","chapter-17","Limits to Computation"
"The action of contrary is logically inconsistent with the assumption that","chapter-17","Limits to Computation"
"halt solves the Halting Problem correctly. There are no other assumptions we","chapter-17","Limits to Computation"
"made that might cause this inconsistency. Thus, by contradiction, we have proved","chapter-17","Limits to Computation"
"that halt cannot solve the Halting Problem correctly, and thus there is no program","chapter-17","Limits to Computation"
"that can solve the Halting Problem.","chapter-17","Limits to Computation"
"Now that we have proved that the Halting Problem is unsolvable, we can use","chapter-17","Limits to Computation"
"reduction arguments to prove that other problems are also unsolvable. The strat-","chapter-17","Limits to Computation"
"egy is to assume the existence of a computer program that solves the problem in","chapter-17","Limits to Computation"
"question and use that program to solve another problem that is already known to be","chapter-17","Limits to Computation"
"unsolvable.","chapter-17","Limits to Computation"
"Example 17.4 Consider the following variation on the Halting Problem.","chapter-17","Limits to Computation"
"Given a computer program, will it halt when its input is the empty string?","chapter-17","Limits to Computation"
"That is, will it halt when it is given no input? To prove that this problem is","chapter-17","Limits to Computation"
"unsolvable, we will employ a standard technique for computability proofs:","chapter-17","Limits to Computation"
"Use a computer program to modify another computer program.","chapter-17","Limits to Computation"
"Proof: Assume that there is a function Ehalt that determines whether","chapter-17","Limits to Computation"
"a given program halts when given no input. Recall that our proof for the","chapter-17","Limits to Computation"
"Halting Problem involved functions that took as parameters a string rep-","chapter-17","Limits to Computation"
"resenting a program and another string representing an input. Consider","chapter-17","Limits to Computation"
"another function combine that takes a program P and an input string I as","chapter-17","Limits to Computation"
"parameters. Function combine modifies P to store I as a static variable S","chapter-17","Limits to Computation"
"and further modifies all calls to input functions within P to instead get their","chapter-17","Limits to Computation"
"input from S. Call the resulting program P","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
". It should take no stretch of the","chapter-17","Limits to Computation"
"imagination to believe that any decent compiler could be modified to take","chapter-17","Limits to Computation"
"computer programs and input strings and produce a new computer program","chapter-17","Limits to Computation"
"that has been modified in this way. Now, take P","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
"and feed it to Ehalt. If","chapter-17","Limits to Computation"
"Ehalt says that P","chapter-17","Limits to Computation"
"0 will halt, then we know that P would halt on input I.","chapter-17","Limits to Computation"
"In other words, we now have a procedure for solving the original Halting","chapter-17","Limits to Computation"
"Problem. The only assumption that we made was the existence of Ehalt.","chapter-17","Limits to Computation"
"Thus, the problem of determining if a program will halt on no input must","chapter-17","Limits to Computation"
"be unsolvable. ✷","chapter-17","Limits to Computation"
"Example 17.5 For arbitrary program P, does there exist any input for","chapter-17","Limits to Computation"
"which P halts?","chapter-17","Limits to Computation"
"Proof: This problem is also uncomputable. Assume that we had a function","chapter-17","Limits to Computation"
"Ahalt that, when given program P as input would determine if there is","chapter-17","Limits to Computation"
"some input for which P halts. We could modify our compiler (or write","chapter-17","Limits to Computation"
"Sec. 17.4 Further Reading 561","chapter-17","Limits to Computation"
"a function as part of a program) to take P and some input string w, and","chapter-17","Limits to Computation"
"modify it so that w is hardcoded inside P, with P reading no input. Call this","chapter-17","Limits to Computation"
"modified program P","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
". Now, P","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
"always behaves the same way regardless of","chapter-17","Limits to Computation"
"its input, because it ignores all input. However, because w is now hardwired","chapter-17","Limits to Computation"
"inside of P","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
", the behavior we get is that of P when given w as input. So, P","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
"will halt on any arbitrary input if and only if P would halt on input w. We","chapter-17","Limits to Computation"
"now feed P","chapter-17","Limits to Computation"
"0","chapter-17","Limits to Computation"
"to function Ahalt. If Ahalt could determine that P","chapter-17","Limits to Computation"
"0 halts","chapter-17","Limits to Computation"
"on some input, then that is the same as determining that P halts on input w.","chapter-17","Limits to Computation"
"But we know that that is impossible. Therefore, Ahalt cannot exist. ✷","chapter-17","Limits to Computation"
"There are many things that we would like to have a computer do that are un-","chapter-17","Limits to Computation"
"solvable. Many of these have to do with program behavior. For example, proving","chapter-17","Limits to Computation"
"that an arbitrary program is “correct,” that is, proving that a program computes a","chapter-17","Limits to Computation"
"particular function, is a proof regarding program behavior. As such, what can be","chapter-17","Limits to Computation"
"accomplished is severely limited. Some other unsolvable problems include:","chapter-17","Limits to Computation"
"• Does a program halt on every input?","chapter-17","Limits to Computation"
"• Does a program compute a particular function?","chapter-17","Limits to Computation"
"• Do two programs compute the same function?","chapter-17","Limits to Computation"
"• Does a particular line in a program get executed?","chapter-17","Limits to Computation"
"This does not mean that a computer program cannot be written that works on","chapter-17","Limits to Computation"
"special cases, possibly even on most programs that we would be interested in check-","chapter-17","Limits to Computation"
"ing. For example, some C compilers will check if the control expression for a","chapter-17","Limits to Computation"
"while loop is a constant expression that evaluates to false. If it is, the compiler","chapter-17","Limits to Computation"
"will issue a warning that the while loop code will never be executed. However, it","chapter-17","Limits to Computation"
"is not possible to write a computer program that can check for all input programs","chapter-17","Limits to Computation"
"whether a specified line of code will be executed when the program is given some","chapter-17","Limits to Computation"
"specified input.","chapter-17","Limits to Computation"
"Another unsolvable problem is whether a program contains a computer virus.","chapter-17","Limits to Computation"
"The property “contains a computer virus” is a matter of behavior. Thus, it is not","chapter-17","Limits to Computation"
"possible to determine positively whether an arbitrary program contains a computer","chapter-17","Limits to Computation"
"virus. Fortunately, there are many good heuristics for determining if a program","chapter-17","Limits to Computation"
"is likely to contain a virus, and it is usually possible to determine if a program","chapter-17","Limits to Computation"
"contains a particular virus, at least for the ones that are now known. Real virus","chapter-17","Limits to Computation"
"checkers do a pretty good job, but, it will always be possible for malicious people","chapter-17","Limits to Computation"
"to invent new viruses that no existing virus checker can recognize.","chapter-17","Limits to Computation"
"17.4 Further Reading","chapter-17","Limits to Computation"
"The classic text on the theory of N P-completeness is Computers and Intractabil-","chapter-17","Limits to Computation"
"ity: A Guide to the Theory of N P-completeness by Garey and Johnston [GJ79].","chapter-17","Limits to Computation"
"The Traveling Salesman Problem, edited by Lawler et al. [LLKS85], discusses","chapter-17","Limits to Computation"
"562 Chap. 17 Limits to Computation","chapter-17","Limits to Computation"
"many approaches to finding an acceptable solution to this particular N P-complete","chapter-17","Limits to Computation"
"problem in a reasonable amount of time.","chapter-17","Limits to Computation"
"For more information about the Collatz function see “On the Ups and Downs","chapter-17","Limits to Computation"
"of Hailstone Numbers” by B. Hayes [Hay84], and “The 3x + 1 Problem and its","chapter-17","Limits to Computation"
"Generalizations” by J.C. Lagarias [Lag85].","chapter-17","Limits to Computation"
"For an introduction to the field of computability and impossible problems, see","chapter-17","Limits to Computation"
"Discrete Structures, Logic, and Computability by James L. Hein [Hei09].","chapter-17","Limits to Computation"
"17.5 Exercises","chapter-17","Limits to Computation"
"17.1 Consider this algorithm for finding the maximum element in an array: First","chapter-17","Limits to Computation"
"sort the array and then select the last (maximum) element. What (if anything)","chapter-17","Limits to Computation"
"does this reduction tell us about the upper and lower bounds to the problem","chapter-17","Limits to Computation"
"of finding the maximum element in a sequence? Why can we not reduce","chapter-17","Limits to Computation"
"SORTING to finding the maximum element?","chapter-17","Limits to Computation"
"17.2 Use a reduction to prove that squaring an n × n matrix is just as expensive","chapter-17","Limits to Computation"
"(asymptotically) as multiplying two n × n matrices.","chapter-17","Limits to Computation"
"17.3 Use a reduction to prove that multiplying two upper triangular n × n matri-","chapter-17","Limits to Computation"
"ces is just as expensive (asymptotically) as multiplying two arbitrary n × n","chapter-17","Limits to Computation"
"matrices.","chapter-17","Limits to Computation"
"17.4 (a) Explain why computing the factorial of n by multiplying all values","chapter-17","Limits to Computation"
"from 1 to n together is an exponential time algorithm.","chapter-17","Limits to Computation"
"(b) Explain why computing an approximation to the factorial of n by mak-","chapter-17","Limits to Computation"
"ing use of Stirling’s formula (see Section 2.2) is a polynomial time","chapter-17","Limits to Computation"
"algorithm.","chapter-17","Limits to Computation"
"17.5 Consider this algorithm for solving the K-CLIQUE problem. First, generate","chapter-17","Limits to Computation"
"all subsets of the vertices containing exactly k vertices. There are O(n","chapter-17","Limits to Computation"
"k","chapter-17","Limits to Computation"
") such","chapter-17","Limits to Computation"
"subsets altogether. Then, check whether any subgraphs induced by these","chapter-17","Limits to Computation"
"subsets is complete. If this algorithm ran in polynomial time, what would","chapter-17","Limits to Computation"
"be its significance? Why is this not a polynomial-time algorithm for the K-","chapter-17","Limits to Computation"
"CLIQUE problem?","chapter-17","Limits to Computation"
"17.6 Write the 3 SAT expression obtained from the reduction of SAT to 3 SAT","chapter-17","Limits to Computation"
"described in Section 17.2.1 for the expression","chapter-17","Limits to Computation"
"(a + b + c + d) · (d) · (b + c) · (a + b) · (a + c) · (b).","chapter-17","Limits to Computation"
"Is this expression satisfiable?","chapter-17","Limits to Computation"
"17.7 Draw the graph obtained by the reduction of SAT to the K-CLIQUE problem","chapter-17","Limits to Computation"
"given in Section 17.2.1 for the expression","chapter-17","Limits to Computation"
"(a + b + c) · (a + b + c) · (a + b + c) · (a + b + c).","chapter-17","Limits to Computation"
"Is this expression satisfiable?","chapter-17","Limits to Computation"
"Sec. 17.5 Exercises 563","chapter-17","Limits to Computation"
"17.8 A Hamiltonian cycle in graph G is a cycle that visits every vertex in the","chapter-17","Limits to Computation"
"graph exactly once before returning to the start vertex. The problem HAMIL-","chapter-17","Limits to Computation"
"TONIAN CYCLE asks whether graph G does in fact contain a Hamiltonian","chapter-17","Limits to Computation"
"cycle. Assuming that HAMILTONIAN CYCLE is N P-complete, prove that","chapter-17","Limits to Computation"
"the decision-problem form of TRAVELING SALESMAN is N P-complete.","chapter-17","Limits to Computation"
"17.9 Use the assumption that VERTEX COVER is N P-complete to prove that K-","chapter-17","Limits to Computation"
"CLIQUE is also N P-complete by finding a polynomial time reduction from","chapter-17","Limits to Computation"
"VERTEX COVER to K-CLIQUE.","chapter-17","Limits to Computation"
"17.10 We define the problem INDEPENDENT SET as follows.","chapter-17","Limits to Computation"
"INDEPENDENT SET","chapter-17","Limits to Computation"
"Input: A graph G and an integer k.","chapter-17","Limits to Computation"
"Output: YES if there is a subset S of the vertices in G of size k or","chapter-17","Limits to Computation"
"greater such that no edge connects any two vertices in S, and NO other-","chapter-17","Limits to Computation"
"wise.","chapter-17","Limits to Computation"
"Assuming that K-CLIQUE is N P-complete, prove that INDEPENDENT","chapter-17","Limits to Computation"
"SET is N P-complete.","chapter-17","Limits to Computation"
"17.11 Define the problem PARTITION as follows:","chapter-17","Limits to Computation"
"PARTITION","chapter-17","Limits to Computation"
"Input: A collection of integers.","chapter-17","Limits to Computation"
"Output: YES if the collection can be split into two such that the sum","chapter-17","Limits to Computation"
"of the integers in each partition sums to the same amount. NO otherwise.","chapter-17","Limits to Computation"
"(a) Assuming that PARTITION is N P-complete, prove that the decision","chapter-17","Limits to Computation"
"form of BIN PACKING is N P-complete.","chapter-17","Limits to Computation"
"(b) Assuming that PARTITION is N P-complete, prove that KNAPSACK","chapter-17","Limits to Computation"
"is N P-complete.","chapter-17","Limits to Computation"
"17.12 Imagine that you have a problem P that you know is N P-complete. For","chapter-17","Limits to Computation"
"this problem you have two algorithms to solve it. For each algorithm, some","chapter-17","Limits to Computation"
"problem instances of P run in polynomial time and others run in exponen-","chapter-17","Limits to Computation"
"tial time (there are lots of heuristic-based algorithms for real N P-complete","chapter-17","Limits to Computation"
"problems with this behavior). You can’t tell beforehand for any given prob-","chapter-17","Limits to Computation"
"lem instance whether it will run in polynomial or exponential time on either","chapter-17","Limits to Computation"
"algorithm. However, you do know that for every problem instance, at least","chapter-17","Limits to Computation"
"one of the two algorithms will solve it in polynomial time.","chapter-17","Limits to Computation"
"(a) What should you do?","chapter-17","Limits to Computation"
"(b) What is the running time of your solution?","chapter-17","Limits to Computation"
"564 Chap. 17 Limits to Computation","chapter-17","Limits to Computation"
"(c) What does it say about the question of P = N P if the conditions","chapter-17","Limits to Computation"
"described in this problem existed?","chapter-17","Limits to Computation"
"17.13 Here is another version of the knapsack problem, which we will call EXACT","chapter-17","Limits to Computation"
"KNAPSACK. Given a set of items each with given integer size, and a knap-","chapter-17","Limits to Computation"
"sack of size integer k, is there a subset of the items which fits exactly within","chapter-17","Limits to Computation"
"the knapsack?","chapter-17","Limits to Computation"
"Assuming that EXACT KNAPSACK is N P-complete, use a reduction argu-","chapter-17","Limits to Computation"
"ment to prove that KNAPSACK is N P-complete.","chapter-17","Limits to Computation"
"17.14 The last paragraph of Section 17.2.3 discusses a strategy for developing a","chapter-17","Limits to Computation"
"solution to a new problem by alternating between finding a polynomial time","chapter-17","Limits to Computation"
"solution and proving the problem N P-complete. Refine the “algorithm for","chapter-17","Limits to Computation"
"designing algorithms” from Section 15.1 to incorporate identifying and deal-","chapter-17","Limits to Computation"
"ing with N P-complete problems.","chapter-17","Limits to Computation"
"17.15 Prove that the set of real numbers is uncountable. Use a proof similar to the","chapter-17","Limits to Computation"
"proof in Section 17.3.1 that the set of integer functions is uncountable.","chapter-17","Limits to Computation"
"17.16 Prove, using a reduction argument such as given in Section 17.3.2, that the","chapter-17","Limits to Computation"
"problem of determining if an arbitrary program will print any output is un-","chapter-17","Limits to Computation"
"solvable.","chapter-17","Limits to Computation"
"17.17 Prove, using a reduction argument such as given in Section 17.3.2, that the","chapter-17","Limits to Computation"
"problem of determining if an arbitrary program executes a particular state-","chapter-17","Limits to Computation"
"ment within that program is unsolvable.","chapter-17","Limits to Computation"
"17.18 Prove, using a reduction argument such as given in Section 17.3.2, that the","chapter-17","Limits to Computation"
"problem of determining if two arbitrary programs halt on exactly the same","chapter-17","Limits to Computation"
"inputs is unsolvable.","chapter-17","Limits to Computation"
"17.19 Prove, using a reduction argument such as given in Section 17.3.2, that the","chapter-17","Limits to Computation"
"problem of determining whether there is some input on which two arbitrary","chapter-17","Limits to Computation"
"programs will both halt is unsolvable.","chapter-17","Limits to Computation"
"17.20 Prove, using a reduction argument such as given in Section 17.3.2, that the","chapter-17","Limits to Computation"
"problem of determining whether an arbitrary program halts on all inputs is","chapter-17","Limits to Computation"
"unsolvable.","chapter-17","Limits to Computation"
"17.21 Prove, using a reduction argument such as given in Section 17.3.2, that the","chapter-17","Limits to Computation"
"problem of determining whether an arbitrary program computes a specified","chapter-17","Limits to Computation"
"function is unsolvable.","chapter-17","Limits to Computation"
"17.22 Consider a program named COMP that takes two strings as input. It returns","chapter-17","Limits to Computation"
"TRUE if the strings are the same. It returns FALSE if the strings are different.","chapter-17","Limits to Computation"
"Why doesn’t the argument that we used to prove that a program to solve the","chapter-17","Limits to Computation"
"halting problem does not exist work to prove that COMP does not exist?","chapter-17","Limits to Computation"
"17.6 Projects","chapter-17","Limits to Computation"
"17.1 Implement VERTEX COVER; that is, given graph G and integer k, answer","chapter-17","Limits to Computation"
"the question of whether or not there is a vertex cover of size k or less. Begin","chapter-17","Limits to Computation"
"Sec. 17.6 Projects 565","chapter-17","Limits to Computation"
"by using a brute-force algorithm that checks all possible sets of vertices of","chapter-17","Limits to Computation"
"size k to find an acceptable vertex cover, and measure the running time on a","chapter-17","Limits to Computation"
"number of input graphs. Then try to reduce the running time through the use","chapter-17","Limits to Computation"
"of any heuristics you can think of. Next, try to find approximate solutions to","chapter-17","Limits to Computation"
"the problem in the sense of finding the smallest set of vertices that forms a","chapter-17","Limits to Computation"
"vertex cover.","chapter-17","Limits to Computation"
"17.2 Implement KNAPSACK (see Section 16.1). Measure its running time on a","chapter-17","Limits to Computation"
"number of inputs. What is the largest practical input size for this problem?","chapter-17","Limits to Computation"
"17.3 Implement an approximation of TRAVELING SALESMAN; that is, given a","chapter-17","Limits to Computation"
"graph G with costs for all edges, find the cheapest cycle that visits all vertices","chapter-17","Limits to Computation"
"in G. Try various heuristics to find the best approximations for a wide variety","chapter-17","Limits to Computation"
"of input graphs.","chapter-17","Limits to Computation"
"17.4 Write a program that, given a positive integer n as input, prints out the Collatz","chapter-17","Limits to Computation"
"sequence for that number. What can you say about the types of integers that","chapter-17","Limits to Computation"
"have long Collatz sequences? What can you say about the length of the","chapter-17","Limits to Computation"
"Collatz sequence for various types of integers?","chapter-17","Limits to Computation"